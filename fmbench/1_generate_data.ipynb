{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data: Gather data, create prompts/payloads of different sizes\n",
    "---------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "### This part of our solution design includes \n",
    "\n",
    "- running and downloading our specific dataset\n",
    "\n",
    "- generating prompts as payloads of different sizes that we will send to our different model endpoints with different combinations of concurrency levels that we will later use to run inference and generate benchmarking metrics and visualizations.\n",
    "\n",
    "#### This file will generate all data on wikiqa (english version) with prompt sizes 300 - 4000 token lengths in different payload sizes to send to the model endpoint during the inference pipeline. You will also be able to generate the normal wikiqa dataset from the actual 'long bench dataset'. This notebook then focuses on 3 main deliverables:\n",
    "\n",
    "1. Loading the dataset that is stored within the dataset in the data directory.\n",
    "\n",
    "\n",
    "2. Generating payloads: This notebook also converts the loaded datasets into payloads based on the input question and records the context length of the prompt to send as a part of the payload during running inferences on the deployed endpoints.\n",
    "\n",
    "    - All of the prompts are saved in this data directory in a file named all_prompts.csv.\n",
    "    \n",
    "\n",
    "3. Constructing different sized payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from typing import Dict, List, Optional\n",
    "from datasets import load_dataset, Dataset\n",
    "import importlib.resources as pkg_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a logger to log all messages while the code runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove existing handlers\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Add a simple handler\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## config.yml file contains information that is used across this benchmarking environment, \n",
    "## such as information about the aws account, prompts, payloads to be used for invocations\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the file path for the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_file_path = \"/\".join([config['s3_read_data']['prompt_template_dir'],\n",
    "                         config['s3_read_data']['prompt_template_file']])\n",
    "\n",
    "## download the file from s3 else check locally and use that version\n",
    "prompt_template_from_s3: str = read_from_s3(config['s3_read_data']['read_bucket'], s3_file_path)\n",
    "\n",
    "prompt_template_dir = Path(pkg_resources.files(FMBENCH_PACKAGE_NAME), config['s3_read_data']['prompt_template_dir'])\n",
    "logger.info(f\"Using fmbench.{config['s3_read_data']['prompt_template_dir']} directory: {prompt_template_dir}\")\n",
    "\n",
    "if prompt_template_from_s3 is None:\n",
    "    prompt_template_fpath: str = os.path.join(prompt_template_dir, config['s3_read_data']['prompt_template_file'])\n",
    "    prompt_template = Path(prompt_template_fpath).read_text()\n",
    "    logger.info(f\"Using the default local prompt template --> {prompt_template}\")\n",
    "else:\n",
    "    prompt_template = prompt_template_from_s3\n",
    "    logger.info(f\"Using the prompt template from S3 --> {prompt_template}\")\n",
    "prompt_template = prompt_template.strip()\n",
    "\n",
    "# Calculate the number of tokens in the prompt template\n",
    "prompt_template_keys = config['datasets']['prompt_template_keys']\n",
    "# get the ground truth key from the dataset section of the config file\n",
    "ground_truth_col_key: Optional[str] = config['datasets'].get('ground_truth_col_key', None)\n",
    "# get the question col key from the dataset section of the config file\n",
    "question_col_key: Optional[str] = config['datasets'].get('question_col_key', None)\n",
    "\n",
    "args = {}\n",
    "if prompt_template_keys:\n",
    "    # if the prompt template keys are provided, then they are formatted into the prompt\n",
    "    for k in prompt_template_keys:\n",
    "        args[k] = \"\"\n",
    "    empty_prompt_template = prompt_template.format(**args)\n",
    "else:\n",
    "    # if there are no formatting placeholders or prompt template keys provided, then the same prompt will be\n",
    "    # used for all of the payloads. This is beneficial when there is an image dataset without any questions and\n",
    "    # a consistent prompt can be used for all of the images\n",
    "    logger.info(\"No prompt template keys provided. Using the prompt template provided for all requests.\")\n",
    "    empty_prompt_template = prompt_template\n",
    "\n",
    "logger.info(f\"empty prompt template = \\\"{empty_prompt_template}\\\"\")\n",
    "empty_prompt_len_in_tokens = count_tokens(empty_prompt_template)\n",
    "\n",
    "# Log the number of tokens\n",
    "logger.info(f\"prompt template length={empty_prompt_len_in_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset: Handle text & multimodal image datasets dynamically\n",
    "---\n",
    "\n",
    "In this portion of the generate data step, FMBench does as follows:\n",
    "\n",
    "1. If the dataset is a Hugging Face dataset (prefixed with an `hf:` in the `source_data_files` of the `s3_read_data` section in the config file), then FMBench downloads the data using the `dataset id` and `hf token`. The data is then converted into `jsonl` format and sent to the s3 read bucket within the `source_data` directory. If the run is on `local mode`, then the data is converted to `jsonl` and sent to `/tmp/fmbench-read/source_data/` directory.\n",
    "\n",
    "2. If there is a custom dataset (not prefixed with `hf:`), then FMBench assumes that those files are already provided by the user in the `source_data` folder of the `EC2` instance or within the s3 bucket, available to be used. If the custom dataset is not in `jsonl` format, then use the `bring_your_own_dataset.ipynb`[fmbench/bring_your_own_dataset.ipynb] notebook to convert your custom dataset into `jsonl` format, place those in s3 or the local read folder, and FMBench will use those files in the generate data step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the HF token. This HF token is required while loading an HF dataset\n",
    "# if the HF token is not provided, an exception might be through at the dataset download step.\n",
    "try:\n",
    "    # HF token file name\n",
    "    hf_token_key: str = os.path.join(config['s3_read_data']['scripts_prefix'], HF_TOKEN_FNAME)\n",
    "    hf_token_content: str = get_s3_object(config['s3_read_data']['read_bucket'], hf_token_key, decode=True)\n",
    "    if hf_token_content is not None:\n",
    "        HF_TOKEN = hf_token_content.strip()\n",
    "        logger.info(f\"HF token content successfully retrieved from S3: {HF_TOKEN_FNAME}\")\n",
    "    else:\n",
    "        logger.info(f\"HF token content not found: {hf_token_content}\")\n",
    "    logger.info(f\"HF toke file found: {HF_TOKEN_FNAME}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"HF token file '{HF_TOKEN_FNAME}' not found.\")\n",
    "    HF_TOKEN = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom JSON encoder for PIL Images\n",
    "class PILImageEncoder(json.JSONEncoder):\n",
    "    \"\"\"\n",
    "    A custom JSON encoder that extends the default JSONEncoder to handle PIL Image objects.\n",
    "\n",
    "    This encoder allows for the serialization of PIL Image objects into JSON-compatible format.\n",
    "    When encountering a PIL Image object, it converts the image to a PNG format, then to a \n",
    "    base64-encoded string. This allows image data to be included in JSON output, which \n",
    "    normally doesn't support binary data.\n",
    "    \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, Image.Image):\n",
    "            buffered = io.BytesIO()\n",
    "            obj.save(buffered, format=DEFAULT_IMAGE_FORMAT)\n",
    "            # get the hex value of the buffered image object\n",
    "            hex_data = buffered.getvalue().hex()\n",
    "            return {\n",
    "                'hex_data': hex_data,\n",
    "                'format': DEFAULT_IMAGE_FORMAT\n",
    "            }\n",
    "        return super(PILImageEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_hf_dataset(dataset_identifier: str,\n",
    "                      HF_TOKEN: Optional[str],\n",
    "                      ds_N: int):\n",
    "    \"\"\"\n",
    "    Load a dataset from Hugging Face if the dataset_identifier starts with 'hf:' prefix.\n",
    "    Returns the dataset as a pandas DataFrame.\n",
    "\n",
    "    :param dataset_identifier: The dataset identifier, e.g., 'hf:derek-thomas/ScienceQA.jsonl'\n",
    "    :param HF_TOKEN: Hugging Face token, if required\n",
    "    :param ds_N: Number of samples to process\n",
    "    :return: Hugging face dataset object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = []\n",
    "        # remove the hf prefix to load the hugging face dataset\n",
    "        ds_id = dataset_identifier[len(HF_DATASET_PREFIX):]\n",
    "        # Check if a specific split is requested\n",
    "        ds_parts = ds_id.split('/')\n",
    "        # Extract ds_id, ds_name, ds_split to load the dataset\n",
    "        if len(ds_parts) >= 2:\n",
    "            ds_id = '/'.join(ds_parts[:2])\n",
    "            logger.info(f\"Going to load the hugging face dataset: {ds_id}\")\n",
    "        else:\n",
    "            logger.error(f\"Invalid hugging face dataset id: {ds_id}. Use a valid dataset id.\")\n",
    "        # fetch the ds subset name and split for loading the dataset. If the subset name is not provided\n",
    "        # then default to 'default'\n",
    "        ds_name = ds_parts[2] if len(ds_parts) >= 3 else 'default'\n",
    "        # If ds_split is not provided, then we will default it to 'train'. If you wish to use\n",
    "        # a desired split, then provide the subset name and split name in this format within the \n",
    "        # config file: hf:dataset-id/subset-name/split-name\n",
    "        ds_split = ds_parts[3] if len(ds_parts) >= 4 else 'train'\n",
    "        # Set up parameters for load_dataset. These parameters are used while loading\n",
    "        # the dataset. The 'streaming' parameter enables you to work with a dataset without downloading it. \n",
    "        # The data is streamed as you iterate over the dataset.\n",
    "        load_dataset_kwargs = {'path': ds_id, 'name': ds_name, 'split': ds_split, 'streaming': True}\n",
    "        logger.info(f\"Using the following parameters to load the HF dataset: {load_dataset_kwargs}\")\n",
    "        if HF_TOKEN:\n",
    "            # initialize the hf token\n",
    "            load_dataset_kwargs['token'] = HF_TOKEN\n",
    "        # Try to load the dataset - if the dataset has no subsets and only spits, then the\n",
    "        # try block will handle it, otherwise it will try loading the dataset using the config name\n",
    "        # if there are subsets in the dataset\n",
    "        try:\n",
    "            logger.info(f\"Loading the HF dataset: {ds_id}\")\n",
    "            logger.info(f\"load_dataset_kwargs: {load_dataset_kwargs}\")\n",
    "            dataset = load_dataset(**load_dataset_kwargs)\n",
    "            logger.info(f\"Done loading the dataset with the provided split {ds_split}: {ds_id}\")\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Error occurred while loading the HF dataset: {e}\")\n",
    "            return\n",
    "        # If the dataset has multiple splits, select one\n",
    "        dataset_split = dataset\n",
    "        # Take only the first ds_N examples\n",
    "        dataset_iter = itertools.islice(dataset_split, ds_N)\n",
    "        dataset_list = list(dataset_iter)\n",
    "        dataset = Dataset.from_list(dataset_list)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading the hugging face dataset: {e}\")\n",
    "        dataset=None\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_dataset(dataset: Dataset,\n",
    "                    config: Dict,\n",
    "                    ds_N: int) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Process the dataset and convert it to JSON Lines format. This is handled for both\n",
    "    text and image datasets.\n",
    "\n",
    "    :param dataset: The Dataset object\n",
    "    :param config: FMBench configuration file\n",
    "    :param ds_N: Number of samples to process\n",
    "    :return: JSON Lines content as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        jsonl_content: Optional[str] = None\n",
    "        df = pd.DataFrame(dataset)\n",
    "        image_col = config['datasets'].get('image_col')\n",
    "        # If image_col is specified and exists in df, filter out rows where image_col is None\n",
    "        if image_col and image_col in df.columns:\n",
    "            initial_shape = df.shape\n",
    "            df = df[df[image_col].notnull()]\n",
    "            logger.info(f\"Filtered dataset to only include rows where '{image_col}' is not None. Shape changed from {initial_shape} to {df.shape}\")\n",
    "\n",
    "        # Subset the data and randomly shuffle it\n",
    "        logger.info(f\"Dataset shape before random subset: {df.shape}\")\n",
    "        df = df.sample(n=min(ds_N, len(df)))\n",
    "        logger.info(f\"Dataset shape after random subset: {df.shape}\")\n",
    "\n",
    "        # Convert to JSON Lines format\n",
    "        if image_col and image_col in df.columns:\n",
    "            logger.info(\"Image column found in the dataset. The data is multimodal - Using the PILImageEncoder to encode images in JSON Lines.\")\n",
    "            jsonl_content = df.to_json(orient='records', lines=True, default_handler=PILImageEncoder().default)\n",
    "        else:\n",
    "            logger.info(\"The data is standard text data, converting to JSON Lines.\")\n",
    "            jsonl_content = df.to_json(orient='records', lines=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing and converting the dataset into jsonl format: {e}\")\n",
    "        jsonl_content = None\n",
    "    return jsonl_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate through all of the source files in the source data file section to load the hf datasets\n",
    "for dataset_file in config['s3_read_data']['source_data_files']:\n",
    "    if dataset_file.startswith(HF_DATASET_PREFIX):\n",
    "        logger.info(f\"{dataset_file} is a hugging face dataset. Going to load and process the dataset now.\")\n",
    "        # Load the dataset\n",
    "        dataset = load_hf_dataset(dataset_file, HF_TOKEN, config['datasets'].get('ds_N', DEFAULT_HF_DS_N_VALUE))\n",
    "        logger.info(f\"dataset: {dataset}\")\n",
    "        if dataset is None:\n",
    "            logger.error(f\"Failed to load dataset: {dataset_file}\")\n",
    "            logger.error(f\"If your dataset does not have a 'default' subset or a 'train' split, then provide a dataset with a valid subset id and split name in the format 'hf:dataset-id/subset-name/split-name'.\")\n",
    "            continue\n",
    "        # Process the dataset and convert to JSON Lines\n",
    "        jsonl_content = process_dataset(dataset, config, config['datasets'].get('ds_N', DEFAULT_HF_DS_N_VALUE))\n",
    "        if jsonl_content is None:\n",
    "            logger.error(f\"Failed to process dataset: {dataset_file}\")\n",
    "            continue\n",
    "        # Prepare the file name. The hf dataset is stored as a jsonl file in the fmbench read directory, \n",
    "        # so we add a jsonl extension which is then stored and used in the benchmarking test\n",
    "        file_name = dataset_file + '.jsonl'\n",
    "        # Upload to S3 or locally within the /tmp/fmbench-read folder\n",
    "        write_to_s3(jsonl_content, config['s3_read_data']['read_bucket'], config['s3_read_data']['source_data_prefix'], \"\", file_name)\n",
    "        logger.info(f\"Finished processing and uploading dataset: {dataset_file}\")\n",
    "    else:\n",
    "        logger.info(f\"The provided source data file {dataset_file} is not a hugging face dataset \"\n",
    "            \"because it is not prefixed with 'hf:'. Assuming that this file is \"\n",
    "            \"already provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt payload generation\n",
    "---\n",
    "\n",
    "In this portion of the `generate_data` step, FMBench fetches `jsonl` data files and uses those to generate prompt payloads using the specified prompt template in the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_files():\n",
    "    response = s3_client.list_objects_v2(Bucket=config['s3_read_data']['read_bucket'], Prefix=config['s3_read_data']['source_data_prefix'])\n",
    "    return [obj['Key'] for obj in response['Contents']]\n",
    "\n",
    "# List all files in the bucket and prefix\n",
    "# s3_files = list_files()\n",
    "s3_files = list_s3_files(config['s3_read_data']['read_bucket'], config['s3_read_data']['source_data_prefix'], '.jsonl')\n",
    "logger.info(f\"s3 paths of the data set -> {s3_files}\")\n",
    "\n",
    "# Log the files you're going to read\n",
    "logger.info(f\"dataset files = {s3_files}\")\n",
    "\n",
    "# Process source data files. If the dataset identifier is an hf\n",
    "# dataset, then we strip out the 'hf:' and append the 'jsonl' file extension, \n",
    "# else we assume the provided jsonl file to be provided by the user in the \n",
    "# local/s3 read path.\n",
    "processed_files = []\n",
    "for source_file in config['s3_read_data']['source_data_files']:\n",
    "    if source_file.startswith(HF_DATASET_PREFIX):\n",
    "        # For Hugging Face datasets, append .jsonl\n",
    "        processed_files.append(source_file.lstrip(HF_DATASET_PREFIX) + '.jsonl')\n",
    "    else:\n",
    "        # For S3 files, use as is\n",
    "        processed_files.append(source_file)\n",
    "\n",
    "# Read and concatenate DataFrames\n",
    "# If there are any hf datasets that are read directly at runtime in this notebook, \n",
    "# then strip the \"hf:\" prefix from the source data file name in the config file and \n",
    "# read that.\n",
    "jsonl_files = [\n",
    "    file_key for file_key in s3_files \n",
    "    if file_key.replace(config['s3_read_data']['source_data_prefix'] + \"/\", \"\") in processed_files\n",
    "]\n",
    "logger.info(f\"jsonl_files={jsonl_files}\")\n",
    "\n",
    "# Read and concatenate only the .jsonl files\n",
    "# df = pd.concat([pd.read_json(io.BytesIO(s3_client.get_object(Bucket=config['s3_read_data']['read_bucket'], Key=file_key)['Body'].read()), lines=True) \n",
    "#                  for file_key in jsonl_files])\n",
    "df = pd.concat([pd.read_json(io.BytesIO(get_s3_object(config['s3_read_data']['read_bucket'], file_key, decode=False)), lines=True) \n",
    "                for file_key in jsonl_files])\n",
    "\n",
    "# Log the source of the dataset and its shape\n",
    "logger.info(f\"dataset read from {s3_files}\\nhas shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View a portion of the df to view inputs, contexts, and more information on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for if the dataset is a multimodal image dataset\n",
    "---\n",
    "\n",
    "In this portion, we check if there is a user provided `image_col` in the `datasets` section of the config file. If there is, the image column is used to generate a new column. This new column contains the `base64` conversion of the image which is later used during the inference step of the benchmarking process. If the user does not provide an image column, then the standard text generation benchmarking test will run as is. If the user has not provided a dataset with an image column but the configuration file contains an image column, an exception will be thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_to_base64(img_dict: Optional[Dict[str, str]]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Convert an image represented as a dictionary with 'hex_data' and 'format' into a base64 encoded string.\n",
    "\n",
    "    Args:\n",
    "        img_dict (Optional[Dict[str, str]]): The input image data as a dictionary containing 'hex_data' and 'format'.\n",
    "                                             If None, the function returns None.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: A base64 encoded string representation of the image in JPEG format.\n",
    "                       Returns None if the input is None or if an error occurs during conversion.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base64_img: Optional[str] = None\n",
    "        if img_dict is None:\n",
    "            logger.error(\"Provided image is None in the dataset. Make sure the dataset contains images.\")\n",
    "            return\n",
    "        # Extract 'hex_data' and 'format' from the image dictionary\n",
    "        hex_data = img_dict.get('hex_data')\n",
    "        img_format = img_dict.get('format', DEFAULT_IMAGE_FORMAT)  # Default to 'JPEG' if format is not specified\n",
    "        if not hex_data:\n",
    "            logger.error(\"Hex data is missing in the image dictionary.\")\n",
    "            return\n",
    "        # Convert hex data back into bytes\n",
    "        img_bytes = bytes.fromhex(hex_data)\n",
    "\n",
    "        # Create a BytesIO object from the image bytes\n",
    "        img_buffer = BytesIO(img_bytes)\n",
    "\n",
    "        # Open the image using PIL\n",
    "        img = Image.open(img_buffer)\n",
    "\n",
    "        # Convert the image to base64\n",
    "        buffered = BytesIO()\n",
    "        img.save(buffered, format=DEFAULT_IMAGE_FORMAT)\n",
    "        base64_img = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred while converting the image into base64: {e}\")\n",
    "        base64_img = None\n",
    "    return base64_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if the image column is provided in the configuration while within the `dataset` section, \n",
    "# apply the base64 conversion function on the image column. If the image column is not provided, \n",
    "# the standard text generation benchmarking test will be used.\n",
    "if config['datasets'].get('image_col') is not None:\n",
    "    logger.info(f\"Image column provided in the dataset: {config['datasets'].get('image_col')},\"\n",
    "                f\"going to convert the image into base64 and generate a new base64 column\")\n",
    "    df['base64_img'] = df[config['datasets'].get('image_col')].apply(image_to_base64)\n",
    "else:\n",
    "    logger.info(\"Going to use the standard text generation benchmarking test. No image columns\"\n",
    "                \"found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display basic statistics on the existing dataset: including count, mean, std, min, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"distribution of the length field in the dataset is as follows ->\\n{df.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataset elements into prompts as payloads for inference purposes\n",
    "\n",
    "Now, we will focus on converting the existing data within our datasets, and extract the informatprompt_templateion to convert it into prompts to be able to send to our deployed model endpoints during the process of testing and benchmarking for results and various metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if config['datasets']['prompt_template_keys']:\n",
    "    df['prompt'] = df.apply(lambda row: process_item(row, config['datasets']['prompt_template_keys'], prompt_template), axis=1)\n",
    "    df['prompt_len'] = df.prompt.map(lambda x: x['prompt_len'])\n",
    "else:\n",
    "    print(\"No prompt template keys provided. Using constant prompt for all rows.\")\n",
    "    constant_prompt = {\n",
    "        'prompt': prompt_template,\n",
    "        'prompt_len': len(prompt_template)\n",
    "    }\n",
    "    df['prompt'] = [constant_prompt] * len(df)\n",
    "    df['prompt_len'] = constant_prompt['prompt_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame to a CSV format string\n",
    "csv_buffer = io.StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "csv_data = csv_buffer.getvalue()\n",
    "all_prompts_file = config['dir_paths']['all_prompts_file']\n",
    "\n",
    "# Write to S3 using the write_to_s3 function\n",
    "write_to_s3(csv_data,\n",
    "            config['aws']['bucket'],\n",
    "            DATA_DIR, config['dir_paths']['prompts_prefix'],\n",
    "            all_prompts_file)\n",
    "\n",
    "# Log where the prompts are saved\n",
    "logger.info(f\"all prompts dataframe of shape {df.shape} saved to \"\n",
    "            f\"s3://{config['aws']['bucket']}/{DATA_DIR}/{os.path.join(config['dir_paths']['prompts_prefix'], all_prompts_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View some of the prompts \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Prompts into Payloads for inference purposes\n",
    "------\n",
    "Now we will prepare data for model inference. It involves converting prompts, created and stored in a specific format, into payloads for inference. We will utilize the prompt file for our model and incorporate the prompt into a payload using that. \n",
    "\n",
    "These payloads are tailored to the needs of deployed model endpoints. The conversion considers prompt sizes and specific configurations to further make our benchmarking more detailed and comprehensive. \n",
    "\n",
    "The goal is to have a set of well-formatted and parameterized payload requests of various sizes ready to be sent to the model endpoints for inference, with the responses to be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to construct a single request payload based on row prompt data and configuration\n",
    "def construct_request_payload(row, config: Dict) -> Dict:\n",
    "\n",
    "    # Deep copy inference parameters from the config.yml file - feel free to change this based on the model type you are using\n",
    "    # parameters = copy.deepcopy(config['inference_parameters']['common'])\n",
    "    # truncate = parameters.get('truncate', None)\n",
    "    # if truncate == TRUNCATE_POLICY.AT_PROMPT_TOKEN_LENGTH:\n",
    "    #     parameters['truncate'] = row['prompt_len']\n",
    "\n",
    "    # Return the constructed payload along with the ground truth if any\n",
    "    # is contained within the dataset\n",
    "    prompt_dict: Optional[Dict] = None\n",
    "    try:\n",
    "        # Construct the base prompt dictionary with the prompt input\n",
    "        prompt_dict = dict(inputs=row['prompt']['prompt'])\n",
    "\n",
    "        # Add ground truth and question_col_key if available\n",
    "        if ground_truth_col_key is not None and ground_truth_col_key in row:\n",
    "            prompt_dict['ground_truth'] = row[ground_truth_col_key]\n",
    "\n",
    "        if question_col_key is not None and question_col_key in row:\n",
    "            prompt_dict['question'] = row[question_col_key]\n",
    "\n",
    "        # Check if 'base64_img' column exists and add it to the prompt_dict\n",
    "        if 'base64_img' in row and row['base64_img'] is not None:\n",
    "            prompt_dict['base64_img'] = row['base64_img']\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prompt template could not be constructed: {e}\")\n",
    "        prompt_dict = None\n",
    "\n",
    "    return prompt_dict\n",
    "\n",
    "\n",
    "# Function to create a dataset payload files from the given dataset file we have\n",
    "def create_dataset_payload_file(df: pd.DataFrame, dataset_info: Dict, config: Dict) -> str:\n",
    "\n",
    "    # First, log the dataset existing information\n",
    "    logger.info(f\"going to create a payload file as dataset_info={json.dumps(dataset_info, indent=2)}\")\n",
    "\n",
    "    # Filter the DataFrame based on prompt length and language given below for constructing payloads of various sizes\n",
    "    df['prompt_len_in_range'] = df.prompt.map(lambda x: x['prompt_len'] >= dataset_info['min_length_in_tokens'] and \\\n",
    "                                                        x['prompt_len'] < dataset_info['max_length_in_tokens'])\n",
    "\n",
    "    # select prompts between pre-configured threshold lengths and are in the selected language\n",
    "    if 'language' in df.columns:\n",
    "        df_filtered = df[(df.language == dataset_info['language']) & (df.prompt_len_in_range)]\n",
    "    else:\n",
    "        df_filtered = df[df.prompt_len_in_range]\n",
    "\n",
    "    logger.info(f\"after filtering for {json.dumps(dataset_info, indent=2)}, shape of dataframe is {df_filtered.shape}\")\n",
    "    if df_filtered.shape[0] == 0:\n",
    "        logger.error(f\"did not find any prompts in the dataframe that matched the filtering criteria, exiting\")\n",
    "        return None\n",
    "    # df_filtered.head()\n",
    "\n",
    "    # Here, we construct request payloads for each row in the filtered DataFrame\n",
    "    df_filtered['request'] = df_filtered.apply(lambda r: construct_request_payload(r, config), axis=1)\n",
    "    logger.info(f\"payload request entry looks like this -> {json.dumps(df_filtered['request'].iloc[0], indent=2)}\")\n",
    "\n",
    "    # Convert the 'request' column of the filtered DataFrame to a JSON Lines string\n",
    "    json_lines_str = df_filtered['request'].to_json(orient='records', lines=True)\n",
    "\n",
    "    lang = dataset_info['language']\n",
    "    min_len = dataset_info['min_length_in_tokens']\n",
    "    max_len = dataset_info['max_length_in_tokens']\n",
    "    file_name = dataset_info['payload_file'].format(lang=lang, min=min_len, max=max_len)\n",
    "\n",
    "    prompts_path = os.path.join(DATA_DIR, config['dir_paths']['prompts_prefix'])\n",
    "\n",
    "    # defining the s3_path these prompts will go to\n",
    "    s3_file_path = os.path.join(prompts_path, file_name)\n",
    "\n",
    "    # Write the JSON Lines string to S3\n",
    "    # get the bucket name, config vars from config file\n",
    "    prefix = f\"{config['dir_paths']['prompts_prefix']}/{config['s3_read_data']['source_data_prefix']}\"\n",
    "    write_to_s3(json_lines_str, config['aws']['bucket'], DATA_DIR, prefix, file_name)\n",
    "\n",
    "    logger.info(f\"dataset of different payload file structures saved to s3://{config['aws']['bucket']}/{s3_file_path}\")\n",
    "    return f\"s3://{config['aws']['bucket']}/{s3_file_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = ((df, d, config) for d in config['datasets']['filters'])\n",
    "\n",
    "# This results in the creation of payload files for each dataset\n",
    "paths: List = list(itertools.starmap(create_dataset_payload_file, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join([p for p in paths if p]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
