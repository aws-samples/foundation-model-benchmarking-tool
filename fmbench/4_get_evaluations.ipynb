{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate candidate models using Majority Voting with PoLL (Panel of LLM Evaluators), gather findings on quantitative metrics (such as _Cosine Similarity, levenshtein distance, and token set ratio_)\n",
    "\n",
    "---\n",
    "\n",
    "_This notebook works best with the conda_python3 kernel on a ml.t3.medium machine_.\n",
    "\n",
    "#### This step of the solution evaluates the quality of responses generated by the candidate models that have to be benchmarked/evaluated. It does so by performing the steps below:\n",
    "\n",
    "- **Fetches the inference request file that contains all results from the inference step**: The inference results file that contains all inferences from each candidate model is fetched along with the associated metrics such as ground truth (if any), source payload file, concurrency level, etc.\n",
    "\n",
    "- **Generates quantitative metrics for evaluation**: This step calculates quantitative metrics to measure similarity and accuracy, using the _Cosine Similarity, levenshtein distance, and token set ratio_ metrics. Cosine similarity is a metric used to measure how similar two vectors are, regardless of their size. Levenshtein distance is a string metric for measuring the difference between two sequences. The Token Set Ratio algorithm tokenizes both input strings, removes duplicate tokens, and calculates the similarity score. This helps in getting a quantitative overall score to get an overall accuracy/similarity score for each model based on its responses. These quantitative metrics can be used for further downstream tasks to measure the trends, patterns and behaviours of different models on the same dataset or the same models hosted on different serving stacks across various AWS Generative AI services.\n",
    "- **Use a _Panel of LLM Evaluators_ approach to get subjective evaluations**: Refer to this [paper](https://arxiv.org/pdf/2404.18796). We use the following ways to evaluate the responses from the `candidate models` (models used to generate inferences)\n",
    "\n",
    "  - **Majority Voting**: When a dataset provides a ground truth, FMBench uses a technique called `Majority Voting`. Here, we use PoLL, _or a panel of LLM evaluators_, from different model families to evaluate each candidate model's response based on whether it generates a `correct` or an `incorrect` answer simply based on its comparison with the ground truth. Using models from different model families as a PoLL, increases it's ability to match a human level evaluation, makes the evaluation process more streamlined, consistent across all the responses, and reduces the latency and cost of evaluating the candidate models over time. The intra model bias during the evaluation process is also eliminated since more than a single model acts as a panel evaluator. FMBench uses [the majority voting evaluation instructions](prompt_template/eval_criteria/evaluation_instructions_majority_vote.txt) that are fed into the prompt templates supplied to different judge models to evalaute responses at runtime.\n",
    "\n",
    "  **Evaluation Process Flow**\n",
    "\n",
    "  1. First, all the quantitative metrics are calculated, such as the _Cosine Similarity, levenshtein similarity, and token set ratio_ for each candidate model. Once all the quantitative metrics are calculated, all the responses from each candidate model on each payload file is sent through a _Panel of LLM Evaluators_. This panel uses the ground truth for each sample from the data and checks the correctness of every candidate model output across the entire dataset. An example of a prompt that is used to perform this evaluation is [here](prompt_template/eval_criteria/claude_eval_prompt_templates/claude_eval_majority_vote.txt). Every LLM evaluator uses its specific prompt template that is populated with the question, context, ground truth, candidate model response and evaluation instructions at runtime. The evaluation instructions that are used for majority voting can be viewed [here](prompt_template/eval_criteria/evaluation_instructions_majority_vote.txt). Every LLM evaluator compares the model output to the given ground truth and gives a verdict as a binary decision ([`correct`/`incorrect`]) based on the correctness of the candidate model response and an explanation for that given verdict. For the purpose of this evaluation, FMBench uses 3 models as LLM evaluators but users/customers can use less or more.\n",
    "\n",
    "  1. Next, all LLM evaluations of the candidate models are sent through another evaluation layer. This layer performs a check on the evaluations done using the _Cosine Similarity Score_. This evaluation step reinforces the correctness or incorrectness of the evaluation made by the panel of LLM evaluators. This categorizes each evaluation into 3 sub categories:\n",
    "\n",
    "     1. If an LLM evaluator evaluates a candidate model response as `incorrect`, then we check if the _Cosine similarity score_ of that response is less than or equal to the `incorrect_verdict_cosine_similarity_threshold`. If so, then we define the evaluation done as correctly incorrect and move to the next.\n",
    "\n",
    "     1. If an LLM evaluator evaluates a candidate model response as `correct`, then we check if the _Cosine similarity score_ of that response is greater than or equal to the `correct_verdict_cosine_similarity_threshold`. If so, then we define the evaluation done as correctly correct and move to the next.\n",
    "\n",
    "     1. If an LLM evaluator evaluates a candidate model response as `correct` or `incorrect`, but none or either of the Cosine Similarity Thresholds are not met, those responses are labelled as `needs_further_human_or_LLM_evaluation`. For responses that are initially `incorrect` and do not satisfy the cosine similarity threshold are categorized as incorrect anyways.\n",
    "\n",
    "     ![](img/llm_eval_flowchart.png)\n",
    "\n",
    "**_All evaluations are generated in a JSON format for further downstream analytics on the evaluation results_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Application.verbose_crash=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import ray\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import yaml\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "import plotly.io as pio\n",
    "from pathlib import Path\n",
    "from statistics import mode\n",
    "import plotly.express as px\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from numpy.linalg import norm\n",
    "from litellm import completion\n",
    "from dataclasses import asdict\n",
    "from typing import List, Optional, Dict\n",
    "import importlib.resources as pkg_resources\n",
    "from botocore.exceptions import ClientError\n",
    "from litellm import completion, RateLimitError\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from litellm.llms.bedrock.common_utils import BedrockError\n",
    "from fmbench.scripts.pricing import load_and_update_pricing\n",
    "from fmbench.scripts.evaluation_scripts.bedrock_llm_evaluators import BedrockEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove existing handlers\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Add a simple handler\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 21:48:54,389\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c1e202f475482a80188dbcd17bfd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.12.8</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.42.1</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.12.8', ray_version='2.42.1', ray_commit='c2e38f7b75be223c0c033986472daada8622d64f')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the ray service to run async calls in parallel to bedrock easily\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:55,862] p528224 {2104575653.py:1} INFO - CONFIG_FILE=configs/bedrock/config-bedrock.yml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_name=us-west-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:56,199] p528224 {2104575653.py:3} INFO - {\n",
      "  \"general\": {\n",
      "    \"name\": \"latest-FMs-fmbench-bedrock\",\n",
      "    \"model_name\": \"FMs available in Amazon Bedrock\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-west-2\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::218208277580:role/adminaccessfortesting\",\n",
      "    \"bucket\": \"sagemaker-fmbench-write-us-west-2-218208277580\"\n",
      "  },\n",
      "  \"dir_paths\": {\n",
      "    \"data_prefix\": \"data\",\n",
      "    \"prompts_prefix\": \"prompts\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\",\n",
      "    \"metrics_dir\": \"metrics\",\n",
      "    \"models_dir\": \"models\",\n",
      "    \"metadata_dir\": \"metadata\"\n",
      "  },\n",
      "  \"s3_read_data\": {\n",
      "    \"read_bucket\": \"sagemaker-fmbench-read-us-west-2-218208277580\",\n",
      "    \"scripts_prefix\": \"scripts\",\n",
      "    \"script_files\": [\n",
      "      \"hf_token.txt\"\n",
      "    ],\n",
      "    \"configs_prefix\": \"configs\",\n",
      "    \"config_files\": [\n",
      "      \"pricing.yml\"\n",
      "    ],\n",
      "    \"source_data_prefix\": \"source_data\",\n",
      "    \"source_data_files\": [\n",
      "      \"2wikimqa_e.jsonl\",\n",
      "      \"2wikimqa.jsonl\",\n",
      "      \"hotpotqa_e.jsonl\",\n",
      "      \"hotpotqa.jsonl\",\n",
      "      \"narrativeqa.jsonl\",\n",
      "      \"triviaqa_e.jsonl\",\n",
      "      \"triviaqa.jsonl\"\n",
      "    ],\n",
      "    \"tokenizer_prefix\": \"llama2_tokenizer\",\n",
      "    \"prompt_template_dir\": \"prompt_template\",\n",
      "    \"prompt_template_file\": \"prompt_template_llama2.txt\"\n",
      "  },\n",
      "  \"run_steps\": {\n",
      "    \"0_setup.ipynb\": true,\n",
      "    \"1_generate_data.ipynb\": true,\n",
      "    \"2_deploy_model.ipynb\": false,\n",
      "    \"3_run_inference.ipynb\": true,\n",
      "    \"4_get_evaluations.ipynb\": true,\n",
      "    \"5_model_metric_analysis.ipynb\": true,\n",
      "    \"6_cleanup.ipynb\": false\n",
      "  },\n",
      "  \"datasets\": {\n",
      "    \"prompt_template_keys\": [\n",
      "      \"input\",\n",
      "      \"context\"\n",
      "    ],\n",
      "    \"ground_truth_col_key\": \"answers\",\n",
      "    \"question_col_key\": \"input\",\n",
      "    \"max_iters_per_combination\": 105,\n",
      "    \"min_iters_per_combination\": 5,\n",
      "    \"filters\": [\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 1,\n",
      "        \"max_length_in_tokens\": 500,\n",
      "        \"payload_file\": \"payload_en_1-500.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 500,\n",
      "        \"max_length_in_tokens\": 1000,\n",
      "        \"payload_file\": \"payload_en_500-1000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 1000,\n",
      "        \"max_length_in_tokens\": 2000,\n",
      "        \"payload_file\": \"payload_en_1000-2000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 2000,\n",
      "        \"max_length_in_tokens\": 3000,\n",
      "        \"payload_file\": \"payload_en_2000-3000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 3000,\n",
      "        \"max_length_in_tokens\": 3840,\n",
      "        \"payload_file\": \"payload_en_3000-3840.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 3000,\n",
      "        \"max_length_in_tokens\": 4000,\n",
      "        \"payload_file\": \"payload_en_3000-4000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 4000,\n",
      "        \"max_length_in_tokens\": 5000,\n",
      "        \"payload_file\": \"payload_en_4000-5000.jsonl\"\n",
      "      },\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"min_length_in_tokens\": 5000,\n",
      "        \"max_length_in_tokens\": 6000,\n",
      "        \"payload_file\": \"payload_en_5000-6000.jsonl\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"model_evaluations\": \"model_eval_all_info.yml\",\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_500-1000\"\n",
      "  },\n",
      "  \"pricing\": \"pricing.yml\",\n",
      "  \"inference_parameters\": {\n",
      "    \"bedrock\": {\n",
      "      \"temperature\": 0.1,\n",
      "      \"max_tokens\": 100,\n",
      "      \"top_p\": 0.92,\n",
      "      \"caching\": false\n",
      "    }\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
      "      \"model_id\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
      "      \"ep_name\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
      "      \"instance_type\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
      "      \"image_uri\": null,\n",
      "      \"deploy\": false,\n",
      "      \"instance_count\": null,\n",
      "      \"deployment_script\": null,\n",
      "      \"inference_script\": \"bedrock_predictor.py\",\n",
      "      \"inference_spec\": {\n",
      "        \"split_input_and_parameters\": false,\n",
      "        \"parameter_set\": \"bedrock\",\n",
      "        \"parameters\": {\n",
      "          \"temperature\": 0.1,\n",
      "          \"max_tokens\": 100,\n",
      "          \"top_p\": 0.92,\n",
      "          \"caching\": false\n",
      "        }\n",
      "      },\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"env\": null,\n",
      "      \"bucket\": \"sagemaker-fmbench-write-us-west-2-218208277580\"\n",
      "    }\n",
      "  ],\n",
      "  \"report\": {\n",
      "    \"latency_budget\": 2,\n",
      "    \"cosine_similarity_budget\": 0.3,\n",
      "    \"accuracy_budget\": 1,\n",
      "    \"accuracy_error_rate_budget\": 0,\n",
      "    \"cost_per_10k_txn_budget\": 50,\n",
      "    \"error_rate_budget\": 0,\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\",\n",
      "    \"txn_count_for_showing_cost\": 10000,\n",
      "    \"v_shift_w_single_instance\": 0.025,\n",
      "    \"v_shift_w_gt_one_instance\": 0.025\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role_arn_from_env=None, using current sts caller identity to set arn_string\n",
      "the sts role is an assumed role, setting arn_string to arn:aws:iam::218208277580:role/adminaccessfortesting\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"CONFIG_FILE={CONFIG_FILE}\")\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the associated pricing config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:56,336] p528224 {685747301.py:9} INFO - Using fmbench.configs directory: /home/ubuntu/byojudge/foundation-model-benchmarking-tool/.fmbench_python311/lib/python3.12/site-packages/fmbench/configs\n",
      "[2025-02-12 21:48:56,338] p528224 {685747301.py:12} INFO - pricing config provided for inference from this model is --> pricing.yml\n",
      "[2025-02-12 21:48:56,339] p528224 {685747301.py:16} INFO - pricing config file path is --> /home/ubuntu/byojudge/foundation-model-benchmarking-tool/.fmbench_python311/lib/python3.12/site-packages/fmbench/configs/pricing.yml\n",
      "[2025-02-12 21:48:56,340] p528224 {685747301.py:26} INFO - Extracted instances from the main config --> ['anthropic.claude-3-haiku-20240307-v1:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_name=us-west-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:56,672] p528224 {pricing.py:113} INFO - Loaded pricing data from /home/ubuntu/byojudge/foundation-model-benchmarking-tool/.fmbench_python311/lib/python3.12/site-packages/fmbench/configs/pricing.yml\n",
      "[2025-02-12 21:48:56,673] p528224 {pricing.py:133} INFO - Token-based pricing for anthropic.claude-3-haiku-20240307-v1:0 already exists. Skipping fetch.\n",
      "[2025-02-12 21:48:56,674] p528224 {685747301.py:31} INFO - pricing config file recorded: {\n",
      "  \"pricing\": {\n",
      "    \"instance_based\": {\n",
      "      \"anthropic.claude-v3-sonnet-pt-nc\": 88,\n",
      "      \"ml.c5.2xlarge\": 0.408,\n",
      "      \"ml.c5.4xlarge\": 0.816,\n",
      "      \"ml.c5.xlarge\": 0.204,\n",
      "      \"ml.c7i.xlarge\": 0.214,\n",
      "      \"ml.g4dn.12xlarge\": 4.89,\n",
      "      \"ml.g4dn.16xlarge\": 5.44,\n",
      "      \"ml.g4dn.2xlarge\": 0.94,\n",
      "      \"ml.g4dn.4xlarge\": 1.505,\n",
      "      \"ml.g4dn.8xlarge\": 2.72,\n",
      "      \"ml.g4dn.xlarge\": 0.7364,\n",
      "      \"ml.g5.12xlarge\": 7.09,\n",
      "      \"ml.g5.24xlarge\": 10.18,\n",
      "      \"ml.g5.2xlarge\": 1.515,\n",
      "      \"ml.g5.48xlarge\": 20.36,\n",
      "      \"ml.g5.4xlarge\": 2.03,\n",
      "      \"ml.g5.8xlarge\": 3.06,\n",
      "      \"ml.g5.xlarge\": 1.4084,\n",
      "      \"ml.g6.12xlarge\": 5.752,\n",
      "      \"ml.g6.16xlarge\": 4.246,\n",
      "      \"ml.g6.24xlarge\": 8.344,\n",
      "      \"ml.g6.2xlarge\": 1.222,\n",
      "      \"ml.g6.48xlarge\": 16.688,\n",
      "      \"ml.inf2.24xlarge\": 7.79,\n",
      "      \"ml.inf2.48xlarge\": 15.58,\n",
      "      \"ml.inf2.8xlarge\": 2.36,\n",
      "      \"ml.inf2.xlarge\": 0.99,\n",
      "      \"ml.m5.xlarge\": 0.23,\n",
      "      \"ml.p3.2xlarge\": 3.825,\n",
      "      \"ml.p4d.24xlarge\": 37.688,\n",
      "      \"ml.p5.48xlarge\": 113.068,\n",
      "      \"ml.trn1.32xlarge\": 28.497,\n",
      "      \"p5e.48xlarge\": 110.92\n",
      "    },\n",
      "    \"token_based\": {\n",
      "      \"ai21.j2-mid-v1\": {\n",
      "        \"input-per-1k-tokens\": 0.0125,\n",
      "        \"output-per-1k-tokens\": 0.0125\n",
      "      },\n",
      "      \"ai21.j2-ultra-v1\": {\n",
      "        \"input-per-1k-tokens\": 0.0188,\n",
      "        \"output-per-1k-tokens\": 0.0188\n",
      "      },\n",
      "      \"amazon.titan-text-express-v1\": {\n",
      "        \"input-per-1k-tokens\": 0.0002,\n",
      "        \"output-per-1k-tokens\": 0.0006\n",
      "      },\n",
      "      \"amazon.titan-text-lite-v1\": {\n",
      "        \"input-per-1k-tokens\": 0.00015,\n",
      "        \"output-per-1k-tokens\": 0.0002\n",
      "      },\n",
      "      \"anthropic.claude-3-5-sonnet-20240620-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.003,\n",
      "        \"output-per-1k-tokens\": 0.015\n",
      "      },\n",
      "      \"anthropic.claude-3-haiku-20240307-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.00025,\n",
      "        \"output-per-1k-tokens\": 0.00125\n",
      "      },\n",
      "      \"anthropic.claude-3-opus-20240229-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.015,\n",
      "        \"output-per-1k-tokens\": 0.075\n",
      "      },\n",
      "      \"anthropic.claude-3-sonnet-20240229-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.003,\n",
      "        \"output-per-1k-tokens\": 0.015\n",
      "      },\n",
      "      \"anthropic.claude-instant-v1\": {\n",
      "        \"input-per-1k-tokens\": 0.0008,\n",
      "        \"output-per-1k-tokens\": 0.0024\n",
      "      },\n",
      "      \"anthropic.claude-v2\": {\n",
      "        \"input-per-1k-tokens\": 0.008,\n",
      "        \"output-per-1k-tokens\": 0.024\n",
      "      },\n",
      "      \"anthropic.claude-v2:1\": {\n",
      "        \"input-per-1k-tokens\": 0.008,\n",
      "        \"output-per-1k-tokens\": 0.024\n",
      "      },\n",
      "      \"cohere.command-light-text-v14\": {\n",
      "        \"input-per-1k-tokens\": 0.0003,\n",
      "        \"output-per-1k-tokens\": 0.0006\n",
      "      },\n",
      "      \"cohere.command-r-plus-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.003,\n",
      "        \"output-per-1k-tokens\": 0.015\n",
      "      },\n",
      "      \"cohere.command-text-v14\": {\n",
      "        \"input-per-1k-tokens\": 0.0015,\n",
      "        \"output-per-1k-tokens\": 0.002\n",
      "      },\n",
      "      \"meta.llama2-13b-chat-v1\": {\n",
      "        \"input-per-1k-tokens\": 0.00075,\n",
      "        \"output-per-1k-tokens\": 0.001\n",
      "      },\n",
      "      \"meta.llama2-70b-chat-v1\": {\n",
      "        \"input-per-1k-tokens\": 0.00195,\n",
      "        \"output-per-1k-tokens\": 0.00256\n",
      "      },\n",
      "      \"meta.llama3-1-405b-instruct-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.00532,\n",
      "        \"output-per-1k-tokens\": 0.016\n",
      "      },\n",
      "      \"us.meta.llama3-1-70b-instruct-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.00072,\n",
      "        \"output-per-1k-tokens\": 0.00072\n",
      "      },\n",
      "      \"us.meta.llama3-1-8b-instruct-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.00022,\n",
      "        \"output-per-1k-tokens\": 0.00022\n",
      "      },\n",
      "      \"meta.llama3-70b-instruct-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.00265,\n",
      "        \"output-per-1k-tokens\": 0.0035\n",
      "      },\n",
      "      \"meta.llama3-8b-instruct-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.0003,\n",
      "        \"output-per-1k-tokens\": 0.0006\n",
      "      },\n",
      "      \"mistral.mistral-7b-instruct-v0:2\": {\n",
      "        \"input-per-1k-tokens\": 0.00015,\n",
      "        \"output-per-1k-tokens\": 0.0002\n",
      "      },\n",
      "      \"mistral.mixtral-8x7b-instruct-v0:1\": {\n",
      "        \"input-per-1k-tokens\": 0.00045,\n",
      "        \"output-per-1k-tokens\": 0.0007\n",
      "      },\n",
      "      \"us.meta.llama3-2-11b-instruct-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.00016,\n",
      "        \"output-per-1k-tokens\": 0.00016\n",
      "      },\n",
      "      \"us.meta.llama3-2-1b-instruct-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.0001,\n",
      "        \"output-per-1k-tokens\": 0.0001\n",
      "      },\n",
      "      \"us.meta.llama3-2-3b-instruct-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.00015,\n",
      "        \"output-per-1k-tokens\": 0.00015\n",
      "      },\n",
      "      \"us.meta.llama3-2-90b-instruct-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.00072,\n",
      "        \"output-per-1k-tokens\": 0.00072\n",
      "      },\n",
      "      \"amazon.nova-micro-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 3.5e-05,\n",
      "        \"output-per-1k-tokens\": 0.00014\n",
      "      },\n",
      "      \"amazon.nova-lite-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 6e-05,\n",
      "        \"output-per-1k-tokens\": 0.00024\n",
      "      },\n",
      "      \"amazon.nova-pro-v1:0\": {\n",
      "        \"input-per-1k-tokens\": 0.0008,\n",
      "        \"output-per-1k-tokens\": 0.0032\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role_arn_from_env=None, using current sts caller identity to set arn_string\n",
      "the sts role is an assumed role, setting arn_string to arn:aws:iam::218208277580:role/adminaccessfortesting\n"
     ]
    }
   ],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "pricing_file_path: str = config[\"pricing\"]\n",
    "\n",
    "# initialize the pricing config file to None\n",
    "pricing_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files(\"fmbench\"), \"configs\")\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "pricing_module = Path(config[\"pricing\"])\n",
    "logger.info(\n",
    "    f\"pricing config provided for inference from this model is --> {pricing_module}\"\n",
    ")\n",
    "pricing_file_path = os.path.join(config_dir, pricing_module)\n",
    "logger.info(f\"pricing config file path is --> {pricing_file_path}\")\n",
    "\n",
    "instance_list = [\n",
    "    experiment.get(\"instance_type\")\n",
    "    for experiment in config.get(\"experiments\", [])\n",
    "    if experiment.get(\"instance_type\")\n",
    "]\n",
    "\n",
    "\n",
    "# Print the extracted instance types\n",
    "logger.info(f\"Extracted instances from the main config --> {instance_list}\")\n",
    "\n",
    "pricing_config = load_and_update_pricing(\n",
    "    pricing_file_path, PRICING_FALLBACK_YAML_PATH, instance_list\n",
    ")\n",
    "logger.info(f\"pricing config file recorded: {json.dumps(pricing_config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model evaluation information\n",
    "\n",
    "---\n",
    "\n",
    "The common model configuration file contains information about which evaluation strategy to use (`majority voting`),\n",
    "the ground truth column if provided by the user in the config file, which FMs on Bedrock to use as LLM as evaluators,\n",
    "the prompt templates used by each LLM evaluator for Majority voting, the quantitative metric thresholds for an evaluation to be correct/incorrect,\n",
    "directory paths, inference parameters and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:56,774] p528224 {1436483999.py:9} INFO - Using fmbench.configs directory: /home/ubuntu/byojudge/foundation-model-benchmarking-tool/.fmbench_python311/lib/python3.12/site-packages/fmbench/configs\n",
      "[2025-02-12 21:48:56,775] p528224 {1436483999.py:12} INFO - eval config provided for evaluation --> model_eval_all_info.yml\n",
      "[2025-02-12 21:48:56,776] p528224 {1436483999.py:14} INFO - eval config file path is --> /home/ubuntu/byojudge/foundation-model-benchmarking-tool/.fmbench_python311/lib/python3.12/site-packages/fmbench/configs/model_eval_all_info.yml\n",
      "[2025-02-12 21:48:56,792] p528224 {1436483999.py:33} INFO - eval config file recorded: {\n",
      "  \"model_evaluations\": {\n",
      "    \"ground_truth_col\": \"answers\",\n",
      "    \"question_col\": \"input\",\n",
      "    \"PoLL_Composition_and_Voting\": {\n",
      "      \"method\": \"majority_vote\",\n",
      "      \"use_quantitative_metrics\": true\n",
      "    },\n",
      "    \"model_eval_dir\": {\n",
      "      \"eval_prompts_dir\": \"eval_criteria\",\n",
      "      \"eval_prompt_template_dir_list\": [\n",
      "        \"claude_eval_prompt_templates\",\n",
      "        \"llama3_eval_prompt_templates\",\n",
      "        \"cohere_eval_prompt_templates\",\n",
      "        \"mistral_eval_prompt_templates\"\n",
      "      ],\n",
      "      \"eval_instructions_dir\": \"eval_instructions\",\n",
      "      \"eval_instructions_files\": [\n",
      "        \"evaluation_instructions_majority_vote.txt\"\n",
      "      ]\n",
      "    },\n",
      "    \"quantitative_eval_info\": {\n",
      "      \"embeddings_model_id\": {\n",
      "        \"model_id\": \"sentence-transformers/all-mpnet-base-v2\"\n",
      "      },\n",
      "      \"incorrect_verdict_cosine_similarity_threshold\": 0.4,\n",
      "      \"correct_verdict_cosine_similarity_threshold\": 0.01\n",
      "    },\n",
      "    \"subjective_eval_info\": {\n",
      "      \"judge_panel_list\": [\n",
      "        {\n",
      "          \"model_id\": \"us.meta.llama3-3-70b-instruct-v1:0\",\n",
      "          \"instance_type\": \"us.meta.llama3-3-70b-instruct-v1:0\",\n",
      "          \"instance_count\": null,\n",
      "          \"eval_prompt_template_dir\": \"llama3_eval_prompt_templates\",\n",
      "          \"eval_prompt_template_name\": \"llama3_eval_majority_vote\"\n",
      "        },\n",
      "        {\n",
      "          \"model_id\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
      "          \"instance_type\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
      "          \"instance_count\": null,\n",
      "          \"eval_prompt_template_dir\": \"claude_eval_prompt_templates\",\n",
      "          \"eval_prompt_template_name\": \"claude_eval_majority_vote\"\n",
      "        },\n",
      "        {\n",
      "          \"model_id\": \"cohere.command-r-plus-v1:0\",\n",
      "          \"instance_type\": \"cohere.command-r-plus-v1:0\",\n",
      "          \"instance_count\": null,\n",
      "          \"eval_prompt_template_dir\": \"cohere_eval_prompt_templates\",\n",
      "          \"eval_prompt_template_name\": \"cohere_eval_majority_vote\"\n",
      "        }\n",
      "      ],\n",
      "      \"run_parallel_inference_count\": 5,\n",
      "      \"inference_parameters\": {\n",
      "        \"temperature\": 0.1,\n",
      "        \"max_tokens\": 300,\n",
      "        \"top_p\": 0.92,\n",
      "        \"caching\": false\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "model_eval_fpath: str = config[\"model_evaluations\"]\n",
    "\n",
    "# initialize the pricing config file to None\n",
    "eval_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files(\"fmbench\"), \"configs\")\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "eval_module = Path(config[\"model_evaluations\"])\n",
    "logger.info(f\"eval config provided for evaluation --> {eval_module}\")\n",
    "eval_file_path = os.path.join(config_dir, eval_module)\n",
    "logger.info(f\"eval config file path is --> {eval_file_path}\")\n",
    "\n",
    "# eval_config = load_config(eval_file_path).format(method_name=config['method_name'])\n",
    "with open(eval_file_path, \"r\") as file:\n",
    "    model_eval_info = file.read()\n",
    "    # load the preliminary unformatted config file to fetch the method name and plug it into\n",
    "    # the prompt template file names\n",
    "    model_eval_info_config = yaml.safe_load(model_eval_info)\n",
    "    model_eval_formatted_content = model_eval_info.format(\n",
    "        ground_truth=config[\"datasets\"].get(\"ground_truth_col_key\", None),\n",
    "        method_name=model_eval_info_config[\"model_evaluations\"][\n",
    "            \"PoLL_Composition_and_Voting\"\n",
    "        ].get(\"method\", None),\n",
    "        question=config[\"datasets\"].get(\"question_col_key\", None),\n",
    "    )\n",
    "    eval_config = yaml.safe_load(model_eval_formatted_content)\n",
    "\n",
    "# view all information that will be used in the evaluation process, which includes the ground truth\n",
    "# in the dataset, the evaluation method (Majority voting) and associated information\n",
    "logger.info(f\"eval config file recorded: {json.dumps(eval_config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:56,911] p528224 {210332609.py:6} INFO - cwd=/home/ubuntu/byojudge/foundation-model-benchmarking-tool/fmbench, METADATA_DIR=metadata, METRICS_PATH_FNAME=metrics_path.txt, metrics_path_file=metadata/metrics_path.txt\n",
      "[2025-02-12 21:48:56,913] p528224 {210332609.py:10} INFO - metrics_path_file=metadata/metrics_path.txt, METRICS_DIR=latest-FMs-fmbench-bedrock-adminaccessfortesting/data/metrics/yyyy=2025/mm=02/dd=12/hh=19/mm=32\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "if debug is True:\n",
    "    metrics_path_file: str = os.path.join(\"..\", \"..\", METADATA_DIR, METRICS_PATH_FNAME)\n",
    "else:\n",
    "    metrics_path_file: str = os.path.join(METADATA_DIR, METRICS_PATH_FNAME)\n",
    "logger.info(\n",
    "    f\"cwd={os.getcwd()}, METADATA_DIR={METADATA_DIR}, METRICS_PATH_FNAME={METRICS_PATH_FNAME}, metrics_path_file={metrics_path_file}\"\n",
    ")\n",
    "METRICS_DIR: str = Path(metrics_path_file).read_text().strip()\n",
    "logger.info(f\"metrics_path_file={metrics_path_file}, METRICS_DIR={METRICS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:57,048] p528224 {1711372933.py:4} INFO - File path containing the metrics per inference folder --> latest-FMs-fmbench-bedrock-adminaccessfortesting/data/metrics/yyyy=2025/mm=02/dd=12/hh=19/mm=32/per_inference_request_results.csv\n",
      "[2025-02-12 21:48:57,186] p528224 {1711372933.py:11} INFO - latest-FMs-fmbench-bedrock-adminaccessfortesting/data/metrics/yyyy=2025/mm=02/dd=12/hh=19/mm=32/per_inference_request_results.csv read into dataframe of shape (5, 25), cols=Index(['endpoint_name', 'prompt', 'question', 'ground_truth', 'base64_img',\n",
      "       'payload_file', 'temperature', 'max_tokens', 'top_p', 'completion',\n",
      "       'prompt_tokens', 'completion_tokens', 'latency', 'time_to_first_token',\n",
      "       'time_per_output_token', 'time_to_last_token', 'uuid',\n",
      "       'experiment_name', 'concurrency', 'instance_type', 'instance_count',\n",
      "       'EndpointName', 'ModelName', 'Image', 'S3Uri'],\n",
      "      dtype='object')\n",
      "[2025-02-12 21:48:57,187] p528224 {1711372933.py:15} INFO - latest-FMs-fmbench-bedrock-adminaccessfortesting/data/metrics/yyyy=2025/mm=02/dd=12/hh=19/mm=32/per_inference_request_results.csv contains results for the following endpoints=['anthropic.claude-3-haiku-20240307-v1:0']\n",
      "[2025-02-12 21:48:57,188] p528224 {1711372933.py:18} INFO -                             endpoint_name  \\\n",
      "0  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "1  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "2  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "3  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "4  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
      "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
      "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
      "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
      "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
      "\n",
      "                                            question  \\\n",
      "0  What family are the genus' Sinofranchetia and ...   \n",
      "1  What family are the genus' Sinofranchetia and ...   \n",
      "2  What family are the genus' Sinofranchetia and ...   \n",
      "3  What family are the genus' Sinofranchetia and ...   \n",
      "4  What family are the genus' Sinofranchetia and ...   \n",
      "\n",
      "                                        ground_truth  base64_img  \\\n",
      "0  a genus of flowering plant in the Lardizabalac...         NaN   \n",
      "1  a genus of flowering plant in the Lardizabalac...         NaN   \n",
      "2  a genus of flowering plant in the Lardizabalac...         NaN   \n",
      "3  a genus of flowering plant in the Lardizabalac...         NaN   \n",
      "4  a genus of flowering plant in the Lardizabalac...         NaN   \n",
      "\n",
      "             payload_file  temperature  max_tokens  top_p  \\\n",
      "0  payload_en_1-500.jsonl          0.1         100   0.92   \n",
      "1  payload_en_1-500.jsonl          0.1         100   0.92   \n",
      "2  payload_en_1-500.jsonl          0.1         100   0.92   \n",
      "3  payload_en_1-500.jsonl          0.1         100   0.92   \n",
      "4  payload_en_1-500.jsonl          0.1         100   0.92   \n",
      "\n",
      "                                          completion  ...  time_to_last_token  \\\n",
      "0  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
      "1  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
      "2  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
      "3  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
      "4  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
      "\n",
      "                               uuid                         experiment_name  \\\n",
      "0  a96963c50c1d44fcb378ea8d0272508e  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "1  ceb0a935cc0b4701a97274496eb9a98b  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "2  1e0cb4bda348496085b241c93c9f81ef  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "3  464f686f0be84d0b97795a9e8138c4f3  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "4  16a48fa517194d32802a561b0d508e8c  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "\n",
      "   concurrency                           instance_type  instance_count  \\\n",
      "0            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
      "1            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
      "2            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
      "3            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
      "4            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
      "\n",
      "  EndpointName ModelName  Image S3Uri  \n",
      "0          NaN       NaN    NaN   NaN  \n",
      "1          NaN       NaN    NaN   NaN  \n",
      "2          NaN       NaN    NaN   NaN  \n",
      "3          NaN       NaN    NaN   NaN  \n",
      "4          NaN       NaN    NaN   NaN  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path: str = os.path.join(\n",
    "    METRICS_DIR, config[\"report\"][\"per_inference_request_file\"]\n",
    ")\n",
    "logger.info(f\"File path containing the metrics per inference folder --> {file_path}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    file_content = get_s3_object(config[\"aws\"][\"bucket\"], file_path)\n",
    "    # Use pandas to read the CSV content\n",
    "    df_per_inference = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(\n",
    "        f\"{file_path} read into dataframe of shape {df_per_inference.shape}, \"\n",
    "        f\"cols={df_per_inference.columns}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"{file_path} contains results for the following endpoints={df_per_inference.endpoint_name.unique()}\"\n",
    "    )\n",
    "    logger.info(df_per_inference.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates in the inference file caused due to higher concurrency levels\n",
    "\n",
    "Calculate the accuracy on a unique set of data for each candidate model. If a given candidate model ran inferences on multiple concurrency levels for benchmarking purposes, FMBench uses only the unique set of prompts used per candidate model to get a measure of accuracy. This in turn reduces the time and cost to get model evaluations through the panel of LLM evalautors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>base64_img</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>temperature</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>top_p</th>\n",
       "      <th>completion</th>\n",
       "      <th>...</th>\n",
       "      <th>time_to_last_token</th>\n",
       "      <th>uuid</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>instance_count</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>Image</th>\n",
       "      <th>S3Uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>What family are the genus' Sinofranchetia and ...</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a96963c50c1d44fcb378ea8d0272508e</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>What family are the genus' Sinofranchetia and ...</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ceb0a935cc0b4701a97274496eb9a98b</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>What family are the genus' Sinofranchetia and ...</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1e0cb4bda348496085b241c93c9f81ef</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>What family are the genus' Sinofranchetia and ...</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>464f686f0be84d0b97795a9e8138c4f3</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>What family are the genus' Sinofranchetia and ...</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16a48fa517194d32802a561b0d508e8c</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            endpoint_name  \\\n",
       "0  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "1  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "2  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "3  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "4  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What family are the genus' Sinofranchetia and ...   \n",
       "1  What family are the genus' Sinofranchetia and ...   \n",
       "2  What family are the genus' Sinofranchetia and ...   \n",
       "3  What family are the genus' Sinofranchetia and ...   \n",
       "4  What family are the genus' Sinofranchetia and ...   \n",
       "\n",
       "                                        ground_truth  base64_img  \\\n",
       "0  a genus of flowering plant in the Lardizabalac...         NaN   \n",
       "1  a genus of flowering plant in the Lardizabalac...         NaN   \n",
       "2  a genus of flowering plant in the Lardizabalac...         NaN   \n",
       "3  a genus of flowering plant in the Lardizabalac...         NaN   \n",
       "4  a genus of flowering plant in the Lardizabalac...         NaN   \n",
       "\n",
       "             payload_file  temperature  max_tokens  top_p  \\\n",
       "0  payload_en_1-500.jsonl          0.1         100   0.92   \n",
       "1  payload_en_1-500.jsonl          0.1         100   0.92   \n",
       "2  payload_en_1-500.jsonl          0.1         100   0.92   \n",
       "3  payload_en_1-500.jsonl          0.1         100   0.92   \n",
       "4  payload_en_1-500.jsonl          0.1         100   0.92   \n",
       "\n",
       "                                          completion  ...  time_to_last_token  \\\n",
       "0  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
       "1  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
       "2  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
       "3  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
       "4  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
       "\n",
       "                               uuid                         experiment_name  \\\n",
       "0  a96963c50c1d44fcb378ea8d0272508e  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "1  ceb0a935cc0b4701a97274496eb9a98b  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "2  1e0cb4bda348496085b241c93c9f81ef  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "3  464f686f0be84d0b97795a9e8138c4f3  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "4  16a48fa517194d32802a561b0d508e8c  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "   concurrency                           instance_type  instance_count  \\\n",
       "0            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
       "1            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
       "2            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
       "3            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
       "4            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
       "\n",
       "  EndpointName ModelName  Image S3Uri  \n",
       "0          NaN       NaN    NaN   NaN  \n",
       "1          NaN       NaN    NaN   NaN  \n",
       "2          NaN       NaN    NaN   NaN  \n",
       "3          NaN       NaN    NaN   NaN  \n",
       "4          NaN       NaN    NaN   NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_per_inference.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:57,456] p528224 {2672332180.py:1} INFO - Inferences recorded from 1 endpoints.\n",
      "[2025-02-12 21:48:57,458] p528224 {2672332180.py:4} INFO - Shape of the inference file before removing duplicate inferences per candidate model: (5, 25)\n",
      "[2025-02-12 21:48:57,463] p528224 {2672332180.py:10} INFO - Shape of the inference file after removing duplicate inferences per candidate model: (1, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>base64_img</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>temperature</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>top_p</th>\n",
       "      <th>completion</th>\n",
       "      <th>...</th>\n",
       "      <th>time_to_last_token</th>\n",
       "      <th>uuid</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>instance_count</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>Image</th>\n",
       "      <th>S3Uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>What family are the genus' Sinofranchetia and ...</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16a48fa517194d32802a561b0d508e8c</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            endpoint_name  \\\n",
       "4  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "                                              prompt  \\\n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
       "\n",
       "                                            question  \\\n",
       "4  What family are the genus' Sinofranchetia and ...   \n",
       "\n",
       "                                        ground_truth  base64_img  \\\n",
       "4  a genus of flowering plant in the Lardizabalac...         NaN   \n",
       "\n",
       "             payload_file  temperature  max_tokens  top_p  \\\n",
       "4  payload_en_1-500.jsonl          0.1         100   0.92   \n",
       "\n",
       "                                          completion  ...  time_to_last_token  \\\n",
       "4  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
       "\n",
       "                               uuid                         experiment_name  \\\n",
       "4  16a48fa517194d32802a561b0d508e8c  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "   concurrency                           instance_type  instance_count  \\\n",
       "4            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
       "\n",
       "  EndpointName ModelName  Image S3Uri  \n",
       "4          NaN       NaN    NaN   NaN  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"Inferences recorded from {len(df_per_inference.endpoint_name.unique())} endpoints.\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Shape of the inference file before removing duplicate inferences per candidate model: {df_per_inference.shape}\"\n",
    ")\n",
    "df_per_inference = df_per_inference.drop_duplicates(\n",
    "    [\"endpoint_name\", \"prompt\"], keep=\"last\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Shape of the inference file after removing duplicate inferences per candidate model: {df_per_inference.shape}\"\n",
    ")\n",
    "df_per_inference.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:57,634] p528224 {3217771916.py:1} INFO - Going to be using this inference file to generate evaluations on ->                             endpoint_name  \\\n",
      "4  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "\n",
      "                                              prompt  \\\n",
      "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
      "\n",
      "                                            question  \\\n",
      "4  What family are the genus' Sinofranchetia and ...   \n",
      "\n",
      "                                        ground_truth  base64_img  \\\n",
      "4  a genus of flowering plant in the Lardizabalac...         NaN   \n",
      "\n",
      "             payload_file  temperature  max_tokens  top_p  \\\n",
      "4  payload_en_1-500.jsonl          0.1         100   0.92   \n",
      "\n",
      "                                          completion  ...  time_to_last_token  \\\n",
      "4  The genus Sinofranchetia and Stauntonia are bo...  ...                 NaN   \n",
      "\n",
      "                               uuid                         experiment_name  \\\n",
      "4  16a48fa517194d32802a561b0d508e8c  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "\n",
      "   concurrency                           instance_type  instance_count  \\\n",
      "4            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
      "\n",
      "  EndpointName ModelName  Image S3Uri  \n",
      "4          NaN       NaN    NaN   NaN  \n",
      "\n",
      "[1 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"Going to be using this inference file to generate evaluations on -> {df_per_inference.head()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between prompt token length and inference latency for different instances and concurrency levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:57,757] p528224 {519360418.py:1} INFO - Information on the inference file being used for evaluations: count    1.00000\n",
      "mean     0.83711\n",
      "std          NaN\n",
      "min      0.83711\n",
      "25%      0.83711\n",
      "50%      0.83711\n",
      "75%      0.83711\n",
      "max      0.83711\n",
      "Name: latency, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"Information on the inference file being used for evaluations: {df_per_inference.latency.describe()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:57,925] p528224 {582292302.py:1} INFO - Total number of inferences to evaluate from candidate models: 1\n"
     ]
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"Total number of inferences to evaluate from candidate models: {df_per_inference.shape[0]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the `sentence-transformers/all-mpnet-base-v2` embeddings model to calculate the _Cosine Similarity_ scores\n",
    "\n",
    "---\n",
    "\n",
    "This portion of the evaluation step does as follows:\n",
    "\n",
    "1. Uses the `sentence-transformers/all-mpnet-base-v2` model from Hugging Face. This is a sentence-transformers model. It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "1. Use the embeddings model to get quantitative metrics from the inferences. This helps to get a similarity score between the ground truth answers from a dataset if any are given and the actual responses from the model received during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the quantitiative evaluation information from the config file, such as the embeddings model\n",
    "# to be used\n",
    "embeddings_model_quantitative_info: Dict = eval_config[\"model_evaluations\"][\n",
    "    \"quantitative_eval_info\"\n",
    "]\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    This function loads the sentence-transformers model based on the provided model ID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = None\n",
    "        model_id = embeddings_model_quantitative_info[\"embeddings_model_id\"].get(\n",
    "            \"model_id\", None\n",
    "        )\n",
    "        if model_id:\n",
    "            model = SentenceTransformer(model_id)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Model ID is not provided or invalid in the configuration.\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"The SentenceTransformer embeddings model could not be loaded: {e}\"\n",
    "        )\n",
    "        model = None\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:48:58,175] p528224 {SentenceTransformer.py:210} INFO - Use pytorch device_name: cpu\n",
      "[2025-02-12 21:48:58,177] p528224 {SentenceTransformer.py:218} INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "[2025-02-12 21:49:01,757] p528224 {3052915936.py:3} INFO - Embeddings model info which will be used to calculate the cosine similarity scores for Majority Voting Eval: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load the embeddings model to calculate the cosine similarity scores\n",
    "model = load_model()\n",
    "logger.info(\n",
    "    f\"Embeddings model info which will be used to calculate the cosine similarity scores for Majority Voting Eval: {model}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between two texts. In this case,\n",
    "    the cosine similarity is the comparison between the ground truth in the given dataset\n",
    "    and the candidate model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cosine_similarity_score: float = None\n",
    "        # returns the embedding for a given text using the sentence-transformers model.\n",
    "        A = model.encode([text1])[0]\n",
    "        B = model.encode([text2])[0]\n",
    "        cosine_similarity_score = dot(A, B) / (norm(A) * norm(B))\n",
    "        logger.info(\n",
    "            f\"Calculating the cosine similarity score, current score: {cosine_similarity_score}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cosine similarity was not calculated at this iteration: {e}\")\n",
    "        cosine_similarity_score = None\n",
    "    return cosine_similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:49:01,985] p528224 {2104989195.py:8} INFO - The evaluation method FMBench is going to use to evaluate different model responses: majority_vote\n",
      "[2025-02-12 21:49:01,986] p528224 {2104989195.py:11} INFO - judge panel being used to evaluate model responses: [{'model_id': 'us.meta.llama3-3-70b-instruct-v1:0', 'instance_type': 'us.meta.llama3-3-70b-instruct-v1:0', 'instance_count': None, 'eval_prompt_template_dir': 'llama3_eval_prompt_templates', 'eval_prompt_template_name': 'llama3_eval_majority_vote'}, {'model_id': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0', 'instance_type': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0', 'instance_count': None, 'eval_prompt_template_dir': 'claude_eval_prompt_templates', 'eval_prompt_template_name': 'claude_eval_majority_vote'}, {'model_id': 'cohere.command-r-plus-v1:0', 'instance_type': 'cohere.command-r-plus-v1:0', 'instance_count': None, 'eval_prompt_template_dir': 'cohere_eval_prompt_templates', 'eval_prompt_template_name': 'cohere_eval_majority_vote'}]\n"
     ]
    }
   ],
   "source": [
    "# get the method that is being used to evaluate the content (which is either Majority voting)\n",
    "model_eval_subjective_info: List[Dict] = eval_config[\"model_evaluations\"][\n",
    "    \"subjective_eval_info\"\n",
    "]\n",
    "method_name: str = eval_config[\"model_evaluations\"][\"PoLL_Composition_and_Voting\"].get(\n",
    "    \"method\", None\n",
    ")\n",
    "logger.info(\n",
    "    f\"The evaluation method FMBench is going to use to evaluate different model responses: {method_name}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"judge panel being used to evaluate model responses: {model_eval_subjective_info.get('judge_panel_list', None)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:49:02,131] p528224 {946843640.py:2} INFO - Valid ground truth column found in the inference file: answers, calculating cosine similarity scores\n",
      "[2025-02-12 21:49:02,133] p528224 {946843640.py:5} INFO - ~Creating embeddings and calculating cosine similarity scores for of all candidate model responses now. This might take a 1-2 minutes~\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f8e38bc00b4868a846a93a1fb37fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2406441d93b49439098594a89da8aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:49:02,492] p528224 {3109731133.py:13} INFO - Calculating the cosine similarity score, current score: 0.6601186394691467\n",
      "[2025-02-12 21:49:02,504] p528224 {946843640.py:24} INFO - Calculated the cosine similarity score:                             endpoint_name  \\\n",
      "4  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "\n",
      "                                              prompt  \\\n",
      "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
      "\n",
      "                                            question  \\\n",
      "4  What family are the genus' Sinofranchetia and ...   \n",
      "\n",
      "                                        ground_truth  base64_img  \\\n",
      "4  a genus of flowering plant in the Lardizabalac...         NaN   \n",
      "\n",
      "             payload_file  temperature  max_tokens  top_p  \\\n",
      "4  payload_en_1-500.jsonl          0.1         100   0.92   \n",
      "\n",
      "                                          completion  ...  \\\n",
      "4  The genus Sinofranchetia and Stauntonia are bo...  ...   \n",
      "\n",
      "                               uuid                         experiment_name  \\\n",
      "4  16a48fa517194d32802a561b0d508e8c  anthropic.claude-3-haiku-20240307-v1:0   \n",
      "\n",
      "   concurrency                           instance_type  instance_count  \\\n",
      "4            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
      "\n",
      "   EndpointName ModelName Image  S3Uri cosine_similarity_score  \n",
      "4           NaN       NaN   NaN    NaN                0.660119  \n",
      "\n",
      "[1 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# calculate the quantitative metrics if evaluation is set to Majority voting\n",
    "logger.info(\n",
    "    f\"Valid ground truth column found in the inference file: {eval_config['model_evaluations'].get('ground_truth_col')}, calculating cosine similarity scores\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"~Creating embeddings and calculating cosine similarity scores for of all candidate model responses now. This might take a 1-2 minutes~\"\n",
    ")\n",
    "ground_truth_col_name: Optional[str] = config[\"datasets\"].get(\n",
    "    \"ground_truth_col_key\", None\n",
    ")\n",
    "\n",
    "# Check for ground truth column and raise an exception if not found\n",
    "if ground_truth_col_name is None:\n",
    "    raise ValueError(\n",
    "        f\"Expected a valid ground truth column name in the config file information, got {ground_truth_col_name}. Cannot continue.\"\n",
    "    )\n",
    "\n",
    "# If we reach this point, we know the ground truth column exists\n",
    "df_per_inference[\"cosine_similarity_score\"] = df_per_inference.apply(\n",
    "    lambda row: calculate_cosine_similarity(row[\"completion\"], row[\"ground_truth\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "logger.info(f\"Calculated the cosine similarity score: {df_per_inference.head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Evaluations: Hierarchical Flow\n",
    "\n",
    "---\n",
    "\n",
    "1. Check for the lexical match/similarity between the ground truth and the answer using one main quantitative metrics: \\_Cosine similarity score.\n",
    "\n",
    "1. Parse each candidate model response through a panel of LLM evaluators to determine the accuracy of that model across the entire dataset.\n",
    "\n",
    "1. Send all evaluations from the LLM evaluators through a final evaluation layer to check if the evaluation is correctly or incorrectly made with the help of quantitative metric thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:49:02,729] p528224 {3733622903.py:18} INFO - Per inference cosine similarity scores saved to s3://sagemaker-fmbench-write-us-west-2-218208277580/latest-FMs-fmbench-bedrock-adminaccessfortesting/data/metrics/yyyy=2025/mm=02/dd=12/hh=19/mm=32/per_inference_quantitative_eval_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>base64_img</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>temperature</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>top_p</th>\n",
       "      <th>completion</th>\n",
       "      <th>...</th>\n",
       "      <th>uuid</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>instance_count</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>Image</th>\n",
       "      <th>S3Uri</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>What family are the genus' Sinofranchetia and ...</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>...</td>\n",
       "      <td>16a48fa517194d32802a561b0d508e8c</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.660119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            endpoint_name  \\\n",
       "4  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "                                              prompt  \\\n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
       "\n",
       "                                            question  \\\n",
       "4  What family are the genus' Sinofranchetia and ...   \n",
       "\n",
       "                                        ground_truth  base64_img  \\\n",
       "4  a genus of flowering plant in the Lardizabalac...         NaN   \n",
       "\n",
       "             payload_file  temperature  max_tokens  top_p  \\\n",
       "4  payload_en_1-500.jsonl          0.1         100   0.92   \n",
       "\n",
       "                                          completion  ...  \\\n",
       "4  The genus Sinofranchetia and Stauntonia are bo...  ...   \n",
       "\n",
       "                               uuid                         experiment_name  \\\n",
       "4  16a48fa517194d32802a561b0d508e8c  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "   concurrency                           instance_type  instance_count  \\\n",
       "4            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
       "\n",
       "   EndpointName ModelName Image  S3Uri cosine_similarity_score  \n",
       "4           NaN       NaN   NaN    NaN                0.660119  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the all_metrics path to send the evaluation metrics to\n",
    "all_metrics_fpath: str = os.path.join(METRICS_DIR, config[\"report\"][\"all_metrics_file\"])\n",
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_cosine_similarity_scores_csv = csv_buffer.getvalue()\n",
    "inference_cosine_similarity_scores_s3_path = os.path.join(\n",
    "    METRICS_DIR, PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES\n",
    ")  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(\n",
    "    df_per_inference_with_cosine_similarity_scores_csv,\n",
    "    BUCKET_NAME,\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES,\n",
    ")\n",
    "logger.info(\n",
    "    f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{inference_cosine_similarity_scores_s3_path}\"\n",
    ")\n",
    "df_per_inference.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use _Panel of LLM Evaluators_ to get Subjective Evaluations on various evaluation criteria\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we run evaluations on all candidate models using a panel of LLM evaluators. We use a main evaluation method: `Majority Voting`. To eliminate intra-model bias, we address this by scoring answer correctness based not on a single judge, but instead on a panel composed of multiple evaluator models.\n",
    "\n",
    "1. **Majority Voting**: We use the PoLL to evaluate candidate model responses by checking its correctness compared to a provided ground truth answer in the dataset. We prompt each PoLL to evaluate and give the response in a JSON structure, giving a verdict on whether the response is correct or incorrect based on its comparison with the ground truth, and an explanation as to why that is. With all verdicts and responses in JSON, we can perform downstream tasks such as:\n",
    "\n",
    "   1. Calculate the overall accuracy of each model using the correct versus the (correct + incorrect) responses\n",
    "\n",
    "   1. Calculate the `error rate` or frequency or incorrect responses\n",
    "\n",
    "   1. Categorize the errors based on the explanations provided by the evaluators. Common categories might include misunderstanding the question, incomplete answers, factual inaccuracies\n",
    "\n",
    "   1. Summary of overall correct/incorrect, and the best model based on the PoLL. Rank the models on Correctness versus Incorrectness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:49:02,882] p528224 {4236313324.py:7} INFO - available llm as a judge evaluation information to use: {\n",
      "  \"judge_panel_list\": [\n",
      "    {\n",
      "      \"model_id\": \"us.meta.llama3-3-70b-instruct-v1:0\",\n",
      "      \"instance_type\": \"us.meta.llama3-3-70b-instruct-v1:0\",\n",
      "      \"instance_count\": null,\n",
      "      \"eval_prompt_template_dir\": \"llama3_eval_prompt_templates\",\n",
      "      \"eval_prompt_template_name\": \"llama3_eval_majority_vote\"\n",
      "    },\n",
      "    {\n",
      "      \"model_id\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
      "      \"instance_type\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
      "      \"instance_count\": null,\n",
      "      \"eval_prompt_template_dir\": \"claude_eval_prompt_templates\",\n",
      "      \"eval_prompt_template_name\": \"claude_eval_majority_vote\"\n",
      "    },\n",
      "    {\n",
      "      \"model_id\": \"cohere.command-r-plus-v1:0\",\n",
      "      \"instance_type\": \"cohere.command-r-plus-v1:0\",\n",
      "      \"instance_count\": null,\n",
      "      \"eval_prompt_template_dir\": \"cohere_eval_prompt_templates\",\n",
      "      \"eval_prompt_template_name\": \"cohere_eval_majority_vote\"\n",
      "    }\n",
      "  ],\n",
      "  \"run_parallel_inference_count\": 5,\n",
      "  \"inference_parameters\": {\n",
      "    \"temperature\": 0.1,\n",
      "    \"max_tokens\": 300,\n",
      "    \"top_p\": 0.92,\n",
      "    \"caching\": false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# get the qualitative/subjective evaluation information from the config file to evaluate answers from different\n",
    "# endpoints on various criteria\n",
    "model_eval_subjective_info: Dict = eval_config[\"model_evaluations\"][\n",
    "    \"subjective_eval_info\"\n",
    "]\n",
    "eval_criteria_list = model_eval_subjective_info.get(\"eval_criteria\", None)\n",
    "logger.info(\n",
    "    f\"available llm as a judge evaluation information to use: {json.dumps(model_eval_subjective_info, indent=2)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:49:03,003] p528224 {331787567.py:5} INFO - Inference parameters that LLM evaluators will use: {'temperature': 0.1, 'max_tokens': 300, 'top_p': 0.92, 'caching': False}\n"
     ]
    }
   ],
   "source": [
    "# get the inference parameters that the LLM judge panel will use while evaluating model candidate responses\n",
    "INFERENCE_PARAMETERS_LLM_PANEL: Dict = eval_config[\"model_evaluations\"][\n",
    "    \"subjective_eval_info\"\n",
    "].get(\"inference_parameters\", None)\n",
    "logger.info(\n",
    "    f\"Inference parameters that LLM evaluators will use: {INFERENCE_PARAMETERS_LLM_PANEL}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_filename(s):\n",
    "    \"\"\"\n",
    "    convert a string to another string that can be used as a filename\n",
    "    i.e. remove white space and non-word chars\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"None\"\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "    # Replace all runs of whitespace with a single dash\n",
    "    s = re.sub(r\"\\s+\", \"-\", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_as_json(x: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Convert a string into a dictionary. Remove any\n",
    "    stray whitespaces which could break the json parsing\n",
    "    \"\"\"\n",
    "    d: Optional[Dict] = None\n",
    "    try:\n",
    "        x = x.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        d = json.loads(x)\n",
    "    except Exception as e:\n",
    "        print(f\"parse_as_json, error parsing string as json, string={x}\")\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>base64_img</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>temperature</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>top_p</th>\n",
       "      <th>candidate_model_response</th>\n",
       "      <th>...</th>\n",
       "      <th>uuid</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>instance_count</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>Image</th>\n",
       "      <th>S3Uri</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>What family are the genus' Sinofranchetia and ...</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>...</td>\n",
       "      <td>16a48fa517194d32802a561b0d508e8c</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>1</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.660119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            endpoint_name  \\\n",
       "4  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "                                              prompt  \\\n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
       "\n",
       "                                            question  \\\n",
       "4  What family are the genus' Sinofranchetia and ...   \n",
       "\n",
       "                                        ground_truth  base64_img  \\\n",
       "4  a genus of flowering plant in the Lardizabalac...         NaN   \n",
       "\n",
       "             payload_file  temperature  max_tokens  top_p  \\\n",
       "4  payload_en_1-500.jsonl          0.1         100   0.92   \n",
       "\n",
       "                            candidate_model_response  ...  \\\n",
       "4  The genus Sinofranchetia and Stauntonia are bo...  ...   \n",
       "\n",
       "                               uuid                         experiment_name  \\\n",
       "4  16a48fa517194d32802a561b0d508e8c  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "   concurrency                           instance_type  instance_count  \\\n",
       "4            1  anthropic.claude-3-haiku-20240307-v1:0             NaN   \n",
       "\n",
       "   EndpointName ModelName Image  S3Uri cosine_similarity_score  \n",
       "4           NaN       NaN   NaN    NaN                0.660119  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_per_inference.rename(\n",
    "    columns={\"completion\": \"candidate_model_response\"}, inplace=True\n",
    ")\n",
    "df_per_inference.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the evaluation prompt payloads\n",
    "\n",
    "---\n",
    "\n",
    "Here, the evaluation prompt template is used by the LLM judge to evaluate the answers on different criteria.\n",
    "This prompt template function uses a set of rules, prompt template, the answer, and ground truth (if any) in the\n",
    "evaluation solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_eval_prompts(\n",
    "    eval_template: str,\n",
    "    answer: str,\n",
    "    rules: str,\n",
    "    ground_truth: Optional[str],\n",
    "    question: Optional[str],\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prepares the evaluation prompts by preparing the standard eval prompt template\n",
    "    with the rules of a given subjective criteria, context, answer and ground truth (if any ground truth is provided)\n",
    "    This function prepares prompt payloads for both evaluation criteria: Majority voting. In the\n",
    "    case of Majority voting, there is no subjective criteria that is inputted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        processed_eval_template: Optional[str] = None\n",
    "        processed_eval_template = eval_template.format(\n",
    "            rules=rules, answer=answer, ground_truth=ground_truth, question=question\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error encountered while generating the evaluation prompt template: {e}\"\n",
    "        )\n",
    "        processed_eval_template = None\n",
    "    return processed_eval_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "\n",
    "# create the metrics directory that stores all of the json files containing evaluations from all Panel of LLM evaluators\n",
    "METRICS_PER_POLL_EVAL_DIR: str = os.path.join(\n",
    "    METRICS_DIR, METRICS_PER_POLL_EVAL_DIR_NAME\n",
    ")\n",
    "_ = list(map(clear_dir, [METRICS_PER_POLL_EVAL_DIR]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_candidate_model_name(model_name: str) -> str:\n",
    "    # the candidate model is actually an endpoint name, so remove the timestamp and -endpoint from\n",
    "    # string so \"Meta-Llama-3-1-8B-Instruct-g5-2024-08-17-01-25-45-284-endpoint\" would become\n",
    "    # \"Meta-Llama-3-1-8B-Instruct-g5\", and no change would happen for Bedrock models as they dont\n",
    "    # contain timestamp, for example anthropic.claude-3-opus-20240229-v1:0 would remain unchanged\n",
    "\n",
    "    # regex to match the timestamp and the endpoint part\n",
    "    regex = r\"-\\d{4}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{3}.*$\"\n",
    "    # removing the matched part\n",
    "    model_name_normalized = re.sub(regex, \"\", model_name)\n",
    "    return model_name_normalized\n",
    "\n",
    "\n",
    "def run_panel_of_llm_evals(\n",
    "    i: int, total: int, row: Dict, model_id: str, instance_type: Optional[str], instance_count: Optional[int], eval_method_name: str, uuid: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Runs the evaluation for one row\n",
    "    The eval prompt is already available in the row dictionary\n",
    "    and we simply want to run the inference against the judge model.\n",
    "    The results are returned in a new dictionary that contains the model\n",
    "    response and some fields from the original dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # initialize the response dictionary that contains the pricing information\n",
    "        # along with other metrics. If there is any error encountered, this response\n",
    "        # dictionary is returned as is\n",
    "        resp = dict(\n",
    "            exception=None,\n",
    "            completion=None,\n",
    "            completion_tokens=None,\n",
    "            prompt_tokens=None,\n",
    "            input_token_cost=None,\n",
    "            output_token_cost=None,\n",
    "            total_cost=None,\n",
    "            model_id=model_id,\n",
    "            latency=None,\n",
    "            candidate_model_response=row[\"candidate_model_response\"],\n",
    "            candidate_model=None,\n",
    "            payload_file=row[\"payload_file\"],\n",
    "            cosine_similarity_score=row[\"cosine_similarity_score\"],\n",
    "            ground_truth=row[\"ground_truth\"],\n",
    "            question=row[\"question\"] if \"question\" in row else None,\n",
    "        )\n",
    "        candidate_model = normalize_candidate_model_name(row[\"endpoint_name\"])\n",
    "        logger.info(\n",
    "            f\"run_eval, row {i}/{total}, judge_model_id={model_id}, candidate model={candidate_model}\"\n",
    "        )\n",
    "        # create the payload for model inference\n",
    "        prompt = row[f\"{model_id}_{method_name}_eval_prompt\"]\n",
    "        # define the BedrockEvaluator class\n",
    "        endpoint_name = row[\"endpoint_name\"]\n",
    "        bedrock_eval = BedrockEvaluation(endpoint_name)\n",
    "        # generate the evaluation on the data using the model judge\n",
    "        resp = bedrock_eval.get_llm_evaluation(model_id, prompt)\n",
    "        print(f\"RESPONSE DEBUG: {resp}\")\n",
    "        # assign the completion from the candidate model to the `candidate_model_response`,\n",
    "        # and the actual evaluation will be contained in a field called `completion`\n",
    "        resp[\"candidate_model_response\"] = row[\"candidate_model_response\"]\n",
    "        resp[\"latency\"] = row[\"latency\"]\n",
    "        resp[\"candidate_model\"] = candidate_model\n",
    "        resp[\"payload_file\"] = row[\"payload_file\"]\n",
    "        resp[\"cosine_similarity_score\"] = row[\"cosine_similarity_score\"]\n",
    "        # Calculate cost based on the number of input and output tokens\n",
    "        total_cost = bedrock_eval.calculate_llm_eval_cost(\n",
    "                        instance_type,\n",
    "                        instance_count,\n",
    "                        pricing_config,\n",
    "                        resp['latency'],\n",
    "                        resp['prompt_tokens'],\n",
    "                        resp['completion_tokens'],\n",
    "                    )\n",
    "        if total_cost is not None:\n",
    "            resp['total_cost'] = total_cost\n",
    "            logger.info(f\"Total cost of this evaluation: {total_cost}\")\n",
    "        # if there is a ground truth (in case of Majority voting) or\n",
    "        # criteria name (in case of average pooline), include those in the json response\n",
    "        resp[\"ground_truth\"] = row[\"ground_truth\"]\n",
    "        if \"question\" in row:\n",
    "            resp[\"question\"] = row[\"question\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encountered while running evaluation: {e}\")\n",
    "        resp[\"exception\"] = str(e)\n",
    "    return resp\n",
    "\n",
    "\n",
    "# we use Ray to parallize\n",
    "@ray.remote\n",
    "def async_run_eval(\n",
    "    i: int, total: int, row: Dict, model_id: str, instance_type: Optional[str], instance_count: Optional[int], eval_method_name: str, uuid: str\n",
    ") -> Dict:\n",
    "    print(\n",
    "        f\"async_run_eval, i={i}, total={total}, judge_model_info={model_id}, eval_method: {eval_method_name}, uuid: {uuid}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"async_run_eval, i={i}, total={total}, judge_model_info={model_id}, eval_method: {eval_method_name}, uuid: {uuid}\"\n",
    "    )\n",
    "    return run_panel_of_llm_evals(i, total, row, model_id, instance_type, instance_count, eval_method_name, uuid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:52:19,144] p528224 {1775823573.py:3} INFO - Total number of candidate models going to be evaluated: 1\n"
     ]
    }
   ],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "df_per_inference_list = json.loads(df_per_inference.to_json(orient=\"records\"))\n",
    "logger.info(\n",
    "    f\"Total number of candidate models going to be evaluated: {len(df_per_inference_list)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare evaluation prompt templates\n",
    "\n",
    "---\n",
    "\n",
    "This portion of the step prepares the evaluation prompt templates that are used in the evaluation process of using `Majority Voting` using the PoLL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'judge_panel_list': [{'model_id': 'us.meta.llama3-3-70b-instruct-v1:0',\n",
       "   'instance_type': 'us.meta.llama3-3-70b-instruct-v1:0',\n",
       "   'instance_count': None,\n",
       "   'eval_prompt_template_dir': 'llama3_eval_prompt_templates',\n",
       "   'eval_prompt_template_name': 'llama3_eval_majority_vote'},\n",
       "  {'model_id': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
       "   'instance_type': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
       "   'instance_count': None,\n",
       "   'eval_prompt_template_dir': 'claude_eval_prompt_templates',\n",
       "   'eval_prompt_template_name': 'claude_eval_majority_vote'},\n",
       "  {'model_id': 'cohere.command-r-plus-v1:0',\n",
       "   'instance_type': 'cohere.command-r-plus-v1:0',\n",
       "   'instance_count': None,\n",
       "   'eval_prompt_template_dir': 'cohere_eval_prompt_templates',\n",
       "   'eval_prompt_template_name': 'cohere_eval_majority_vote'}],\n",
       " 'run_parallel_inference_count': 5,\n",
       " 'inference_parameters': {'temperature': 0.1,\n",
       "  'max_tokens': 300,\n",
       "  'top_p': 0.92,\n",
       "  'caching': False}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval_subjective_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:52:19,874] p528224 {3525940305.py:1} INFO - Number of judges being used for this model evaluation: 3\n",
      "[2025-02-12 21:52:19,875] p528224 {3525940305.py:4} INFO - Inference Parameters that are going to be used by the judge panels while evaluating candidate models: {'temperature': 0.1, 'max_tokens': 300, 'top_p': 0.92, 'caching': False}\n"
     ]
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"Number of judges being used for this model evaluation: {len(model_eval_subjective_info.get('judge_panel_list', None))}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Inference Parameters that are going to be used by the judge panels while evaluating candidate models: {model_eval_subjective_info.get('inference_parameters', None)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare prompt payloads\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the step, FMBench iterates through each of the row containing the model response and prepares the corresponding prompt payloads. In this step, the prompt template for a given evaluation method is used. For Majority voting, a standard prompt template is used with evaluation instructions and candidate model responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:52:20,261] p528224 {3863069304.py:25} INFO - evaluation prompt template file path being used for us.meta.llama3-3-70b-instruct-v1:0: /home/ubuntu/byojudge/foundation-model-benchmarking-tool/.fmbench_python311/lib/python3.12/site-packages/fmbench/prompt_template/eval_criteria/llama3_eval_prompt_templates/llama3_eval_majority_vote.txt\n",
      "[2025-02-12 21:52:20,262] p528224 {3863069304.py:28} INFO - evaluation prompt template file name: llama3_eval_majority_vote.txt\n",
      "[2025-02-12 21:52:20,263] p528224 {3863069304.py:32} INFO - Evaluation prompt template being used: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "Your role is to evaluate the correctness of the candidate model output provided in the candidate model output section\n",
      "based on whether it aligns with the ground truth answer provided in the ground truth section in answering the \n",
      "question in the question section.\n",
      "\n",
      "Refer to the question that you have to use while evaluating the correctness of the candidate model response in alignment to the ground truth:\n",
      "Question: {question}\n",
      "\n",
      "Refer to the candidate model response to be evaluated below:\n",
      "candidate model response: {answer}\n",
      "\n",
      "\n",
      "Refer to the ground truth to the question in the context below in the ground_truth section:\n",
      "ground_truth: {ground_truth}\n",
      "\n",
      "Follow the instructions below while giving your evaluation in the evaluation_instructions section below:\n",
      "\n",
      "evaluation_instructions:\n",
      "{rules}\n",
      "\n",
      "Do not add anything else in your response. Give your response only in a correct formatted json starting with an \n",
      "opening and ending with a bracket.\n",
      "\n",
      "Do not add any pre filler words. Your response should contain the JSON structured output only.\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "[2025-02-12 21:52:20,264] p528224 {3863069304.py:47} INFO - rules: 1. Your response should be a JSON containing two elements: \"verdict\" and \"explanation\". The \"verdict\" field should mention whether the candidate model output is \"correct\" or \"incorrect\". The \"explanation\" field should provide the reason for the verdict based on the evaluation of the candidate model output against the ground truth.\n",
      "\n",
      "2. If the ground truth is a comma-separated list with multiple options for responses, the verdict is correct if the candidate model output \n",
      "aligns or matches with any of the options in the comma separated list and answers the question in the similar way as the ground truth. \n",
      "It does not have to match all, but if it aligns or matches with one of the options, then the verdict is correct. If the ground truth contains a single option for a response, \n",
      "then only compare the candidate model response with that to check if the candidate model response is correct or not.\n",
      "\n",
      "3. The candidate model output does not have to use the exact wording of the ground truth but should correctly answer the question in a semantically equivalent manner. If there is more content in the candidate model response than required but it answers the question correctly and aligns or matches\n",
      "with the ground truth, the verdict is \"correct\". Otherwise, it is \"incorrect\".\n",
      "\n",
      "4. If the candidate model output does not match or align with the ground truth, or any option in the ground truth if the ground truth is \n",
      "a comma separated list of options, then the verdict is \"incorrect\".\n",
      "\n",
      "5. If the candidate model output has more context than required compared to the ground truth in answering the question, but still contains the relevant answer \n",
      "like the ground truth, then the verdict is correct.\n",
      "\n",
      "[2025-02-12 21:52:20,265] p528224 {3863069304.py:25} INFO - evaluation prompt template file path being used for us.anthropic.claude-3-5-sonnet-20241022-v2:0: /home/ubuntu/byojudge/foundation-model-benchmarking-tool/.fmbench_python311/lib/python3.12/site-packages/fmbench/prompt_template/eval_criteria/claude_eval_prompt_templates/claude_eval_majority_vote.txt\n",
      "[2025-02-12 21:52:20,266] p528224 {3863069304.py:28} INFO - evaluation prompt template file name: claude_eval_majority_vote.txt\n",
      "[2025-02-12 21:52:20,267] p528224 {3863069304.py:32} INFO - Evaluation prompt template being used: Human: Your role is to evaluate the correctness of the candidate model output provided in the <candidate model output></candidate model output> \n",
      "tags based on whether it aligns with the ground truth answer provided in the <ground_truth></ground_truth> xml tags in answering the \n",
      "question in the <question></question> xml tags.\n",
      "\n",
      "Refer to the question that you have to use while evaluating the correctness of the candidate model response in alignment to the ground truth:\n",
      "<question>\n",
      "{question}\n",
      "</question> \n",
      "\n",
      "Refer to the candidate model response to be evaluated in the <candidate model output></candidate model output> tags below:\n",
      "<candidate model output>\n",
      "{answer}\n",
      "</candidate model output> \n",
      "\n",
      "Refer to the ground truth below in the <ground_truth></ground_truth> xml tags while evaluating the candidate model output:\n",
      "<ground_truth>\n",
      "{ground_truth}\n",
      "</ground_truth> \n",
      "\n",
      "Follow the instructions below while giving your evaluation of the candidate model output in the <evaluation_instructions></evaluation_instructions>\n",
      "tags:\n",
      "\n",
      "<evaluation_instructions>\n",
      "{rules}\n",
      "</evaluation_instructions>\n",
      "\n",
      "Your response should only be in JSON format. Your response should NOT have any tags, and should start with the starting bracket of the JSON\n",
      "structure and end with the ending bracket. There should only be the JSON in your response without any other words outside of it, should not\n",
      "contain any tags, only the JSON structure.\n",
      "\n",
      "Assistant: Sure, here is my evaluation in JSON:\n",
      "[2025-02-12 21:52:20,269] p528224 {3863069304.py:47} INFO - rules: 1. Your response should be a JSON containing two elements: \"verdict\" and \"explanation\". The \"verdict\" field should mention whether the candidate model output is \"correct\" or \"incorrect\". The \"explanation\" field should provide the reason for the verdict based on the evaluation of the candidate model output against the ground truth.\n",
      "\n",
      "2. If the ground truth is a comma-separated list with multiple options for responses, the verdict is correct if the candidate model output \n",
      "aligns or matches with any of the options in the comma separated list and answers the question in the similar way as the ground truth. \n",
      "It does not have to match all, but if it aligns or matches with one of the options, then the verdict is correct. If the ground truth contains a single option for a response, \n",
      "then only compare the candidate model response with that to check if the candidate model response is correct or not.\n",
      "\n",
      "3. The candidate model output does not have to use the exact wording of the ground truth but should correctly answer the question in a semantically equivalent manner. If there is more content in the candidate model response than required but it answers the question correctly and aligns or matches\n",
      "with the ground truth, the verdict is \"correct\". Otherwise, it is \"incorrect\".\n",
      "\n",
      "4. If the candidate model output does not match or align with the ground truth, or any option in the ground truth if the ground truth is \n",
      "a comma separated list of options, then the verdict is \"incorrect\".\n",
      "\n",
      "5. If the candidate model output has more context than required compared to the ground truth in answering the question, but still contains the relevant answer \n",
      "like the ground truth, then the verdict is correct.\n",
      "\n",
      "[2025-02-12 21:52:20,271] p528224 {3863069304.py:25} INFO - evaluation prompt template file path being used for cohere.command-r-plus-v1:0: /home/ubuntu/byojudge/foundation-model-benchmarking-tool/.fmbench_python311/lib/python3.12/site-packages/fmbench/prompt_template/eval_criteria/cohere_eval_prompt_templates/cohere_eval_majority_vote.txt\n",
      "[2025-02-12 21:52:20,271] p528224 {3863069304.py:28} INFO - evaluation prompt template file name: cohere_eval_majority_vote.txt\n",
      "[2025-02-12 21:52:20,273] p528224 {3863069304.py:32} INFO - Evaluation prompt template being used: Your role is to evaluate the correctness of the candidate model output provided in the candidate model output section\n",
      "based on whether it aligns with the ground truth answer provided in the ground truth section in answering the \n",
      "question in the question section.\n",
      "\n",
      "Refer to the question that you have to use while evaluating the correctness of the candidate model response in alignment to the ground truth:\n",
      "Question: {question}\n",
      "\n",
      "Refer to the candidate model response to be evaluated below:\n",
      "candidate model response: {answer}\n",
      "\n",
      "\n",
      "Refer to the ground truth to the question in the context below in the ground_truth section:\n",
      "ground_truth: {ground_truth}\n",
      "\n",
      "Follow the instructions below while giving your evaluation in the evaluation_instructions section below:\n",
      "\n",
      "## evaluation_instructions:\n",
      "{rules}\n",
      "\n",
      "## Response Instructions:\n",
      "\n",
      "Only give your response in a JSON format. Do not prefix your response with ``` json .....  ``` and so on. \n",
      "\n",
      "Do not add anything else but the format above in your evaluation response. Your response should just start with \n",
      "an opening JSON bracket, the fields, and then a closing JSON bracket, nothing else.\n",
      "\n",
      "[2025-02-12 21:52:20,274] p528224 {3863069304.py:47} INFO - rules: 1. Your response should be a JSON containing two elements: \"verdict\" and \"explanation\". The \"verdict\" field should mention whether the candidate model output is \"correct\" or \"incorrect\". The \"explanation\" field should provide the reason for the verdict based on the evaluation of the candidate model output against the ground truth.\n",
      "\n",
      "2. If the ground truth is a comma-separated list with multiple options for responses, the verdict is correct if the candidate model output \n",
      "aligns or matches with any of the options in the comma separated list and answers the question in the similar way as the ground truth. \n",
      "It does not have to match all, but if it aligns or matches with one of the options, then the verdict is correct. If the ground truth contains a single option for a response, \n",
      "then only compare the candidate model response with that to check if the candidate model response is correct or not.\n",
      "\n",
      "3. The candidate model output does not have to use the exact wording of the ground truth but should correctly answer the question in a semantically equivalent manner. If there is more content in the candidate model response than required but it answers the question correctly and aligns or matches\n",
      "with the ground truth, the verdict is \"correct\". Otherwise, it is \"incorrect\".\n",
      "\n",
      "4. If the candidate model output does not match or align with the ground truth, or any option in the ground truth if the ground truth is \n",
      "a comma separated list of options, then the verdict is \"incorrect\".\n",
      "\n",
      "5. If the candidate model output has more context than required compared to the ground truth in answering the question, but still contains the relevant answer \n",
      "like the ground truth, then the verdict is correct.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>base64_img</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>temperature</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>top_p</th>\n",
       "      <th>candidate_model_response</th>\n",
       "      <th>...</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>instance_count</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>Image</th>\n",
       "      <th>S3Uri</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>us.meta.llama3-3-70b-instruct-v1:0_majority_vote_eval_prompt</th>\n",
       "      <th>us.anthropic.claude-3-5-sonnet-20241022-v2:0_majority_vote_eval_prompt</th>\n",
       "      <th>cohere.command-r-plus-v1:0_majority_vote_eval_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>What family are the genus' Sinofranchetia and ...</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>Human: Your role is to evaluate the correctnes...</td>\n",
       "      <td>Your role is to evaluate the correctness of th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            endpoint_name  \\\n",
       "4  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "                                              prompt  \\\n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
       "\n",
       "                                            question  \\\n",
       "4  What family are the genus' Sinofranchetia and ...   \n",
       "\n",
       "                                        ground_truth  base64_img  \\\n",
       "4  a genus of flowering plant in the Lardizabalac...         NaN   \n",
       "\n",
       "             payload_file  temperature  max_tokens  top_p  \\\n",
       "4  payload_en_1-500.jsonl          0.1         100   0.92   \n",
       "\n",
       "                            candidate_model_response  ...  \\\n",
       "4  The genus Sinofranchetia and Stauntonia are bo...  ...   \n",
       "\n",
       "                            instance_type  instance_count  EndpointName  \\\n",
       "4  anthropic.claude-3-haiku-20240307-v1:0             NaN           NaN   \n",
       "\n",
       "   ModelName  Image  S3Uri cosine_similarity_score  \\\n",
       "4        NaN    NaN    NaN                0.660119   \n",
       "\n",
       "  us.meta.llama3-3-70b-instruct-v1:0_majority_vote_eval_prompt  \\\n",
       "4  <|begin_of_text|><|start_header_id|>user<|end_...             \n",
       "\n",
       "   us.anthropic.claude-3-5-sonnet-20241022-v2:0_majority_vote_eval_prompt  \\\n",
       "4  Human: Your role is to evaluate the correctnes...                        \n",
       "\n",
       "  cohere.command-r-plus-v1:0_majority_vote_eval_prompt  \n",
       "4  Your role is to evaluate the correctness of th...    \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "model_eval_dir: Optional[str] = eval_config[\"model_evaluations\"][\"model_eval_dir\"]\n",
    "eval_prompts_dir: str = Path(\n",
    "    pkg_resources.files(\"fmbench\"),\n",
    "    f\"{config['s3_read_data']['prompt_template_dir']}/{model_eval_dir.get('eval_prompts_dir', None)}\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Iterate through each LLM as a judge and each evaluation criterion\n",
    "    for llm_info in model_eval_subjective_info.get(\"judge_panel_list\", []):\n",
    "        model_id: str = llm_info[\"model_id\"]\n",
    "        method_name: str = eval_config[\"model_evaluations\"][\n",
    "            \"PoLL_Composition_and_Voting\"\n",
    "        ].get(\"method\", None)\n",
    "        eval_prompt_template_fname: str = (\n",
    "            f\"{llm_info.get('eval_prompt_template_name', None)}.txt\"\n",
    "        )\n",
    "\n",
    "        # Use the evaluation prompt template path to read in the standard prompt template that\n",
    "        # is used in the creation of prompt payloads\n",
    "        eval_prompt_template_dir = llm_info.get(\"eval_prompt_template_dir\", None)\n",
    "        eval_prompt_template_path = os.path.join(\n",
    "            eval_prompts_dir, eval_prompt_template_dir, eval_prompt_template_fname\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"evaluation prompt template file path being used for {model_id}: {eval_prompt_template_path}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"evaluation prompt template file name: {eval_prompt_template_fname}\"\n",
    "        )\n",
    "        eval_prompt_template = Path(eval_prompt_template_path).read_text()\n",
    "        logger.info(f\"Evaluation prompt template being used: {eval_prompt_template}\")\n",
    "\n",
    "        # There is a standard instructions file for both Majority voting on how to evaluate the\n",
    "        # model responses (whether it should be a binary decision or rating on a scale of 1-5)\n",
    "        eval_instructions_fname = next(\n",
    "            (\n",
    "                rule\n",
    "                for rule in model_eval_dir.get(\"eval_instructions_files\", None)\n",
    "                if method_name in rule\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        rules = Path(\n",
    "            os.path.join(eval_prompts_dir, eval_instructions_fname)\n",
    "        ).read_text()\n",
    "        logger.info(f\"rules: {rules}\")\n",
    "        column_name = f\"{model_id}_{method_name}_eval_prompt\"\n",
    "        df_per_inference[column_name] = df_per_inference.apply(\n",
    "            lambda r: prepare_eval_prompts(\n",
    "                eval_prompt_template,\n",
    "                r[\"candidate_model_response\"],\n",
    "                rules,\n",
    "                r[\"ground_truth\"],\n",
    "                r[\"question\"],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error occurred in the creation of prompt payloads: {e}\")\n",
    "    df_per_inference = None\n",
    "\n",
    "df_per_inference.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:52:21,751] p528224 {961315896.py:16} INFO - Per inference cosine similarity scores saved to s3://sagemaker-fmbench-write-us-west-2-218208277580/latest-FMs-fmbench-bedrock-adminaccessfortesting/data/metrics/yyyy=2025/mm=02/dd=12/hh=21/mm=48/processed_eval_prompts_for_inference.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>base64_img</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>temperature</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>top_p</th>\n",
       "      <th>candidate_model_response</th>\n",
       "      <th>...</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>instance_count</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>Image</th>\n",
       "      <th>S3Uri</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>us.meta.llama3-3-70b-instruct-v1:0_majority_vote_eval_prompt</th>\n",
       "      <th>us.anthropic.claude-3-5-sonnet-20241022-v2:0_majority_vote_eval_prompt</th>\n",
       "      <th>cohere.command-r-plus-v1:0_majority_vote_eval_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>What family are the genus' Sinofranchetia and ...</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.92</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_...</td>\n",
       "      <td>Human: Your role is to evaluate the correctnes...</td>\n",
       "      <td>Your role is to evaluate the correctness of th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            endpoint_name  \\\n",
       "4  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "                                              prompt  \\\n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...   \n",
       "\n",
       "                                            question  \\\n",
       "4  What family are the genus' Sinofranchetia and ...   \n",
       "\n",
       "                                        ground_truth  base64_img  \\\n",
       "4  a genus of flowering plant in the Lardizabalac...         NaN   \n",
       "\n",
       "             payload_file  temperature  max_tokens  top_p  \\\n",
       "4  payload_en_1-500.jsonl          0.1         100   0.92   \n",
       "\n",
       "                            candidate_model_response  ...  \\\n",
       "4  The genus Sinofranchetia and Stauntonia are bo...  ...   \n",
       "\n",
       "                            instance_type  instance_count  EndpointName  \\\n",
       "4  anthropic.claude-3-haiku-20240307-v1:0             NaN           NaN   \n",
       "\n",
       "   ModelName  Image  S3Uri cosine_similarity_score  \\\n",
       "4        NaN    NaN    NaN                0.660119   \n",
       "\n",
       "  us.meta.llama3-3-70b-instruct-v1:0_majority_vote_eval_prompt  \\\n",
       "4  <|begin_of_text|><|start_header_id|>user<|end_...             \n",
       "\n",
       "   us.anthropic.claude-3-5-sonnet-20241022-v2:0_majority_vote_eval_prompt  \\\n",
       "4  Human: Your role is to evaluate the correctnes...                        \n",
       "\n",
       "  cohere.command-r-plus-v1:0_majority_vote_eval_prompt  \n",
       "4  Your role is to evaluate the correctness of th...    \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_eval_prompt_payloads = csv_buffer.getvalue()\n",
    "eval_prompt_payloads_for_inference = os.path.join(\n",
    "    METRICS_DIR, PROCESSED_EVAL_PROMPT_PAYLOADS\n",
    ")  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(\n",
    "    df_per_inference_with_eval_prompt_payloads,\n",
    "    BUCKET_NAME,\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    PROCESSED_EVAL_PROMPT_PAYLOADS,\n",
    ")\n",
    "logger.info(\n",
    "    f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{eval_prompt_payloads_for_inference}\"\n",
    ")\n",
    "df_per_inference.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 29)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_per_inference.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:52:21,985] p528224 {3623529742.py:3} INFO - Total number evaluations to be done: 1\n"
     ]
    }
   ],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "eval_records_list = json.loads(df_per_inference.to_json(orient=\"records\"))\n",
    "logger.info(f\"Total number evaluations to be done: {len(eval_records_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the hierarchy of Model Evaluations\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the step, FMBench performs the following actions:\n",
    "\n",
    "1. For `Majority Voting` - We suppose that a ground truth already exists in the dataset. We first calculate quantitative metrics.\n",
    "\n",
    "1. We use the LLM panel of judges (in this case 3 judges), to give a verdict on whether the `answer` from the candidate models during inference is `correct` or `incorrect`. The panel of LLM judges also gives an explanation as to why it evaluated a candidate model response as correct or incorrect.\n",
    "\n",
    "1. Each model response is given in a JSON structure which is further used for downstream analytics, to decide the comparision of evaluation results between different model candidates and more.\n",
    "\n",
    "1. The evaluations are sent through a final layer to decide if an evaluation made using an LLM evaluator is made correctly/incorrectly.\n",
    "\n",
    "**_This step takes a couple of minutes to complete based on the size of the dataset and the judge models. Model completion time depends on the PoLL models being used. `Llama3-70b`, `Cohere command-r-v1` and `claude 3 Sonnet` were used for this example_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:52:22,632] p528224 {585958162.py:3} INFO - The judge panel list contains 3 judges. Their information: [{'model_id': 'us.meta.llama3-3-70b-instruct-v1:0', 'instance_type': 'us.meta.llama3-3-70b-instruct-v1:0', 'instance_count': None, 'eval_prompt_template_dir': 'llama3_eval_prompt_templates', 'eval_prompt_template_name': 'llama3_eval_majority_vote'}, {'model_id': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0', 'instance_type': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0', 'instance_count': None, 'eval_prompt_template_dir': 'claude_eval_prompt_templates', 'eval_prompt_template_name': 'claude_eval_majority_vote'}, {'model_id': 'cohere.command-r-plus-v1:0', 'instance_type': 'cohere.command-r-plus-v1:0', 'instance_count': None, 'eval_prompt_template_dir': 'cohere_eval_prompt_templates', 'eval_prompt_template_name': 'cohere_eval_majority_vote'}]\n"
     ]
    }
   ],
   "source": [
    "# get the llm as a judge panel list\n",
    "judge_panel_list: List[Dict] = model_eval_subjective_info.get(\"judge_panel_list\", None)\n",
    "logger.info(\n",
    "    f\"The judge panel list contains {len(judge_panel_list)} judges. Their information: {judge_panel_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:52:22,917] p528224 {1632128982.py:1} INFO - ~Panel of LLM evaluators are going to start evaluating responses. This might take a couple of minutes depending on the size of the dataset and candidate model responses~\n"
     ]
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"~Panel of LLM evaluators are going to start evaluating responses. This might take a couple of minutes depending on the size of the dataset and candidate model responses~\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:52:23,140] p528224 {3912179960.py:4} INFO - Are quantitative metrics going to be used to make a final eval decision: True\n"
     ]
    }
   ],
   "source": [
    "is_quantitative_eval_enabled: bool = eval_config[\"model_evaluations\"][\n",
    "    \"PoLL_Composition_and_Voting\"\n",
    "].get(\"use_quantitative_metrics\", False)\n",
    "logger.info(\n",
    "    f\"Are quantitative metrics going to be used to make a final eval decision: {is_quantitative_eval_enabled}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the evaluation process\n",
    "\n",
    "---\n",
    "\n",
    "This process loops through the evaluation prompt payloads that are prepared. For Majority voting, a JSON containing 2 elements is generated: \"verdict\" of whether the given answer is correct or incorrect and an \"explanation\".\n",
    "\n",
    "Responses from either evaluation processes are sent for further downstream processes to determine the most accurate\n",
    "and subjectively correct model based on domain specific use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 21:52:24,013] p528224 {4165010176.py:12} INFO - ============Running inference for judge panelist us.meta.llama3-3-70b-instruct-v1:0 for majority_vote ============\n",
      "[2025-02-12 21:52:24,015] p528224 {4165010176.py:17} INFO - Getting inference for list 1/1, size of list=1\n",
      "[2025-02-12 21:52:26,514] p528224 {4165010176.py:42} INFO - ~Sleeping for one second before the next Panel of LLM evaluates the responses~\n",
      "[2025-02-12 21:52:27,515] p528224 {4165010176.py:12} INFO - ============Running inference for judge panelist us.anthropic.claude-3-5-sonnet-20241022-v2:0 for majority_vote ============\n",
      "[2025-02-12 21:52:27,516] p528224 {4165010176.py:17} INFO - Getting inference for list 1/1, size of list=1\n",
      "[2025-02-12 21:52:27,610] p528224 {4165010176.py:42} INFO - ~Sleeping for one second before the next Panel of LLM evaluates the responses~\n",
      "[2025-02-12 21:52:28,611] p528224 {4165010176.py:12} INFO - ============Running inference for judge panelist cohere.command-r-plus-v1:0 for majority_vote ============\n",
      "[2025-02-12 21:52:28,612] p528224 {4165010176.py:17} INFO - Getting inference for list 1/1, size of list=1\n",
      "[2025-02-12 21:52:30,208] p528224 {4165010176.py:42} INFO - ~Sleeping for one second before the next Panel of LLM evaluates the responses~\n",
      "[2025-02-12 21:52:31,209] p528224 {4165010176.py:48} INFO - Total elapsed time for inference: 7.20 seconds\n",
      "[2025-02-12 21:52:31,210] p528224 {4165010176.py:49} INFO - Total erroneous lists: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m async_run_eval, i=1, total=1, judge_model_info=us.meta.llama3-3-70b-instruct-v1:0, eval_method: majority_vote, uuid: 16a48fa517194d32802a561b0d508e8c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:httpx:HTTP Request: POST https://bedrock-runtime.us-west-2.amazonaws.com/model/us.meta.llama3-3-70b-instruct-v1%3A0/converse \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \u001b[92m21:52:26 - LiteLLM:INFO\u001b[0m: utils.py:1120 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:fmbench.scripts.evaluation_scripts.bedrock_llm_evaluators:Response json: {'generated_text': '{\"verdict\": \"correct\", \"explanation\": \"The candidate model output states that the genus Sinofranchetia and Stauntonia are from the family Lardizabalaceae, which aligns with the ground truth that mentions the Lardizabalaceae family.\"}'}\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m ERROR:fmbench.utils:Error encountered while running evaluation: 'total_cost'\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:fmbench.utils:async_run_eval, i=1, total=1, judge_model_info=us.anthropic.claude-3-5-sonnet-20241022-v2:0, eval_method: majority_vote, uuid: 16a48fa517194d32802a561b0d508e8c\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:fmbench.utils:run_eval, row 1/1, judge_model_id=us.anthropic.claude-3-5-sonnet-20241022-v2:0, candidate model=anthropic.claude-3-haiku-20240307-v1:0\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:fmbench.scripts.evaluation_scripts.bedrock_llm_evaluators:Getting evaluation for prompt: Human: Your role is to evaluate the correctness of the candidate model output provided in the <candidate model output></candidate model output> \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m tags based on whether it aligns with the ground truth answer provided in the <ground_truth></ground_truth> xml tags in answering the \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m question in the <question></question> xml tags.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Refer to the question that you have to use while evaluating the correctness of the candidate model response in alignment to the ground truth:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m <question>\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m What family are the genus' Sinofranchetia and Stauntonia from?\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m </question> \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Refer to the candidate model response to be evaluated in the <candidate model output></candidate model output> tags below:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m <candidate model output>\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m The genus Sinofranchetia and Stauntonia are both from the family Lardizabalaceae.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m </candidate model output> \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Refer to the ground truth below in the <ground_truth></ground_truth> xml tags while evaluating the candidate model output:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m <ground_truth>\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m a genus of flowering plant in the Lardizabalaceae family\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m </ground_truth> \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Follow the instructions below while giving your evaluation of the candidate model output in the <evaluation_instructions></evaluation_instructions>\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m tags:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m <evaluation_instructions>\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m 1. Your response should be a JSON containing two elements: \"verdict\" and \"explanation\". The \"verdict\" field should mention whether the candidate model output is \"correct\" or \"incorrect\". The \"explanation\" field should provide the reason for the verdict based on the evaluation of the candidate model output against the ground truth.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m 2. If the ground truth is a comma-separated list with multiple options for responses, the verdict is correct if the candidate model output \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m aligns or matches with any of the options in the comma separated list and answers the question in the similar way as the ground truth. \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m It does not have to match all, but if it aligns or matches with one of the options, then the verdict is correct. If the ground truth contains a single option for a response, \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m then only compare the candidate model response with that to check if the candidate model response is correct or not.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m 3. The candidate model output does not have to use the exact wording of the ground truth but should correctly answer the question in a semantically equivalent manner. If there is more content in the candidate model response than required but it answers the question correctly and aligns or matches\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m with the ground truth, the verdict is \"correct\". Otherwise, it is \"incorrect\".\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m 4. If the candidate model output does not match or align with the ground truth, or any option in the ground truth if the ground truth is \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m a comma separated list of options, then the verdict is \"incorrect\".\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m 5. If the candidate model output has more context than required compared to the ground truth in answering the question, but still contains the relevant answer \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m like the ground truth, then the verdict is correct.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m </evaluation_instructions>\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Your response should only be in JSON format. Your response should NOT have any tags, and should start with the starting bracket of the JSON\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m structure and end with the ending bracket. There should only be the JSON in your response without any other words outside of it, should not\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m contain any tags, only the JSON structure.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Assistant: Sure, here is my evaluation in JSON:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:fmbench.scripts.evaluation_scripts.bedrock_llm_evaluators:Invoking bedrock/anthropic.claude-3-haiku-20240307-v1:0 to get inference\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \u001b[92m21:52:27 - LiteLLM:INFO\u001b[0m: utils.py:2944 - \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m LiteLLM completion() model= anthropic.claude-3-haiku-20240307-v1:0; provider = bedrock\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:LiteLLM:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m LiteLLM completion() model= anthropic.claude-3-haiku-20240307-v1:0; provider = bedrock\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:httpx:HTTP Request: POST https://bedrock-runtime.us-west-2.amazonaws.com/model/us.anthropic.claude-3-5-sonnet-20241022-v2%3A0/converse \"HTTP/1.1 503 Service Unavailable\"\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m ERROR:fmbench.scripts.evaluation_scripts.bedrock_llm_evaluators:Unexpected error during prediction, endpoint_name=anthropic.claude-3-haiku-20240307-v1:0, exception=litellm.ServiceUnavailableError: BedrockException - {\"message\":\"Bedrock is unable to process your request.\"}\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m ERROR:fmbench.utils:Error encountered while running evaluation: litellm.ServiceUnavailableError: BedrockException - {\"message\":\"Bedrock is unable to process your request.\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m async_run_eval, i=1, total=1, judge_model_info=us.anthropic.claude-3-5-sonnet-20241022-v2:0, eval_method: majority_vote, uuid: 16a48fa517194d32802a561b0d508e8c\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:fmbench.utils:async_run_eval, i=1, total=1, judge_model_info=cohere.command-r-plus-v1:0, eval_method: majority_vote, uuid: 16a48fa517194d32802a561b0d508e8c\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:fmbench.utils:run_eval, row 1/1, judge_model_id=cohere.command-r-plus-v1:0, candidate model=anthropic.claude-3-haiku-20240307-v1:0\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:fmbench.scripts.evaluation_scripts.bedrock_llm_evaluators:Getting evaluation for prompt: Your role is to evaluate the correctness of the candidate model output provided in the candidate model output section\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m based on whether it aligns with the ground truth answer provided in the ground truth section in answering the \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m question in the question section.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Refer to the question that you have to use while evaluating the correctness of the candidate model response in alignment to the ground truth:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Question: What family are the genus' Sinofranchetia and Stauntonia from?\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Refer to the candidate model response to be evaluated below:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m candidate model response: The genus Sinofranchetia and Stauntonia are both from the family Lardizabalaceae.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Refer to the ground truth to the question in the context below in the ground_truth section:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m ground_truth: a genus of flowering plant in the Lardizabalaceae family\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Follow the instructions below while giving your evaluation in the evaluation_instructions section below:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m ## evaluation_instructions:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m 1. Your response should be a JSON containing two elements: \"verdict\" and \"explanation\". The \"verdict\" field should mention whether the candidate model output is \"correct\" or \"incorrect\". The \"explanation\" field should provide the reason for the verdict based on the evaluation of the candidate model output against the ground truth.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m 2. If the ground truth is a comma-separated list with multiple options for responses, the verdict is correct if the candidate model output \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m aligns or matches with any of the options in the comma separated list and answers the question in the similar way as the ground truth. \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m It does not have to match all, but if it aligns or matches with one of the options, then the verdict is correct. If the ground truth contains a single option for a response, \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m then only compare the candidate model response with that to check if the candidate model response is correct or not.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m 3. The candidate model output does not have to use the exact wording of the ground truth but should correctly answer the question in a semantically equivalent manner. If there is more content in the candidate model response than required but it answers the question correctly and aligns or matches\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m with the ground truth, the verdict is \"correct\". Otherwise, it is \"incorrect\".\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m 4. If the candidate model output does not match or align with the ground truth, or any option in the ground truth if the ground truth is \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m a comma separated list of options, then the verdict is \"incorrect\".\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m 5. If the candidate model output has more context than required compared to the ground truth in answering the question, but still contains the relevant answer \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m like the ground truth, then the verdict is correct.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m ## Response Instructions:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Only give your response in a JSON format. Do not prefix your response with ``` json .....  ``` and so on. \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m Do not add anything else but the format above in your evaluation response. Your response should just start with \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m an opening JSON bracket, the fields, and then a closing JSON bracket, nothing else.\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:fmbench.scripts.evaluation_scripts.bedrock_llm_evaluators:Invoking bedrock/anthropic.claude-3-haiku-20240307-v1:0 to get inference\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \u001b[92m21:52:28 - LiteLLM:INFO\u001b[0m: utils.py:2944 - \n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m LiteLLM completion() model= anthropic.claude-3-haiku-20240307-v1:0; provider = bedrock\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:LiteLLM:\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m LiteLLM completion() model= anthropic.claude-3-haiku-20240307-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m async_run_eval, i=1, total=1, judge_model_info=cohere.command-r-plus-v1:0, eval_method: majority_vote, uuid: 16a48fa517194d32802a561b0d508e8c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:httpx:HTTP Request: POST https://bedrock-runtime.us-west-2.amazonaws.com/model/cohere.command-r-plus-v1%3A0/converse \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m \u001b[92m21:52:30 - LiteLLM:INFO\u001b[0m: utils.py:1120 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m INFO:fmbench.scripts.evaluation_scripts.bedrock_llm_evaluators:Response json: {'generated_text': '{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"The candidate model output is correct as it states that the genus\\' Sinofranchetia and Stauntonia are from the family Lardizabalaceae, which aligns with the ground truth answer.\"\\n}'}\n",
      "\u001b[36m(async_run_eval pid=528450)\u001b[0m ERROR:fmbench.utils:Error encountered while running evaluation: 'total_cost'\n"
     ]
    }
   ],
   "source": [
    "n: int = model_eval_subjective_info.get(\"run_parallel_inference_count\", 5)\n",
    "list_of_lists = [\n",
    "    eval_records_list[i * n : (i + 1) * n]\n",
    "    for i in range((len(eval_records_list) + n - 1) // n)\n",
    "]\n",
    "resp_list = []\n",
    "erroneous_count: int = 0\n",
    "st: float = time.perf_counter()\n",
    "\n",
    "# Iterate over the judge panel and sublists\n",
    "for judge_panelist_info in judge_panel_list:\n",
    "    logger.info(\n",
    "        f\"============Running inference for judge panelist {judge_panelist_info['model_id']} for {method_name} ============\"\n",
    "    )\n",
    "    for idx, sublist in enumerate(list_of_lists):\n",
    "        model_id: str = judge_panelist_info[\"model_id\"]\n",
    "        logger.info(\n",
    "            f\"Getting inference for list {idx + 1}/{len(list_of_lists)}, size of list={len(sublist)}\"\n",
    "        )\n",
    "        try:\n",
    "            resp_list.extend(\n",
    "                ray.get(\n",
    "                    [\n",
    "                        async_run_eval.remote(\n",
    "                            i + 1,\n",
    "                            len(sublist),\n",
    "                            record,\n",
    "                            model_id,\n",
    "                            judge_panelist_info['instance_type'], \n",
    "                            judge_panelist_info['instance_count'],\n",
    "                            method_name,\n",
    "                            record[\"uuid\"],\n",
    "                        )\n",
    "                        for i, record in enumerate(sublist)\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing list {idx + 1}/{len(list_of_lists)}: {e}\")\n",
    "            erroneous_count += 1\n",
    "    # Sleep for two seconds before moving on to the next model\n",
    "    logger.info(\n",
    "        f\"~Sleeping for one second before the next Panel of LLM evaluates the responses~\"\n",
    "    )\n",
    "    time.sleep(1)\n",
    "\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"Total elapsed time for inference: {elapsed_time:.2f} seconds\")\n",
    "logger.info(f\"Total erroneous lists: {erroneous_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'exception': \"'total_cost'\",\n",
       "  'completion': None,\n",
       "  'completion_tokens': None,\n",
       "  'prompt_tokens': None,\n",
       "  'input_token_cost': None,\n",
       "  'output_token_cost': None,\n",
       "  'total_cost': None,\n",
       "  'model_id': 'us.meta.llama3-3-70b-instruct-v1:0',\n",
       "  'latency': None,\n",
       "  'candidate_model_response': 'The genus Sinofranchetia and Stauntonia are both from the family Lardizabalaceae.',\n",
       "  'candidate_model': None,\n",
       "  'payload_file': 'payload_en_1-500.jsonl',\n",
       "  'cosine_similarity_score': 0.6601186395,\n",
       "  'ground_truth': 'a genus of flowering plant in the Lardizabalaceae family',\n",
       "  'question': \"What family are the genus' Sinofranchetia and Stauntonia from?\"},\n",
       " {'exception': 'litellm.ServiceUnavailableError: BedrockException - {\"message\":\"Bedrock is unable to process your request.\"}',\n",
       "  'completion': None,\n",
       "  'completion_tokens': None,\n",
       "  'prompt_tokens': None,\n",
       "  'input_token_cost': None,\n",
       "  'output_token_cost': None,\n",
       "  'total_cost': None,\n",
       "  'model_id': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
       "  'latency': None,\n",
       "  'candidate_model_response': 'The genus Sinofranchetia and Stauntonia are both from the family Lardizabalaceae.',\n",
       "  'candidate_model': None,\n",
       "  'payload_file': 'payload_en_1-500.jsonl',\n",
       "  'cosine_similarity_score': 0.6601186395,\n",
       "  'ground_truth': 'a genus of flowering plant in the Lardizabalaceae family',\n",
       "  'question': \"What family are the genus' Sinofranchetia and Stauntonia from?\"},\n",
       " {'exception': \"'total_cost'\",\n",
       "  'completion': None,\n",
       "  'completion_tokens': None,\n",
       "  'prompt_tokens': None,\n",
       "  'input_token_cost': None,\n",
       "  'output_token_cost': None,\n",
       "  'total_cost': None,\n",
       "  'model_id': 'cohere.command-r-plus-v1:0',\n",
       "  'latency': None,\n",
       "  'candidate_model_response': 'The genus Sinofranchetia and Stauntonia are both from the family Lardizabalaceae.',\n",
       "  'candidate_model': None,\n",
       "  'payload_file': 'payload_en_1-500.jsonl',\n",
       "  'cosine_similarity_score': 0.6601186395,\n",
       "  'ground_truth': 'a genus of flowering plant in the Lardizabalaceae family',\n",
       "  'question': \"What family are the genus' Sinofranchetia and Stauntonia from?\"}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send all Panel of LLM evaluator responses to S3 as `JSON` files\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect all of the panel of LLM evals and send them all as JSON files to S3\n",
    "if resp_list:\n",
    "    save_s3_list = []\n",
    "    try:\n",
    "        for resp in resp_list:\n",
    "            if resp:\n",
    "                llm_eval_response = json.dumps(resp, indent=2)\n",
    "                candidate_model_id = resp.get(\"candidate_model\", None)\n",
    "                if candidate_model_id:  # Ensure candidate_model_id is not None\n",
    "                    # Extract a few words from the poll eval response to append to the file name\n",
    "                    response_excerpt = \" \".join(\n",
    "                        resp.get(\"candidate_model_response\", \"\").split()[:5]\n",
    "                    )\n",
    "                    sanitized_response_excerpt = \"\".join(\n",
    "                        [c if c.isalnum() else \"_\" for c in response_excerpt]\n",
    "                    )\n",
    "                    llm_eval_json_fname = f\"{candidate_model_id}_{time.time()}_{sanitized_response_excerpt}.json\"\n",
    "                    response_s3_path = os.path.join(\n",
    "                        METRICS_PER_POLL_EVAL_DIR, llm_eval_json_fname\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"Sending model eval result files to s3 path prefix: {response_s3_path}\"\n",
    "                    )\n",
    "                    save_s3_list.append(\n",
    "                        (\n",
    "                            llm_eval_response,\n",
    "                            config[\"aws\"][\"bucket\"],\n",
    "                            \"\",\n",
    "                            METRICS_PER_POLL_EVAL_DIR,\n",
    "                            llm_eval_json_fname,\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"candidate_model_id is None, skipping this response.\"\n",
    "                    )\n",
    "            else:\n",
    "                logger.warning(\"Response is None, skipping this entry.\")\n",
    "        if save_s3_list:\n",
    "            # Split the save_s3_list into smaller batches to get\n",
    "            # rid of the cannot write to s3 bucket - request rate was hitting maximum threshold\n",
    "            batch_size: int = 50\n",
    "            delay: float = 1\n",
    "            for i in range(0, len(save_s3_list), batch_size):\n",
    "                batch = save_s3_list[i : i + batch_size]\n",
    "                # write a batch of evaluation result files to s3\n",
    "                write_multiple_to_s3(batch)\n",
    "                time.sleep(delay)  # Delay between batches\n",
    "        else:\n",
    "            logger.error(\"No valid responses to write to S3.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing or writing to S3: {e}\")\n",
    "else:\n",
    "    logger.info(\"No responses to write to S3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save All Results: Perform downstream analytical tasks on each PoLL evaluation result\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the evaluation step:\n",
    "\n",
    "1. We compile all metrics gathered from the Majority Voting experiment, and send them as `CSV`, `txt` files to s3.\n",
    "\n",
    "1. These metrics include: Quantitative metrics and binary decision scores (for Majority Voting).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the results list into a dataframe for easy analytics\n",
    "df_eval_results = pd.DataFrame(resp_list)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how=\"all\")\n",
    "# the exception, judge model id, prompt token count, will be NaN for the verdicts decided\n",
    "# using the lexical match and not moved forward to the panel of LLM evaluators\n",
    "df_eval_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parse out the completion from LLM as a judge and column bind\n",
    "# the fields of the dictionary to the original results dataframe\n",
    "df_eval_results_only = (\n",
    "    df_eval_results[\"completion\"].apply(parse_as_json).apply(pd.Series)\n",
    ")\n",
    "df_eval_results_only.dropna(axis=1, how=\"all\")\n",
    "df_eval_results = pd.concat([df_eval_results, df_eval_results_only], axis=1)\n",
    "df_eval_results.rename(columns={\"model_id\": \"judge_model_id\"}, inplace=True)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how=\"all\")\n",
    "df_eval_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new column and assign the original verdict to this column\n",
    "df_eval_results[\"original_verdict\"] = df_eval_results[\"verdict\"]\n",
    "df_eval_results.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the correctness of LLM Evaluators using quantitative metrics\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the evaluation step, we perform the following steps:\n",
    "\n",
    "1. Evaluate whether the LLM evaluators sent in the correct evaluations using another layer of checks with _Cosine Similarity Score_.\n",
    "\n",
    "1. If the verdicts decided by the LLM evaluators (`correct` or `incorrect`) do not meet the respective cosine similarity thresholds, then they are sent into another file for further analysis for human or another LLM evaluation loop.\n",
    "\n",
    "There are two possible cases for this evaluation:\n",
    "\n",
    "1. **Incorrect Verdicts**: If the verdict from the judge model is incorrect, then check if the cosine similarity of that\n",
    "   incorrectly identified verdict is less than the `incorrect_verdict_cosine_similarity_threshold`. If so, then it is\n",
    "   finally sent in as is into the dataframe. If the LLM evaluator defines a verdict as incorrect but if it has a higher cosine\n",
    "   similarity than the incorrect cosine similarity threshold, then it is marked for \"needing further evaluation using a human\" or\n",
    "   another LLM evalution.\n",
    "\n",
    "2. **Correct Verdicts**: If the verdict from the judge model is correct and if it exceeds the correctness cosine similarity threshold,\n",
    "   then the model is evaluated as correct and sent in for further downstream analytics. For the correct verdicts identified by the judge models\n",
    "   that do not meet the correctness cosine similarity threshold, are defined as \"needed further human/LLM evaluation\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantitative_verdict_cosine_similarity_decision(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Given an LLM evaluator response, this function checks for whether a verdict provided by an LLM evaluator\n",
    "    is correctly evaluated using a cosine similarity metric threshold for correct and incorrect verdicts. These\n",
    "    are the two cases that this function handles for each evaluation done using LLM as evaluators:\n",
    "\n",
    "    1. Incorrect Verdicts: If the verdict from the judge model is incorrect, then check if the cosine similarity of that\n",
    "    incorrectly identified verdict is less than the `incorrect_verdict_cosine_similarity_threshold`. If so, then it is\n",
    "    finally sent in as is into the dataframe. If the LLM evaluator defines a verdict as incorrect but if it has a higher cosine\n",
    "    similarity than the incorrect cosine similarity threshold, then it is marked for \"needing further evaluation using a human\" or\n",
    "    another LLM evalution.\n",
    "\n",
    "    2. Correct Verdicts: If the verdict from the judge model is correct and if it exceeds the correctness cosine similarity threshold,\n",
    "    then the model is evaluated as correct and sent in for further downstream analytics. For the correct verdicts identified by the judge models\n",
    "    that do not meet the correctness cosine similarity threshold, are defined as \"needed further human/LLM evaluation\".\n",
    "\n",
    "    This function is used if the evaluation method being used is Majority voting, specifically in the case\n",
    "    of when ground truth is provided.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This is a boolean value that is returned defining whether a given verdict is valid based on\n",
    "        # the comparison of its respective cosine similarity score and cosine similarity threshold for correctness/incorrectness\n",
    "        is_eval_done_correctly: Optional[bool] = None\n",
    "        correct_cosine_similarity_threshold: Optional[float] = None\n",
    "        incorrect_cosine_similarity_threshold: Optional[float] = None\n",
    "\n",
    "        # Check if the evaluation method is Majority voting and if the customer has enabled\n",
    "        # evaluation decisions to also be made by quantitative metric thresholds\n",
    "        if is_quantitative_eval_enabled:\n",
    "            # Retrieve the information that is going to be used to check for whether a verdict is\n",
    "            # incorrectly identified as correct or incorrect\n",
    "            judge_model_id: str = row[\"judge_model_id\"]\n",
    "            verdict: str = row[\"verdict\"]\n",
    "            explanation: str = row[\"explanation\"]\n",
    "            cosine_similarity_score: float = row[\"cosine_similarity_score\"]\n",
    "\n",
    "            # Get the correctness and incorrectness cosine similarity threshold scores\n",
    "            correct_cosine_similarity_threshold = eval_config[\"model_evaluations\"][\n",
    "                \"quantitative_eval_info\"\n",
    "            ].get(\"correct_verdict_cosine_similarity_threshold\", None)\n",
    "            incorrect_cosine_similarity_threshold = eval_config[\"model_evaluations\"][\n",
    "                \"quantitative_eval_info\"\n",
    "            ].get(\"incorrect_verdict_cosine_similarity_threshold\", None)\n",
    "\n",
    "            # If the verdict is correct and is greater than or equal to the correct cosine similarity threshold, then\n",
    "            # the verdict is correct. If not, the verdict is identified to need further evaluation\n",
    "\n",
    "            # include the original verdict here\n",
    "            if verdict == \"correct\":\n",
    "                if cosine_similarity_score >= correct_cosine_similarity_threshold:\n",
    "                    row[\"explanation\"] = (\n",
    "                        f\"{explanation} Cosine similarity is {cosine_similarity_score}, which does meets and is above the threshold of {correct_cosine_similarity_threshold}.\"\n",
    "                    )\n",
    "                    is_eval_done_correctly = True\n",
    "                else:\n",
    "                    row[\"verdict\"] = \"needs_further_human_or_LLM_evaluation\"\n",
    "                    row[\"explanation\"] = (\n",
    "                        f\"{explanation} Cosine similarity is {cosine_similarity_score}, which does not meet the threshold of {correct_cosine_similarity_threshold}. Evaluate it further to determine the correct answer.\"\n",
    "                    )\n",
    "                    is_eval_done_correctly = False\n",
    "\n",
    "            # If the verdict is incorrect and is less than or equal to the incorrect cosine similarity threshold, then\n",
    "            # the verdict is correctly identified as incorrect. If not, the verdict is identified to need further evaluation\n",
    "            elif verdict == \"incorrect\":\n",
    "                if cosine_similarity_score <= incorrect_cosine_similarity_threshold:\n",
    "                    row[\"explanation\"] = (\n",
    "                        f\"{explanation} Cosine similarity is {cosine_similarity_score}, which does is below the threshold of {incorrect_cosine_similarity_threshold}.\"\n",
    "                    )\n",
    "                    is_eval_done_correctly = True\n",
    "                else:\n",
    "                    row[\"verdict\"] = \"needs_further_human_or_LLM_evaluation\"\n",
    "                    # if the verdict needs further evaluation but was incorrect originally, then reset the verdict to incorrect\n",
    "                    if row[\"verdict\"] == \"needs_further_human_or_LLM_evaluation\":\n",
    "                        row[\"verdict\"] = \"incorrect\"\n",
    "                        row[\"explanation\"] = (\n",
    "                            f\"{explanation} Cosine Similarity of {cosine_similarity_score} >= {incorrect_cosine_similarity_threshold} incorrect cosine similarity threshold, does not meet threshold.\"\n",
    "                        )\n",
    "                        is_eval_done_correctly = True\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Error in quantitative_verdict_cosine_similarity_decision: {str(e)}\"\n",
    "        )\n",
    "        is_eval_done_correctly = None\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the layer of another evaluation filter on the dataframe containing all LLM as evaluator results\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if df_eval_results is not None:\n",
    "    df_eval_results = df_eval_results.apply(\n",
    "        lambda r: quantitative_verdict_cosine_similarity_decision(r), axis=1\n",
    "    )\n",
    "df_eval_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send the raw results as a csv file to the S3 bucket\n",
    "csv_buffer = io.StringIO()\n",
    "df_eval_results.to_csv(csv_buffer, index=False)\n",
    "eval_llm_as_a_judge_results = csv_buffer.getvalue()\n",
    "eval_results_csv_fpath = os.path.join(\n",
    "    METRICS_DIR, MODEL_EVAL_COMPLETIONS_CSV\n",
    ")  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(\n",
    "    eval_llm_as_a_judge_results,\n",
    "    BUCKET_NAME,\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    MODEL_EVAL_COMPLETIONS_CSV,\n",
    ")\n",
    "logger.info(\n",
    "    f\"Per PoLL model responses saved as a csv to s3://{BUCKET_NAME}/{eval_results_csv_fpath}\"\n",
    ")\n",
    "df_eval_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"Total number of evaluations that are done using different panel of LLM evaluators: {df_eval_results.shape[0]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate evaluation cost per LLM evaluator per candidate model\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the evaluation step, the evaluation cost is calculated. The cost for the input and output tokens processed per LLM evaluator for each evaluation for each candidate model is summed up to give a total cost for evaluating the dataset using each evaluator. The total cost is added up in the final model metrics step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_cost_df = (\n",
    "    df_eval_results.groupby(\"judge_model_id\")[\n",
    "        [\"total_cost\", \"prompt_token_count\", \"completion_token_count\"]\n",
    "    ]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "eval_cost_df = eval_cost_df.sort_values(\"total_cost\", ascending=False)\n",
    "eval_cost_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send the cost calculation for running each evaluator to s3. This CSV file contains the total cost (which is the\n",
    "# summation of the input and output tokens across all evaluations across all candidate models), the total prompt token counts\n",
    "# and the total completion token counts across the entire dataset\n",
    "try:\n",
    "    eval_cost_df = (\n",
    "        df_eval_results.groupby(\"judge_model_id\")[\n",
    "            [\"total_cost\", \"prompt_token_count\", \"completion_token_count\"]\n",
    "        ]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    eval_cost_df = eval_cost_df.sort_values(\"total_cost\", ascending=False)\n",
    "    eval_cost_df[\"total_cost\"] = round(eval_cost_df[\"total_cost\"], 4)\n",
    "    csv_buffer = io.StringIO()\n",
    "    eval_cost_df.to_csv(csv_buffer, index=False)\n",
    "    eval_cost_df_responses = csv_buffer.getvalue()\n",
    "    eval_cost_df_responses_fpath = os.path.join(METRICS_DIR, EVAL_COST_PER_JUDGE_MODEL)\n",
    "    write_to_s3(\n",
    "        eval_cost_df_responses, BUCKET_NAME, \"\", METRICS_DIR, EVAL_COST_PER_JUDGE_MODEL\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Cost calculations for running each LLM evaluator to evaluate candidate models is sent to s3://{BUCKET_NAME}/{eval_cost_df_responses_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not calculate the total cost for running each LLM evaluator to evaluate candidate models: {e}\"\n",
    "    )\n",
    "\n",
    "if eval_cost_df is not None:\n",
    "    eval_cost_df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Voting Results: Send the incorrect and correct responses to S3 separately in `CSV` files for downstream analytics for each model judge\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the step, we will send the model responses as CSV, txt files to s3 for further downstream processing and report generations\n",
    "\n",
    "1. We calculate the majority vote done using the verdicts from each panel of LLM judges\n",
    "\n",
    "1. Calculate the majority vote accuracy ranking for each candidate model, i.e., which candidate model ranked at the top using majority correct votes from panel of LLM evaluators and so on.\n",
    "\n",
    "1. Generate metrics on a final `candidate_model_accuracy` table containing insights into accuracy of a model per judge per candidate model as well as accuracy of that given model across all judges as per majority vote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For Majority Voting - all responses from the panel of LLM as evaluators are sent\n",
    "# to s3 as a csv file\n",
    "try:\n",
    "    logger.info(\n",
    "        f\"Method name is {method_name}, sending the correct and incorrect verdicts to s3\"\n",
    "    )\n",
    "    verdict_types: List[str] = [\n",
    "        \"incorrect\",\n",
    "        \"correct\",\n",
    "        \"needs_further_human_or_LLM_evaluation\",\n",
    "    ]\n",
    "    all_llm_eval_responses_df: Optional[pd.DataFrame] = None\n",
    "    # iterate through each of the verdict tupe and save each verdict type responses from each evaluator in different\n",
    "    # csv files. For example, a csv files containing only incorrect verdicts from all model judges, whereas another\n",
    "    # csv file containing only the correct verdicts.\n",
    "    for verdict in verdict_types:\n",
    "        df_verdicts = df_eval_results[df_eval_results[\"verdict\"] == verdict]\n",
    "        all_llm_eval_responses_df = pd.concat(\n",
    "            [all_llm_eval_responses_df, df_verdicts], ignore_index=True\n",
    "        )\n",
    "        if not df_verdicts.empty:\n",
    "            csv_buffer = io.StringIO()\n",
    "            df_verdicts.to_csv(csv_buffer, index=False)\n",
    "            verdict_responses = csv_buffer.getvalue()\n",
    "            verdict_file = (\n",
    "                INCORRECT_VERDICT_RESPONSES_FILE\n",
    "                if verdict == \"incorrect\"\n",
    "                else (\n",
    "                    CORRECT_VERDICT_RESPONSES_FILE\n",
    "                    if verdict == \"correct\"\n",
    "                    else NEEDS_FURTHER_EVAL_FILE\n",
    "                )\n",
    "            )\n",
    "            verdict_responses_fpath = os.path.join(METRICS_DIR, verdict_file)\n",
    "            write_to_s3(verdict_responses, BUCKET_NAME, \"\", METRICS_DIR, verdict_file)\n",
    "            logger.info(\n",
    "                f\"{verdict.capitalize()} verdict responses sent to s3://{BUCKET_NAME}/{verdict_responses_fpath}\"\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Number of {verdict} responses in total: {df_verdicts.shape[0]}\"\n",
    "            )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error encountered while writing the evaluation responses to s3: {e}\")\n",
    "    all_llm_eval_responses_df = None\n",
    "\n",
    "all_llm_eval_responses_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the number of unique judges\n",
    "num_judge_models: int = len(all_llm_eval_responses_df.judge_model_id.unique())\n",
    "logger.info(f\"there are {num_judge_models} LLM judge models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For Majority Voting - send all incorrect and correct verdicts as txt files to s3 for readability purposes\n",
    "try:\n",
    "    logger.info(\n",
    "        f\"Method name is {method_name}, sending the correct and incorrect verdicts to s3\"\n",
    "    )\n",
    "    verdict_types: List[str] = [\n",
    "        \"incorrect\",\n",
    "        \"correct\",\n",
    "        \"needs_further_human_or_LLM_evaluation\",\n",
    "    ]\n",
    "    judge_model_ids = df_eval_results[\"judge_model_id\"].unique()\n",
    "    # save each judge model's correct and incorrect verdict files as txt files\n",
    "    # for downstream analytics and readability purposes\n",
    "    for judge_model_id in judge_model_ids:\n",
    "        for verdict in verdict_types:\n",
    "            df_judge_verdict = df_eval_results[\n",
    "                (df_eval_results[\"verdict\"] == verdict)\n",
    "                & (df_eval_results[\"judge_model_id\"] == judge_model_id)\n",
    "            ]\n",
    "            if not df_judge_verdict.empty:\n",
    "                txt_buffer = io.StringIO()\n",
    "                for index, row in df_judge_verdict.iterrows():\n",
    "                    txt_buffer.write(\n",
    "                        f\"candidate model: {row['candidate_model']}\\n\"\n",
    "                        f\"Question: {row['question']}\\n\"\n",
    "                        f\"candidate model response: {row['candidate_model_response']}\\n\"\n",
    "                        f\"ground truth: {row['ground_truth']}\\n\"\n",
    "                        f\"verdict: {row['verdict']}\\n\"\n",
    "                        f\"explanation: {row['explanation']}\\n\"\n",
    "                        f\"cosine similarity: {row['cosine_similarity_score']}\\n\\n\"\n",
    "                    )\n",
    "                judge_verdict_responses = txt_buffer.getvalue()\n",
    "                verdict_file = f\"{judge_model_id}_{verdict}_verdicts_evaluation.txt\"\n",
    "                judge_verdict_responses_fpath = os.path.join(METRICS_DIR, verdict_file)\n",
    "                write_to_s3(\n",
    "                    judge_verdict_responses, BUCKET_NAME, \"\", METRICS_DIR, verdict_file\n",
    "                )\n",
    "                logger.info(\n",
    "                    f\"{verdict.capitalize()} verdict responses for judge {judge_model_id} saved to s3://{BUCKET_NAME}/{judge_verdict_responses_fpath}\"\n",
    "                )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error encountered while writing the evaluation responses to s3: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the overall quantitate metrics of each model scored by the PoLL\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mean cosine similarity score, levenshtein distance and token set ratio\n",
    "try:\n",
    "    panel_summary_responses_df = (\n",
    "        df_eval_results.groupby([\"judge_model_id\", \"candidate_model\", \"verdict\"])\n",
    "        .agg(\n",
    "            count=(\"verdict\", \"size\"),\n",
    "            mean_cosine_similarity=(\"cosine_similarity_score\", \"mean\"),\n",
    "        )\n",
    "        .unstack(fill_value=0)\n",
    "        .stack()\n",
    "        .reset_index()\n",
    "    )\n",
    "    csv_buffer = io.StringIO()\n",
    "    panel_summary_responses_df.to_csv(csv_buffer, index=False)\n",
    "    panel_summary_responses = csv_buffer.getvalue()\n",
    "    llm_as_a_judge_per_eval_summary_fpath = os.path.join(\n",
    "        METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES\n",
    "    )\n",
    "    write_to_s3(\n",
    "        panel_summary_responses,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        LLM_JUDGE_PANEL_RESPONSE_SUMMARIES,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Summary on each eval (Majority voting) for each panel judge sent to s3://{BUCKET_NAME}/{llm_as_a_judge_per_eval_summary_fpath}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"View information on the accuracy metrics: {panel_summary_responses_df.head()}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not calculate the overall accuracy metrics for Majority Voting: {e}\"\n",
    "    )\n",
    "panel_summary_responses_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def majority_vote(row):\n",
    "    \"\"\"\n",
    "    This function calculates the majority vote based on whether the candidate model response is correct or incorrect\n",
    "    based on the vote from the panel of judges. It only returns 'correct' if there are more 'correct' votes than 'incorrect'\n",
    "    and 'NaN' values combined, and similarly for 'incorrect'. Otherwise, it returns 'no_majority_vote'.\n",
    "    \"\"\"\n",
    "    verdict_columns = [col for col in row.index if col.endswith(\"_verdict\")]\n",
    "    # find majority vote\n",
    "    verdicts = [row[c] for c in verdict_columns]\n",
    "    majority_vote = mode(verdicts)\n",
    "    return majority_vote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the majority voting pivot table along with the majority vote decision\n",
    "try:\n",
    "    majority_vote_pivoted_df = df_eval_results.pivot_table(\n",
    "        index=[\"question\", \"candidate_model\", \"payload_file\"],\n",
    "        columns=\"judge_model_id\",\n",
    "        values=[\"verdict\"],\n",
    "        aggfunc=\"first\",\n",
    "    )\n",
    "\n",
    "    majority_vote_pivoted_df.columns = [\n",
    "        f\"{judge_model}_{col}\" for col, judge_model in majority_vote_pivoted_df.columns\n",
    "    ]\n",
    "    majority_vote_pivoted_df.reset_index(inplace=True)\n",
    "    majority_vote_pivoted_df[\"majority_vote\"] = majority_vote_pivoted_df.apply(\n",
    "        majority_vote, axis=1\n",
    "    )\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_pivoted_df.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_raw_results = csv_buffer.getvalue()\n",
    "    majority_vote_raw_results_metrics_fpath = os.path.join(\n",
    "        METRICS_DIR, MAJORITY_VOTE_DF_RAW_RESULTS_FILE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        majority_vote_raw_results,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        MAJORITY_VOTE_DF_RAW_RESULTS_FILE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Majority results file containing raw results sent to s3://{BUCKET_NAME}/{majority_vote_raw_results_metrics_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate the raw responses for Majority Voting: {e}\")\n",
    "\n",
    "majority_vote_pivoted_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the total number of correct and incorrect count for the model for each payload file\n",
    "majority_vote_data_df_per_payload = pd.DataFrame()\n",
    "majority_vote_data_df_per_payload[\"correct_count\"] = majority_vote_pivoted_df.groupby(\n",
    "    [\"candidate_model\", \"payload_file\"]\n",
    ")[\"majority_vote\"].apply(lambda x: (x == \"correct\").sum())\n",
    "majority_vote_data_df_per_payload[\"incorrect_count\"] = majority_vote_pivoted_df.groupby(\n",
    "    [\"candidate_model\", \"payload_file\"]\n",
    ")[\"majority_vote\"].apply(lambda x: (x == \"incorrect\").sum())\n",
    "majority_vote_data_df_per_payload.reset_index(inplace=True)\n",
    "majority_vote_data_df_per_payload.sort_values(by=\"correct_count\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy as per the Majority Vote per Payload file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the accuracy of the model based on majority voting\n",
    "if (\n",
    "    \"correct_count\"\n",
    "    and \"incorrect_count\"\n",
    "    in majority_vote_data_df_per_payload.sort_values(\n",
    "        by=\"correct_count\", ascending=False\n",
    "    ).columns\n",
    "):\n",
    "    majority_vote_data_df_per_payload[\"majority_voting_accuracy\"] = round(\n",
    "        (\n",
    "            majority_vote_data_df_per_payload[\"correct_count\"]\n",
    "            / (\n",
    "                majority_vote_data_df_per_payload[\"correct_count\"]\n",
    "                + majority_vote_data_df_per_payload[\"incorrect_count\"]\n",
    "            )\n",
    "        )\n",
    "        * 100,\n",
    "        2,\n",
    "    )\n",
    "\n",
    "majority_vote_data_df_per_payload.sort_values(\n",
    "    by=\"majority_voting_accuracy\", ascending=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the per candidate model accuracy per judge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the df on judge model id, candidate model and verdict, and then calculate the accuracy of each judge model\n",
    "df_per_model_accuracy_counts_df = (\n",
    "    df_eval_results.groupby(\n",
    "        [\"judge_model_id\", \"candidate_model\", \"payload_file\", \"verdict\"]\n",
    "    )\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# get the accuracy for each candidate model\n",
    "df_per_model_accuracy_counts_df[\"accuracy\"] = (\n",
    "    df_per_model_accuracy_counts_df.get(\"correct\", 0)\n",
    "    / (\n",
    "        df_per_model_accuracy_counts_df.get(\"incorrect\", 0)\n",
    "        + df_per_model_accuracy_counts_df.get(\"needs_further_evaluation\", 0)\n",
    "        + df_per_model_accuracy_counts_df.get(\"correct\", 0)\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "\n",
    "df_per_model_accuracy_counts_df[\"accuracy\"] = round(\n",
    "    df_per_model_accuracy_counts_df[\"accuracy\"], 2\n",
    ")\n",
    "df_per_model_accuracy_counts_df.reset_index(inplace=True)\n",
    "df_per_model_accuracy_counts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_accuracy_df = df_per_model_accuracy_counts_df.pivot_table(\n",
    "    index=[\"candidate_model\", \"payload_file\"],\n",
    "    columns=\"judge_model_id\",\n",
    "    values=\"accuracy\",\n",
    ")\n",
    "overall_accuracy_df.reset_index(inplace=True)\n",
    "print(overall_accuracy_df.columns)\n",
    "overall_accuracy_df.columns = [\"candidate_model\", \"payload_file\"] + [\n",
    "    f\"judge_{col}_accuracy\" for col in overall_accuracy_df.columns[2:]\n",
    "]\n",
    "overall_accuracy_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge both panel voting and per model eval df to get all metrics together\n",
    "merged_accuracy_df = pd.merge(\n",
    "    overall_accuracy_df,\n",
    "    majority_vote_data_df_per_payload,\n",
    "    on=[\"candidate_model\", \"payload_file\"],\n",
    ")\n",
    "merged_accuracy_df = merged_accuracy_df.drop(\n",
    "    columns=[\"correct_count\", \"incorrect_count\"], axis=1\n",
    ")\n",
    "merged_accuracy_df = merged_accuracy_df.sort_values(\n",
    "    by=\"majority_voting_accuracy\", ascending=False\n",
    ")\n",
    "\n",
    "# Send the accuracy metrics to S3\n",
    "csv_buffer = io.StringIO()\n",
    "merged_accuracy_df.to_csv(csv_buffer, index=False)\n",
    "per_model_per_payload_accuracy_counts = csv_buffer.getvalue()\n",
    "per_model_per_payload_accuracy_counts_fpath = os.path.join(\n",
    "    METRICS_DIR, PER_PAYLOAD_MODEL_ACCURACY_MAJORITY_VOTING\n",
    ")\n",
    "\n",
    "write_to_s3(\n",
    "    per_model_per_payload_accuracy_counts,\n",
    "    BUCKET_NAME,\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    PER_PAYLOAD_MODEL_ACCURACY_MAJORITY_VOTING,\n",
    ")\n",
    "logger.info(\n",
    "    f\"Per model per payload majority vote accuracy scores sent to s3://{BUCKET_NAME}/{per_model_per_payload_accuracy_counts_fpath}\"\n",
    ")\n",
    "merged_accuracy_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the majority voting accuracy per model\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the majority voting accuracy per model based on the number of correct and incorrect verdicts\n",
    "try:\n",
    "    majority_vote_data_df = pd.DataFrame()\n",
    "    majority_vote_data_df[\"correct_count\"] = majority_vote_pivoted_df.groupby(\n",
    "        \"candidate_model\"\n",
    "    )[\"majority_vote\"].apply(lambda x: (x == \"correct\").sum())\n",
    "    majority_vote_data_df[\"incorrect_count\"] = majority_vote_pivoted_df.groupby(\n",
    "        \"candidate_model\"\n",
    "    )[\"majority_vote\"].apply(lambda x: (x == \"incorrect\").sum())\n",
    "    majority_vote_data_df.reset_index(inplace=True)\n",
    "    majority_vote_data_df.sort_values(by=\"correct_count\", ascending=False)\n",
    "\n",
    "    if \"correct_count\" and \"incorrect_count\" in majority_vote_data_df.columns:\n",
    "        majority_vote_data_df[\"majority_voting_accuracy\"] = round(\n",
    "            (\n",
    "                majority_vote_data_df[\"correct_count\"]\n",
    "                / (\n",
    "                    majority_vote_data_df[\"correct_count\"]\n",
    "                    + majority_vote_data_df[\"incorrect_count\"]\n",
    "                )\n",
    "            )\n",
    "            * 100,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    majority_vote_data_df = majority_vote_data_df.sort_values(\n",
    "        by=\"majority_voting_accuracy\", ascending=False\n",
    "    )\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_data_df.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_per_model_accuracy = csv_buffer.getvalue()\n",
    "    majority_vote_per_model_accuracy_metrics_fpath = os.path.join(\n",
    "        METRICS_DIR, PER_MODEL_ACCURACY_POLL\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        majority_vote_per_model_accuracy,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        PER_MODEL_ACCURACY_POLL,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Per model PoLL accuracy sent to to s3://{BUCKET_NAME}/{majority_vote_per_model_accuracy_metrics_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate per model PoLL accuracy: {e}\")\n",
    "\n",
    "majority_vote_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the majority vote per payload file\n",
    "try:\n",
    "    # Group by payload_file and candidate_model to calculate correct and incorrect counts\n",
    "    majority_vote_payload_df = (\n",
    "        majority_vote_pivoted_df.groupby([\"payload_file\", \"candidate_model\"])[\n",
    "            \"majority_vote\"\n",
    "        ]\n",
    "        .apply(lambda x: (x == \"correct\").sum())\n",
    "        .reset_index(name=\"correct_count\")\n",
    "    )\n",
    "    majority_vote_payload_df[\"incorrect_count\"] = (\n",
    "        majority_vote_pivoted_df.groupby([\"payload_file\", \"candidate_model\"])[\n",
    "            \"majority_vote\"\n",
    "        ]\n",
    "        .apply(lambda x: (x == \"incorrect\").sum())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if \"correct_count\" and \"incorrect_count\" in majority_vote_data_df.columns:\n",
    "        majority_vote_payload_df[\"majority_voting_accuracy\"] = round(\n",
    "            (\n",
    "                majority_vote_payload_df[\"correct_count\"]\n",
    "                / (\n",
    "                    majority_vote_payload_df[\"correct_count\"]\n",
    "                    + majority_vote_payload_df[\"incorrect_count\"]\n",
    "                )\n",
    "            )\n",
    "            * 100,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    # Sort by accuracy for better readability\n",
    "    majority_vote_payload_df = majority_vote_payload_df.sort_values(\n",
    "        by=\"majority_voting_accuracy\", ascending=False\n",
    "    )\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_payload_df.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_per_payload_accuracy = csv_buffer.getvalue()\n",
    "    majority_vote_per_payload_accuracy_metrics_fpath = os.path.join(\n",
    "        METRICS_DIR, PER_PAYLOAD_PER_MODEL_POLL_ACCURACY\n",
    "    )\n",
    "    write_to_s3(\n",
    "        majority_vote_per_payload_accuracy,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        PER_PAYLOAD_PER_MODEL_POLL_ACCURACY,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Per payload file accuracy sent to s3://{BUCKET_NAME}/{majority_vote_per_payload_accuracy_metrics_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate per payload file accuracy: {e}\")\n",
    "\n",
    "majority_vote_payload_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the per candidate model accuracy per panel of LLM evaluator\n",
    "try:\n",
    "    df_per_model_accuracy_counts_df = (\n",
    "        df_eval_results.groupby([\"judge_model_id\", \"candidate_model\", \"verdict\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    # get the accuracy for each candidate model\n",
    "    df_per_model_accuracy_counts_df[\"accuracy\"] = (\n",
    "        df_per_model_accuracy_counts_df.get(\"correct\", 0)\n",
    "        / (\n",
    "            df_per_model_accuracy_counts_df.get(\"incorrect\", 0)\n",
    "            + df_per_model_accuracy_counts_df.get(\"needs_further_evaluation\", 0)\n",
    "            + df_per_model_accuracy_counts_df.get(\"correct\", 0)\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    df_per_model_accuracy_counts_df[\"accuracy\"] = round(\n",
    "        df_per_model_accuracy_counts_df[\"accuracy\"], 2\n",
    "    )\n",
    "    df_per_model_accuracy_counts_df.reset_index(inplace=True)\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    df_per_model_accuracy_counts_df.to_csv(csv_buffer, index=False)\n",
    "    df_per_model_accuracy_counts = csv_buffer.getvalue()\n",
    "    df_per_model_accuracy_counts_metrics_fpath = os.path.join(\n",
    "        METRICS_DIR, PER_MODEL_ACCURACY_PER_EVAL_JUDGE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        df_per_model_accuracy_counts,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        PER_MODEL_ACCURACY_PER_EVAL_JUDGE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Per model accuracy per eval judge sent to s3://{BUCKET_NAME}/{df_per_model_accuracy_counts_metrics_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate per model accuracy per eval judge: {e}\")\n",
    "\n",
    "df_per_model_accuracy_counts_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the summary table\n",
    "\n",
    "---\n",
    "\n",
    "Fetch the summary table containing the per judge accuracy per candidate model and the per model accuracy based on majority vote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the per candidate model accuracy per panel of LLM evaluator\n",
    "try:\n",
    "    overall_accuracy_df = df_per_model_accuracy_counts_df.pivot_table(\n",
    "        index=\"candidate_model\", columns=\"judge_model_id\", values=\"accuracy\"\n",
    "    )\n",
    "    overall_accuracy_df.reset_index(inplace=True)\n",
    "    overall_accuracy_df.columns = [\"candidate_model\"] + [\n",
    "        f\"judge_{col}_accuracy\" for col in overall_accuracy_df.columns[1:]\n",
    "    ]\n",
    "\n",
    "    merged_accuracy_df = pd.merge(\n",
    "        overall_accuracy_df, majority_vote_data_df, on=\"candidate_model\"\n",
    "    )\n",
    "    merged_accuracy_df = merged_accuracy_df.drop(\n",
    "        columns=[\"correct_count\", \"incorrect_count\"], axis=1\n",
    "    )\n",
    "    merged_accuracy_df = merged_accuracy_df.sort_values(\n",
    "        by=\"majority_voting_accuracy\", ascending=False\n",
    "    )\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    merged_accuracy_df.to_csv(csv_buffer, index=False)\n",
    "    merged_accuracy_df_val = csv_buffer.getvalue()\n",
    "    merged_accuracy_df_metrics_fpath = os.path.join(\n",
    "        METRICS_DIR, CANDIDATE_MODEL_ACCURACY_FILE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        merged_accuracy_df_val,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        CANDIDATE_MODEL_ACCURACY_FILE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Per model accuracy per eval judge sent to s3://{BUCKET_NAME}/{merged_accuracy_df_metrics_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate per model accuracy per eval judge: {e}\")\n",
    "\n",
    "merged_accuracy_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Verdict Type: Overlap Analysis\n",
    "\n",
    "---\n",
    "\n",
    "In this portion, we check the `final verdict type`. This generates a verdict which is categorized into the following 4 main parts:\n",
    "\n",
    "1. correct_by_unanimous_decision: If all the panel of LLM judges evalaute a candidate model response as `correct`, then the final verdict is correct by unanimous decision.\n",
    "\n",
    "1. incorrect_by_unanimous_decision: If all the panel of LLM judges evalaute a candidate model response as `incorrect`, then the final verdict is incorrect by unanimous decision.\n",
    "\n",
    "1. correct_by_majority_vote_w_disagreement: If the panel of LLMs have diverse verdicts, but the majority vote is correct for a given candidate model response, then the final verdict is correct_by_majority_vote_w_disagreement.\n",
    "\n",
    "1. incorrect_by_majority_vote_w_disagreement: If the panel of LLMs have diverse verdicts, but the majority vote is incorrect for a given candidate model response, then the final verdict is incorrect_by_majority_vote_w_disagreement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_overlap_of_PoLL(row):\n",
    "    \"\"\"\n",
    "    This function checks how many judges overlapped in rating responses from the candidate model on questions\n",
    "    \"correctly\" and how many did not overlap (where one judge said correct and another said incorrect).\n",
    "    It only returns 'all_correct' if all columns ending with '_verdict' have a 'correct' value (i.e., no NaNs or incorrect votes).\n",
    "    \"\"\"\n",
    "    # Filter columns that end with '_verdict'\n",
    "    verdict_columns = [col for col in row.index if col.endswith(\"_verdict\")]\n",
    "\n",
    "    # Initialize the counts based on the filtered columns\n",
    "    correct_count = (row[verdict_columns] == \"correct\").sum()\n",
    "    incorrect_count = (row[verdict_columns] == \"incorrect\").sum()\n",
    "    nan_count = row[verdict_columns].isna().sum()\n",
    "    # check for when all models rate\n",
    "    total_judges: int = len(verdict_columns)\n",
    "\n",
    "    # Determine the overlap based on the counts\n",
    "    if correct_count == total_judges:\n",
    "        return \"correct_by_unanimous_decision\"\n",
    "    elif incorrect_count == total_judges:\n",
    "        return \"incorrect_by_unanimous_decision\"\n",
    "    elif row[\"majority_vote\"] == \"correct\":\n",
    "        return f\"correct_by_majority_vote_w_{incorrect_count+nan_count}_dissagreement\"\n",
    "    elif row[\"majority_vote\"] == \"incorrect\":\n",
    "        return f\"incorrect_by_majority_vote_w_{correct_count+nan_count}_dissagreement\"\n",
    "    else:\n",
    "        return \"no_overlaps\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    majority_vote_pivoted_df[\"verdict_type\"] = majority_vote_pivoted_df.apply(\n",
    "        check_overlap_of_PoLL, axis=1\n",
    "    )\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_pivoted_df.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_pivoted_final_verdict = csv_buffer.getvalue()\n",
    "    majority_vote_pivoted_final_verdict_fpath = os.path.join(\n",
    "        METRICS_DIR, PER_MODEL_ACCURACY_W_VERDICT_TYPE_FILE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        majority_vote_pivoted_final_verdict,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        PER_MODEL_ACCURACY_W_VERDICT_TYPE_FILE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Majority vote data and final verdicts are sent to s3://{BUCKET_NAME}/{majority_vote_pivoted_final_verdict_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate majority vote data and final verdicts: {e}\")\n",
    "\n",
    "majority_vote_pivoted_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now calculate the verdict breakdown for correct responses and verdict breakdown for\n",
    "# incorrect responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for incorrect responses\n",
    "try:\n",
    "    majority_vote_df_for_incorrect_verdict_analysis = majority_vote_pivoted_df.copy()\n",
    "    majority_vote_df_for_incorrect_verdict_analysis = (\n",
    "        majority_vote_df_for_incorrect_verdict_analysis[\n",
    "            majority_vote_df_for_incorrect_verdict_analysis.majority_vote == \"incorrect\"\n",
    "        ][\"verdict_type\"]\n",
    "        .value_counts(normalize=True)\n",
    "        .rename_axis(\"verdict_type_breakdown_for_incorrect\")\n",
    "        .reset_index(name=\"counts\")\n",
    "    )\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_df_for_incorrect_verdict_analysis.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_pivoted_df_incorrect = csv_buffer.getvalue()\n",
    "    majority_vote_pivoted_df_incorrect_fpath = os.path.join(\n",
    "        METRICS_DIR, VERDICT_TYPE_BREAKDOWN_FOR_INCORRECT_FILE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        majority_vote_pivoted_df_incorrect,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        VERDICT_TYPE_BREAKDOWN_FOR_INCORRECT_FILE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Majority vote with incorrect verdict breakdown is sent to s3://{BUCKET_NAME}/{majority_vote_pivoted_df_incorrect_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not calculate majority vote with incorrect verdict breakdown: {e}\"\n",
    "    )\n",
    "\n",
    "majority_vote_df_for_incorrect_verdict_analysis.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for correct responses\n",
    "try:\n",
    "    majority_vote_df_for_correct_verdict_analysis = majority_vote_pivoted_df.copy()\n",
    "    majority_vote_df_for_correct_verdict_analysis = (\n",
    "        majority_vote_df_for_correct_verdict_analysis[\n",
    "            majority_vote_df_for_correct_verdict_analysis.majority_vote == \"correct\"\n",
    "        ][\"verdict_type\"]\n",
    "        .value_counts(normalize=True)\n",
    "        .rename_axis(\"verdict_type_breakdown_for_correct\")\n",
    "        .reset_index(name=\"counts\")\n",
    "    )\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_df_for_correct_verdict_analysis.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_pivoted_df_correct = csv_buffer.getvalue()\n",
    "    majority_vote_pivoted_df_correct_fpath = os.path.join(\n",
    "        METRICS_DIR, VERDICT_TYPE_BREAKDOWN_FOR_CORRECT_FILE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        majority_vote_pivoted_df_correct,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        VERDICT_TYPE_BREAKDOWN_FOR_CORRECT_FILE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Majority vote with correct verdict breakdown is sent to s3://{BUCKET_NAME}/{majority_vote_pivoted_df_correct_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not calculate majority vote with correct verdict breakdown: {e}\"\n",
    "    )\n",
    "\n",
    "majority_vote_df_for_correct_verdict_analysis.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_eval_results#### Send all responses from the evaluation process to S3 as a txt file for further downstream processing and readability purposes\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Write all explanations to a file and send to S3\n",
    "    explanations_txt_buffer = io.StringIO()\n",
    "    for index, row in df_eval_results.iterrows():\n",
    "        explanations_txt_buffer.write(\n",
    "            f\"candidate model: {row['candidate_model']}\\n\"\n",
    "            f\"judge model: {row['judge_model_id']}\\n\"\n",
    "            f\"Question: {row['question']}\\n\"\n",
    "            f\"candidate model response: {row['candidate_model_response']}\\n\"\n",
    "            f\"ground truth: {row['ground_truth']}\\n\"\n",
    "            f\"verdict: {row['verdict']}\\n\"\n",
    "            f\"explanation: {row['explanation']}\\n\"\n",
    "            f\"cosine similarity: {row['cosine_similarity_score']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    explanations_txt_file_content = explanations_txt_buffer.getvalue()\n",
    "    explanations_fpath = os.path.join(METRICS_DIR, ALL_EVALUATIONS_IN_TXT)\n",
    "    write_to_s3(\n",
    "        explanations_txt_file_content,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        ALL_EVALUATIONS_IN_TXT,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"All text eval content from the llm judge panelists sent to s3://{BUCKET_NAME}/{explanations_fpath}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"All of the content including the candidate model responses, ground truth, evaluation are written: {explanations_txt_file_content}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not calculate the overall accuracy metrics for Majority Voting: {e}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": ".fmbench_python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
