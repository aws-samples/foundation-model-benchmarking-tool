{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Evaluations on all inference files and gather findings on quantitative metrics (such as _Cosine Similarity_) and subjective metrics on various criteria using _LLM as a judge_ - Max Voting & Average Pooling using PoLL (Panel of LLM Evaluators)\n",
    "\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of the solution focusses on getting evaluations on the quality of responses. It does so by gathering the following information and performing the steps below:\n",
    "\n",
    "- **Gets all per inference request file**: This step first accesses and gets all of the per inference request files into a dataframe, containing the response from the LLM as well as the ground truth, if any is provided. \n",
    "\n",
    "- **Generates quantitative metrics for evaluation**: Calculate quantitative metrics to measure similarity and accuracy, for example _Cosine Similarity_. This helps in getting a quantitative overall score to the entire dataset in terms of which model generates outputs that are most similar and accurate to the ground truth (if any is provided). With this statistic, customers and users of the open source community can make business level judgements. \n",
    "\n",
    "- **Uses an _LLM as a judge_ approach to get subjective evaluations**: Refer to this [paper](https://arxiv.org/pdf/2404.18796). We use the following ways to evaluate the responses from the `candidate models` (models used to generate inferences)\n",
    "\n",
    "    1. **Max Voting**: When a dataset provides a ground truth, we use a technique called `Max Voting`. Here, we use PoLL, or a panel of LLM evaluators, from different model families to evaluate each candidate model's response based on whether it generates a `correct` or an `incorrect` answer simply based on ground truth comparisons. Using models from different families as a PoLL, increases it's evaluation ability to be close to that of a human evaluation, and eliminates the intra model bias.\n",
    "    \n",
    "    2. **Average Pooling**: When a dataset does not provide a ground truth, or if a task being evaluated needs to be given subjective level judgements, that is when we use `Average Pooling`. We use a specific subjective level criteria and then evaluate the candidate model responses on a scale of 1-5 for each PoLL. Using this, we get an average score and then can evaluate how each candidate model was scored based on the PoLL evaluations.\n",
    "    \n",
    "FMBench uses this approach of PoLL to eradicate intra model bias by using models as judges from different model families. This brings the evaluation results closer to that of a human evaluation, makes the evaluation process more streamlined, consistent across all the responses, and reduces the latency and cost of evaluating the candidate models over time.\n",
    "    \n",
    "***All evaluations are generated in a JSON format for further downstream analytics on the evaluation results***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import ray\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from fuzzywuzzy import fuzz\n",
    "from numpy.linalg import norm\n",
    "from litellm import completion\n",
    "from difflib import SequenceMatcher as SM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import seaborn and other related libraries for visualizations and plotting charts\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from typing import List, Optional, Dict\n",
    "import importlib.resources as pkg_resources\n",
    "from fmbench import __version__ as fmbench_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the ray service to run async calls in parallel to bedrock easily\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"CONFIG_FILE={CONFIG_FILE}\")\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the associated pricing config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "pricing_file_path: str = config['pricing'] \n",
    "\n",
    "# initialize the pricing config file to None\n",
    "pricing_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files('fmbench'), 'configs')\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "pricing_module = Path(config['pricing'])\n",
    "logger.info(f\"pricing config provided for inference from this model is --> {pricing_module}\")\n",
    "pricing_file_path = os.path.join(config_dir, pricing_module)\n",
    "logger.info(f\"pricing config file path is --> {pricing_file_path}\")\n",
    "\n",
    "pricing_config = load_config(pricing_file_path)\n",
    "logger.info(f\"pricing config file recorded: {json.dumps(pricing_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug is True:\n",
    "    metrics_path_file: str = os.path.join(\"..\", \"..\", METADATA_DIR, METRICS_PATH_FNAME)\n",
    "else:\n",
    "    metrics_path_file: str = os.path.join(METADATA_DIR, METRICS_PATH_FNAME)\n",
    "logger.info(f\"cwd={os.getcwd()}, METADATA_DIR={METADATA_DIR}, METRICS_PATH_FNAME={METRICS_PATH_FNAME}, metrics_path_file={metrics_path_file}\")\n",
    "METRICS_DIR: str = Path(metrics_path_file).read_text().strip()\n",
    "logger.info(f\"metrics_path_file={metrics_path_file}, METRICS_DIR={METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join(METRICS_DIR, config[\"report\"][\"per_inference_request_file\"])\n",
    "# file_path='fmbench-bedrock-fmbench-1-us-east-1-role/data/metrics/yyyy=2024/mm=07/dd=17/hh=02/mm=43/per_inference_request_results.csv'\n",
    "logger.info(f\"File path containing the metrics per inference folder --> {file_path}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    file_content = get_s3_object(config['aws']['bucket'], file_path)\n",
    "    # Use pandas to read the CSV content\n",
    "    df_per_inference = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(f\"{file_path} read into dataframe of shape {df_per_inference.shape}, \"\n",
    "                f\"cols={df_per_inference.columns}\")\n",
    "    logger.info(f\"{file_path} contains results for the following endpoints={df_per_inference.endpoint_name.unique()}\")\n",
    "    logger.info(df_per_inference.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between prompt token length and inference latency for different instances and concurrency levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.latency.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the `sentence-transformers/all-mpnet-base-v2` embeddings model to calculate the _Cosine Similarity_ scores \n",
    "---\n",
    "\n",
    "This portion of the evaluation step does as follows:\n",
    "\n",
    "1. Uses the `sentence-transformers/all-mpnet-base-v2` model from Hugging Face. This is a sentence-transformers model. It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "1. Use the embeddings model to get quantitative metrics from the inferences. This helps to get a similarity score between the ground truth answers from a dataset if any are given and the actual responses from the model received during inference.\n",
    "\n",
    "1. If no ground truth is provided, cosine similarity is calculated between the response and the content provided to answer the question.embeddings_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the quantitiative evaluation information from the config file, such as the embeddings model\n",
    "# to be used\n",
    "embeddings_model_quantitative_info: Dict = config['model_evaluations']['quantitative_eval_info']\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    This function loads the sentence-transformers model based on the provided model ID.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        model=None\n",
    "        model_id = embeddings_model_quantitative_info['embeddings_model_id'].get('model_id', None)\n",
    "        if model_id:\n",
    "            model = SentenceTransformer(model_id)\n",
    "        else:\n",
    "            raise ValueError(\"Model ID is not provided or invalid in the configuration.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"The SentenceTransformer embeddings model could not be loaded: {e}\")\n",
    "        model=None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the embeddings model to calculate the cosine similarity scores\n",
    "model = load_model()\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between two texts. In this case, \n",
    "    the cosine similarity is the comparison between the ground truth in the given dataset\n",
    "    and the candidate model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cosine: float = None\n",
    "        # returns the embedding for a given text using the sentence-transformers model.\n",
    "        A = model.encode([text1])[0]\n",
    "        B = model.encode([text2])[0]\n",
    "        cosine = dot(A, B) / (norm(A) * norm(B))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cosine similarity was not calculated at this iteration: {e}\")\n",
    "        cosine=None\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"~Creating embeddings of all candidate model responses now. This might take a couple minutes~\")\n",
    "\n",
    "# Assuming df_per_inference is your DataFrame\n",
    "df_per_inference['cosine_similarity_score'] = df_per_inference.apply(\n",
    "    lambda row: calculate_cosine_similarity(row['completion'], row['ground_truth']), axis=1\n",
    ")\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lexical Match & Cosine Similarity Score Accuracy Evaluation Filter\n",
    "---\n",
    "\n",
    "Before having the Panel of LLM Evaluators evaluate each candidate model response, we pass those responses through a filtering step. In this step we use a threshold for a `Lexical match` and a `Cosine Similarity` score to define whether that answer is correct without having an LLM evaluate it. The thresholds for correctness is defined in the configuration files. \n",
    "\n",
    "The reason to do this is to make the evaluation process more like a hierarchy of checks, to make sure each and every candidate model response is evaluated appropriately. Additionally, filter steps to check for these scores to determine whether a candidate model reponse is correct, will narrow down the evaluation checks for the PoLL reducing the time and cost to complete all evaluations. This is specific to the `Ground Truth based approach`. \n",
    "\n",
    "For the lexical match, we use the `fuzzy` match approach `token_set_ratio` library to determine what percent of the two texts are similar.\n",
    "\n",
    "**Note**: `Token_set_ratio` algorithm tokenizes both input strings, removes duplicate tokens, and calculates the similarity score based on the intersection and union of the token sets. It captures the essence of the strings’ content rather than their specific order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_token_set_ratio(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the partial token match or fuzz ratio between two strings.\n",
    "    If the fuzz ratio exceeds the threshold and the cosine similarity matches or exceeds the threshold, \n",
    "    then the answer is correct and it is not evaluated using a judge. If it is not, then it\n",
    "    is parsed through the PoLL process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        token_set_ratio: float = None\n",
    "        if text1 and text2:\n",
    "            token_set_ratio = fuzz.token_set_ratio(text1, text2) / 100.0\n",
    "        else:\n",
    "            token_set_ratio=None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in calculating token set ratio: {e}\")\n",
    "        token_set_ratio=None\n",
    "    return token_set_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein distance algorithm\n",
    "---\n",
    "In information theory, linguistics, and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. The Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def levenshtein_distance(s, t):\n",
    "\tm, n = len(s), len(t)\n",
    "\tif m < n:\n",
    "\t\ts, t = t, s\n",
    "\t\tm, n = n, m\n",
    "\td = [list(range(n + 1))] + [[i] + [0] * n for i in range(1, m + 1)]\n",
    "\tfor j in range(1, n + 1):\n",
    "\t\tfor i in range(1, m + 1):\n",
    "\t\t\tif s[i - 1] == t[j - 1]:\n",
    "\t\t\t\td[i][j] = d[i - 1][j - 1]\n",
    "\t\t\telse:\n",
    "\t\t\t\td[i][j] = min(d[i - 1][j], d[i][j - 1], d[i - 1][j - 1]) + 1\n",
    "\treturn d[m][n]\n",
    "\n",
    "\n",
    "def calculate_levenshtein_distance(input_string, reference_string):\n",
    "    distance = levenshtein_distance(input_string, reference_string)\n",
    "    max_length = max(len(input_string), len(reference_string))\n",
    "    similarity = 1 - (distance / max_length)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # These are examples from the LongBench dataset for testing purposes\n",
    "# candidate_model_response: str = \"Both Sinofranchetia and Stauntonia are from the Lardizabalaceae family. This information is mentioned in the passages for both genera.\"\n",
    "# ground_truth: str = \"a genus of flowering plant in the Lardizabalaceae family\"\n",
    "# ratio = calculate_levenshtein_distance(candidate_model_response, ground_truth)\n",
    "# print(f\"ratio calculated: {ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the token set ratio for each row and add it as a new column\n",
    "df_per_inference['token_set_ratio_value'] = df_per_inference.apply(\n",
    "    lambda row: calculate_token_set_ratio(row['completion'], row['ground_truth']), axis=1\n",
    ")\n",
    "\n",
    "df_per_inference['levenshtein_distance'] = df_per_inference.apply(\n",
    "    lambda row: calculate_levenshtein_distance(row['completion'], row['ground_truth']), axis=1\n",
    ")\n",
    "\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the all_metrics path to send the evaluation metrics to\n",
    "all_metrics_fpath = os.path.join(METRICS_DIR, config[\"report\"][\"all_metrics_file\"])\n",
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_cosine_similarity_scores_csv = csv_buffer.getvalue()\n",
    "inference_cosine_similarity_scores_s3_path = os.path.join(METRICS_DIR, PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(df_per_inference_with_cosine_similarity_scores_csv, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES)\n",
    "logger.info(f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{inference_cosine_similarity_scores_s3_path}\")\n",
    "\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use _Panel of LLM Evaluators_ to get Subjective Evaluations on various evaluation criteria\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we run evaluations on the content generated by different candidate models. We use two main evaluation methods: `Max Voting` and `Average Pooling`. To eliminate intra-model bias, we address this by scoring answer correctness based not on a single judge, but instead on a panel composed of multiple evaluator models. Similar pooling techniques are used to reduce variance in human annotations by normalizing out both natural variation in human judgements caused by their own subjective biases as well as human error. We use the following two techniques:\n",
    "\n",
    "1. **Max Voting**: We use the PoLL to evaluate candidate model responses by checking its correctness compared to a provided ground truth answer in the dataset. We prompt each PoLL to evaluate and give the response in a JSON structure, giving a verdict on whether the response is correct or incorrect, and an explanation as to why that is. Using this, we can perform downstream analytics such as: \n",
    "\n",
    "    1. Calculate the overall accuracy of each model using the correct versus the (correct + incorrect) responses\n",
    "    \n",
    "    1. Calculate the `error rate` or frequency or incorrect responses\n",
    "    \n",
    "    1. Categorize the errors based on the explanations provided by the evaluators. Common categories might include misunderstanding the question, incomplete answers, factual inaccuracies\n",
    "    \n",
    "    1. Summary of overall correct/incorrect, and the best model based on the PoLL. Rank the models on Correctness versus Incorrectness.\n",
    "\n",
    "1. **Average Pooling**: We use the PoLL to rate the response of each candidate model on a more subjective criteria. Here, we have the candidate model responses rated on a scale of 1-5 based on the subjective criteria and then get an explanation to that. Using this we can do as follows:\n",
    "\n",
    "    1. Calculate the average score for each model across all questions to get an overall performance measure.\n",
    "    \n",
    "    1. Compute the standard deviation of the scores to understand the consistency of the model's performance.\n",
    "\n",
    "1. Towards the end of all evaluations, a final layer of evaluation is added at the end. This layer utilizes another LLM that acts as a final summarizer. It takes in the ratings, answers generated from each unique model that was used in inference, to give a list of trends, overall patterns and observations as to which model is suited for a given task for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the qualitative/subjective evaluation information from the config file to evaluate answers from different\n",
    "# endpoints on various criteria\n",
    "model_eval_subjective_info: Dict = config['model_evaluations']['subjective_eval_info']\n",
    "eval_criteria_list = model_eval_subjective_info.get('eval_criteria', None)\n",
    "logger.info(f\"available llm as a judge evaluation information to use: {json.dumps(model_eval_subjective_info, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the inference parameters that the LLM judge panel will use while evaluating model candidate responses\n",
    "INFERENCE_PARAMETERS_LLM_PANEL: Dict = config['model_evaluations']['subjective_eval_info'].get('inference_parameters', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_panel_of_llm_evaluation(model_id: str,\n",
    "                                  prompt: str):\n",
    "    \"\"\"\n",
    "    Get inference using LiteLLM. This get's inference on the answers provided and evaluates each\n",
    "    answer based on a given evaluation prompt template and the specific set of rules for each\n",
    "    evaluation criteria.\n",
    "    \"\"\"\n",
    "    # represents the service name\n",
    "    print(f\"get_inference, model_id={model_id}\")\n",
    "    service_name: str = \"bedrock\"\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name \n",
    "    # initialize the response dict\n",
    "    ret = dict(exception=None,\n",
    "               prompt=prompt,\n",
    "               completion=None,\n",
    "               completion_token_count=None,\n",
    "               prompt_token_count=None,\n",
    "               model_id=model_id)\n",
    "    body = ret['prompt']\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region\n",
    "    try:\n",
    "        # Represents calling the litellm completion/messaging api utilizing the completion/embeddings API\n",
    "        print(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(model=bedrock_model,\n",
    "                              messages=[{\"content\": body,\"role\": \"user\"}],\n",
    "                              temperature=INFERENCE_PARAMETERS_LLM_PANEL.get('temperature', 0.1),\n",
    "                              max_tokens=INFERENCE_PARAMETERS_LLM_PANEL.get('max_tokens', 100),\n",
    "                              caching=INFERENCE_PARAMETERS_LLM_PANEL.get('caching', False))\n",
    "        print(f\"response: {response}\")\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens        \n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret['exception'] = e\n",
    "    logger.info(f\"completion: {ret['completion']}\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_filename(s):\n",
    "    \"\"\"\n",
    "    convert a string to another string that can be used as a filename\n",
    "    i.e. remove white space and non-word chars\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"None\"\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespace with a single dash\n",
    "    s = re.sub(r\"\\s+\", '-', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_as_json(x: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Convert a string into a dictionary. Remove any\n",
    "    stray whitespaces which could break the json parsing\n",
    "    \"\"\"\n",
    "    d: Optional[Dict] = None\n",
    "    try:\n",
    "        x = x.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        d = json.loads(x)\n",
    "    except Exception as e:\n",
    "        print(f\"parse_as_json, error parsing string as json, string={x}\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the latest dataframe and run LLM as a judge evaluations on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the evaluation prompt payloads\n",
    "---\n",
    "\n",
    "Here, the evaluation prompt template is used by the LLM judge to evaluate the answers on different criteria.\n",
    "This prompt template function uses a set of rules, prompt template, the answer, and ground truth (if any) in the\n",
    "evaluation solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_eval_prompts(eval_template: str,\n",
    "                         answer: str, \n",
    "                         rules: str, \n",
    "                         context: str, \n",
    "                         ground_truth: Optional[str]):\n",
    "    \"\"\"\n",
    "    This function prepares the evaluation prompts by preparing the standard eval prompt template\n",
    "    with the rules of a given subjective criteria, context, answer and ground truth (if any ground truth is provided)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        processed_eval_template: Optional[str] = None\n",
    "        processed_eval_template = eval_template.format(\n",
    "            rules=rules,\n",
    "            answer=answer,\n",
    "            context=context,\n",
    "            ground_truth=ground_truth)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encountered while generating the evaluation prompt template: {e}\")\n",
    "        processed_eval_template=None\n",
    "    return processed_eval_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "# create the metrics directory that stores all of the json files containing evaluations from all Panel of LLM evaluators\n",
    "METRICS_PER_POLL_EVAL_DIR: str = os.path.join(METRICS_DIR, METRICS_PER_POLL_EVAL_DIR_NAME)\n",
    "_ = list(map(clear_dir, [METRICS_PER_POLL_EVAL_DIR]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_llm_evals(i: int, total: int, row: Dict,  model_id: str, eval_method_name: str, uuid: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Runs the evaluation for one row \n",
    "    The eval prompt is already available in the row dictionary\n",
    "    and we simply want to run the inference against the judge model.\n",
    "    The results are returned in a new dictionary that contains the model \n",
    "    response and some fields from the original dictionary\n",
    "    \"\"\"\n",
    "    try: \n",
    "        # save all the responses from the model in a dictionary\n",
    "        resp: Dict = {}\n",
    "        print(f\"run_eval, row {i}/{total}, judge_model_id={model_id}, candidate model={row['endpoint_name']}\")\n",
    "        candidate_model_response: str = row['completion']\n",
    "        # create the payload for model inference\n",
    "        prompt = row[f'{model_id}_{method_name}_eval_prompt']\n",
    "        # generate the evaluation on the data using the model judge\n",
    "        resp = get_panel_of_llm_evaluation(model_id, prompt)\n",
    "        # assign the completion from the candidate model to the `candidate_model_response`, \n",
    "        # and the actual evaluation will be contained in a field called `completion`\n",
    "        resp['candidate_model_response'] = candidate_model_response\n",
    "        logger.info(f\"Panel of LLM evaluator {model_id} completion: {resp['completion']}\")\n",
    "        resp['candidate_model'] = row['endpoint_name']\n",
    "        resp['cosine_similarity_score'] = row['cosine_similarity_score']\n",
    "        resp['levenshtein_distance'] = row['levenshtein_distance']\n",
    "        resp['token_set_ratio_value'] = row['token_set_ratio_value']\n",
    "        # if there is a ground truth, include that in the json response\n",
    "        if 'ground_truth' in row:\n",
    "            resp['ground_truth'] = row['ground_truth']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encountered while running evaluation: {e}\")\n",
    "        resp=None\n",
    "    return resp\n",
    "\n",
    "# we use Ray to parallize\n",
    "@ray.remote\n",
    "def async_run_eval(i: int, total: int, row: Dict, model_id: str, eval_method_name: str, uuid: str) -> Dict:\n",
    "    print(f\"async_run_eval, i={i}, total={total}, judge_model_info={model_id}, eval_method: {eval_method_name}, uuid: {uuid}\")\n",
    "    return run_llm_evals(i, total, row, model_id, eval_method_name, uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "df_per_inference_list = json.loads(df_per_inference.to_json(orient='records'))\n",
    "logger.info(f\"eval_records_list has {len(df_per_inference_list)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare evaluation prompt templates\n",
    "---\n",
    "\n",
    "This portion of the step prepares the evaluation prompt templates that are used in the evaluation process of using `Max Voting` or `Average Pooling` using the PoLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_eval_subjective_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the method that is being used to evaluate the content (which is either \n",
    "# max voting or average pooling)\n",
    "method_name: str = config['model_evaluations']['PoLL_Composition_and_Voting'].get('method', None)\n",
    "logger.info(f\"The evaluation method FMBench is going to use to evaluate different model responses: {method_name}\")\n",
    "logger.info(f\"judge panel being used to evaluate model responses: {model_eval_subjective_info.get('judge_panel_list', None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "eval_prompts_dir = Path(pkg_resources.files('fmbench'), f\"{config['s3_read_data']['prompt_template_dir']}/{config['s3_read_data']['eval_prompts_dir']}\")\n",
    "# Iterate through each LLM as a judge and each evaluation criterion\n",
    "for llm_info in model_eval_subjective_info.get('judge_panel_list', None):\n",
    "    model_id = llm_info['model_id']\n",
    "    eval_prompt_template_fname: str = f\"{llm_info.get('eval_prompt_template_name_max_voting', None)}.txt\"\n",
    "    eval_prompt_template_dir = llm_info.get('eval_prompt_template_dir', None)\n",
    "    eval_prompt_template_path: str = os.path.join(eval_prompts_dir, eval_prompt_template_dir, eval_prompt_template_fname)\n",
    "    logger.info(f\"evaluation prompt template file path being used for {model_id}: {eval_prompt_template_path}\")\n",
    "    logger.info(f\"evaluation prompt template file name: {eval_prompt_template_fname}\")\n",
    "    try:\n",
    "        eval_prompt_template = Path(eval_prompt_template_path).read_text()\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {eval_prompt_template_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluation prompt template being used: {eval_prompt_template}\")\n",
    "    eval_instructions_fname: str = next((rule for rule in config['s3_read_data']['eval_instructions_files'] if method_name in rule), None)\n",
    "    rules = Path(os.path.join(eval_prompts_dir, eval_instructions_fname)).read_text()\n",
    "    logger.info(f\"rules: {rules}\")\n",
    "    column_name = f\"{model_id}_{method_name}_eval_prompt\"\n",
    "    logger.info(f\"column_name: {column_name}\")\n",
    "    # prepare the prompt payloads with the existing candidate model responses, ground truth (if any), \n",
    "    # prompt template for the given criteria and rules to be used\n",
    "    df_per_inference[column_name] = df_per_inference.apply(\n",
    "        lambda r: prepare_eval_prompts(\n",
    "            eval_prompt_template,\n",
    "            r['completion'],\n",
    "            rules,\n",
    "            r['prompt'],\n",
    "            r['ground_truth']\n",
    "        ),\n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_eval_prompt_payloads = csv_buffer.getvalue()\n",
    "eval_prompt_payloads_for_inference = os.path.join(METRICS_DIR, PROCESSED_EVAL_PROMPT_PAYLOADS)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(df_per_inference_with_eval_prompt_payloads, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, PROCESSED_EVAL_PROMPT_PAYLOADS)\n",
    "logger.info(f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{eval_prompt_payloads_for_inference}\")\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "eval_records_list = json.loads(df_per_inference.to_json(orient='records'))\n",
    "logger.info(f\"eval_records_list has {len(eval_records_list)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LLM as a Judge Evaluations\n",
    "---\n",
    "\n",
    "In this portion of the step, FMBench performs the following actions:\n",
    "\n",
    "1. If the method of evaluation is `Max Voting`, then in that case we suppose that a ground truth to the question from the context or task is pre existing in the dataset. We use the LLM panel of judges (in this case 3 judges), to give a verdict on whether the `answer` from the candidate models during inference is `correct` or `incorrect`. If the response is correct, then it gives it a `correct` and if not, then `incorrect`.\n",
    "\n",
    "1. If the method of evaluation is `Average Pooling`, then in that case we suppose that the completion from the candidate models are supposed to be evlauated on a more subjective criteria rather than just deciding whether it is correct or incorrect compared to the ground truth. In this case, the average pooling prompt templates are used by the Judge Panel to give a rating out of 1-5 to each model completion on different criteria, such as relevancy, helpfulness, correctness, and so on.\n",
    "\n",
    "1. Each model response is given in a JSON structure which is further used for downstream analytics, to decide the comparision of evaluation results between different model candidates and more.\n",
    "\n",
    "***This step takes about ~6 minutes to complete. Model completion time depends on the PoLL models being used. `Llama3-70b`, `Cohere command-r-v1` and `claude 3 haiku` were used for this example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the llm as a judge panel list\n",
    "judge_panel_list = model_eval_subjective_info.get('judge_panel_list', None)\n",
    "logger.info(f\"The judge panel list contains {len(judge_panel_list)} judges: {judge_panel_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"~Panel of LLM evaluators are going to start evaluating responses. This might take a couple of minutes~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_record_for_lexical_similarity(record):\n",
    "    \"\"\"\n",
    "    Given a record, this function calculates the average of token set ratio and Levenshtein distance,\n",
    "    and checks the cosine similarity. If the cosine similarity meets or exceeds the specified threshold,\n",
    "    or if the average of token set ratio and Levenshtein distance meets or exceeds the specified threshold,\n",
    "    the completion is correct and an explanation is given without going through an LLM evaluator\n",
    "    \"\"\"\n",
    "    try:\n",
    "        token_set_ratio = record['token_set_ratio_value']\n",
    "        levenshtein_ratio = record['levenshtein_distance']\n",
    "        cosine_similarity_score = record['cosine_similarity_score']\n",
    "\n",
    "        is_correct = (cosine_similarity_score >= config['report']['cosine_similarity_threshold'] or \n",
    "                      token_set_ratio >= config['report']['token_set_ratio_threshold'] or\n",
    "                      levenshtein_ratio >= config['report']['levenshtein_distance_threshold'])\n",
    "\n",
    "        if is_correct:\n",
    "            verdict = \"correct\"\n",
    "            explanation = (\n",
    "                f\"Lexical match check passed with: \"\n",
    "                f\"Token set ratio = {token_set_ratio * 100}%, \"\n",
    "                f\"Levenshtein similarity match = {levenshtein_ratio * 100}%, \"\n",
    "                f\"Cosine similarity = {cosine_similarity_score:.3f}, not going through a panel of LLM evaluator.\"\n",
    "            )\n",
    "            record.update({\n",
    "                'candidate_model': record['endpoint_name'],\n",
    "                'candidate_model_response': record['completion'],\n",
    "                'completion': f'{{\\n  \"verdict\": \"{verdict}\",\\n  \"explanation\": \"{explanation}\"\\n}}',\n",
    "            })\n",
    "        else:\n",
    "            is_correct = False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred while checking for text similarity: {e}\")\n",
    "        record = None\n",
    "        is_correct = False\n",
    "    return record, is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = model_eval_subjective_info.get('run_parallel_inference_count', 5)\n",
    "list_of_lists = [eval_records_list[i * n:(i + 1) * n] for i in range((len(eval_records_list) + n - 1) // n)]\n",
    "resp_list = []\n",
    "erroneous_count = 0\n",
    "st = time.perf_counter()\n",
    "\n",
    "# Iterate over the judge panel and sublists\n",
    "for judge_panelist_info in judge_panel_list:\n",
    "    logger.info(f\"============Running inference for judge panelist {judge_panelist_info['model_id']} for {method_name} ============\")\n",
    "    for idx, sublist in enumerate(list_of_lists):\n",
    "        model_id = judge_panelist_info['model_id']\n",
    "        logger.info(f\"Getting inference for list {idx + 1}/{len(list_of_lists)}, size of list={len(sublist)}\")\n",
    "        non_matching_records = []\n",
    "\n",
    "        for record in sublist:\n",
    "            # First, check if the current content of the record matches the threshold for \n",
    "            # token set ratio or cosine similarity\n",
    "            processed_record, is_correct = process_record_for_lexical_similarity(record)\n",
    "            if is_correct:\n",
    "                resp_list.append(processed_record)\n",
    "            else:\n",
    "                non_matching_records.append(record)\n",
    "        try:\n",
    "            if non_matching_records:\n",
    "                # Run inference in parallel for non-matching records\n",
    "                resp_list.extend(ray.get([async_run_eval.remote(i + 1, len(non_matching_records), record, model_id, method_name, record['uuid'])\n",
    "                                   for i, record in enumerate(non_matching_records)]))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing list {idx + 1}/{len(list_of_lists)}: {e}\")\n",
    "            erroneous_count += 1\n",
    "\n",
    "    # Sleep for two seconds before moving on to the next model\n",
    "    logger.info(f\"~Sleeping for one second before the next Panel of LLM evaluates the responses~\")\n",
    "    time.sleep(1)\n",
    "\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"Total elapsed time for inference: {elapsed_time:.2f} seconds\")\n",
    "logger.info(f\"Total erroneous lists: {erroneous_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send all Panel of LLM evaluator responses to S3 as `JSON` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sanitize_filename(text):\n",
    "    return \"\".join([c if c.isalnum() else \"_\" for c in text])\n",
    "\n",
    "\n",
    "# Collect all of the panel of LLM evals and send them all as JSON files\n",
    "# to s3\n",
    "if resp_list:\n",
    "    save_s3_list = []\n",
    "\n",
    "    try:\n",
    "        for resp in resp_list:\n",
    "            llm_eval_response = json.dumps(resp, indent=2)\n",
    "            candidate_model_id = resp.get('candidate_model', None)\n",
    "            # Extract a few words from the poll eval response to append to the file name\n",
    "            response_excerpt = \" \".join(resp.get('candidate_model_response', \"\").split()[:5])\n",
    "            sanitized_response_excerpt = sanitize_filename(response_excerpt)\n",
    "            timestamp = time.time()\n",
    "            llm_eval_json_fname = f\"{candidate_model_id}_{timestamp}_{sanitized_response_excerpt}.json\"\n",
    "            response_s3_path = os.path.join(METRICS_PER_POLL_EVAL_DIR, llm_eval_json_fname)\n",
    "            logger.info(f\"sending llm eval response to s3 path prefix: {response_s3_path}\")\n",
    "            save_s3_list.append((llm_eval_response,\n",
    "                                config['aws']['bucket'],\n",
    "                                \"\",\n",
    "                                METRICS_PER_POLL_EVAL_DIR,\n",
    "                                llm_eval_json_fname))\n",
    "        write_multiple_to_s3(save_s3_list)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing or writing to S3: {e}\")\n",
    "else:\n",
    "    logger.info(\"No responses to write to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Perform downstream analytical tasks on each PoLL evaluation result\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the results list into a dataframe for easy analytics\n",
    "df_eval_results = pd.DataFrame(resp_list)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how='all')\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parse out the completion from LLM as a judge and column bind\n",
    "# the fields of the dictionary to the original results dataframe\n",
    "df_eval_results_only = df_eval_results['completion'].apply(parse_as_json).apply(pd.Series)\n",
    "df_eval_results_only.dropna(axis=1, how='all')\n",
    "df_eval_results = pd.concat([df_eval_results, df_eval_results_only], axis=1)\n",
    "df_eval_results.rename(columns={'model_id': 'judge_model_id'}, inplace=True)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how='all')\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send the raw results as a csv file to the S3 bucket\n",
    "csv_buffer = io.StringIO()\n",
    "df_eval_results.to_csv(csv_buffer, index=False)\n",
    "eval_llm_as_a_judge_results = csv_buffer.getvalue()\n",
    "eval_results_csv_fpath = os.path.join(METRICS_DIR, MODEL_EVAL_COMPLETIONS_CSV)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(eval_llm_as_a_judge_results, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, MODEL_EVAL_COMPLETIONS_CSV)\n",
    "logger.info(f\"Per PoLL model responses saved as a csv to s3://{BUCKET_NAME}/{eval_results_csv_fpath}\")\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_eval_results['verdict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_eval_results['verdict'].isin(['incorrect']).any()\n",
    "df_eval_results[df_eval_results['verdict'] == 'incorrect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the incorrect and correct responses to S3 separately in `CSV` files for downstream analytics for each model judge\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_incorrect_verdicts = pd.DataFrame()\n",
    "if df_eval_results['verdict'].isin(['incorrect']).any():\n",
    "    df_incorrect_verdicts = df_eval_results[df_eval_results['verdict'] == 'incorrect']\n",
    "    csv_buffer = io.StringIO()\n",
    "    df_incorrect_verdicts.to_csv(csv_buffer, index=False)\n",
    "    incorrect_verdict_responses = csv_buffer.getvalue()\n",
    "    incorrect_verdict_responses_fpath = os.path.join(METRICS_DIR, INCORRECT_VERDICT_RESPONSES_FILE)  # Define full S3 path\n",
    "    write_to_s3(incorrect_verdict_responses, BUCKET_NAME, \"\", \n",
    "                METRICS_DIR, INCORRECT_VERDICT_RESPONSES_FILE)\n",
    "    logger.info(f\"Incorrect verdict responses from the PoLL evaluation sent to s3://{BUCKET_NAME}/{incorrect_verdict_responses_fpath}\")\n",
    "df_incorrect_verdicts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Number of incorrect responses in total: {df_incorrect_verdicts.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_correct_verdicts = pd.DataFrame() \n",
    "if df_eval_results['verdict'].isin(['correct']).any():\n",
    "    # save the correct responses from the model evaluation process in a csv file to analyze where each model went right\n",
    "    df_correct_verdicts = df_eval_results[df_eval_results['verdict'] == 'correct']\n",
    "    csv_buffer = io.StringIO()\n",
    "    df_correct_verdicts.to_csv(csv_buffer, index=False)\n",
    "    correct_verdict_responses = csv_buffer.getvalue()\n",
    "    correct_verdict_responses_fpath = os.path.join(METRICS_DIR, CORRECT_VERDICT_RESPONSES_FILE)  # Define full S3 path\n",
    "    write_to_s3(correct_verdict_responses, BUCKET_NAME, \"\", \n",
    "                METRICS_DIR, CORRECT_VERDICT_RESPONSES_FILE)\n",
    "    logger.info(f\"Correct verdict responses from the PoLL evaluation sent to s3://{BUCKET_NAME}/{correct_verdict_responses_fpath}\")\n",
    "df_correct_verdicts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Number of correct responses in total: {df_correct_verdicts.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "judge_model_ids = df_eval_results['judge_model_id'].unique()\n",
    "for judge_model_id in judge_model_ids:\n",
    "    # Filter for incorrect and correct verdicts\n",
    "    df_judge_incorrect = df_incorrect_verdicts[df_incorrect_verdicts['judge_model_id'] == judge_model_id]\n",
    "    df_judge_correct = df_correct_verdicts[df_correct_verdicts['judge_model_id'] == judge_model_id]\n",
    "\n",
    "    # Save incorrect verdicts for each judge model for further analysis to s3 \n",
    "    if not df_judge_incorrect.empty:\n",
    "        txt_buffer_incorrect = io.StringIO()\n",
    "        for index, row in df_judge_incorrect.iterrows():\n",
    "            txt_buffer_incorrect.write(f\"candidate model: {row['candidate_model']}\\ncandidate model response: {row['candidate_model_response']}\\nground truth: {row['ground_truth']}\\nverdict and explanation: {row['completion']}\\n\\n\")\n",
    "        incorrect_verdict_responses = txt_buffer_incorrect.getvalue()\n",
    "        incorrect_verdict_responses_fpath = os.path.join(METRICS_DIR, f\"{judge_model_id}_incorrect_verdicts_evaluation.txt\")\n",
    "        write_to_s3(incorrect_verdict_responses, BUCKET_NAME, \"\", METRICS_DIR, f\"{judge_model_id}_incorrect_verdicts_evaluation.txt\")\n",
    "        logger.info(f\"Incorrect verdict responses for judge {judge_model_id} saved to s3://{BUCKET_NAME}/{incorrect_verdict_responses_fpath}\")\n",
    "\n",
    "    # Save correct verdicts for each judge model for further analysis to s3 \n",
    "    if not df_judge_correct.empty:\n",
    "        txt_buffer_correct = io.StringIO()\n",
    "        for index, row in df_judge_correct.iterrows():\n",
    "            txt_buffer_correct.write(f\"candidate model: {row['candidate_model']}\\ncandidate model response: {row['candidate_model_response']}\\nground truth: {row['ground_truth']}\\nverdict and explanation: {row['completion']}\\n\\n\")\n",
    "        correct_verdict_responses = txt_buffer_correct.getvalue()\n",
    "        correct_verdict_responses_fpath = os.path.join(METRICS_DIR, f\"{judge_model_id}_correct_verdicts_evaluation.txt\")\n",
    "        write_to_s3(correct_verdict_responses, BUCKET_NAME, \"\", METRICS_DIR, f\"{judge_model_id}_correct_verdicts_evaluation.txt\")\n",
    "        logger.info(f\"Correct verdict responses for judge {judge_model_id} saved to s3://{BUCKET_NAME}/{correct_verdict_responses_fpath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for each panel of LLM evaluator's verdict count on the dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "panel_summary_responses_df = df_eval_results.groupby(['judge_model_id', 'candidate_model', 'verdict']).agg(\n",
    "    count=('verdict', 'size'),\n",
    "    mean_cosine_similarity=('cosine_similarity_score', 'mean'),\n",
    "    mean_levenshtein_distance=('levenshtein_distance', 'mean'),\n",
    "    mean_token_set_ratio=('token_set_ratio_value', 'mean')\n",
    ").unstack(fill_value=0).stack().reset_index()\n",
    "\n",
    "# Save the raw results to a CSV file and upload to S3\n",
    "csv_buffer = io.StringIO()\n",
    "panel_summary_responses_df.to_csv(csv_buffer, index=False)\n",
    "panel_summary_responses = csv_buffer.getvalue()\n",
    "llm_as_a_judge_per_eval_summary_fpath = os.path.join(METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES)\n",
    "\n",
    "write_to_s3(panel_summary_responses, BUCKET_NAME, \"\", METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES)\n",
    "logger.info(f\"Summary on each eval (max voting/average pooling) for each panel judge sent to s3://{BUCKET_NAME}/{llm_as_a_judge_per_eval_summary_fpath}\")\n",
    "panel_summary_responses_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the overall accuracy of each model scored by the PoLL\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "per_panel_judgement_result_df = panel_summary_responses_df.pivot_table(\n",
    "    index=['candidate_model', 'judge_model_id'],\n",
    "    columns='verdict',\n",
    "    values='count',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Ensure 'correct' and 'incorrect' columns exist\n",
    "if 'correct' not in per_panel_judgement_result_df.columns:\n",
    "    per_panel_judgement_result_df['correct'] = 0\n",
    "if 'incorrect' not in per_panel_judgement_result_df.columns:\n",
    "    per_panel_judgement_result_df['incorrect'] = 0\n",
    "\n",
    "# Calculate accuracy and error rate\n",
    "per_panel_judgement_result_df['accuracy'] = (\n",
    "    per_panel_judgement_result_df.apply(\n",
    "        lambda row: 100 if row['incorrect'] == 0 else round(row['correct'] / (row['correct'] + row['incorrect']), 2) * 100,\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "per_panel_judgement_result_df['error_rate'] = (\n",
    "    per_panel_judgement_result_df.apply(\n",
    "        lambda row: 0 if row['incorrect'] == 0 else round(row['incorrect'] / (row['correct'] + row['incorrect']), 2) * 100,\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "per_panel_judgement_result_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_cosine_similarity = df_eval_results.groupby('candidate_model')['cosine_similarity_score'].mean().reset_index().rename(columns={'cosine_similarity_score': 'mean_cosine_similarity'})\n",
    "mean_levenshtein_distance = df_eval_results.groupby('candidate_model')['levenshtein_distance'].mean().reset_index().rename(columns={'levenshtein_distance': 'mean_levenshtein_distance'})\n",
    "mean_token_set_ratio = df_eval_results.groupby('candidate_model')['token_set_ratio_value'].mean().reset_index().rename(columns={'token_set_ratio_value': 'mean_token_set_ratio_value'})\n",
    "overall_accuracy_grouped_panel_df = per_panel_judgement_result_df.groupby('candidate_model')[['accuracy', 'error_rate']].mean().reset_index()\n",
    "overall_accuracy_grouped_panel_df = (\n",
    "    pd.merge(mean_cosine_similarity, overall_accuracy_grouped_panel_df, on='candidate_model')\n",
    "      .merge(mean_levenshtein_distance, on='candidate_model')\n",
    "      .merge(mean_token_set_ratio, on='candidate_model')\n",
    "      .sort_values(by='accuracy', ascending=False)\n",
    ")\n",
    "overall_accuracy_grouped_panel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send the accuracy metrics to s3\n",
    "csv_buffer = io.StringIO()\n",
    "overall_accuracy_grouped_panel_df.to_csv(csv_buffer, index=False)\n",
    "overall_panel_result = csv_buffer.getvalue()\n",
    "overall_panel_accuracy_metrics_fpath = os.path.join(METRICS_DIR, PER_MODEL_ACCURACY_POLL)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(overall_panel_result, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, PER_MODEL_ACCURACY_POLL)\n",
    "logger.info(f\"Overall accuracy and error rates results of each model sent to s3://{BUCKET_NAME}/{overall_panel_accuracy_metrics_fpath}\")\n",
    "overall_accuracy_grouped_panel_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get accuracy statements\n",
    "# Rank models by accuracy\n",
    "ranked_models = overall_accuracy_grouped_panel_df.sort_values(by='accuracy', ascending=False)\n",
    "highest_accuracy = ranked_models['accuracy'].max()\n",
    "\n",
    "# Group models with the highest accuracy\n",
    "top_performers = ranked_models[ranked_models['accuracy'] == highest_accuracy]\n",
    "other_models = ranked_models[ranked_models['accuracy'] < highest_accuracy]\n",
    "final_ranking = pd.concat([top_performers, other_models])\n",
    "unique_judge_model_ids = per_panel_judgement_result_df['judge_model_id'].unique()\n",
    "PoLL_model_ids = ', '.join(map(str, unique_judge_model_ids))\n",
    "top_performing_model_ids = ', '.join(top_performers['candidate_model'].tolist())\n",
    "\n",
    "# cosine similarity score data\n",
    "highest_cosine_model = final_ranking.loc[final_ranking['mean_cosine_similarity'].idxmax()]\n",
    "highest_cosine_model_name = highest_cosine_model['candidate_model']\n",
    "highest_cosine_similarity = highest_cosine_model['mean_cosine_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if other_models.empty:\n",
    "    other_models = f\"All models performed the same with an accuracy of {highest_accuracy:.2f}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_statement = MAX_VOTING_RESULT_STATEMENT.format(\n",
    "    judge_model_ids=PoLL_model_ids,\n",
    "    highest_accuracy=highest_accuracy,\n",
    "    top_models=top_performers.to_string(index=False),\n",
    "    highest_cosine_similarity=round(highest_cosine_similarity, 4),\n",
    "    top_cosine_similarity_model=highest_cosine_model_name,\n",
    "    ranked_models=other_models,\n",
    "    top_performing_model_ids=top_performing_model_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send the overall accuracy report to s3\n",
    "txt_buffer = io.StringIO()\n",
    "txt_buffer.write(accuracy_statement)\n",
    "poll_txt_file_content = txt_buffer.getvalue()\n",
    "overall_panel_accuracy_metrics_fpath = os.path.join(METRICS_DIR, OVERALL_POLL_REPORT)\n",
    "write_to_s3(poll_txt_file_content, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, OVERALL_POLL_REPORT)\n",
    "logger.info(f\"Overall accuracy and error rates results of each model sent to s3://{BUCKET_NAME}/{overall_panel_accuracy_metrics_fpath}\")\n",
    "print(accuracy_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final analysis of incorrect responses to spot trends, patterns & observations\n",
    "---\n",
    "\n",
    "In this portion of the step, we do the following:\n",
    "\n",
    "1. We add another layer of a final LLM that goes above all of the verdicts, questions, ground truth, and then gives a set of trends and observations as to how the candidate models performed and what to do to improve their performance.\n",
    "\n",
    "1. This makes scaling efficient. After getting a quick `vibe check` from the PoLL evaluation results, users can read through the thorough report to get a sense of what to change to make a particular model perform better than the rest and make an efficient evaluation decision/run this evaluation pipeline again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write all explanations to a file and send to S3\n",
    "explanations_txt_buffer = io.StringIO()\n",
    "for index, row in df_eval_results.iterrows():\n",
    "    explanations_txt_buffer.write(f\"candidate model: {row['candidate_model']}\\ncandidate model response: {row['candidate_model_response']}\\nground truth: {row['ground_truth']}\\nverdict and explanation: {row['completion']}\\n\\n\")\n",
    "\n",
    "explanations_txt_file_content = explanations_txt_buffer.getvalue()\n",
    "explanations_fpath = os.path.join(METRICS_DIR, ALL_EVALUATIONS_IN_TXT)\n",
    "write_to_s3(explanations_txt_file_content, BUCKET_NAME, \"\", METRICS_DIR, ALL_EVALUATIONS_IN_TXT)\n",
    "logger.info(f\"All text eval content from the llm judge panelists sent to s3://{BUCKET_NAME}/{explanations_fpath}\")\n",
    "print(explanations_txt_file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open the prompt template and prepare it for inference\n",
    "with open(os.path.join(eval_prompts_dir, model_eval_subjective_info.get('final_evaluation_prompt_template', None)), 'r') as file:\n",
    "    final_summary_prompt = file.read()\n",
    "    processed_summary_eval_prompt: str = final_summary_prompt.format(context=explanations_txt_file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_evaluation_summarizer: str = model_eval_subjective_info.get('final_evaluation_summarizer', None)\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Trying to get a final summary from the inferences, verdicts and explanations\")\n",
    "    final_analysis = completion(\n",
    "      model=final_evaluation_summarizer,\n",
    "      messages=[{ \"content\": processed_summary_eval_prompt,\"role\": \"user\"}]\n",
    "    )\n",
    "\n",
    "    final_summary_buffer = io.StringIO()\n",
    "    final_summary_buffer.write(final_analysis['choices'][0]['message']['content'] + \"\\n\")\n",
    "    final_summary_content = final_summary_buffer.getvalue()\n",
    "    final_summary_fpath = os.path.join(METRICS_DIR, FINAL_ANALYSIS_REPORT)\n",
    "    write_to_s3(final_summary_content, BUCKET_NAME, \"\", METRICS_DIR, FINAL_ANALYSIS_REPORT)\n",
    "    logger.info(f\"Final summary analysis sent to s3://{BUCKET_NAME}/{final_summary_fpath}\")\n",
    "    print(final_summary_content)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not get the final summary analysis on the evaluations: {e}\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
