{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Evaluations on inferences generated by candidate models in the inference step, gather findings on quantitative metrics (such as _Cosine Similarity, levenshtein distance, and token set ratio_) and subjective metrics on various criteria using Max Voting & Average Pooling with PoLL (Panel of LLM Evaluators)\n",
    "\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of the solution focusses on getting evaluations on the quality of responses. It does so by gathering the following information and performing the steps below:\n",
    "\n",
    "- **Gets the inference request file that contains all inferences from the inference step**: This step first accesses and gets all the inference request file into a dataframe which contains the responses from the candidate models, ground truth (if any), and other information, such as the source payload file, concurrency level, etc.\n",
    "\n",
    "- **Generates quantitative metrics for evaluation**: Calculate quantitative metrics to measure similarity and accuracy, for example _Cosine Similarity, levenshtein distance, and token set ratio_. This helps in getting a quantitative overall score to the entire dataset in terms of which model generates outputs that are most similar and accurate to the ground truth (if any is provided). We use these metrics to build a hierarchy evaluation decision tree to move up to the next step of evaluation if the correctness of an answer is not obviously determined. \n",
    "    \n",
    "    The steps that are followed as a part of this evaluation hierarchy (for Max Voting) is as given below:\n",
    "    \n",
    "    1. For this, we check if either the _Cosine Similarity, levenshtein similarity, or token set ratio_ values exceed a given threshold, and if they do, we assume that the answer to the question is correct and do not parse it through the next step. This saves on latency, cost, and also acts as an evaluation filter.\n",
    "    \n",
    "    1. For the rest of the answers that are not obviously correct or do not have any semantic relation with the ground truth, we move to the next step in the hierarchical tree, which is using a panel of LLM evaluators.\n",
    "\n",
    "- **Uses a _Panel of LLM Evaluator_ approach to get subjective evaluations**: Refer to this [paper](https://arxiv.org/pdf/2404.18796). We use the following ways to evaluate the responses from the `candidate models` (models used to generate inferences)\n",
    "\n",
    "    1. **Max Voting**: When a dataset provides a ground truth, we use a technique called `Max Voting`. Here, we use PoLL, or a panel of LLM evaluators, from different model families to evaluate each candidate model's response based on whether it generates a `correct` or an `incorrect` answer simply based on its comparison with the ground truth. Using models from different families as a PoLL, increases it's evaluation ability to be close to that of a human evaluation, and eliminates intra model bias during the evaluation process.\n",
    "    \n",
    "    2. **Average Pooling**: When a dataset does not provide a ground truth, or if a task being evaluated needs to be given deeper subjective level judgements, that is when we use `Average Pooling`. In this, we use specific subjective level criteria and then evaluate the candidate model responses on a scale of 1-5 for each PoLL. Using this, we get an average score on each criteria and then can evaluate how each candidate model was scored based on the PoLL evaluations.\n",
    "    \n",
    "FMBench uses this approach of PoLL to eradicate intra model bias by using models as judges from different model families. This brings the evaluation results closer to that of a human evaluation, makes the evaluation process more streamlined, consistent across all the responses, and reduces the latency and cost of evaluating the candidate models over time.\n",
    "    \n",
    "***All evaluations are generated in a JSON format for further downstream analytics on the evaluation results***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import ray\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from fuzzywuzzy import fuzz\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from numpy.linalg import norm\n",
    "from litellm import completion\n",
    "from typing import List, Optional, Dict\n",
    "from difflib import SequenceMatcher as SM\n",
    "import importlib.resources as pkg_resources\n",
    "from fmbench import __version__ as fmbench_version\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger to get logs\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the ray service to run async calls in parallel to bedrock easily\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"CONFIG_FILE={CONFIG_FILE}\")\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the associated pricing config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "pricing_file_path: str = config['pricing'] \n",
    "\n",
    "# initialize the pricing config file to None\n",
    "pricing_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files('fmbench'), 'configs')\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "pricing_module = Path(config['pricing'])\n",
    "logger.info(f\"pricing config provided for inference from this model is --> {pricing_module}\")\n",
    "pricing_file_path = os.path.join(config_dir, pricing_module)\n",
    "logger.info(f\"pricing config file path is --> {pricing_file_path}\")\n",
    "\n",
    "pricing_config = load_config(pricing_file_path)\n",
    "logger.info(f\"pricing config file recorded: {json.dumps(pricing_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model evaluation information\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "model_eval_fpath: str = config['model_evaluations'] \n",
    "\n",
    "# initialize the pricing config file to None\n",
    "eval_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files('fmbench'), 'configs')\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "eval_module = Path(config['model_evaluations'])\n",
    "logger.info(f\"eval config provided for evaluation --> {eval_module}\")\n",
    "eval_file_path = os.path.join(config_dir, eval_module)\n",
    "logger.info(f\"eval config file path is --> {eval_file_path}\")\n",
    "\n",
    "# eval_config = load_config(eval_file_path).format(method_name=config['method_name'])\n",
    "with open(eval_file_path, 'r') as file:\n",
    "    model_eval_info = file.read()\n",
    "    model_eval_formatted_content = model_eval_info.format(method_name=config['PoLL_Composition_and_Voting'].get('method', None),\n",
    "                                                         ground_truth=config['PoLL_Composition_and_Voting'].get('ground_truth_col', None), \n",
    "                                                         criteria=config['PoLL_Composition_and_Voting'].get('subjective_eval_criteria', None))\n",
    "    eval_config = yaml.safe_load(model_eval_formatted_content)\n",
    "logger.info(f\"eval config file recorded: {json.dumps(eval_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug is True:\n",
    "    metrics_path_file: str = os.path.join(\"..\", \"..\", METADATA_DIR, METRICS_PATH_FNAME)\n",
    "else:\n",
    "    metrics_path_file: str = os.path.join(METADATA_DIR, METRICS_PATH_FNAME)\n",
    "logger.info(f\"cwd={os.getcwd()}, METADATA_DIR={METADATA_DIR}, METRICS_PATH_FNAME={METRICS_PATH_FNAME}, metrics_path_file={metrics_path_file}\")\n",
    "METRICS_DIR: str = Path(metrics_path_file).read_text().strip()\n",
    "logger.info(f\"metrics_path_file={metrics_path_file}, METRICS_DIR={METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file_path: str = os.path.join(METRICS_DIR, config[\"report\"][\"per_inference_request_file\"])\n",
    "file_path='fmbench-bedrock-anthropic-models-fmbench-1-us-east-1-role/data/metrics/yyyy=2024/mm=07/dd=23/hh=22/mm=19/per_inference_request_results.csv'\n",
    "logger.info(f\"File path containing the metrics per inference folder --> {file_path}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    file_content = get_s3_object(config['aws']['bucket'], file_path)\n",
    "    # Use pandas to read the CSV content\n",
    "    df_per_inference = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(f\"{file_path} read into dataframe of shape {df_per_inference.shape}, \"\n",
    "                f\"cols={df_per_inference.columns}\")\n",
    "    logger.info(f\"{file_path} contains results for the following endpoints={df_per_inference.endpoint_name.unique()}\")\n",
    "    logger.info(df_per_inference.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Going to be using this inference file to generate evaluations on -> {df_per_inference.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between prompt token length and inference latency for different instances and concurrency levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Information on the inference file being used for evaluations: {df_per_inference.latency.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the `sentence-transformers/all-mpnet-base-v2` embeddings model to calculate the _Cosine Similarity_ scores \n",
    "---\n",
    "\n",
    "This portion of the evaluation step does as follows:\n",
    "\n",
    "1. Uses the `sentence-transformers/all-mpnet-base-v2` model from Hugging Face. This is a sentence-transformers model. It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "1. Use the embeddings model to get quantitative metrics from the inferences. This helps to get a similarity score between the ground truth answers from a dataset if any are given and the actual responses from the model received during inference.\n",
    "\n",
    "1. If no ground truth is provided, cosine similarity is calculated between the response and the content provided to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the quantitiative evaluation information from the config file, such as the embeddings model\n",
    "# to be used\n",
    "embeddings_model_quantitative_info: Dict = eval_config['model_evaluations']['quantitative_eval_info']\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    This function loads the sentence-transformers model based on the provided model ID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model=None\n",
    "        model_id = embeddings_model_quantitative_info['embeddings_model_id'].get('model_id', None)\n",
    "        if model_id:\n",
    "            model = SentenceTransformer(model_id)\n",
    "        else:\n",
    "            raise ValueError(\"Model ID is not provided or invalid in the configuration.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"The SentenceTransformer embeddings model could not be loaded: {e}\")\n",
    "        model=None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the embeddings model to calculate the cosine similarity scores\n",
    "model = load_model()\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between two texts. In this case, \n",
    "    the cosine similarity is the comparison between the ground truth in the given dataset\n",
    "    and the candidate model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cosine: float = None\n",
    "        # returns the embedding for a given text using the sentence-transformers model.\n",
    "        A = model.encode([text1])[0]\n",
    "        B = model.encode([text2])[0]\n",
    "        cosine = dot(A, B) / (norm(A) * norm(B))\n",
    "        logger.info(f\"Calculating the cosine similarity score, current score: {cosine}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cosine similarity was not calculated at this iteration: {e}\")\n",
    "        cosine=None\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the method that is being used to evaluate the content (which is either \n",
    "# max voting or average pooling)\n",
    "model_eval_subjective_info: List[Dict] = eval_config['model_evaluations']['subjective_eval_info']\n",
    "method_name: str = eval_config['model_evaluations']['PoLL_Composition_and_Voting'].get('method', None)\n",
    "logger.info(f\"The evaluation method FMBench is going to use to evaluate different model responses: {method_name}\")\n",
    "logger.info(f\"judge panel being used to evaluate model responses: {model_eval_subjective_info.get('judge_panel_list', None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"~Creating embeddings of all candidate model responses now. This might take a 1-2 minutes~\")\n",
    "\n",
    "# calculate the quantitative metrics if evaluation is set to max voting\n",
    "if method_name == \"max_voting\":\n",
    "    logger.info(f\"ground truth column found: {eval_config['model_evaluations'].get('ground_truth_col')}, calculating cosine similarity scores\")\n",
    "    # Assuming df_per_inference is your DataFrame\n",
    "    df_per_inference['cosine_similarity_score'] = df_per_inference.apply(\n",
    "        lambda row: calculate_cosine_similarity(row['completion'], row['ground_truth']), axis=1\n",
    "    )\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluations: Hierarchical Flow\n",
    "--- \n",
    "\n",
    "For this portion of the step, we start with the model evaluation process. Here we perform the following steps:\n",
    "\n",
    "1. Check for the lexical match/similarity between the ground truth (if any) and the answer.\n",
    "\n",
    "1. Compute the similarity score using three main quantitative metrics: Cosine similarity score, Levenshtein similarity, and Token set ratio. If the thresholds of any of these are passed, the model evaluation is complete and answer is correct. \n",
    "\n",
    "1. If the answer is not obvious, i.e., none of the three thresholds of quantitative evaluations are met, then the data moves to the Panel of LLM Evaluators for a further deep dive into the evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Evaluation Part 1: Lexical Match & Cosine Similarity Score Accuracy Evaluation Filter\n",
    "---\n",
    "\n",
    "Before having the Panel of LLM Evaluators evaluate each candidate model response, we pass those responses through a filtering step. In this step we use a threshold for a `Lexical match`, `Cosine Similarity`, and `Levenshtein Similarity` scores to define whether that answer is correct without having an LLM evaluate it. The thresholds for correctness is defined in the configuration files. \n",
    "\n",
    "The reason to do this is to make the evaluation process more like a hierarchy of checks, to make sure each and every candidate model response is evaluated appropriately. Additionally, filter steps to check for these scores to determine whether a candidate model reponse is correct, will narrow down the evaluation checks for the PoLL reducing the time and cost to complete all evaluations. This is specific to the `Ground Truth based approach`. \n",
    "\n",
    "For the lexical match, we use the `fuzzy` match approach `token_set_ratio` library to determine what percent of the two texts are similar.\n",
    "\n",
    "**Note**: `Token_set_ratio` algorithm tokenizes both input strings, removes duplicate tokens, and calculates the similarity score based on the intersection and union of the token sets. It captures the essence of the strings’ content rather than their specific order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_token_set_ratio(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the partial token match or fuzz ratio between two strings.\n",
    "    If the fuzz ratio exceeds the threshold and the cosine similarity matches or exceeds the threshold, \n",
    "    then the answer is correct and it is not evaluated using a judge. If it is not, then it\n",
    "    is parsed through the PoLL process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        token_set_ratio: float = None\n",
    "        if text1 and text2:\n",
    "            token_set_ratio = fuzz.token_set_ratio(text1, text2) / 100.0\n",
    "        else:\n",
    "            token_set_ratio=None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in calculating token set ratio: {e}\")\n",
    "        token_set_ratio=None\n",
    "    return token_set_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein distance algorithm\n",
    "---\n",
    "In information theory, linguistics, and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. The Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def levenshtein_distance(s: str, t: str):\n",
    "    \"\"\"\n",
    "    Here, we use Dynamic Programming (DP) to compute the levenshtein distance\n",
    "    between two strings\n",
    "    \"\"\"\n",
    "    # Initialize lengths of both strings\n",
    "    m, n = len(s), len(t)\n",
    "\n",
    "    # Ensure s is the longer string\n",
    "    if m < n:\n",
    "        s, t = t, s\n",
    "        m, n = n, m\n",
    "\n",
    "    # Initialize the distance matrix with dimensions (m+1) x (n+1)\n",
    "    d = [list(range(n + 1))] + [[i] + [0] * n for i in range(1, m + 1)]\n",
    "\n",
    "    # Populate the matrix\n",
    "    for j in range(1, n + 1):\n",
    "        for i in range(1, m + 1):\n",
    "            # If characters match, no cost is added\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                # Otherwise, take the minimum cost from insert, delete, or replace operations\n",
    "                d[i][j] = min(d[i - 1][j], d[i][j - 1], d[i - 1][j - 1]) + 1\n",
    "    # Return the computed Levenshtein distance (bottom-right cell of the matrix)\n",
    "    return d[m][n]\n",
    "\n",
    "\n",
    "def calculate_levenshtein_distance(input_string: str, reference_string: str) -> float:\n",
    "    \"\"\"\n",
    "    In this function, we calculate the levenshtein distance between the input string (candidate model response) and \n",
    "    the reference string (which can be the ground truth or the context provided to answer the question).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        similarity: Optional[float]=None\n",
    "        distance = levenshtein_distance(input_string, reference_string)\n",
    "        max_length = max(len(input_string), len(reference_string))\n",
    "        similarity = 1 - (distance / max_length)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not compute the levenshtein similarity score: {e}\")\n",
    "        similarity=None\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # These are examples from the LongBench dataset for testing purposes\n",
    "# candidate_model_response: str = \"Both Sinofranchetia and Stauntonia are from the Lardizabalaceae family. This information is mentioned in the passages for both genera.\"\n",
    "# ground_truth: str = \"a genus of flowering plant in the Lardizabalaceae family\"\n",
    "# ratio = calculate_levenshtein_distance(candidate_model_response, ground_truth)\n",
    "# print(f\"ratio calculated: {ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the token set ratio for each row and add it as a new column\n",
    "# In this case, the ground truth is used as context to calculate the levenshtein distance\n",
    "# and the token set ratio if the ground truth is not provided\n",
    "\n",
    "# calculate the quantitative metrics if evaluation is set to max voting\n",
    "if method_name == \"max_voting\":\n",
    "    logger.info(f\"ground truth column is found: {eval_config['model_evaluations'].get('ground_truth_col')}, calculating token set ratio and levenshtein distance\")\n",
    "    df_per_inference = df_per_inference.assign(\n",
    "        token_set_ratio_value=lambda df: df.apply(lambda row: calculate_token_set_ratio(row['completion'], row['ground_truth']), axis=1),\n",
    "        levenshtein_distance=lambda df: df.apply(lambda row: calculate_levenshtein_distance(row['completion'], row['ground_truth']), axis=1)\n",
    "    )\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the all_metrics path to send the evaluation metrics to\n",
    "all_metrics_fpath: str = os.path.join(METRICS_DIR, config[\"report\"][\"all_metrics_file\"])\n",
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_cosine_similarity_scores_csv = csv_buffer.getvalue()\n",
    "inference_cosine_similarity_scores_s3_path = os.path.join(METRICS_DIR, PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(df_per_inference_with_cosine_similarity_scores_csv, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES)\n",
    "logger.info(f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{inference_cosine_similarity_scores_s3_path}\")\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Evaluation Part 2: Use _Panel of LLM Evaluators_ to get Subjective Evaluations on various evaluation criteria\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we run evaluations on the content generated by different candidate models. We use two main evaluation methods: `Max Voting` and `Average Pooling`. To eliminate intra-model bias, we address this by scoring answer correctness based not on a single judge, but instead on a panel composed of multiple evaluator models. Similar pooling techniques are used to reduce variance in human annotations by normalizing out both natural variation in human judgements caused by their own subjective biases as well as human error. We use the following two techniques:\n",
    "\n",
    "1. **Max Voting**: We use the PoLL to evaluate candidate model responses by checking its correctness compared to a provided ground truth answer in the dataset. We prompt each PoLL to evaluate and give the response in a JSON structure, giving a verdict on whether the response is correct or incorrect, and an explanation as to why that is. Using this, we can perform downstream analytics such as: \n",
    "\n",
    "    1. Calculate the overall accuracy of each model using the correct versus the (correct + incorrect) responses\n",
    "    \n",
    "    1. Calculate the `error rate` or frequency or incorrect responses\n",
    "    \n",
    "    1. Categorize the errors based on the explanations provided by the evaluators. Common categories might include misunderstanding the question, incomplete answers, factual inaccuracies\n",
    "    \n",
    "    1. Summary of overall correct/incorrect, and the best model based on the PoLL. Rank the models on Correctness versus Incorrectness.\n",
    "\n",
    "1. **Average Pooling**: We use the PoLL to rate the response of each candidate model on a more subjective criteria. Here, we have the candidate model responses rated on a scale of 1-5 based on the subjective criteria and then get an explanation to that. Using this we can do as follows:\n",
    "\n",
    "    1. Calculate the average score for each model across all questions to get an overall performance measure.\n",
    "    \n",
    "    1. Compute the standard deviation of the scores to understand the consistency of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the qualitative/subjective evaluation information from the config file to evaluate answers from different\n",
    "# endpoints on various criteria\n",
    "model_eval_subjective_info: Dict = eval_config['model_evaluations']['subjective_eval_info']\n",
    "eval_criteria_list = model_eval_subjective_info.get('eval_criteria', None)\n",
    "logger.info(f\"available llm as a judge evaluation information to use: {json.dumps(model_eval_subjective_info, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the inference parameters that the LLM judge panel will use while evaluating model candidate responses\n",
    "INFERENCE_PARAMETERS_LLM_PANEL: Dict = eval_config['model_evaluations']['subjective_eval_info'].get('inference_parameters', None)\n",
    "logger.info(f\"Inference parameters that LLM evaluators will use: {INFERENCE_PARAMETERS_LLM_PANEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_panel_of_llm_evaluation(model_id: str,\n",
    "                                  prompt: str):\n",
    "    \"\"\"\n",
    "    Get inference using LiteLLM. This function is called by each evaluator on the panel of \n",
    "    llm evaluators to get a response on a given prompt. This is in the case of where there is \n",
    "    max voting or average pooling enabled\n",
    "    \"\"\"\n",
    "    # represents the service name\n",
    "    logger.info(f\"get_inference, model_id={model_id}\")\n",
    "    service_name: str = \"bedrock\"\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name \n",
    "    # initialize the response dict\n",
    "    ret = dict(exception=None,\n",
    "               prompt=prompt,\n",
    "               completion=None,\n",
    "               completion_token_count=None,\n",
    "               prompt_token_count=None,\n",
    "               model_id=model_id)\n",
    "    body = ret['prompt']\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region\n",
    "    try:\n",
    "        # Represents calling the litellm completion/messaging api utilizing the completion/embeddings API\n",
    "        print(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(model=bedrock_model,\n",
    "                              messages=[{\"content\": body,\"role\": \"user\"}],\n",
    "                              temperature=INFERENCE_PARAMETERS_LLM_PANEL.get('temperature', 0.1),\n",
    "                              max_tokens=INFERENCE_PARAMETERS_LLM_PANEL.get('max_tokens', 100),\n",
    "                              caching=INFERENCE_PARAMETERS_LLM_PANEL.get('caching', False))\n",
    "        print(f\"response: {response}\")\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens        \n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret['exception'] = e\n",
    "    logger.info(f\"completion: {ret['completion']}\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_filename(s):\n",
    "    \"\"\"\n",
    "    convert a string to another string that can be used as a filename\n",
    "    i.e. remove white space and non-word chars\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"None\"\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespace with a single dash\n",
    "    s = re.sub(r\"\\s+\", '-', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_as_json(x: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Convert a string into a dictionary. Remove any\n",
    "    stray whitespaces which could break the json parsing\n",
    "    \"\"\"\n",
    "    d: Optional[Dict] = None\n",
    "    try:\n",
    "        x = x.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        d = json.loads(x)\n",
    "    except Exception as e:\n",
    "        print(f\"parse_as_json, error parsing string as json, string={x}\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.rename(columns={'completion': 'candidate_model_response'}, inplace=True)\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the evaluation prompt payloads\n",
    "---\n",
    "\n",
    "Here, the evaluation prompt template is used by the LLM judge to evaluate the answers on different criteria.\n",
    "This prompt template function uses a set of rules, prompt template, the answer, and ground truth (if any) in the\n",
    "evaluation solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_eval_prompts(eval_template: str,\n",
    "                         answer: str, \n",
    "                         rules: str, \n",
    "                         context: str, \n",
    "                         ground_truth: Optional[str], \n",
    "                         subjective_criteria: Optional[str]):\n",
    "    \"\"\"\n",
    "    This function prepares the evaluation prompts by preparing the standard eval prompt template\n",
    "    with the rules of a given subjective criteria, context, answer and ground truth (if any ground truth is provided)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        processed_eval_template: Optional[str] = None\n",
    "        processed_eval_template = eval_template.format(\n",
    "            rules=rules,\n",
    "            answer=answer,\n",
    "            context=context,\n",
    "            ground_truth=ground_truth, \n",
    "            subjective_criteria=subjective_criteria)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encountered while generating the evaluation prompt template: {e}\")\n",
    "        processed_eval_template=None\n",
    "    return processed_eval_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "# create the metrics directory that stores all of the json files containing evaluations from all Panel of LLM evaluators\n",
    "METRICS_PER_POLL_EVAL_DIR: str = os.path.join(METRICS_DIR, METRICS_PER_POLL_EVAL_DIR_NAME)\n",
    "_ = list(map(clear_dir, [METRICS_PER_POLL_EVAL_DIR]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_llm_evals(i: int, total: int, row: Dict,  model_id: str, eval_method_name: str, uuid: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Runs the evaluation for one row \n",
    "    The eval prompt is already available in the row dictionary\n",
    "    and we simply want to run the inference against the judge model.\n",
    "    The results are returned in a new dictionary that contains the model \n",
    "    response and some fields from the original dictionary\n",
    "    \"\"\"\n",
    "    try: \n",
    "        # save all the responses from the model in a dictionary\n",
    "        resp: Dict = {}\n",
    "        print(f\"run_eval, row {i}/{total}, judge_model_id={model_id}, candidate model={row['endpoint_name']}\")\n",
    "        # create the payload for model inference\n",
    "        prompt = row[f'{model_id}_{method_name}_eval_prompt']\n",
    "        # generate the evaluation on the data using the model judge\n",
    "        resp = get_panel_of_llm_evaluation(model_id, prompt)\n",
    "        # assign the completion from the candidate model to the `candidate_model_response`, \n",
    "        # and the actual evaluation will be contained in a field called `completion`\n",
    "        resp['candidate_model_response'] = row['candidate_model_response']\n",
    "        logger.info(f\"Panel of LLM evaluator {model_id} completion: {resp['completion']}\")\n",
    "        resp['candidate_model'] = row['endpoint_name']\n",
    "        if eval_method_name == \"max_voting\":\n",
    "            resp['cosine_similarity_score'] = row['cosine_similarity_score']\n",
    "            resp['levenshtein_distance'] = row['levenshtein_distance']\n",
    "            resp['token_set_ratio_value'] = row['token_set_ratio_value']\n",
    "            resp['token_set_ratio_value'] = row['token_set_ratio_value']\n",
    "        resp['payload_file'] = row['payload_file']\n",
    "        # if there is a ground truth (in case of max voting) or \n",
    "        # criteria name (in case of average pooline), include those in the json response\n",
    "        if 'ground_truth' in row:\n",
    "            resp['ground_truth'] = row['ground_truth']\n",
    "        if 'criteria_name' in row:\n",
    "            resp['criteria_name'] = row['criteria_name']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encountered while running evaluation: {e}\")\n",
    "        resp=None\n",
    "    return resp\n",
    "\n",
    "# we use Ray to parallize\n",
    "@ray.remote\n",
    "def async_run_eval(i: int, total: int, row: Dict, model_id: str, eval_method_name: str, uuid: str) -> Dict:\n",
    "    print(f\"async_run_eval, i={i}, total={total}, judge_model_info={model_id}, eval_method: {eval_method_name}, uuid: {uuid}\")\n",
    "    return run_llm_evals(i, total, row, model_id, eval_method_name, uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "df_per_inference_list = json.loads(df_per_inference.to_json(orient='records'))\n",
    "logger.info(f\"Total number of candidate models going to be evaluated: {len(df_per_inference_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare evaluation prompt templates\n",
    "---\n",
    "\n",
    "This portion of the step prepares the evaluation prompt templates that are used in the evaluation process of using `Max Voting` or `Average Pooling` using the PoLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_eval_subjective_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_eval_subjective_info.get('subjective_eval_criteria', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "eval_prompts_dir: str = Path(pkg_resources.files('fmbench'), f\"{config['s3_read_data']['prompt_template_dir']}/{config['s3_read_data']['eval_prompts_dir']}\")\n",
    "# Iterate through each LLM as a judge and each evaluation criterion\n",
    "for llm_info in model_eval_subjective_info.get('judge_panel_list', []):\n",
    "    model_id = llm_info['model_id']\n",
    "    method_name = eval_config['model_evaluations']['PoLL_Composition_and_Voting'].get(\"method\", None)\n",
    "    eval_prompt_template_fname = f\"{llm_info.get('eval_prompt_template_name', None)}.txt\"\n",
    "\n",
    "    eval_prompt_template_dir = llm_info.get('eval_prompt_template_dir', None)\n",
    "    eval_prompt_template_path = os.path.join(eval_prompts_dir, eval_prompt_template_dir, eval_prompt_template_fname)\n",
    "    logger.info(f\"evaluation prompt template file path being used for {model_id}: {eval_prompt_template_path}\")\n",
    "    logger.info(f\"evaluation prompt template file name: {eval_prompt_template_fname}\")\n",
    "\n",
    "    try:\n",
    "        eval_prompt_template = Path(eval_prompt_template_path).read_text()\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {eval_prompt_template_path}\")\n",
    "        continue\n",
    "\n",
    "    logger.info(f\"Evaluation prompt template being used: {eval_prompt_template}\")\n",
    "\n",
    "    eval_instructions_fname = next((rule for rule in config['s3_read_data']['eval_instructions_files'] if method_name in rule), None)\n",
    "    rules = Path(os.path.join(eval_prompts_dir, eval_instructions_fname)).read_text()\n",
    "    logger.info(f\"rules: {rules}\")\n",
    "\n",
    "    if method_name == \"max_voting\":\n",
    "        column_name = f\"{model_id}_{method_name}_eval_prompt\"\n",
    "        df_per_inference[column_name] = df_per_inference.apply(\n",
    "            lambda r: prepare_eval_prompts(\n",
    "                eval_prompt_template,\n",
    "                r['candidate_model_response'],\n",
    "                rules,\n",
    "                r['prompt'],\n",
    "                r['ground_truth'],\n",
    "                \"\"\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    elif method_name == \"avg_pooling\":\n",
    "        criteria_info = model_eval_subjective_info.get('subjective_eval_criteria', None)\n",
    "        criteria_dir = criteria_info.get('criteria_dir', None)\n",
    "        criteria_files = criteria_info.get('criteria', None)\n",
    "        logger.info(f\"Iterating through criteria in directory: {criteria_dir}\")\n",
    "\n",
    "        # List to store DataFrames for each criteria\n",
    "        all_dataframes = []\n",
    "\n",
    "        # loop through each criteria to form a prompt template for each\n",
    "        for criteria in criteria_files:\n",
    "            criteria_file = f\"{criteria}.txt\"\n",
    "            criteria_path = os.path.join(eval_prompts_dir, criteria_dir, criteria_file)\n",
    "            logger.info(f\"path to the evaluation criteria: {criteria_path}\")\n",
    "\n",
    "            try:\n",
    "                subjective_criteria_content = Path(criteria_path).read_text()\n",
    "                logger.info(f\"subjective_criteria_content: {subjective_criteria_content}\")\n",
    "            except FileNotFoundError:\n",
    "                logger.error(f\"Subjective criteria file not found: {criteria_path}\")\n",
    "                continue\n",
    "\n",
    "            # Create a copy of the original DataFrame for this criteria\n",
    "            df_criteria = df_per_inference.copy()\n",
    "            df_criteria['criteria_name'] = criteria\n",
    "\n",
    "            column_name = f\"{model_id}_{method_name}_eval_prompt\"\n",
    "            df_criteria[column_name] = df_criteria.apply(\n",
    "                lambda r: prepare_eval_prompts(\n",
    "                    eval_prompt_template,\n",
    "                    r['candidate_model_response'],\n",
    "                    rules,\n",
    "                    r['prompt'],\n",
    "                    \"\",\n",
    "                    subjective_criteria_content\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            all_dataframes.append(df_criteria)\n",
    "\n",
    "        # Concatenate all the DataFrames\n",
    "        df_per_inference = pd.concat(all_dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_eval_prompt_payloads = csv_buffer.getvalue()\n",
    "eval_prompt_payloads_for_inference = os.path.join(METRICS_DIR, PROCESSED_EVAL_PROMPT_PAYLOADS)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(df_per_inference_with_eval_prompt_payloads, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, PROCESSED_EVAL_PROMPT_PAYLOADS)\n",
    "logger.info(f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{eval_prompt_payloads_for_inference}\")\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "eval_records_list = json.loads(df_per_inference.to_json(orient='records'))\n",
    "logger.info(f\"Total number evaluations to be done: {len(eval_records_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the hierarchy of Model Evaluations\n",
    "---\n",
    "\n",
    "In this portion of the step, FMBench performs the following actions:\n",
    "\n",
    "1. If the method of evaluation is `Max Voting`, then in that case we suppose that a ground truth to the question from the context or task is pre existing in the dataset. We first calculate quantitative metrics. If the desired correctness threshold for either of the cosine similarity, levenshtein distance or token set ratio is exceeded, we end the evaluation of that data and move to the next one. If none of the quantitative metrics satisfy the threshold, the data moves to the panel of LLM evaluators for next steps.\n",
    "\n",
    "1. We use the LLM panel of judges (in this case 3 judges), to give a verdict on whether the `answer` from the candidate models during inference is `correct` or `incorrect`. If the response is correct, then it gives it a `correct` and if not, then `incorrect`.\n",
    "\n",
    "1. If the method of evaluation is `Average Pooling`, then in that case we suppose that the completion from the candidate models are supposed to be evlauated on a more subjective criteria rather than just deciding whether it is correct or incorrect compared to the ground truth. In this case, the average pooling prompt templates are used by the Judge Panel to give a rating out of 1-5 to each model completion on different criteria, such as relevancy, helpfulness, correctness, and so on.\n",
    "\n",
    "1. Each model response is given in a JSON structure which is further used for downstream analytics, to decide the comparision of evaluation results between different model candidates and more.\n",
    "\n",
    "***This step takes about ~6 minutes to complete. Model completion time depends on the PoLL models being used. `Llama3-70b`, `Cohere command-r-v1` and `claude 3 haiku` were used for this example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the llm as a judge panel list\n",
    "judge_panel_list: List[Dict] = model_eval_subjective_info.get('judge_panel_list', None)\n",
    "logger.info(f\"The judge panel list contains {len(judge_panel_list)} judges. Their information: {judge_panel_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"~Panel of LLM evaluators are going to start evaluating responses. This might take a couple of minutes depending on the size of the dataset and candidate model responses~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_record_for_lexical_similarity(method_name: str, record):\n",
    "    \"\"\"\n",
    "    Given a record, this function calculates the average of token set ratio and Levenshtein distance,\n",
    "    and checks the cosine similarity. If the cosine similarity meets or exceeds the specified threshold,\n",
    "    or if the average of token set ratio and Levenshtein distance meets or exceeds the specified threshold,\n",
    "    the completion is correct and an explanation is given without going through an LLM evaluator.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if method_name == \"max_voting\":\n",
    "            # Get the quantitative metrics\n",
    "            token_set_ratio = record['token_set_ratio_value']\n",
    "            levenshtein_ratio = record['levenshtein_distance']\n",
    "            cosine_similarity_score = record['cosine_similarity_score']\n",
    "\n",
    "            # Calculate the average of the metrics\n",
    "            average_score = (token_set_ratio + levenshtein_ratio + cosine_similarity_score) / 3\n",
    "\n",
    "            # Check if any of the quantitative metrics thresholds are met\n",
    "            is_quantitative_threshold_met = (\n",
    "                cosine_similarity_score >= eval_config['model_evaluations']['quantitative_eval_info'].get('cosine_similarity_threshold', None) or \n",
    "                token_set_ratio >= eval_config['model_evaluations']['quantitative_eval_info'].get('token_set_ratio_threshold', None) or\n",
    "                levenshtein_ratio >= eval_config['model_evaluations']['quantitative_eval_info'].get('levenshtein_distance_threshold', None)\n",
    "            )\n",
    "\n",
    "            # If no individual threshold is met, check if the average meets the overall threshold\n",
    "            if not is_quantitative_threshold_met:\n",
    "                is_quantitative_threshold_met = (\n",
    "                    average_score >= eval_config['model_evaluations']['quantitative_eval_info'].get('overall_eval_threshold', None)\n",
    "                )\n",
    "\n",
    "            if is_quantitative_threshold_met:\n",
    "                verdict = \"correct\"\n",
    "                explanation = (\n",
    "                    f\"Lexical match check passed with: \"\n",
    "                    f\"Token set ratio = {token_set_ratio * 100}%, \"\n",
    "                    f\"Levenshtein similarity match = {levenshtein_ratio * 100}%, \"\n",
    "                    f\"Cosine similarity = {cosine_similarity_score:.3f}, \"\n",
    "                    f\"Average score = {average_score * 100:.2f}%, not going through a panel of LLM evaluator.\"\n",
    "                )\n",
    "                record.update({\n",
    "                    'candidate_model': record['endpoint_name'],\n",
    "                    'completion': f'{{\\n  \"verdict\": \"{verdict}\",\\n  \"explanation\": \"{explanation}\"\\n}}',\n",
    "                })\n",
    "        else:\n",
    "            is_quantitative_threshold_met=False\n",
    "            record=record\n",
    "        logger.debug(f\"Processed record: {record}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred while checking for text similarity: {e}\")\n",
    "        record = None\n",
    "        is_quantitative_threshold_met = False\n",
    "    return record, is_quantitative_threshold_met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the evaluation process\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n: int = model_eval_subjective_info.get('run_parallel_inference_count', 5)\n",
    "list_of_lists = [eval_records_list[i * n:(i + 1) * n] for i in range((len(eval_records_list) + n - 1) // n)]\n",
    "resp_list = []\n",
    "erroneous_count: int = 0\n",
    "st: float = time.perf_counter()\n",
    "\n",
    "# Iterate over the judge panel and sublists\n",
    "for judge_panelist_info in judge_panel_list:\n",
    "    logger.info(f\"============Running inference for judge panelist {judge_panelist_info['model_id']} for {method_name} ============\")\n",
    "    for idx, sublist in enumerate(list_of_lists):\n",
    "        model_id: str = judge_panelist_info['model_id']\n",
    "        logger.info(f\"Getting inference for list {idx + 1}/{len(list_of_lists)}, size of list={len(sublist)}\")\n",
    "        # this list will hold all of the records that do not pass the metrics\n",
    "        # threshold test. The records that will be populated in this list will be used\n",
    "        # by the LLM evaluators to evaluate\n",
    "        records_not_meeting_quantitative_metric_threshold = []\n",
    "        for record in sublist:\n",
    "            # First, check if the current content of the record matches the threshold for \n",
    "            # token set ratio/cosine similarity/levenshtein similarity\n",
    "            processed_record, is_quantitative_threshold_met = process_record_for_lexical_similarity(method_name, record)\n",
    "            if is_quantitative_threshold_met:\n",
    "                # if the quantitative threshold is met, append the updated record with the \n",
    "                # already decided verdict in the response list\n",
    "                resp_list.append(processed_record)\n",
    "            else:\n",
    "                records_not_meeting_quantitative_metric_threshold.append(record)\n",
    "        try:\n",
    "            # If the quantitative metric thresholds are not meet, we parse them through all LLM\n",
    "            # evaluators to dive deep and correctly evaluate whether the model candidate response is \n",
    "            # correct or incorrect\n",
    "            if records_not_meeting_quantitative_metric_threshold:\n",
    "                # Run inference in parallel for non-matching records\n",
    "                resp_list.extend(ray.get([async_run_eval.remote(i + 1, len(records_not_meeting_quantitative_metric_threshold), record, model_id, method_name, record['uuid'])\n",
    "                                   for i, record in enumerate(records_not_meeting_quantitative_metric_threshold)]))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing list {idx + 1}/{len(list_of_lists)}: {e}\")\n",
    "            erroneous_count += 1\n",
    "    # Sleep for two seconds before moving on to the next model\n",
    "    logger.info(f\"~Sleeping for one second before the next Panel of LLM evaluates the responses~\")\n",
    "    time.sleep(1)\n",
    "\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"Total elapsed time for inference: {elapsed_time:.2f} seconds\")\n",
    "logger.info(f\"Total erroneous lists: {erroneous_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send all Panel of LLM evaluator responses to S3 as `JSON` files\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect all of the panel of LLM evals and send them all as JSON files\n",
    "# to s3\n",
    "if resp_list:\n",
    "    save_s3_list = []\n",
    "    try:\n",
    "        for resp in resp_list:\n",
    "            llm_eval_response = json.dumps(resp, indent=2)\n",
    "            candidate_model_id = resp.get('candidate_model', None)\n",
    "            # Extract a few words from the poll eval response to append to the file name\n",
    "            response_excerpt = \" \".join(resp.get('candidate_model_response', \"\").split()[:5])\n",
    "            sanitized_response_excerpt = \"\".join([c if c.isalnum() else \"_\" for c in response_excerpt])\n",
    "            llm_eval_json_fname = f\"{candidate_model_id}_{time.time()}_{sanitized_response_excerpt}.json\"\n",
    "            response_s3_path = os.path.join(METRICS_PER_POLL_EVAL_DIR, llm_eval_json_fname)\n",
    "            logger.info(f\"Sending model eval result files to s3 path prefix: {response_s3_path}\")\n",
    "            save_s3_list.append((llm_eval_response,\n",
    "                                config['aws']['bucket'],\n",
    "                                \"\",\n",
    "                                METRICS_PER_POLL_EVAL_DIR,\n",
    "                                llm_eval_json_fname))\n",
    "        write_multiple_to_s3(save_s3_list)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing or writing to S3: {e}\")\n",
    "else:\n",
    "    logger.info(\"No responses to write to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Perform downstream analytical tasks on each PoLL evaluation result\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the results list into a dataframe for easy analytics\n",
    "df_eval_results = pd.DataFrame(resp_list)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how='all')\n",
    "# the exception, judge model id, prompt token count, will be NaN for the verdicts decided\n",
    "# using the lexical match and not moved forward to the panel of LLM evaluators\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parse out the completion from LLM as a judge and column bind\n",
    "# the fields of the dictionary to the original results dataframe\n",
    "df_eval_results_only = df_eval_results['completion'].apply(parse_as_json).apply(pd.Series)\n",
    "df_eval_results_only.dropna(axis=1, how='all')\n",
    "df_eval_results = pd.concat([df_eval_results, df_eval_results_only], axis=1)\n",
    "df_eval_results.rename(columns={'model_id': 'judge_model_id'}, inplace=True)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how='all')\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send the raw results as a csv file to the S3 bucket\n",
    "csv_buffer = io.StringIO()\n",
    "df_eval_results.to_csv(csv_buffer, index=False)\n",
    "eval_llm_as_a_judge_results = csv_buffer.getvalue()\n",
    "eval_results_csv_fpath = os.path.join(METRICS_DIR, MODEL_EVAL_COMPLETIONS_CSV)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(eval_llm_as_a_judge_results, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, MODEL_EVAL_COMPLETIONS_CSV)\n",
    "logger.info(f\"Per PoLL model responses saved as a csv to s3://{BUCKET_NAME}/{eval_results_csv_fpath}\")\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Shape of the dataframe containing all evaluations: {df_eval_results.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the incorrect and correct responses to S3 separately in `CSV` files for downstream analytics for each model judge\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_evaluation_verdicts_and_ratings_to_s3(df_eval_results: pd.DataFrame, method_name: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function sends verdict responses separately to s3. This function is for max voting. \n",
    "    It uses a method name (which is either max_voting or avg_pooling). For max voting evaluation types, \n",
    "    all of the correct and incorrect verdicts are saved in separate CSV files using this function\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result_df: Optional[pd.DataFrame] = None\n",
    "        if method_name == 'max_voting':\n",
    "            logger.info(f\"Method name is {method_name}, sending the correct and incorrect verdicts to s3\")\n",
    "            verdict_types: List[str] = ['incorrect', 'correct']\n",
    "            all_verdicts_df = pd.DataFrame()\n",
    "            for verdict in verdict_types:\n",
    "                df_verdicts = df_eval_results[df_eval_results['verdict'] == verdict]\n",
    "                all_verdicts_df = pd.concat([all_verdicts_df, df_verdicts])\n",
    "\n",
    "                if not df_verdicts.empty:\n",
    "                    csv_buffer = io.StringIO()\n",
    "                    df_verdicts.to_csv(csv_buffer, index=False)\n",
    "                    verdict_responses = csv_buffer.getvalue()\n",
    "                    verdict_file = INCORRECT_VERDICT_RESPONSES_FILE if verdict == 'incorrect' else CORRECT_VERDICT_RESPONSES_FILE\n",
    "                    verdict_responses_fpath = os.path.join(METRICS_DIR, verdict_file)\n",
    "                    write_to_s3(verdict_responses, BUCKET_NAME, \"\", METRICS_DIR, verdict_file)\n",
    "                    logger.info(f\"{verdict.capitalize()} verdict responses sent to s3://{BUCKET_NAME}/{verdict_responses_fpath}\")\n",
    "                    logger.info(f\"Number of {verdict} responses in total: {df_verdicts.shape[0]}\")\n",
    "\n",
    "            result_df = all_verdicts_df\n",
    "\n",
    "            judge_model_ids = df_eval_results['judge_model_id'].unique()\n",
    "            for judge_model_id in judge_model_ids:\n",
    "                for verdict in verdict_types:\n",
    "                    df_judge_verdict = df_eval_results[(df_eval_results['verdict'] == verdict) & (df_eval_results['judge_model_id'] == judge_model_id)]\n",
    "\n",
    "                    if not df_judge_verdict.empty:\n",
    "                        txt_buffer = io.StringIO()\n",
    "                        for index, row in df_judge_verdict.iterrows():\n",
    "                            txt_buffer.write(\n",
    "                                f\"candidate model: {row['candidate_model']}\\n\"\n",
    "                                f\"candidate model response: {row['candidate_model_response']}\\n\"\n",
    "                                f\"ground truth: {row['ground_truth']}\\n\"\n",
    "                                f\"verdict and explanation: {row['completion']}\\n\\n\"\n",
    "                            )\n",
    "                        judge_verdict_responses = txt_buffer.getvalue()\n",
    "                        verdict_file = f\"{judge_model_id}_{verdict}_verdicts_evaluation.txt\"\n",
    "                        judge_verdict_responses_fpath = os.path.join(METRICS_DIR, verdict_file)\n",
    "                        write_to_s3(judge_verdict_responses, BUCKET_NAME, \"\", METRICS_DIR, verdict_file)\n",
    "                        logger.info(f\"{verdict.capitalize()} verdict responses for judge {judge_model_id} saved to s3://{BUCKET_NAME}/{judge_verdict_responses_fpath}\")\n",
    "\n",
    "        # if the eval method is average pooling, get the pivoted table containing each eval criteria, and\n",
    "        # the overall rating for that candidate model response\n",
    "        elif method_name == 'avg_pooling':\n",
    "            logger.info(f\"Method name is {method_name}, sending the different criteria evals to s3\")\n",
    "            df_avg_pooling = df_eval_results.pivot_table(\n",
    "                index=['candidate_model', 'judge_model_id', 'candidate_model_response', 'payload_file'],\n",
    "                columns='criteria_name',\n",
    "                values='eval_rating',\n",
    "                aggfunc='mean'\n",
    "            ).reset_index()\n",
    "\n",
    "            # Ensure all columns are numeric for mean calculation\n",
    "            numeric_columns = df_avg_pooling.select_dtypes(include='number').columns\n",
    "            df_avg_pooling['overall_eval_rating'] = df_avg_pooling[numeric_columns].mean(axis=1)\n",
    "            csv_buffer = io.StringIO()\n",
    "            df_avg_pooling.to_csv(csv_buffer, index=False)\n",
    "            avg_pooling_eval_responses = csv_buffer.getvalue()\n",
    "            avg_pooling_responses_fpath = os.path.join(METRICS_DIR, AVERAGE_POOLING_ALL_EVALS)\n",
    "            write_to_s3(avg_pooling_eval_responses, BUCKET_NAME, \"\", METRICS_DIR, AVERAGE_POOLING_ALL_EVALS)\n",
    "            logger.info(f\"Average pooling evaluation responses sent to s3://{BUCKET_NAME}/{avg_pooling_responses_fpath}\")\n",
    "            result_df = df_avg_pooling\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encountered while writing the evaluation responses to s3: {e}\")\n",
    "        result_df = None\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save correct and incorrect verdict files to S3 if the eval method being used is max_voting, else move on\n",
    "avg_pooling_eval_df = save_evaluation_verdicts_and_ratings_to_s3(df_eval_results, method_name)\n",
    "avg_pooling_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def non_perfect_overall_rating_counts(df_eval_results: pd.DataFrame, method_name: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function sends counts and returns a dataframe that does not contain an overall evaluation rating of 5 for\n",
    "    when the method name is \"avg_pooling\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result_df: Optional[pd.DataFrame] = None\n",
    "        if method_name == 'max_voting':\n",
    "            logger.info(f\"Method name is {method_name}. Cannot get evaluation ratings since that is done for average pooling\")\n",
    "            return\n",
    "        # if the eval method is average pooling, get the pivoted table containing each eval criteria, and\n",
    "        # the overall rating for that candidate model response\n",
    "        elif method_name == 'avg_pooling':\n",
    "            logger.info(f\"Method name is {method_name}, extracting entries with non perfect ratings (< 5) and sending them to s3\")\n",
    "            result_df = df_eval_results[df_eval_results['overall_eval_rating'] < 5]\n",
    "            csv_buffer = io.StringIO()\n",
    "            result_df.to_csv(csv_buffer, index=False)\n",
    "            non_perfect_ratings_avg_pooling_eval_responses = csv_buffer.getvalue()\n",
    "            non_perfect_ratings_avg_pooling_responses_fpath = os.path.join(METRICS_DIR, NON_PERFECT_RATING_RESPONSES)\n",
    "            write_to_s3(non_perfect_ratings_avg_pooling_eval_responses, BUCKET_NAME, \"\", METRICS_DIR, NON_PERFECT_RATING_RESPONSES)\n",
    "            logger.info(f\"All evaluation ratings below 5 are sent to s3://{BUCKET_NAME}/{non_perfect_ratings_avg_pooling_responses_fpath}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encountered while writing the non perfect evaluation ratings to s3: {e}\")\n",
    "        result_df = None\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_perfect_overall_rating_counts(avg_pooling_eval_df, method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for each panel of LLM evaluator's verdict count on the dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_panel_summary_responses(df_eval_results: pd.DataFrame, method_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used for when the evaluation type is max voting. Here, it takes in a method name, \n",
    "    and then based on the verdicts for each candidate model, gives the mean cosine similarity score, levenshtein\n",
    "    distance and toekn set ratio\n",
    "    \"\"\"\n",
    "    if method_name != 'max_voting':\n",
    "        logger.info(f\"Evaluation method is set to {method_name}, exiting out of this function\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    panel_summary_responses_df = df_eval_results.groupby(['judge_model_id', 'candidate_model', 'verdict']).agg(\n",
    "        count=('verdict', 'size'),\n",
    "        mean_cosine_similarity=('cosine_similarity_score', 'mean'),\n",
    "        mean_levenshtein_distance=('levenshtein_distance', 'mean'),\n",
    "        mean_token_set_ratio=('token_set_ratio_value', 'mean')\n",
    "    ).unstack(fill_value=0).stack().reset_index()\n",
    "    csv_buffer = io.StringIO()\n",
    "    panel_summary_responses_df.to_csv(csv_buffer, index=False)\n",
    "    panel_summary_responses = csv_buffer.getvalue()\n",
    "    llm_as_a_judge_per_eval_summary_fpath = os.path.join(METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES)\n",
    "\n",
    "    write_to_s3(panel_summary_responses, BUCKET_NAME, \"\", METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES)\n",
    "    logger.info(f\"Summary on each eval (max voting/average pooling) for each panel judge sent to s3://{BUCKET_NAME}/{llm_as_a_judge_per_eval_summary_fpath}\")\n",
    "\n",
    "    return panel_summary_responses_df\n",
    "\n",
    "panel_summary_responses_df = generate_panel_summary_responses(df_eval_results, method_name)\n",
    "panel_summary_responses_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the overall accuracy of each model scored by the PoLL\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_per_panel_judgement_result_df(panel_summary_responses_df: pd.DataFrame, method_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function is used to get the per panel judgement on each candidate model in terms of the \n",
    "    how many responses where correct (accuracy) and how many were incorrect (error rate)\n",
    "    \"\"\"\n",
    "    if method_name != 'max_voting':\n",
    "        logger.info(f\"Evaluation method is set to {method_name}, exiting out of this function\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    per_panel_judgement_result_df = panel_summary_responses_df.pivot_table(\n",
    "        index=['candidate_model', 'judge_model_id'],\n",
    "        columns='verdict',\n",
    "        values='count',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "\n",
    "    # Ensure 'correct' and 'incorrect' columns exist\n",
    "    if 'correct' not in per_panel_judgement_result_df.columns:\n",
    "        per_panel_judgement_result_df['correct'] = 0\n",
    "    if 'incorrect' not in per_panel_judgement_result_df.columns:\n",
    "        per_panel_judgement_result_df['incorrect'] = 0\n",
    "\n",
    "    # Calculate accuracy and error rate\n",
    "    per_panel_judgement_result_df = per_panel_judgement_result_df.assign(\n",
    "        accuracy=lambda df: df.apply(lambda row: 100 if row['incorrect'] == 0 else round(row['correct'] / (row['correct'] + row['incorrect']), 2) * 100, axis=1),\n",
    "        error_rate=lambda df: df.apply(lambda row: 0 if row['incorrect'] == 0 else round(row['incorrect'] / (row['correct'] + row['incorrect']), 2) * 100, axis=1)\n",
    "    )\n",
    "\n",
    "    return per_panel_judgement_result_df\n",
    "\n",
    "per_panel_judgement_result_df = generate_per_panel_judgement_result_df(panel_summary_responses_df, method_name)\n",
    "per_panel_judgement_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_overall_accuracy_metrics(df_eval_results: pd.DataFrame, per_panel_judgement_result_df: pd.DataFrame, method_name: str) -> pd.DataFrame:\n",
    "    if method_name != 'max_voting':\n",
    "        logger.info(f\"Evaluation method is set to {method_name}, exiting out of this function\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mean_cosine_similarity = df_eval_results.groupby('candidate_model')['cosine_similarity_score'].mean().reset_index().rename(columns={'cosine_similarity_score': 'mean_cosine_similarity'})\n",
    "    mean_levenshtein_distance = df_eval_results.groupby('candidate_model')['levenshtein_distance'].mean().reset_index().rename(columns={'levenshtein_distance': 'mean_levenshtein_distance'})\n",
    "    mean_token_set_ratio = df_eval_results.groupby('candidate_model')['token_set_ratio_value'].mean().reset_index().rename(columns={'token_set_ratio_value': 'mean_token_set_ratio_value'})\n",
    "\n",
    "    overall_accuracy_grouped_panel_df = per_panel_judgement_result_df.groupby('candidate_model')[['accuracy', 'error_rate']].mean().reset_index()\n",
    "    overall_accuracy_grouped_panel_df = (\n",
    "        pd.merge(mean_cosine_similarity, overall_accuracy_grouped_panel_df, on='candidate_model')\n",
    "        .merge(mean_levenshtein_distance, on='candidate_model')\n",
    "        .merge(mean_token_set_ratio, on='candidate_model')\n",
    "        .sort_values(by='accuracy', ascending=False)\n",
    "    )\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    overall_accuracy_grouped_panel_df.to_csv(csv_buffer, index=False)\n",
    "    overall_panel_result = csv_buffer.getvalue()\n",
    "    overall_panel_accuracy_metrics_fpath = os.path.join(METRICS_DIR, PER_MODEL_ACCURACY_POLL)\n",
    "\n",
    "    write_to_s3(overall_panel_result, BUCKET_NAME, \"\", METRICS_DIR, PER_MODEL_ACCURACY_POLL)\n",
    "    logger.info(f\"Overall accuracy and error rates results of each model sent to s3://{BUCKET_NAME}/{overall_panel_accuracy_metrics_fpath}\")\n",
    "\n",
    "    return overall_accuracy_grouped_panel_df\n",
    "\n",
    "overall_accuracy_grouped_panel_df = save_overall_accuracy_metrics(df_eval_results, per_panel_judgement_result_df, method_name)\n",
    "overall_accuracy_grouped_panel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_save_accuracy_statement(overall_accuracy_grouped_panel_df: pd.DataFrame, per_panel_judgement_result_df: pd.DataFrame, method_name: str):\n",
    "    if method_name != 'max_voting':\n",
    "        logger.info(f\"Evaluation method is set to {method_name}, exiting out of this function\")\n",
    "        return\n",
    "\n",
    "    # Rank models by accuracy\n",
    "    ranked_models = overall_accuracy_grouped_panel_df.sort_values(by='accuracy', ascending=False)\n",
    "    highest_accuracy = ranked_models['accuracy'].max()\n",
    "\n",
    "    # Group models with the highest accuracy\n",
    "    top_performers = ranked_models[ranked_models['accuracy'] == highest_accuracy]\n",
    "    other_models = ranked_models[ranked_models['accuracy'] < highest_accuracy]\n",
    "    final_ranking = pd.concat([top_performers, other_models])\n",
    "    unique_judge_model_ids = per_panel_judgement_result_df['judge_model_id'].unique()\n",
    "    PoLL_model_ids = ', '.join(map(str, unique_judge_model_ids))\n",
    "    top_performing_model_ids = ', '.join(top_performers['candidate_model'].tolist())\n",
    "\n",
    "    # Cosine similarity score data\n",
    "    highest_cosine_model = final_ranking.loc[final_ranking['mean_cosine_similarity'].idxmax()]\n",
    "    highest_cosine_model_name = highest_cosine_model['candidate_model']\n",
    "    highest_cosine_similarity = highest_cosine_model['mean_cosine_similarity']\n",
    "\n",
    "    # Levenshtein distance data\n",
    "    highest_levenshtein_model = final_ranking.loc[final_ranking['mean_levenshtein_distance'].idxmin()]\n",
    "    highest_levenshtein_model_name = highest_levenshtein_model['candidate_model']\n",
    "    highest_levenshtein_distance = highest_levenshtein_model['mean_levenshtein_distance']\n",
    "\n",
    "    # Token set ratio data\n",
    "    highest_token_set_ratio_model = final_ranking.loc[final_ranking['mean_token_set_ratio_value'].idxmax()]\n",
    "    highest_token_set_ratio_model_name = highest_token_set_ratio_model['candidate_model']\n",
    "    highest_token_set_ratio_value = highest_token_set_ratio_model['mean_token_set_ratio_value']\n",
    "\n",
    "    if other_models.empty:\n",
    "        other_models_statement = f\"All models performed the same with an accuracy of {highest_accuracy:.2f}.\"\n",
    "    else:\n",
    "        other_models_statement = other_models.to_string(index=False)\n",
    "\n",
    "    # Create the accuracy statement\n",
    "    accuracy_statement = MAX_VOTING_RESULT_STATEMENT.format(\n",
    "        judge_model_ids=PoLL_model_ids,\n",
    "        highest_accuracy=highest_accuracy,\n",
    "        top_models=top_performers.to_string(index=False),\n",
    "        highest_cosine_similarity=round(highest_cosine_similarity, 4),\n",
    "        top_cosine_similarity_model=highest_cosine_model_name,\n",
    "        highest_levenshtein_distance=round(highest_levenshtein_distance, 4),\n",
    "        top_levenshtein_model=highest_levenshtein_model_name,\n",
    "        highest_token_set_ratio_value=round(highest_token_set_ratio_value, 4),\n",
    "        top_token_set_ratio_model=highest_token_set_ratio_model_name,\n",
    "        ranked_models=other_models_statement,\n",
    "        top_performing_model_ids=top_performing_model_ids\n",
    "    )\n",
    "\n",
    "    # Send the overall accuracy report to S3\n",
    "    txt_buffer = io.StringIO()\n",
    "    txt_buffer.write(accuracy_statement)\n",
    "    poll_txt_file_content = txt_buffer.getvalue()\n",
    "    overall_panel_accuracy_metrics_fpath = os.path.join(METRICS_DIR, OVERALL_POLL_REPORT)\n",
    "    write_to_s3(poll_txt_file_content, BUCKET_NAME, \"\", METRICS_DIR, OVERALL_POLL_REPORT)\n",
    "    logger.info(f\"Overall accuracy and error rates results of each model sent to s3://{BUCKET_NAME}/{overall_panel_accuracy_metrics_fpath}\")\n",
    "    print(accuracy_statement)\n",
    "\n",
    "generate_and_save_accuracy_statement(overall_accuracy_grouped_panel_df, per_panel_judgement_result_df, method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send all responses from the evaluation process to S3 as a txt file for further downstream processing and readability purposes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_all_evaluations_to_s3(df_eval_results: pd.DataFrame, method_name: str) -> str:\n",
    "    if method_name != 'max_voting':\n",
    "        logger.info(f\"Evaluation method is set to {method_name}, exiting out of this function\")\n",
    "        return None\n",
    "    # Write all explanations to a file and send to S3\n",
    "    explanations_txt_buffer = io.StringIO()\n",
    "    for index, row in df_eval_results.iterrows():\n",
    "        explanations_txt_buffer.write(\n",
    "            f\"candidate model: {row['candidate_model']}\\n\"\n",
    "            f\"candidate model response: {row['candidate_model_response']}\\n\"\n",
    "            f\"ground truth: {row['ground_truth']}\\n\"\n",
    "            f\"verdict and explanation: {row['completion']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    explanations_txt_file_content = explanations_txt_buffer.getvalue()\n",
    "    explanations_fpath = os.path.join(METRICS_DIR, ALL_EVALUATIONS_IN_TXT)\n",
    "    write_to_s3(explanations_txt_file_content, BUCKET_NAME, \"\", METRICS_DIR, ALL_EVALUATIONS_IN_TXT)\n",
    "    logger.info(f\"All text eval content from the llm judge panelists sent to s3://{BUCKET_NAME}/{explanations_fpath}\")\n",
    "    logger.info(f\"All of the content including the candidate model responses, ground truth, evaluation are written: {evaluations_content}\")\n",
    "    return explanations_txt_file_content\n",
    "\n",
    "write_all_evaluations_to_s3(df_eval_results, method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Evaluations (for max voting debugging purposes)\n",
    "---\n",
    "\n",
    "Going over each of the evaluation is a tedious task, while it is an option, in this portion of the step:\n",
    "\n",
    "1. All of the evaluations are parsed in chunks through a model on Amazon Bedrock, to provide whether all evaluations are done correctly. For example, if all verdicts for Max Voting (either correct or incorrect) are correctly mentioned depending on the comparison between the candidate model response and the ground truth.\n",
    "\n",
    "1. Scan for any false positive or false negative evaluations, if any, then we can prompt engineer the evaluation prompts to give more accurate results.\n",
    "\n",
    "1. Give a summary of all evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get the content of the explanations\n",
    "# explanations_txt_file_content = evaluations_content.getvalue()\n",
    "# final_summarizer_model_id: str = model_eval_subjective_info.get(\"final_evaluation_summarizer\", None)\n",
    "\n",
    "# # Split the explanations into chunks of 50,000 words with some overlap\n",
    "# chunk_size: int = 50000\n",
    "# chunk_overlap: int = 10000\n",
    "\n",
    "\n",
    "# def split_into_chunks(text, chunk_size, chunk_overlap):\n",
    "#     words = text.split()\n",
    "#     chunks = []\n",
    "#     for i in range(0, len(words), chunk_size - chunk_overlap):\n",
    "#         chunk = ' '.join(words[i:i + chunk_size])\n",
    "#         chunks.append(chunk)\n",
    "#     return chunks\n",
    "\n",
    "\n",
    "# chunks = split_into_chunks(explanations_txt_file_content, chunk_size, chunk_overlap)\n",
    "# num_chunks = len(chunks)\n",
    "\n",
    "# # Log the number of chunks created\n",
    "# logger.info(f\"Number of chunks created: {num_chunks}\")\n",
    "\n",
    "# # Load the final evaluation prompt template\n",
    "# final_eval_prompt_template = os.path.join(eval_prompts_dir, model_eval_subjective_info.get(\"final_evaluation_prompt_template\", None))\n",
    "# final_eval_prompt_content = Path(final_eval_prompt_template).read_text()\n",
    "# logger.info(f\"Prompt being used to evaluate all evaluations: {final_eval_prompt_content}\")\n",
    "\n",
    "# # Evaluate each chunk with the model on Amazon Bedrock and collect the responses\n",
    "# responses = []\n",
    "# for chunk in chunks:\n",
    "#     prompt_content = final_eval_prompt_content.format(context=chunk)\n",
    "#     response = completion(model=final_summarizer_model_id,\n",
    "#                           messages=[{\"content\": prompt_content, \"role\": \"user\"}],\n",
    "#                           temperature=INFERENCE_PARAMETERS_LLM_PANEL.get('temperature', 0.1),\n",
    "#                           max_tokens=1000,\n",
    "#                           caching=INFERENCE_PARAMETERS_LLM_PANEL.get('caching', False))\n",
    "#     responses.append(response['choices'][0]['message']['content'])\n",
    "\n",
    "# # Combine all the responses into a single text\n",
    "# combined_response = '\\n\\n'.join(responses)\n",
    "\n",
    "# # Write the combined response to a new file and upload to S3\n",
    "# combined_response_buffer = io.StringIO(combined_response)\n",
    "# combined_response_content = combined_response_buffer.getvalue()\n",
    "# combined_response_fpath = os.path.join(METRICS_DIR, \"eval_of_evaluations_summary_debug.txt\")\n",
    "# write_to_s3(combined_response_content, BUCKET_NAME, \"\", METRICS_DIR, \"eval_of_evaluations_summary_debug.txt\")\n",
    "\n",
    "# # Log the S3 path and print the combined response\n",
    "# logger.info(f\"Combined evaluations response sent to s3://{BUCKET_NAME}/{combined_response_fpath}\")\n",
    "# print(combined_response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(combined_response)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
