{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Evaluations on all inference files and gather findings on quantitative metrics (such as _Cosine Similarity_) and subjective metrics on various criteria using _LLM as a judge_ - Max Voting & Average Pooling using PoLL (Panel of LLM Evaluators)\n",
    "\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of the solution focusses on getting evaluations on the quality of responses. It does so by gathering the following information and performing the steps below:\n",
    "\n",
    "- **Gets all per inference request file**: This step first accesses and gets all of the per inference request files into a dataframe, containing the response from the LLM as well as the ground truth, if any is provided. \n",
    "\n",
    "- **Generates quantitative metrics for evaluation**: Calculate quantitative metrics to measure similarity and accuracy, for example _Cosine Similarity_. This helps in getting a quantitative overall score to the entire dataset in terms of which model generates outputs that are most similar and accurate to the ground truth (if any is provided). With this statistic, customers and users of the open source community can make business level judgements. \n",
    "\n",
    "- **Uses an _LLM as a judge_ approach to get subjective evaluations**: Refer to this [paper](https://arxiv.org/pdf/2404.18796). We use the following ways to evaluate the responses from the `candidate models` (models used to generate inferences)\n",
    "\n",
    "    1. **Max Voting**: When a dataset provides a ground truth, we use a technique called `Max Voting`. Here, we use PoLL, or a panel of LLM evaluators, from different model families to evaluate each candidate model's response based on whether it generates a `correct` or an `incorrect` answer simply based on ground truth comparisons. Using models from different families as a PoLL, increases it's evaluation ability to be close to that of a human evaluation, and eliminates the intra model bias.\n",
    "    \n",
    "    2. **Average Pooling**: When a dataset does not provide a ground truth, or if a task being evaluated needs to be given subjective level judgements, that is when we use `Average Pooling`. We use a specific subjective level criteria and then evaluate the candidate model responses on a scale of 1-5 for each PoLL. Using this, we get an average score and then can evaluate how each candidate model was scored based on the PoLL evaluations.\n",
    "    \n",
    "FMBench uses this approach of PoLL to eradicate intra model bias by using models as judges from different model families. This brings the evaluation results closer to that of a human evaluation, makes the evaluation process more streamlined, consistent across all the responses, and reduces the latency and cost of evaluating the candidate models over time.\n",
    "    \n",
    "***All evaluations are generated in a JSON format for further downstream analytics on the evaluation results***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import ray\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import tempfile\n",
    "import datetime\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from litellm import completion\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import seaborn and other related libraries for visualizations and plotting charts\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tomark import Tomark\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from dateutil.parser import parse\n",
    "from typing import List, Optional, Dict\n",
    "import importlib.resources as pkg_resources\n",
    "from fmbench import __version__ as fmbench_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the ray service to run async calls in parallel to bedrock easily\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"CONFIG_FILE={CONFIG_FILE}\")\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the associated pricing config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "pricing_file_path: str = config['pricing'] \n",
    "\n",
    "# initialize the pricing config file to None\n",
    "pricing_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files('fmbench'), 'configs')\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "pricing_module = Path(config['pricing'])\n",
    "logger.info(f\"pricing config provided for inference from this model is --> {pricing_module}\")\n",
    "pricing_file_path = os.path.join(config_dir, pricing_module)\n",
    "logger.info(f\"pricing config file path is --> {pricing_file_path}\")\n",
    "\n",
    "pricing_config = load_config(pricing_file_path)\n",
    "logger.info(f\"pricing config file recorded: {json.dumps(pricing_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug is True:\n",
    "    metrics_path_file: str = os.path.join(\"..\", \"..\", METADATA_DIR, METRICS_PATH_FNAME)\n",
    "else:\n",
    "    metrics_path_file: str = os.path.join(METADATA_DIR, METRICS_PATH_FNAME)\n",
    "logger.info(f\"cwd={os.getcwd()}, METADATA_DIR={METADATA_DIR}, METRICS_PATH_FNAME={METRICS_PATH_FNAME}, metrics_path_file={metrics_path_file}\")\n",
    "METRICS_DIR: str = Path(metrics_path_file).read_text().strip()\n",
    "logger.info(f\"metrics_path_file={metrics_path_file}, METRICS_DIR={METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join(METRICS_DIR, config[\"report\"][\"per_inference_request_file\"])\n",
    "logger.info(f\"File path containing the metrics per inference folder --> {file_path}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    file_content = get_s3_object(config['aws']['bucket'], file_path)\n",
    "    # Use pandas to read the CSV content\n",
    "    df_per_inference = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(f\"{file_path} read into dataframe of shape {df_per_inference.shape}, \"\n",
    "                f\"cols={df_per_inference.columns}\")\n",
    "    logger.info(f\"{file_path} contains results for the following endpoints={df_per_inference.endpoint_name.unique()}\")\n",
    "    logger.info(df_per_inference.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between prompt token length and inference latency for different instances and concurrency levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.latency.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the `sentence-transformers/all-mpnet-base-v2` embeddings model to calculate the _Cosine Similarity_ scores \n",
    "---\n",
    "\n",
    "This portion of the evaluation step does as follows:\n",
    "\n",
    "1. Uses the `sentence-transformers/all-mpnet-base-v2` model from Hugging Face. This is a sentence-transformers model. It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "1. Use the embeddings model to get quantitative metrics from the inferences. This helps to get a similarity score between the ground truth answers from a dataset if any are given and the actual responses from the model received during inference.\n",
    "\n",
    "1. If no ground truth is provided, cosine similarity is calculated between the response and the content provided to answer the question.embeddings_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the quantitiative evaluation information from the config file, such as the embeddings model\n",
    "# to be used\n",
    "embeddings_model_quantitative_info: Dict = config['model_evaluations']['quantitative_eval_info']\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    This function loads the sentence-transformers model based on the provided model ID.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        model=None\n",
    "        model_id = embeddings_model_quantitative_info['embeddings_model_id'].get('model_id', None)\n",
    "        if model_id:\n",
    "            model = SentenceTransformer(model_id)\n",
    "        else:\n",
    "            raise ValueError(\"Model ID is not provided or invalid in the configuration.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"The SentenceTransformer embeddings model could not be loaded: {e}\")\n",
    "        model=None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the embeddings model to calculate the cosine similarity scores\n",
    "model = load_model()\n",
    "\n",
    "\n",
    "def get_cosine_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between two texts. In this case, \n",
    "    the cosine similarity is the comparison between the ground truth in the given dataset\n",
    "    and the candidate model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cosine: float = None\n",
    "        # returns the embedding for a given text using the sentence-transformers model.\n",
    "        A = model.encode([text1])[0]\n",
    "        B = model.encode([text2])[0]\n",
    "        cosine = dot(A, B) / (norm(A) * norm(B))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cosine similarity was not calculated at this iteration: {e}\")\n",
    "        cosine=None\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"~Creating embeddings of all candidate model responses now. This might take a couple minutes~\")\n",
    "\n",
    "# Assuming df_per_inference is your DataFrame\n",
    "df_per_inference['cosine_similarity_score'] = df_per_inference.apply(\n",
    "    lambda row: get_cosine_similarity(row['completion'], row['ground_truth']), axis=1\n",
    ")\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "METRICS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the all_metrics path to send the evaluation metrics to\n",
    "all_metrics_fpath = os.path.join(METRICS_DIR, config[\"report\"][\"all_metrics_file\"])\n",
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_cosine_similarity_scores_csv = csv_buffer.getvalue()\n",
    "inference_cosine_similarity_scores_s3_path = os.path.join(METRICS_DIR, PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(df_per_inference_with_cosine_similarity_scores_csv, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES)\n",
    "logger.info(f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{inference_cosine_similarity_scores_s3_path}\")\n",
    "\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use _Panel of LLM Evaluators_ to get Subjective Evaluations on various evaluation criteria\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we run evaluations on the content generated by different candidate models. We use two main evaluation methods: `Max Voting` and `Average Pooling`. To eliminate intra-model bias, we address this by scoring answer correctness based not on a single judge, but instead on a panel composed of multiple evaluator models. Similar pooling techniques are used to reduce variance in human annotations by normalizing out both natural variation in human judgements caused by their own subjective biases as well as human error. We use the following two techniques:\n",
    "\n",
    "1. **Max Voting**: We use the PoLL to evaluate candidate model responses by checking its correctness compared to a provided ground truth answer in the dataset. We prompt each PoLL to evaluate and give the response in a JSON structure, giving a verdict on whether the response is correct or incorrect, and an explanation as to why that is. Using this, we can perform downstream analytics such as: \n",
    "\n",
    "    1. Calculate the overall accuracy of each model using the correct versus the (correct + incorrect) responses\n",
    "    \n",
    "    1. Calculate the `error rate` or frequency or incorrect responses\n",
    "    \n",
    "    1. Categorize the errors based on the explanations provided by the evaluators. Common categories might include misunderstanding the question, incomplete answers, factual inaccuracies\n",
    "    \n",
    "    1. Summary of overall correct/incorrect, and the best model based on the PoLL. Rank the models on Correctness versus Incorrectness.\n",
    "\n",
    "1. **Average Pooling**: We use the PoLL to rate the response of each candidate model on a more subjective criteria. Here, we have the candidate model responses rated on a scale of 1-5 based on the subjective criteria and then get an explanation to that. Using this we can do as follows:\n",
    "\n",
    "    1. Calculate the average score for each model across all questions to get an overall performance measure.\n",
    "    \n",
    "    1. Compute the standard deviation of the scores to understand the consistency of the model's performance.\n",
    "\n",
    "1. Towards the end of all evaluations, a final layer of evaluation is added at the end. This layer utilizes another LLM that acts as a final summarizer. It takes in the ratings, answers generated from each unique model that was used in inference, to give a list of trends, overall patterns and observations as to which model is suited for a given task for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the qualitative/subjective evaluation information from the config file to evaluate answers from different\n",
    "# endpoints on various criteria\n",
    "model_eval_subjective_info: Dict = config['model_evaluations']['subjective_eval_info']\n",
    "eval_criteria_list = model_eval_subjective_info.get('eval_criteria', None)\n",
    "logger.info(f\"available llm as a judge evaluation information to use: {json.dumps(model_eval_subjective_info, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the inference parameters that the LLM judge panel will use while evaluating model candidate responses\n",
    "INFERENCE_PARAMETERS_LLM_PANEL: Dict = config['model_evaluations']['subjective_eval_info'].get('inference_parameters', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_inference(model_id: str,\n",
    "                  prompt: str):\n",
    "    \"\"\"\n",
    "    Get inference using LiteLLM. This get's inference on the answers provided and evaluates each\n",
    "    answer based on a given evaluation prompt template and the specific set of rules for each\n",
    "    evaluation criteria.\n",
    "    \"\"\"\n",
    "    # represents the service name\n",
    "    print(f\"get_inference, model_id={model_id}\")\n",
    "    service_name: str = \"bedrock\"\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name \n",
    "    # initialize the response dict\n",
    "    ret = dict(exception=None,\n",
    "               prompt=prompt,\n",
    "               completion=None,\n",
    "               completion_token_count=None,\n",
    "               prompt_token_count=None,\n",
    "               model_id=model_id)\n",
    "    body = ret['prompt']\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region\n",
    "    try:\n",
    "        # Represents calling the litellm completion/messaging api utilizing the completion/embeddings API\n",
    "        print(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(model=bedrock_model,\n",
    "                              messages=[{\"content\": body,\"role\": \"user\"}],\n",
    "                              temperature=INFERENCE_PARAMETERS_LLM_PANEL.get('temperature', 0.1),\n",
    "                              max_tokens=INFERENCE_PARAMETERS_LLM_PANEL.get('max_tokens', 100),\n",
    "                              caching=INFERENCE_PARAMETERS_LLM_PANEL.get('caching', False))\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens        \n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret['exception'] = e\n",
    "    logger.info(f\"completion: {ret['completion']}\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_filename(s):\n",
    "    \"\"\"\n",
    "    convert a string to another string that can be used as a filename\n",
    "    i.e. remove white space and non-word chars\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"None\"\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespace with a single dash\n",
    "    s = re.sub(r\"\\s+\", '-', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_as_json(x: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Convert a string into a dictionary. Remove any\n",
    "    stray whitespaces which could break the json parsing\n",
    "    \"\"\"\n",
    "    d: Optional[Dict] = None\n",
    "    try:\n",
    "        x = x.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        d = json.loads(x)\n",
    "    except Exception as e:\n",
    "        print(f\"parse_as_json, error parsing string as json, string={x}\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the latest dataframe and run LLM as a judge evaluations on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the evaluation prompt payloads\n",
    "---\n",
    "\n",
    "Here, the evaluation prompt template is used by the LLM judge to evaluate the answers on different criteria.\n",
    "This prompt template function uses a set of rules, prompt template, the answer, and ground truth (if any) in the\n",
    "evaluation solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_eval_prompts(eval_template: str,\n",
    "                         answer: str, \n",
    "                         rules: str, \n",
    "                         context: str, \n",
    "                         ground_truth: Optional[str]):\n",
    "    \"\"\"\n",
    "    This function prepares the evaluation prompts by preparing the standard eval prompt template\n",
    "    with the rules of a given subjective criteria, context, answer and ground truth (if any ground truth is provided)\n",
    "    \"\"\"\n",
    "    processed_eval_template: Optional[str] = None\n",
    "    processed_eval_template = eval_template.format(\n",
    "        rules=rules,\n",
    "        answer=answer,\n",
    "        context=context,\n",
    "        ground_truth=ground_truth)\n",
    "    return processed_eval_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "METRICS_PER_POLL_EVAL_DIR: str = os.path.join(METRICS_DIR, METRICS_PER_POLL_EVAL_DIR_NAME)\n",
    "_ = list(map(clear_dir, [METRICS_PER_POLL_EVAL_DIR]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_eval(i: int, total: int, row: Dict,  model_id: str, eval_method_name: str, uuid: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Runs the evaluation for one row \n",
    "    The eval prompt is already available in the row dictionary\n",
    "    and we simply want to run the inference against the judge model.\n",
    "    The results are returned in a new dictionary that contains the model \n",
    "    response and some fields from the original dictionary\n",
    "    \"\"\"\n",
    "    # save all the responses from the model in a dictionary\n",
    "    resp: Dict = {}\n",
    "    print(f\"run_eval, row {i}/{total}, judge_model_id={model_id}, candidate model={row['endpoint_name']}\")\n",
    "    candidate_model_response: str = row['completion']\n",
    "    # create the payload for model inference\n",
    "    prompt = row[f'{model_id}_{method_name}_eval_prompt']\n",
    "    # generate the chapter title based on the given chapter in the prompt \n",
    "    resp = get_inference(model_id, prompt)\n",
    "    resp['candidate_model_response'] = candidate_model_response\n",
    "    logger.info(f\"Panel of LLM evaluator {model_id} completion: {resp['completion']}\")\n",
    "    resp['candidate_model'] = row['endpoint_name']\n",
    "    resp['cosine_similarity_score'] = row['cosine_similarity_score']\n",
    "    if 'ground_truth' in row:\n",
    "        resp['ground_truth'] = row['ground_truth']\n",
    "    return resp\n",
    "\n",
    "\n",
    "# we use Ray to parallize\n",
    "@ray.remote\n",
    "def async_run_eval(i: int, total: int, row: Dict, model_id: str, eval_method_name: str, uuid: str) -> Dict:\n",
    "    print(f\"async_run_eval, i={i}, total={total}, judge_model_info={model_id}, eval_method: {eval_method_name}, uuid: {uuid}\")\n",
    "    return run_eval(i, total, row, model_id, eval_method_name, uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "df_per_inference_list = json.loads(df_per_inference.to_json(orient='records'))\n",
    "logger.info(f\"eval_records_list has {len(df_per_inference_list)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare evaluation prompt templates\n",
    "---\n",
    "\n",
    "This portion of the step prepares the evaluation prompt templates that are used in the evaluation process of using `Max Voting` or `Average Pooling` using the PoLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_eval_subjective_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the method that is being used to evaluate the content (which is either \n",
    "# max voting or average pooling)\n",
    "method_name: str = config['model_evaluations']['PoLL_Composition_and_Voting'].get('method', None)\n",
    "logger.info(f\"The evaluation method FMBench is going to use to evaluate different model responses: {method_name}\")\n",
    "logger.info(f\"judge panel being used to evaluate model responses: {model_eval_subjective_info.get('judge_panel_list', None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "eval_prompts_dir = Path(pkg_resources.files('fmbench'), f\"{config['s3_read_data']['prompt_template_dir']}/{config['s3_read_data']['eval_prompts_dir']}\")\n",
    "# Iterate through each LLM as a judge and each evaluation criterion\n",
    "for llm_info in model_eval_subjective_info.get('judge_panel_list', None):\n",
    "    model_id = llm_info['model_id']\n",
    "    eval_prompt_template_fname: str = f\"{llm_info.get('eval_prompt_template_name_max_voting', None)}.txt\"\n",
    "    eval_prompt_template_dir = llm_info.get('eval_prompt_template_dir', None)\n",
    "    eval_prompt_template_path: str = os.path.join(eval_prompts_dir, eval_prompt_template_dir, eval_prompt_template_fname)\n",
    "    logger.info(f\"evaluation prompt template file path being used for {model_id}: {eval_prompt_template_path}\")\n",
    "    logger.info(f\"evaluation prompt template file name: {eval_prompt_template_fname}\")\n",
    "    try:\n",
    "        eval_prompt_template = Path(eval_prompt_template_path).read_text()\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {eval_prompt_template_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluation prompt template being used: {eval_prompt_template}\")\n",
    "    eval_instructions_fname: str = next((rule for rule in config['s3_read_data']['eval_instructions_files'] if method_name in rule), None)\n",
    "    rules = Path(os.path.join(eval_prompts_dir, eval_instructions_fname)).read_text()\n",
    "    logger.info(f\"rules: {rules}\")\n",
    "    column_name = f\"{model_id}_{method_name}_eval_prompt\"\n",
    "    logger.info(f\"column_name: {column_name}\")\n",
    "\n",
    "    df_per_inference[column_name] = df_per_inference.apply(\n",
    "        lambda r: prepare_eval_prompts(\n",
    "            eval_prompt_template,\n",
    "            r['completion'],\n",
    "            rules,\n",
    "            r['prompt'],\n",
    "            r['ground_truth']\n",
    "        ),\n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_eval_prompt_payloads = csv_buffer.getvalue()\n",
    "eval_prompt_payloads_for_inference = os.path.join(METRICS_DIR, PROCESSED_EVAL_PROMPT_PAYLOADS)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(df_per_inference_with_eval_prompt_payloads, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, PROCESSED_EVAL_PROMPT_PAYLOADS)\n",
    "logger.info(f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{eval_prompt_payloads_for_inference}\")\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "eval_records_list = json.loads(df_per_inference.to_json(orient='records'))\n",
    "logger.info(f\"eval_records_list has {len(eval_records_list)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LLM as a Judge Evaluations\n",
    "---\n",
    "\n",
    "In this portion of the step, FMBench performs the following actions:\n",
    "\n",
    "1. If the method of evaluation is `Max Voting`, then in that case we suppose that a ground truth to the question from the context or task is pre existing in the dataset. We use the LLM panel of judges (in this case 3 judges), to give a verdict on whether the `answer` from the candidate models during inference is `correct` or `incorrect`. If the response is correct, then it gives it a `correct` and if not, then `incorrect`.\n",
    "\n",
    "1. If the method of evaluation is `Average Pooling`, then in that case we suppose that the completion from the candidate models are supposed to be evlauated on a more subjective criteria rather than just deciding whether it is correct or incorrect compared to the ground truth. In this case, the average pooling prompt templates are used by the Judge Panel to give a rating out of 1-5 to each model completion on different criteria, such as relevancy, helpfulness, correctness, and so on.\n",
    "\n",
    "1. Each model response is given in a JSON structure which is further used for downstream analytics, to decide the comparision of evaluation results between different model candidates and more.\n",
    "\n",
    "***This step takes about ~6 minutes to complete. Model completion time depends on the PoLL models being used. `Llama3-70b`, `Cohere command-r-v1` and `claude 3 haiku` were used for this example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the llm as a judge panel list\n",
    "judge_panel_list = model_eval_subjective_info.get('judge_panel_list', None)\n",
    "logger.info(f\"The judge panel list contains {len(judge_panel_list)} judges: {judge_panel_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"~Panel of LLM evaluators are going to start evaluating responses. This might take a couple of minutes~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = model_eval_subjective_info.get('run_parallel_inference_count', 5)\n",
    "list_of_lists = [eval_records_list[i * n:(i + 1) * n] for i in range((len(eval_records_list) + n - 1) // n)]\n",
    "resp_list = []\n",
    "erroneous_count = 0  # To keep track of errors\n",
    "st = time.perf_counter()\n",
    "\n",
    "# Iterate over the judge panel and sublists\n",
    "for judge_panelist_info in judge_panel_list:\n",
    "    logger.info(f\"============Running inference for judge panelist {judge_panelist_info['model_id']} for {method_name} ============\")\n",
    "    for idx, sublist in enumerate(list_of_lists):\n",
    "        model_id = judge_panelist_info['model_id']\n",
    "        logger.info(f\"getting inference for list {idx + 1}/{len(list_of_lists)}, size of list={len(sublist)}\")\n",
    "        try:\n",
    "            # Run inference in parallel\n",
    "            resp_list.extend(ray.get([async_run_eval.remote(i + 1, len(sublist), record, model_id, method_name, record['uuid'])\n",
    "                                      for i, record in enumerate(sublist)]))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing list {idx + 1}/{len(list_of_lists)}: {e}\")\n",
    "            erroneous_count += 1\n",
    "    # sleep for two seconds before moving on to the next model\n",
    "    logger.info(f\"~Sleeping for two seconds before the next Panel of LLM evaluates the responses~\")\n",
    "    time.sleep(2)\n",
    "\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"Total elapsed time for inference: {elapsed_time:.2f} seconds\")\n",
    "logger.info(f\"Total erroneous lists: {erroneous_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send all Panel of LLM evaluator responses to S3 as `JSON` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sanitize_filename(text):\n",
    "    return \"\".join([c if c.isalnum() else \"_\" for c in text])\n",
    "# Collect responses to send to S3\n",
    "if resp_list:\n",
    "    for resp in resp_list:\n",
    "        try:\n",
    "            llm_eval_response = json.dumps(resp, indent=2)\n",
    "            candidate_model_id = resp.get('candidate_model', None)\n",
    "            # Extract a few words from the poll eval response to append to the file name\n",
    "            response_excerpt = \" \".join(resp.get('candidate_model_response', None).split()[:5])\n",
    "            sanitized_response_excerpt = sanitize_filename(response_excerpt)\n",
    "            timestamp = time.time()\n",
    "            llm_eval_json_fname = f\"{candidate_model_id}_{timestamp}_{sanitized_response_excerpt}.json\"\n",
    "            response_s3_path = os.path.join(METRICS_PER_POLL_EVAL_DIR, llm_eval_json_fname)\n",
    "            logger.info(f\"sending llm eval response to s3 path prefix: {response_s3_path}\")\n",
    "            write_to_s3(llm_eval_response, config['aws']['bucket'], \"\", METRICS_PER_POLL_EVAL_DIR, llm_eval_json_fname)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preparing response for S3: {e}\")\n",
    "else:\n",
    "    logger.info(\"No responses to write to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform downstream analytical tasks on each PoLL evaluation result\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 18:35:29,463] p16430 {3877528743.py:3} INFO - df_eval_results shape=(194, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>completion_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>model_id</th>\n",
       "      <th>candidate_model_response</th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>58</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...</td>\n",
       "      <td>55</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...</td>\n",
       "      <td>55</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>63</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>57</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exception                                             prompt  \\\n",
       "0      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "1      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "2      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "3      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "4      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "\n",
       "                                          completion  completion_token_count  \\\n",
       "0  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      58   \n",
       "1  {\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...                      55   \n",
       "2  {\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...                      55   \n",
       "3  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      63   \n",
       "4  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      57   \n",
       "\n",
       "   prompt_token_count                         model_id  \\\n",
       "0                1502  mistral.mistral-large-2402-v1:0   \n",
       "1                1502  mistral.mistral-large-2402-v1:0   \n",
       "2                1502  mistral.mistral-large-2402-v1:0   \n",
       "3                1502  mistral.mistral-large-2402-v1:0   \n",
       "4                1502  mistral.mistral-large-2402-v1:0   \n",
       "\n",
       "                            candidate_model_response  \\\n",
       "0  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "1  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "2  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "3  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "4  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "\n",
       "                          candidate_model  cosine_similarity_score  \\\n",
       "0  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "1  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "2  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "3  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "4  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  a genus of flowering plant in the Lardizabalac...  \n",
       "1  a genus of flowering plant in the Lardizabalac...  \n",
       "2  a genus of flowering plant in the Lardizabalac...  \n",
       "3  a genus of flowering plant in the Lardizabalac...  \n",
       "4  a genus of flowering plant in the Lardizabalac...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the results list into a dataframe for easy analytics\n",
    "df_eval_results = pd.DataFrame(resp_list)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how='all')\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 18:35:30,741] p16430 {3063319009.py:7} INFO - df_eval_results shape=(194, 12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_as_json, error parsing string as json, string={\"verdict\": \"correct\",\"explanation\": \"The answer correctly defines the term 'velocity' in physics as the rate of change of position, which is the first temporal derivative of displacement. The answer also correctly mentions that velocity is a vector quantity that measures the rate and direction of motion.\"}``````\n",
      "parse_as_json, error parsing string as json, string={\"verdict\": \"incorrect\",\"explanation\": \"The answer is incorrect because Ding Yaping was a Chinese and German former international table tennis player, while Johann Christian Gustav Lucae was a German anatomist. Therefore, they were not of the same nationality.\"}``````\n",
      "parse_as_json, error parsing string as json, string={\"verdict\": \"correct\",\"explanation\": \"The answer correctly states that a traditional hoy boat was powered by sails. The passage mentions that hoys were \"sloop-rigged\" and that the mainsail could be fitted with or without a boom. It also states that before the development of steam engines, the passage of boats in places like the Thames estuary and the estuaries of the Netherlands \"required the skillful use of tides as much as of the wind.\" This indicates that hoys relied primarily on wind power.\"}\n",
      "parse_as_json, error parsing string as json, string={\"verdict\": \"correct\",\"explanation\": \"The answer correctly answers the question in the same way as the ground truth. The answer mentions that the song \"Dakota\" by the Welsh band Stereophonics was the UK #1 single released by the band in 2005. The ground truth also mentions \"Dakota\" as the correct answer.\"}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>completion_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>judge_model_id</th>\n",
       "      <th>candidate_model_response</th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>verdict</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>58</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly identifies that both the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...</td>\n",
       "      <td>55</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...</td>\n",
       "      <td>55</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>63</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>57</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exception                                             prompt  \\\n",
       "0      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "1      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "2      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "3      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "4      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "\n",
       "                                          completion  completion_token_count  \\\n",
       "0  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      58   \n",
       "1  {\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...                      55   \n",
       "2  {\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...                      55   \n",
       "3  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      63   \n",
       "4  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      57   \n",
       "\n",
       "   prompt_token_count                   judge_model_id  \\\n",
       "0                1502  mistral.mistral-large-2402-v1:0   \n",
       "1                1502  mistral.mistral-large-2402-v1:0   \n",
       "2                1502  mistral.mistral-large-2402-v1:0   \n",
       "3                1502  mistral.mistral-large-2402-v1:0   \n",
       "4                1502  mistral.mistral-large-2402-v1:0   \n",
       "\n",
       "                            candidate_model_response  \\\n",
       "0  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "1  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "2  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "3  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "4  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "\n",
       "                          candidate_model  cosine_similarity_score  \\\n",
       "0  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "1  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "2  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "3  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "4  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "\n",
       "                                        ground_truth  verdict  \\\n",
       "0  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "1  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "2  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "3  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "4  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "\n",
       "                                         explanation  \n",
       "0  The answer correctly identifies that both the ...  \n",
       "1  The answer correctly states that both the genu...  \n",
       "2  The answer correctly states that both the genu...  \n",
       "3  The answer correctly states that both the genu...  \n",
       "4  The answer correctly states that both the genu...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse out the completion from LLM as a judge and column bind\n",
    "# the fields of the dictionary to the original results dataframe\n",
    "df_eval_results_only = df_eval_results['completion'].apply(parse_as_json).apply(pd.Series)\n",
    "df_eval_results_only.dropna(axis=1, how='all')\n",
    "df_eval_results = pd.concat([df_eval_results, df_eval_results_only], axis=1)\n",
    "df_eval_results.rename(columns={'model_id': 'judge_model_id'}, inplace=True)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how='all')\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 18:35:34,297] p16430 {3367761925.py:10} INFO - Per PoLL model responses saved as a csv to s3://sagemaker-fmbench-write-us-east-1-988564344122/fmbench-bedrock-fmbench-1-us-east-1-role/data/metrics/yyyy=2024/mm=07/dd=15/hh=18/mm=01/raw_llm_as_a_judge_evals.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>completion_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>judge_model_id</th>\n",
       "      <th>candidate_model_response</th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>verdict</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>58</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly identifies that both the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...</td>\n",
       "      <td>55</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...</td>\n",
       "      <td>55</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>63</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>57</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exception                                             prompt  \\\n",
       "0      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "1      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "2      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "3      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "4      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "\n",
       "                                          completion  completion_token_count  \\\n",
       "0  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      58   \n",
       "1  {\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...                      55   \n",
       "2  {\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...                      55   \n",
       "3  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      63   \n",
       "4  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      57   \n",
       "\n",
       "   prompt_token_count                   judge_model_id  \\\n",
       "0                1502  mistral.mistral-large-2402-v1:0   \n",
       "1                1502  mistral.mistral-large-2402-v1:0   \n",
       "2                1502  mistral.mistral-large-2402-v1:0   \n",
       "3                1502  mistral.mistral-large-2402-v1:0   \n",
       "4                1502  mistral.mistral-large-2402-v1:0   \n",
       "\n",
       "                            candidate_model_response  \\\n",
       "0  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "1  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "2  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "3  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "4  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "\n",
       "                          candidate_model  cosine_similarity_score  \\\n",
       "0  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "1  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "2  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "3  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "4  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "\n",
       "                                        ground_truth  verdict  \\\n",
       "0  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "1  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "2  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "3  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "4  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "\n",
       "                                         explanation  \n",
       "0  The answer correctly identifies that both the ...  \n",
       "1  The answer correctly states that both the genu...  \n",
       "2  The answer correctly states that both the genu...  \n",
       "3  The answer correctly states that both the genu...  \n",
       "4  The answer correctly states that both the genu...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send the raw results as a csv file to the S3 bucket\n",
    "csv_buffer = io.StringIO()\n",
    "df_eval_results.to_csv(csv_buffer, index=False)\n",
    "eval_llm_as_a_judge_results = csv_buffer.getvalue()\n",
    "eval_results_csv_fpath = os.path.join(METRICS_DIR, MODEL_EVAL_COMPLETIONS_CSV)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(eval_llm_as_a_judge_results, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, MODEL_EVAL_COMPLETIONS_CSV)\n",
    "logger.info(f\"Per PoLL model responses saved as a csv to s3://{BUCKET_NAME}/{eval_results_csv_fpath}\")\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        correct\n",
       "1        correct\n",
       "2        correct\n",
       "3        correct\n",
       "4        correct\n",
       "         ...    \n",
       "189      correct\n",
       "190    incorrect\n",
       "191      correct\n",
       "192      correct\n",
       "193      correct\n",
       "Name: verdict, Length: 194, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_results['verdict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>completion_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>judge_model_id</th>\n",
       "      <th>candidate_model_response</th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>verdict</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"incorrect\",\\n  \"explanation\":...</td>\n",
       "      <td>65</td>\n",
       "      <td>3907</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.525871</td>\n",
       "      <td>Guy II, Count of Soissons</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. The ground t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>81</td>\n",
       "      <td>3192</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.396625</td>\n",
       "      <td>Altuğ Çelikbilek</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer is incorrect because it states that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>100</td>\n",
       "      <td>3676</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.415288</td>\n",
       "      <td>Do Musafir</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect because it st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>86</td>\n",
       "      <td>3577</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The passage does not provide any information a...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.190640</td>\n",
       "      <td>5 January 1964</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect as it states ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>78</td>\n",
       "      <td>2954</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The passage does not provide any information a...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.276481</td>\n",
       "      <td>8 December 2002</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer is incorrect because the passage do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>88</td>\n",
       "      <td>3748</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Both Pueraria and Pleiospilos are flowering pl...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.451170</td>\n",
       "      <td>Pleiospilos</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect because the q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>110</td>\n",
       "      <td>3841</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Unfortunately, the passage provided does not c...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>-0.009911</td>\n",
       "      <td>Midtribulation rapture,Midtribulationism,Pre-t...</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect because it st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>125</td>\n",
       "      <td>5132</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.477352</td>\n",
       "      <td>Ruel Redinger</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided states that the informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>109</td>\n",
       "      <td>5013</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.480927</td>\n",
       "      <td>William Pooley</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. The ground t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>87</td>\n",
       "      <td>4345</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>According to the information provided in the p...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.291852</td>\n",
       "      <td>3 September 1992</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect because the d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>94</td>\n",
       "      <td>5562</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The passage does not mention the place of birt...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>-0.017602</td>\n",
       "      <td>Somb</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect because the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>112</td>\n",
       "      <td>5646</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.366061</td>\n",
       "      <td>Dos Basuras</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The ground truth provided is incorrect. The di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>69</td>\n",
       "      <td>4751</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>No, the Kakwa River and the Bighead River are ...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.083248</td>\n",
       "      <td>yes</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer is incorrect. The Kakwa River and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>108</td>\n",
       "      <td>4630</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.460808</td>\n",
       "      <td>Kerem İnan</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect because it st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>64</td>\n",
       "      <td>3901</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>No, Yadboro River and Pigna Barney River are n...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.065030</td>\n",
       "      <td>yes</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. According to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>67</td>\n",
       "      <td>4649</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The radio station that serves the Lakes Region...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.156154</td>\n",
       "      <td>WANH</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. The ground t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>68</td>\n",
       "      <td>3988</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.285275</td>\n",
       "      <td>Dusty Dvoracek</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided states that there is not e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>100</td>\n",
       "      <td>4908</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.360721</td>\n",
       "      <td>The Ten-Year Lunch</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. According to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"incorrect\",\\n  \"explanation\":...</td>\n",
       "      <td>86</td>\n",
       "      <td>4639</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>According to the information provided, the 200...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.297598</td>\n",
       "      <td>179 women</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided states that there is not e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>71</td>\n",
       "      <td>3601</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>David Bowie's final album released two days be...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.241898</td>\n",
       "      <td>BLACKSTAR,Blackstar (TV series),BlackStar,Powe...</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect as the name o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>132</td>\n",
       "      <td>4742</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Unfortunately, the passage provided does not c...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>-0.012336</td>\n",
       "      <td>Midtribulation rapture,Midtribulationism,Pre-t...</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect as it states ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>66</td>\n",
       "      <td>5829</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The passage does not provide information about...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.144619</td>\n",
       "      <td>Kristiania</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer states that the passage does not pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>103</td>\n",
       "      <td>6222</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.323060</td>\n",
       "      <td>Seven In The Sun</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect because the q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>102</td>\n",
       "      <td>5860</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.469359</td>\n",
       "      <td>Cuchillos De Fuego</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect as it states ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>104</td>\n",
       "      <td>5520</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.356666</td>\n",
       "      <td>De As</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. The ground t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>72</td>\n",
       "      <td>6150</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>According to the information provided in the p...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.075632</td>\n",
       "      <td>Hezzelin I</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. According to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>68</td>\n",
       "      <td>6277</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The passage does not provide any information a...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>-0.014242</td>\n",
       "      <td>Theni</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer is incorrect because the passage do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>82</td>\n",
       "      <td>6347</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>According to the information provided in the p...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.498310</td>\n",
       "      <td>Elisabeth of Meissen</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. According to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>85</td>\n",
       "      <td>6151</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>According to the information provided in the p...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.573669</td>\n",
       "      <td>Albert, Count of Chiny</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The ground truth states that the paternal gran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>71</td>\n",
       "      <td>6141</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The passage does not provide any information a...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.056017</td>\n",
       "      <td>American</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided states that the passage do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>99</td>\n",
       "      <td>6532</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The passage does not provide any information a...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.057973</td>\n",
       "      <td>1070</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer is incorrect because the passage do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>68</td>\n",
       "      <td>5190</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>No, The Bloom of Yesterday is a German-Austria...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.161794</td>\n",
       "      <td>yes</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer is incorrect because The Bloom of Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>88</td>\n",
       "      <td>5601</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided, I do not ha...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.315818</td>\n",
       "      <td>College Lovers</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer is incorrect because it states that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>106</td>\n",
       "      <td>5321</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.285403</td>\n",
       "      <td>duck</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided does not correctly answer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>81</td>\n",
       "      <td>5209</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.358886</td>\n",
       "      <td>Sinclair Hill</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. According to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>97</td>\n",
       "      <td>5321</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.285403</td>\n",
       "      <td>duck</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided does not correctly answer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>108</td>\n",
       "      <td>5851</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.471493</td>\n",
       "      <td>Ferdi Taygan</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect as it states ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>71</td>\n",
       "      <td>5725</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.667468</td>\n",
       "      <td>Jean Acker</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The ground truth states that Jean Acker was bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>107</td>\n",
       "      <td>5283</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>According to the passage, the 40 days after th...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.516593</td>\n",
       "      <td>Pentecost Sunday,Pentekoste,Whitsun Tide,The f...</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. The Christia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>70</td>\n",
       "      <td>5559</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>According to the passage, Boris Becker won the...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.114842</td>\n",
       "      <td>four,4</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer is incorrect because the passage do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    exception                                             prompt  \\\n",
       "10       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "11       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "14       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "16       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "17       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "19       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "37       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "43       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "44       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "47       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "49       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "53       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "54       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "55       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "60       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "63       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "64       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "73       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "74       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "82       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "83       None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "109      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "111      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "115      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "116      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "123      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "126      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "128      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "132      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "134      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "136      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "137      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "142      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "145      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "150      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "159      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "161      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "163      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "176      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "190      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "\n",
       "                                            completion  \\\n",
       "10   {\\n  \"verdict\": \"incorrect\",\\n  \"explanation\":...   \n",
       "11   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "14   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "16   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "17   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "19   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "37   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "43   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "44   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "47   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "49   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "53   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "54   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "55   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "60   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "63   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "64   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "73   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "74   {\\n  \"verdict\": \"incorrect\",\\n  \"explanation\":...   \n",
       "82   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "83   {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "109  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "111  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "115  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "116  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "123  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "126  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "128  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "132  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "134  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "136  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "137  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "142  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "145  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "150  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "159  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "161  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "163  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "176  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "190  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...   \n",
       "\n",
       "     completion_token_count  prompt_token_count  \\\n",
       "10                       65                3907   \n",
       "11                       81                3192   \n",
       "14                      100                3676   \n",
       "16                       86                3577   \n",
       "17                       78                2954   \n",
       "19                       88                3748   \n",
       "37                      110                3841   \n",
       "43                      125                5132   \n",
       "44                      109                5013   \n",
       "47                       87                4345   \n",
       "49                       94                5562   \n",
       "53                      112                5646   \n",
       "54                       69                4751   \n",
       "55                      108                4630   \n",
       "60                       64                3901   \n",
       "63                       67                4649   \n",
       "64                       68                3988   \n",
       "73                      100                4908   \n",
       "74                       86                4639   \n",
       "82                       71                3601   \n",
       "83                      132                4742   \n",
       "109                      66                5829   \n",
       "111                     103                6222   \n",
       "115                     102                5860   \n",
       "116                     104                5520   \n",
       "123                      72                6150   \n",
       "126                      68                6277   \n",
       "128                      82                6347   \n",
       "132                      85                6151   \n",
       "134                      71                6141   \n",
       "136                      99                6532   \n",
       "137                      68                5190   \n",
       "142                      88                5601   \n",
       "145                     106                5321   \n",
       "150                      81                5209   \n",
       "159                      97                5321   \n",
       "161                     108                5851   \n",
       "163                      71                5725   \n",
       "176                     107                5283   \n",
       "190                      70                5559   \n",
       "\n",
       "                      judge_model_id  \\\n",
       "10   mistral.mistral-large-2402-v1:0   \n",
       "11   mistral.mistral-large-2402-v1:0   \n",
       "14   mistral.mistral-large-2402-v1:0   \n",
       "16   mistral.mistral-large-2402-v1:0   \n",
       "17   mistral.mistral-large-2402-v1:0   \n",
       "19   mistral.mistral-large-2402-v1:0   \n",
       "37   mistral.mistral-large-2402-v1:0   \n",
       "43   mistral.mistral-large-2402-v1:0   \n",
       "44   mistral.mistral-large-2402-v1:0   \n",
       "47   mistral.mistral-large-2402-v1:0   \n",
       "49   mistral.mistral-large-2402-v1:0   \n",
       "53   mistral.mistral-large-2402-v1:0   \n",
       "54   mistral.mistral-large-2402-v1:0   \n",
       "55   mistral.mistral-large-2402-v1:0   \n",
       "60   mistral.mistral-large-2402-v1:0   \n",
       "63   mistral.mistral-large-2402-v1:0   \n",
       "64   mistral.mistral-large-2402-v1:0   \n",
       "73   mistral.mistral-large-2402-v1:0   \n",
       "74   mistral.mistral-large-2402-v1:0   \n",
       "82   mistral.mistral-large-2402-v1:0   \n",
       "83   mistral.mistral-large-2402-v1:0   \n",
       "109  mistral.mistral-large-2402-v1:0   \n",
       "111  mistral.mistral-large-2402-v1:0   \n",
       "115  mistral.mistral-large-2402-v1:0   \n",
       "116  mistral.mistral-large-2402-v1:0   \n",
       "123  mistral.mistral-large-2402-v1:0   \n",
       "126  mistral.mistral-large-2402-v1:0   \n",
       "128  mistral.mistral-large-2402-v1:0   \n",
       "132  mistral.mistral-large-2402-v1:0   \n",
       "134  mistral.mistral-large-2402-v1:0   \n",
       "136  mistral.mistral-large-2402-v1:0   \n",
       "137  mistral.mistral-large-2402-v1:0   \n",
       "142  mistral.mistral-large-2402-v1:0   \n",
       "145  mistral.mistral-large-2402-v1:0   \n",
       "150  mistral.mistral-large-2402-v1:0   \n",
       "159  mistral.mistral-large-2402-v1:0   \n",
       "161  mistral.mistral-large-2402-v1:0   \n",
       "163  mistral.mistral-large-2402-v1:0   \n",
       "176  mistral.mistral-large-2402-v1:0   \n",
       "190  mistral.mistral-large-2402-v1:0   \n",
       "\n",
       "                              candidate_model_response  \\\n",
       "10   Based on the information provided in the passa...   \n",
       "11   Based on the information provided in the passa...   \n",
       "14   Based on the information provided in the passa...   \n",
       "16   The passage does not provide any information a...   \n",
       "17   The passage does not provide any information a...   \n",
       "19   Both Pueraria and Pleiospilos are flowering pl...   \n",
       "37   Unfortunately, the passage provided does not c...   \n",
       "43   Based on the information provided in the passa...   \n",
       "44   Based on the information provided in the passa...   \n",
       "47   According to the information provided in the p...   \n",
       "49   The passage does not mention the place of birt...   \n",
       "53   Based on the information provided in the passa...   \n",
       "54   No, the Kakwa River and the Bighead River are ...   \n",
       "55   Based on the information provided in the passa...   \n",
       "60   No, Yadboro River and Pigna Barney River are n...   \n",
       "63   The radio station that serves the Lakes Region...   \n",
       "64   Based on the information provided in the passa...   \n",
       "73   Based on the information provided in the passa...   \n",
       "74   According to the information provided, the 200...   \n",
       "82   David Bowie's final album released two days be...   \n",
       "83   Unfortunately, the passage provided does not c...   \n",
       "109  The passage does not provide information about...   \n",
       "111  Based on the information provided in the passa...   \n",
       "115  Based on the information provided in the passa...   \n",
       "116  Based on the information provided in the passa...   \n",
       "123  According to the information provided in the p...   \n",
       "126  The passage does not provide any information a...   \n",
       "128  According to the information provided in the p...   \n",
       "132  According to the information provided in the p...   \n",
       "134  The passage does not provide any information a...   \n",
       "136  The passage does not provide any information a...   \n",
       "137  No, The Bloom of Yesterday is a German-Austria...   \n",
       "142  Based on the information provided, I do not ha...   \n",
       "145  Based on the information provided in the passa...   \n",
       "150  Based on the information provided in the passa...   \n",
       "159  Based on the information provided in the passa...   \n",
       "161  Based on the information provided in the passa...   \n",
       "163  Based on the information provided in the passa...   \n",
       "176  According to the passage, the 40 days after th...   \n",
       "190  According to the passage, Boris Becker won the...   \n",
       "\n",
       "                            candidate_model  cosine_similarity_score  \\\n",
       "10   anthropic.claude-3-haiku-20240307-v1:0                 0.525871   \n",
       "11   anthropic.claude-3-haiku-20240307-v1:0                 0.396625   \n",
       "14   anthropic.claude-3-haiku-20240307-v1:0                 0.415288   \n",
       "16   anthropic.claude-3-haiku-20240307-v1:0                 0.190640   \n",
       "17   anthropic.claude-3-haiku-20240307-v1:0                 0.276481   \n",
       "19   anthropic.claude-3-haiku-20240307-v1:0                 0.451170   \n",
       "37   anthropic.claude-3-haiku-20240307-v1:0                -0.009911   \n",
       "43   anthropic.claude-3-haiku-20240307-v1:0                 0.477352   \n",
       "44   anthropic.claude-3-haiku-20240307-v1:0                 0.480927   \n",
       "47   anthropic.claude-3-haiku-20240307-v1:0                 0.291852   \n",
       "49   anthropic.claude-3-haiku-20240307-v1:0                -0.017602   \n",
       "53   anthropic.claude-3-haiku-20240307-v1:0                 0.366061   \n",
       "54   anthropic.claude-3-haiku-20240307-v1:0                 0.083248   \n",
       "55   anthropic.claude-3-haiku-20240307-v1:0                 0.460808   \n",
       "60   anthropic.claude-3-haiku-20240307-v1:0                 0.065030   \n",
       "63   anthropic.claude-3-haiku-20240307-v1:0                 0.156154   \n",
       "64   anthropic.claude-3-haiku-20240307-v1:0                 0.285275   \n",
       "73   anthropic.claude-3-haiku-20240307-v1:0                 0.360721   \n",
       "74   anthropic.claude-3-haiku-20240307-v1:0                 0.297598   \n",
       "82   anthropic.claude-3-haiku-20240307-v1:0                 0.241898   \n",
       "83   anthropic.claude-3-haiku-20240307-v1:0                -0.012336   \n",
       "109  anthropic.claude-3-haiku-20240307-v1:0                 0.144619   \n",
       "111  anthropic.claude-3-haiku-20240307-v1:0                 0.323060   \n",
       "115  anthropic.claude-3-haiku-20240307-v1:0                 0.469359   \n",
       "116  anthropic.claude-3-haiku-20240307-v1:0                 0.356666   \n",
       "123  anthropic.claude-3-haiku-20240307-v1:0                 0.075632   \n",
       "126  anthropic.claude-3-haiku-20240307-v1:0                -0.014242   \n",
       "128  anthropic.claude-3-haiku-20240307-v1:0                 0.498310   \n",
       "132  anthropic.claude-3-haiku-20240307-v1:0                 0.573669   \n",
       "134  anthropic.claude-3-haiku-20240307-v1:0                 0.056017   \n",
       "136  anthropic.claude-3-haiku-20240307-v1:0                 0.057973   \n",
       "137  anthropic.claude-3-haiku-20240307-v1:0                 0.161794   \n",
       "142  anthropic.claude-3-haiku-20240307-v1:0                 0.315818   \n",
       "145  anthropic.claude-3-haiku-20240307-v1:0                 0.285403   \n",
       "150  anthropic.claude-3-haiku-20240307-v1:0                 0.358886   \n",
       "159  anthropic.claude-3-haiku-20240307-v1:0                 0.285403   \n",
       "161  anthropic.claude-3-haiku-20240307-v1:0                 0.471493   \n",
       "163  anthropic.claude-3-haiku-20240307-v1:0                 0.667468   \n",
       "176  anthropic.claude-3-haiku-20240307-v1:0                 0.516593   \n",
       "190  anthropic.claude-3-haiku-20240307-v1:0                 0.114842   \n",
       "\n",
       "                                          ground_truth    verdict  \\\n",
       "10                           Guy II, Count of Soissons  incorrect   \n",
       "11                                    Altuğ Çelikbilek  incorrect   \n",
       "14                                          Do Musafir  incorrect   \n",
       "16                                      5 January 1964  incorrect   \n",
       "17                                     8 December 2002  incorrect   \n",
       "19                                         Pleiospilos  incorrect   \n",
       "37   Midtribulation rapture,Midtribulationism,Pre-t...  incorrect   \n",
       "43                                       Ruel Redinger  incorrect   \n",
       "44                                      William Pooley  incorrect   \n",
       "47                                    3 September 1992  incorrect   \n",
       "49                                                Somb  incorrect   \n",
       "53                                         Dos Basuras  incorrect   \n",
       "54                                                 yes  incorrect   \n",
       "55                                          Kerem İnan  incorrect   \n",
       "60                                                 yes  incorrect   \n",
       "63                                                WANH  incorrect   \n",
       "64                                      Dusty Dvoracek  incorrect   \n",
       "73                                  The Ten-Year Lunch  incorrect   \n",
       "74                                           179 women  incorrect   \n",
       "82   BLACKSTAR,Blackstar (TV series),BlackStar,Powe...  incorrect   \n",
       "83   Midtribulation rapture,Midtribulationism,Pre-t...  incorrect   \n",
       "109                                         Kristiania  incorrect   \n",
       "111                                   Seven In The Sun  incorrect   \n",
       "115                                 Cuchillos De Fuego  incorrect   \n",
       "116                                              De As  incorrect   \n",
       "123                                         Hezzelin I  incorrect   \n",
       "126                                              Theni  incorrect   \n",
       "128                               Elisabeth of Meissen  incorrect   \n",
       "132                             Albert, Count of Chiny  incorrect   \n",
       "134                                           American  incorrect   \n",
       "136                                               1070  incorrect   \n",
       "137                                                yes  incorrect   \n",
       "142                                     College Lovers  incorrect   \n",
       "145                                               duck  incorrect   \n",
       "150                                      Sinclair Hill  incorrect   \n",
       "159                                               duck  incorrect   \n",
       "161                                       Ferdi Taygan  incorrect   \n",
       "163                                         Jean Acker  incorrect   \n",
       "176  Pentecost Sunday,Pentekoste,Whitsun Tide,The f...  incorrect   \n",
       "190                                             four,4  incorrect   \n",
       "\n",
       "                                           explanation  \n",
       "10   The answer provided is incorrect. The ground t...  \n",
       "11   The answer is incorrect because it states that...  \n",
       "14   The answer provided is incorrect because it st...  \n",
       "16   The answer provided is incorrect as it states ...  \n",
       "17   The answer is incorrect because the passage do...  \n",
       "19   The answer provided is incorrect because the q...  \n",
       "37   The answer provided is incorrect because it st...  \n",
       "43   The answer provided states that the informatio...  \n",
       "44   The answer provided is incorrect. The ground t...  \n",
       "47   The answer provided is incorrect because the d...  \n",
       "49   The answer provided is incorrect because the p...  \n",
       "53   The ground truth provided is incorrect. The di...  \n",
       "54   The answer is incorrect. The Kakwa River and t...  \n",
       "55   The answer provided is incorrect because it st...  \n",
       "60   The answer provided is incorrect. According to...  \n",
       "63   The answer provided is incorrect. The ground t...  \n",
       "64   The answer provided states that there is not e...  \n",
       "73   The answer provided is incorrect. According to...  \n",
       "74   The answer provided states that there is not e...  \n",
       "82   The answer provided is incorrect as the name o...  \n",
       "83   The answer provided is incorrect as it states ...  \n",
       "109  The answer states that the passage does not pr...  \n",
       "111  The answer provided is incorrect because the q...  \n",
       "115  The answer provided is incorrect as it states ...  \n",
       "116  The answer provided is incorrect. The ground t...  \n",
       "123  The answer provided is incorrect. According to...  \n",
       "126  The answer is incorrect because the passage do...  \n",
       "128  The answer provided is incorrect. According to...  \n",
       "132  The ground truth states that the paternal gran...  \n",
       "134  The answer provided states that the passage do...  \n",
       "136  The answer is incorrect because the passage do...  \n",
       "137  The answer is incorrect because The Bloom of Y...  \n",
       "142  The answer is incorrect because it states that...  \n",
       "145  The answer provided does not correctly answer ...  \n",
       "150  The answer provided is incorrect. According to...  \n",
       "159  The answer provided does not correctly answer ...  \n",
       "161  The answer provided is incorrect as it states ...  \n",
       "163  The ground truth states that Jean Acker was bo...  \n",
       "176  The answer provided is incorrect. The Christia...  \n",
       "190  The answer is incorrect because the passage do...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_results['verdict'].isin(['incorrect']).any()\n",
    "df_eval_results[df_eval_results['verdict'] == 'incorrect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the incorrect and correct responses to S3 separately in `CSV` files for downstream analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 18:35:50,032] p16430 {3872311551.py:10} INFO - Incorrect verdict responses from the PoLL evaluation sent to s3://sagemaker-fmbench-write-us-east-1-988564344122/fmbench-bedrock-fmbench-1-us-east-1-role/data/metrics/yyyy=2024/mm=07/dd=15/hh=18/mm=01/incorrect_verdict_responses.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>completion_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>judge_model_id</th>\n",
       "      <th>candidate_model_response</th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>verdict</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"incorrect\",\\n  \"explanation\":...</td>\n",
       "      <td>65</td>\n",
       "      <td>3907</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.525871</td>\n",
       "      <td>Guy II, Count of Soissons</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect. The ground t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>81</td>\n",
       "      <td>3192</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.396625</td>\n",
       "      <td>Altuğ Çelikbilek</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer is incorrect because it states that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>100</td>\n",
       "      <td>3676</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>Based on the information provided in the passa...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.415288</td>\n",
       "      <td>Do Musafir</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect because it st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>86</td>\n",
       "      <td>3577</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The passage does not provide any information a...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.190640</td>\n",
       "      <td>5 January 1964</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer provided is incorrect as it states ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...</td>\n",
       "      <td>78</td>\n",
       "      <td>2954</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The passage does not provide any information a...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.276481</td>\n",
       "      <td>8 December 2002</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The answer is incorrect because the passage do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exception                                             prompt  \\\n",
       "10      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "11      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "14      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "16      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "17      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "\n",
       "                                           completion  completion_token_count  \\\n",
       "10  {\\n  \"verdict\": \"incorrect\",\\n  \"explanation\":...                      65   \n",
       "11  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...                      81   \n",
       "14  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...                     100   \n",
       "16  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...                      86   \n",
       "17  {\\n\"verdict\": \"incorrect\",\\n\"explanation\": \"Th...                      78   \n",
       "\n",
       "    prompt_token_count                   judge_model_id  \\\n",
       "10                3907  mistral.mistral-large-2402-v1:0   \n",
       "11                3192  mistral.mistral-large-2402-v1:0   \n",
       "14                3676  mistral.mistral-large-2402-v1:0   \n",
       "16                3577  mistral.mistral-large-2402-v1:0   \n",
       "17                2954  mistral.mistral-large-2402-v1:0   \n",
       "\n",
       "                             candidate_model_response  \\\n",
       "10  Based on the information provided in the passa...   \n",
       "11  Based on the information provided in the passa...   \n",
       "14  Based on the information provided in the passa...   \n",
       "16  The passage does not provide any information a...   \n",
       "17  The passage does not provide any information a...   \n",
       "\n",
       "                           candidate_model  cosine_similarity_score  \\\n",
       "10  anthropic.claude-3-haiku-20240307-v1:0                 0.525871   \n",
       "11  anthropic.claude-3-haiku-20240307-v1:0                 0.396625   \n",
       "14  anthropic.claude-3-haiku-20240307-v1:0                 0.415288   \n",
       "16  anthropic.claude-3-haiku-20240307-v1:0                 0.190640   \n",
       "17  anthropic.claude-3-haiku-20240307-v1:0                 0.276481   \n",
       "\n",
       "                 ground_truth    verdict  \\\n",
       "10  Guy II, Count of Soissons  incorrect   \n",
       "11           Altuğ Çelikbilek  incorrect   \n",
       "14                 Do Musafir  incorrect   \n",
       "16             5 January 1964  incorrect   \n",
       "17            8 December 2002  incorrect   \n",
       "\n",
       "                                          explanation  \n",
       "10  The answer provided is incorrect. The ground t...  \n",
       "11  The answer is incorrect because it states that...  \n",
       "14  The answer provided is incorrect because it st...  \n",
       "16  The answer provided is incorrect as it states ...  \n",
       "17  The answer is incorrect because the passage do...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incorrect_verdicts = pd.DataFrame()\n",
    "if df_eval_results['verdict'].isin(['incorrect']).any():\n",
    "    df_incorrect_verdicts = df_eval_results[df_eval_results['verdict'] == 'incorrect']\n",
    "    csv_buffer = io.StringIO()\n",
    "    df_incorrect_verdicts.to_csv(csv_buffer, index=False)\n",
    "    incorrect_verdict_responses = csv_buffer.getvalue()\n",
    "    incorrect_verdict_responses_fpath = os.path.join(METRICS_DIR, INCORRECT_VERDICT_RESPONSES_FILE)  # Define full S3 path\n",
    "    write_to_s3(incorrect_verdict_responses, BUCKET_NAME, \"\", \n",
    "                METRICS_DIR, INCORRECT_VERDICT_RESPONSES_FILE)\n",
    "    logger.info(f\"Incorrect verdict responses from the PoLL evaluation sent to s3://{BUCKET_NAME}/{incorrect_verdict_responses_fpath}\")\n",
    "df_incorrect_verdicts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 18:35:52,803] p16430 {42602085.py:1} INFO - Number of incorrect responses in total: 40\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Number of incorrect responses in total: {df_incorrect_verdicts.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 18:35:53,703] p16430 {1608207437.py:11} INFO - Correct verdict responses from the PoLL evaluation sent to s3://sagemaker-fmbench-write-us-east-1-988564344122/fmbench-bedrock-fmbench-1-us-east-1-role/data/metrics/yyyy=2024/mm=07/dd=15/hh=18/mm=01/correct_verdict_responses.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>completion_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>judge_model_id</th>\n",
       "      <th>candidate_model_response</th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>verdict</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>58</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly identifies that both the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...</td>\n",
       "      <td>55</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...</td>\n",
       "      <td>55</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>63</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>&lt;s&gt;[INST]\\nYou are a judge who evaluates the c...</td>\n",
       "      <td>{\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...</td>\n",
       "      <td>57</td>\n",
       "      <td>1502</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>The genus Sinofranchetia and Stauntonia are bo...</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.660119</td>\n",
       "      <td>a genus of flowering plant in the Lardizabalac...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The answer correctly states that both the genu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exception                                             prompt  \\\n",
       "0      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "1      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "2      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "3      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "4      None  <s>[INST]\\nYou are a judge who evaluates the c...   \n",
       "\n",
       "                                          completion  completion_token_count  \\\n",
       "0  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      58   \n",
       "1  {\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...                      55   \n",
       "2  {\\n\"verdict\": \"correct\",\\n\"explanation\": \"The ...                      55   \n",
       "3  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      63   \n",
       "4  {\\n  \"verdict\": \"correct\",\\n  \"explanation\": \"...                      57   \n",
       "\n",
       "   prompt_token_count                   judge_model_id  \\\n",
       "0                1502  mistral.mistral-large-2402-v1:0   \n",
       "1                1502  mistral.mistral-large-2402-v1:0   \n",
       "2                1502  mistral.mistral-large-2402-v1:0   \n",
       "3                1502  mistral.mistral-large-2402-v1:0   \n",
       "4                1502  mistral.mistral-large-2402-v1:0   \n",
       "\n",
       "                            candidate_model_response  \\\n",
       "0  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "1  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "2  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "3  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "4  The genus Sinofranchetia and Stauntonia are bo...   \n",
       "\n",
       "                          candidate_model  cosine_similarity_score  \\\n",
       "0  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "1  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "2  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "3  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "4  anthropic.claude-3-haiku-20240307-v1:0                 0.660119   \n",
       "\n",
       "                                        ground_truth  verdict  \\\n",
       "0  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "1  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "2  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "3  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "4  a genus of flowering plant in the Lardizabalac...  correct   \n",
       "\n",
       "                                         explanation  \n",
       "0  The answer correctly identifies that both the ...  \n",
       "1  The answer correctly states that both the genu...  \n",
       "2  The answer correctly states that both the genu...  \n",
       "3  The answer correctly states that both the genu...  \n",
       "4  The answer correctly states that both the genu...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_correct_verdicts = pd.DataFrame() \n",
    "if df_eval_results['verdict'].isin(['correct']).any():\n",
    "    # save the correct responses from the model evaluation process in a csv file to analyze where each model went right\n",
    "    df_correct_verdicts = df_eval_results[df_eval_results['verdict'] == 'correct']\n",
    "    csv_buffer = io.StringIO()\n",
    "    df_correct_verdicts.to_csv(csv_buffer, index=False)\n",
    "    correct_verdict_responses = csv_buffer.getvalue()\n",
    "    correct_verdict_responses_fpath = os.path.join(METRICS_DIR, CORRECT_VERDICT_RESPONSES_FILE)  # Define full S3 path\n",
    "    write_to_s3(correct_verdict_responses, BUCKET_NAME, \"\", \n",
    "                METRICS_DIR, CORRECT_VERDICT_RESPONSES_FILE)\n",
    "    logger.info(f\"Correct verdict responses from the PoLL evaluation sent to s3://{BUCKET_NAME}/{correct_verdict_responses_fpath}\")\n",
    "df_correct_verdicts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 18:35:55,557] p16430 {2360764180.py:1} INFO - Number of correct responses in total: 150\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Number of correct responses in total: {df_correct_verdicts.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 18:35:56,418] p16430 {1895158047.py:13} INFO - Summary on each eval (max voting/average pooling) for each panel judge sent to s3://sagemaker-fmbench-write-us-east-1-988564344122/fmbench-bedrock-fmbench-1-us-east-1-role/data/metrics/yyyy=2024/mm=07/dd=15/hh=18/mm=01/llm_as_a_judge_per_eval_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judge_model_id</th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>verdict</th>\n",
       "      <th>count</th>\n",
       "      <th>mean_cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>correct</td>\n",
       "      <td>150</td>\n",
       "      <td>0.408023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>40</td>\n",
       "      <td>0.287548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    judge_model_id                         candidate_model  \\\n",
       "0  mistral.mistral-large-2402-v1:0  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "1  mistral.mistral-large-2402-v1:0  anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "     verdict  count  mean_cosine_similarity  \n",
       "0    correct    150                0.408023  \n",
       "1  incorrect     40                0.287548  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panel_summary_responses_df = df_eval_results.groupby(['judge_model_id', 'candidate_model', 'verdict']).agg(\n",
    "    count=('verdict', 'size'),\n",
    "    mean_cosine_similarity=('cosine_similarity_score', 'mean')\n",
    ").unstack(fill_value=0).stack().reset_index()\n",
    "\n",
    "# Save the raw results to a CSV file and upload to S3\n",
    "csv_buffer = io.StringIO()\n",
    "panel_summary_responses_df.to_csv(csv_buffer, index=False)\n",
    "panel_summary_responses = csv_buffer.getvalue()\n",
    "llm_as_a_judge_per_eval_summary_fpath = os.path.join(METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES)\n",
    "\n",
    "write_to_s3(panel_summary_responses, BUCKET_NAME, \"\", METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES)\n",
    "logger.info(f\"Summary on each eval (max voting/average pooling) for each panel judge sent to s3://{BUCKET_NAME}/{llm_as_a_judge_per_eval_summary_fpath}\")\n",
    "panel_summary_responses_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the overall accuracy of each model scored by the PoLL\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>verdict</th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>judge_model_id</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>error_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>mistral.mistral-large-2402-v1:0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "verdict                         candidate_model  \\\n",
       "0        anthropic.claude-3-haiku-20240307-v1:0   \n",
       "\n",
       "verdict                   judge_model_id  correct  incorrect  accuracy  \\\n",
       "0        mistral.mistral-large-2402-v1:0    150.0       40.0      79.0   \n",
       "\n",
       "verdict  error_rate  \n",
       "0              21.0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_panel_judgement_result_df = panel_summary_responses_df.pivot_table(\n",
    "    index=['candidate_model', 'judge_model_id'],\n",
    "    columns='verdict',\n",
    "    values='count',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Ensure 'correct' and 'incorrect' columns exist\n",
    "if 'correct' not in per_panel_judgement_result_df.columns:\n",
    "    per_panel_judgement_result_df['correct'] = 0\n",
    "if 'incorrect' not in per_panel_judgement_result_df.columns:\n",
    "    per_panel_judgement_result_df['incorrect'] = 0\n",
    "\n",
    "# Calculate accuracy and error rate\n",
    "per_panel_judgement_result_df['accuracy'] = (\n",
    "    per_panel_judgement_result_df.apply(\n",
    "        lambda row: 100 if row['incorrect'] == 0 else round(row['correct'] / (row['correct'] + row['incorrect']), 2) * 100,\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "per_panel_judgement_result_df['error_rate'] = (\n",
    "    per_panel_judgement_result_df.apply(\n",
    "        lambda row: 0 if row['incorrect'] == 0 else round(row['incorrect'] / (row['correct'] + row['incorrect']), 2) * 100,\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "per_panel_judgement_result_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>mean_cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.381169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          candidate_model  mean_cosine_similarity\n",
       "0  anthropic.claude-3-haiku-20240307-v1:0                0.381169"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean cosine similarity for each candidate model\n",
    "mean_cosine_similarity = df_eval_results.groupby('candidate_model')['cosine_similarity_score'].mean().reset_index()\n",
    "mean_cosine_similarity = mean_cosine_similarity.rename(columns={'cosine_similarity_score': 'mean_cosine_similarity'})\n",
    "mean_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>mean_cosine_similarity</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>error_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.381169</td>\n",
       "      <td>79.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          candidate_model  mean_cosine_similarity  accuracy  \\\n",
       "0  anthropic.claude-3-haiku-20240307-v1:0                0.381169      79.0   \n",
       "\n",
       "   error_rate  \n",
       "0        21.0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_accuracy_grouped_panel_df = per_panel_judgement_result_df.groupby('candidate_model')[['accuracy', 'error_rate']].mean().reset_index()\n",
    "overall_accuracy_grouped_panel_df = pd.merge(mean_cosine_similarity, overall_accuracy_grouped_panel_df, on='candidate_model')\n",
    "overall_accuracy_grouped_panel_df = overall_accuracy_grouped_panel_df.sort_values(by='accuracy', ascending=False)\n",
    "overall_accuracy_grouped_panel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 18:38:27,233] p16430 {1651915844.py:10} INFO - Overall accuracy and error rates results of each model sent to s3://sagemaker-fmbench-write-us-east-1-988564344122/fmbench-bedrock-fmbench-1-us-east-1-role/data/metrics/yyyy=2024/mm=07/dd=15/hh=18/mm=01/PoLL_per_model_accuracy.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_model</th>\n",
       "      <th>mean_cosine_similarity</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>error_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anthropic.claude-3-haiku-20240307-v1:0</td>\n",
       "      <td>0.381169</td>\n",
       "      <td>79.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          candidate_model  mean_cosine_similarity  accuracy  \\\n",
       "0  anthropic.claude-3-haiku-20240307-v1:0                0.381169      79.0   \n",
       "\n",
       "   error_rate  \n",
       "0        21.0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send the accuracy metrics to s3\n",
    "csv_buffer = io.StringIO()\n",
    "overall_accuracy_grouped_panel_df.to_csv(csv_buffer, index=False)\n",
    "overall_panel_result = csv_buffer.getvalue()\n",
    "overall_panel_accuracy_metrics_fpath = os.path.join(METRICS_DIR, PER_MODEL_ACCURACY_POLL)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(overall_panel_result, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, PER_MODEL_ACCURACY_POLL)\n",
    "logger.info(f\"Overall accuracy and error rates results of each model sent to s3://{BUCKET_NAME}/{overall_panel_accuracy_metrics_fpath}\")\n",
    "overall_accuracy_grouped_panel_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get accuracy statements\n",
    "# Rank models by accuracy\n",
    "ranked_models = overall_accuracy_grouped_panel_df.sort_values(by='accuracy', ascending=False)\n",
    "highest_accuracy = ranked_models['accuracy'].max()\n",
    "\n",
    "# Group models with the highest accuracy\n",
    "top_performers = ranked_models[ranked_models['accuracy'] == highest_accuracy]\n",
    "other_models = ranked_models[ranked_models['accuracy'] < highest_accuracy]\n",
    "final_ranking = pd.concat([top_performers, other_models])\n",
    "unique_judge_model_ids = per_panel_judgement_result_df['judge_model_id'].unique()\n",
    "PoLL_model_ids = ', '.join(map(str, unique_judge_model_ids))\n",
    "top_performing_model_ids = ', '.join(top_performers['candidate_model'].tolist())\n",
    "\n",
    "# cosine similarity score data\n",
    "highest_cosine_model = final_ranking.loc[final_ranking['mean_cosine_similarity'].idxmax()]\n",
    "highest_cosine_model_name = highest_cosine_model['candidate_model']\n",
    "highest_cosine_similarity = highest_cosine_model['mean_cosine_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if other_models.empty:\n",
    "    other_models = f\"All models performed the same with an accuracy of {highest_accuracy:.2f}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_statement = MAX_VOTING_RESULT_STATEMENT.format(\n",
    "    judge_model_ids=PoLL_model_ids,\n",
    "    highest_accuracy=highest_accuracy,\n",
    "    top_models=top_performers.to_string(index=False),\n",
    "    highest_cosine_similarity=round(highest_cosine_similarity, 4),\n",
    "    top_cosine_similarity_model=highest_cosine_model_name,\n",
    "    ranked_models=other_models,\n",
    "    top_performing_model_ids=top_performing_model_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-15 18:38:29,672] p16430 {84484965.py:8} INFO - Overall accuracy and error rates results of each model sent to s3://sagemaker-fmbench-write-us-east-1-988564344122/fmbench-bedrock-fmbench-1-us-east-1-role/data/metrics/yyyy=2024/mm=07/dd=15/hh=18/mm=01/overall_PoLL_report.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "A Detailed Analysis of Model Performance Based on Accuracy using Panel of LLM Evaluators (PoLL):\n",
      "\n",
      "This accuracy benchmarking was done using a Panel of LLM evaluators. mistral.mistral-large-2402-v1:0 were used as judges. \n",
      "This analysis is on the entire data that candidate models used to generate inference.\n",
      "\n",
      "Top Performing Models for Accuracy (79.0% Accuracy):\n",
      "                       candidate_model  mean_cosine_similarity  accuracy  error_rate\n",
      "anthropic.claude-3-haiku-20240307-v1:0                0.381169      79.0        21.0\n",
      "\n",
      "Top Performing Model for Cosine Similarity (0.3812 Cosine Similarity Score):\n",
      "anthropic.claude-3-haiku-20240307-v1:0\n",
      "\n",
      "Other Ranked Models:\n",
      "All models performed the same with an accuracy of 79.00.\n",
      "\n",
      "Summary:\n",
      "The top-performing models, including anthropic.claude-3-haiku-20240307-v1:0, have achieved 79.0% accuracy, setting a benchmark for accuracy. The remaining models are ranked based on their accuracy and error rates. \n",
      "\n",
      "The top-performing model in quantitative evaluation, using the Cosine similarity\n",
      "score, is anthropic.claude-3-haiku-20240307-v1:0 with a score of 0.3812.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# send the overall accuracy report to s3\n",
    "txt_buffer = io.StringIO()\n",
    "txt_buffer.write(accuracy_statement)\n",
    "poll_txt_file_content = txt_buffer.getvalue()\n",
    "overall_panel_accuracy_metrics_fpath = os.path.join(METRICS_DIR, OVERALL_POLL_REPORT)\n",
    "write_to_s3(poll_txt_file_content, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, OVERALL_POLL_REPORT)\n",
    "logger.info(f\"Overall accuracy and error rates results of each model sent to s3://{BUCKET_NAME}/{overall_panel_accuracy_metrics_fpath}\")\n",
    "print(accuracy_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final analysis of incorrect responses to spot trends, patterns & observations\n",
    "---\n",
    "\n",
    "In this portion of the step, we do the following:\n",
    "\n",
    "1. We add another layer of a final LLM that goes above all of the verdicts, questions, ground truth, and then gives a set of trends and observations as to how the candidate models performed and what to do to improve their performance.\n",
    "\n",
    "1. This makes scaling efficient. After getting a quick `vibe check` from the PoLL evaluation results, users can read through the thorough report to get a sense of what to change to make a particular model perform better than the rest and make an efficient evaluation decision/run this evaluation pipeline again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write all explanations to a file and send to S3\n",
    "explanations_txt_buffer = io.StringIO()\n",
    "for index, row in df_eval_results.iterrows():\n",
    "    explanations_txt_buffer.write(f\"candidate model: {row['candidate_model']}\\ncandidate model response: {row['candidate_model_response']}\\nground truth: {row['ground_truth']}\\nverdict and explanation: {row['completion']}\\n\\n\")\n",
    "\n",
    "explanations_txt_file_content = explanations_txt_buffer.getvalue()\n",
    "explanations_fpath = os.path.join(METRICS_DIR, ALL_EVALUATIONS_IN_TXT)\n",
    "write_to_s3(explanations_txt_file_content, BUCKET_NAME, \"\", METRICS_DIR, ALL_EVALUATIONS_IN_TXT)\n",
    "logger.info(f\"All text eval content from the llm judge panelists sent to s3://{BUCKET_NAME}/{explanations_fpath}\")\n",
    "print(explanations_txt_file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open the prompt template and prepare it for inference\n",
    "with open(os.path.join(eval_prompts_dir, model_eval_subjective_info.get('final_evaluation_prompt_template', None)), 'r') as file:\n",
    "    final_summary_prompt = file.read()\n",
    "    processed_summary_eval_prompt: str = final_summary_prompt.format(context=explanations_txt_file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_evaluation_summarizer: str = model_eval_subjective_info.get('final_evaluation_summarizer', None)\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Trying to get a final summary from the inferences, verdicts and explanations\")\n",
    "    final_analysis = completion(\n",
    "      model=final_evaluation_summarizer,\n",
    "      messages=[{ \"content\": processed_summary_eval_prompt,\"role\": \"user\"}]\n",
    "    )\n",
    "\n",
    "    final_summary_buffer = io.StringIO()\n",
    "    final_summary_buffer.write(final_analysis['choices'][0]['message']['content'] + \"\\n\")\n",
    "    final_summary_content = final_summary_buffer.getvalue()\n",
    "    final_summary_fpath = os.path.join(METRICS_DIR, FINAL_ANALYSIS_REPORT)\n",
    "    write_to_s3(final_summary_content, BUCKET_NAME, \"\", METRICS_DIR, FINAL_ANALYSIS_REPORT)\n",
    "    logger.info(f\"Final summary analysis sent to s3://{BUCKET_NAME}/{final_summary_fpath}\")\n",
    "    print(final_summary_content)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not get the final summary analysis on the evaluations: {e}\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
