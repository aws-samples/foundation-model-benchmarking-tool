{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data: Gather data, create prompts/payloads of different sizes\n",
    "---------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "### This part of our solution design includes \n",
    "\n",
    "- running and downloading our specific dataset\n",
    "\n",
    "- generating prompts as payloads of different sizes that we will send to our different model endpoints with different combinations of concurrency levels that we will later use to run inference and generate benchmarking metrics and visualizations.\n",
    "\n",
    "#### This file will generate all data on wikiqa (english version) with prompt sizes 300 - 4000 token lengths in different payload sizes to send to the model endpoint during the inference pipeline. You will also be able to generate the normal wikiqa dataset from the actual 'long bench dataset'. This notebook then focuses on 3 main deliverables:\n",
    "\n",
    "1. Loading the dataset that is stored within the dataset in the data directory.\n",
    "\n",
    "\n",
    "2. Generating payloads: This notebook also converts the loaded datasets into payloads based on the input question and records the context length of the prompt to send as a part of the payload during running inferences on the deployed endpoints.\n",
    "\n",
    "    - All of the prompts are saved in this data directory in a file named all_prompts.csv.\n",
    "    \n",
    "\n",
    "3. Constructing different sized payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from typing import Dict, List, Optional\n",
    "import importlib.resources as pkg_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a logger to log all messages while the code runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## config.yml file contains information that is used across this benchmarking environment, \n",
    "## such as information about the aws account, prompts, payloads to be used for invocations\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the file path for the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_file_path = \"/\".join([config['s3_read_data']['prompt_template_dir'],\n",
    "                         config['s3_read_data']['prompt_template_file']])\n",
    "\n",
    "## download the file from s3 else check locally and use that version\n",
    "prompt_template_from_s3: str = read_from_s3(config['s3_read_data']['read_bucket'], s3_file_path)\n",
    "\n",
    "prompt_template_dir = Path(pkg_resources.files(FMBENCH_PACKAGE_NAME), config['s3_read_data']['prompt_template_dir'])\n",
    "logger.info(f\"Using fmbench.{config['s3_read_data']['prompt_template_dir']} directory: {prompt_template_dir}\")\n",
    "\n",
    "if prompt_template_from_s3 is None:\n",
    "    promtp_template_fpath: str = os.path.join(prompt_template_dir, config['s3_read_data']['prompt_template_file'])\n",
    "    prompt_template = Path(promtp_template_fpath).read_text()\n",
    "    logger.info(f\"Using the default local prompt template --> {prompt_template}\")\n",
    "else:\n",
    "    prompt_template = prompt_template_from_s3\n",
    "    logger.info(f\"Using the prompt template from S3 --> {prompt_template}\")\n",
    "prompt_template = prompt_template.strip()\n",
    "\n",
    "# Calculate the number of tokens in the prompt template\n",
    "prompt_template_keys = config['datasets']['prompt_template_keys']\n",
    "# get the ground truth key from the dataset section of the config file\n",
    "ground_truth_col_key: Optional[str] = config['datasets'].get('ground_truth_col_key', None)\n",
    "# get the question col key from the dataset section of the config file\n",
    "question_col_key: Optional[str] = config['datasets'].get('question_col_key', None)\n",
    "\n",
    "args = {}\n",
    "for k in prompt_template_keys:\n",
    "    args[k] = \"\"\n",
    "empty_prompt_template = prompt_template.format(**args)\n",
    "logger.info(f\"empty prompt template = \\\"{empty_prompt_template}\\\"\")\n",
    "empty_prompt_len_in_tokens = count_tokens(empty_prompt_template)\n",
    "\n",
    "# Log the number of tokens\n",
    "logger.info(f\"prompt template length={empty_prompt_len_in_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_files():\n",
    "    response = s3_client.list_objects_v2(Bucket=config['s3_read_data']['read_bucket'], Prefix=config['s3_read_data']['source_data_prefix'])\n",
    "    return [obj['Key'] for obj in response['Contents']]\n",
    "\n",
    "# List all files in the bucket and prefix\n",
    "# s3_files = list_files()\n",
    "s3_files = list_s3_files(config['s3_read_data']['read_bucket'], config['s3_read_data']['source_data_prefix'], '.jsonl')\n",
    "logger.info(f\"s3 paths of the data set -> {s3_files}\")\n",
    "\n",
    "# Log the files you're going to read\n",
    "logger.info(f\"dataset files = {s3_files}\")\n",
    "\n",
    "# Read and concatenate DataFrames\n",
    "if is_read_local():\n",
    "    jsonl_files = s3_files\n",
    "else:\n",
    "    jsonl_files = [file_key for file_key in s3_files if file_key.replace(config['s3_read_data']['source_data_prefix'] + \"/\", \"\") in config['s3_read_data']['source_data_files']]\n",
    "logger.info(f\"jsonl_files={jsonl_files}\")\n",
    "# Read and concatenate only the .jsonl files\n",
    "# df = pd.concat([pd.read_json(io.BytesIO(s3_client.get_object(Bucket=config['s3_read_data']['read_bucket'], Key=file_key)['Body'].read()), lines=True) \n",
    "#                  for file_key in jsonl_files])\n",
    "df = pd.concat([pd.read_json(io.BytesIO(get_s3_object(config['s3_read_data']['read_bucket'], file_key, decode=False)), lines=True) \n",
    "                for file_key in jsonl_files])\n",
    "\n",
    "# Log the source of the dataset and its shape\n",
    "logger.info(f\"dataset read from {s3_files}\\nhas shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View a portion of the df to view inputs, contexts, and more information on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display basic statistics on the existing dataset: including count, mean, std, min, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"distribution of the length field in the dataset is as follows ->\\n{df.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataset elements into prompts as payloads for inference purposes\n",
    "\n",
    "Now, we will focus on converting the existing data within our datasets, and extract the information to convert it into prompts to be able to send to our deployed model endpoints during the process of testing and benchmarking for results and various metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df['prompt'] = df.apply(lambda row: process_item(row, config['datasets']['prompt_template_keys'], prompt_template), axis=1)\n",
    "df['prompt_len'] = df.prompt.map(lambda x: x['prompt_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame to a CSV format string\n",
    "csv_buffer = io.StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "csv_data = csv_buffer.getvalue()\n",
    "all_prompts_file = config['dir_paths']['all_prompts_file']\n",
    "\n",
    "# Write to S3 using the write_to_s3 function\n",
    "write_to_s3(csv_data,\n",
    "            config['aws']['bucket'],\n",
    "            DATA_DIR, config['dir_paths']['prompts_prefix'],\n",
    "            all_prompts_file)\n",
    "\n",
    "# Log where the prompts are saved\n",
    "logger.info(f\"all prompts dataframe of shape {df.shape} saved to \"\n",
    "            f\"s3://{config['aws']['bucket']}/{DATA_DIR}/{os.path.join(config['dir_paths']['prompts_prefix'], all_prompts_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View some of the prompts \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Prompts into Payloads for inference purposes\n",
    "------\n",
    "Now we will prepare data for model inference. It involves converting prompts, created and stored in a specific format, into payloads for inference. We will utilize the prompt file for our model and incorporate the prompt into a payload using that. \n",
    "\n",
    "These payloads are tailored to the needs of deployed model endpoints. The conversion considers prompt sizes and specific configurations to further make our benchmarking more detailed and comprehensive. \n",
    "\n",
    "The goal is to have a set of well-formatted and parameterized payload requests of various sizes ready to be sent to the model endpoints for inference, with the responses to be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to construct a single request payload based on row prompt data and configuration\n",
    "def construct_request_payload(row, config: Dict) -> Dict:\n",
    "\n",
    "    # Deep copy inference parameters from the config.yml file - feel free to change this based on the model type you are using\n",
    "    # parameters = copy.deepcopy(config['inference_parameters']['common'])\n",
    "    # truncate = parameters.get('truncate', None)\n",
    "    # if truncate == TRUNCATE_POLICY.AT_PROMPT_TOKEN_LENGTH:\n",
    "    #     parameters['truncate'] = row['prompt_len']\n",
    "\n",
    "    # Return the constructed payload along with the ground truth if any\n",
    "    # is contained within the dataset\n",
    "    prompt_dict: Optional[Dict] = None\n",
    "    try:\n",
    "        # Construct the base prompt dictionary with the prompt input\n",
    "        prompt_dict = dict(inputs=row['prompt']['prompt'])\n",
    "\n",
    "        # Add ground truth and question_col_key if available\n",
    "        if ground_truth_col_key is not None and ground_truth_col_key in row:\n",
    "            prompt_dict['ground_truth'] = row[ground_truth_col_key]\n",
    "\n",
    "        if question_col_key is not None and question_col_key in row:\n",
    "            prompt_dict['question'] = row[question_col_key]\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prompt template could not be constructed: {e}\")\n",
    "        prompt_dict = None\n",
    "\n",
    "    return prompt_dict\n",
    "\n",
    "\n",
    "# Function to create a dataset payload files from the given dataset file we have\n",
    "def create_dataset_payload_file(df: pd.DataFrame, dataset_info: Dict, config: Dict) -> str:\n",
    "\n",
    "    # First, log the dataset existing information\n",
    "    logger.info(f\"going to create a payload file as dataset_info={json.dumps(dataset_info, indent=2)}\")\n",
    "\n",
    "    # Filter the DataFrame based on prompt length and language given below for constructing payloads of various sizes\n",
    "    df['prompt_len_in_range'] = df.prompt.map(lambda x: x['prompt_len'] >= dataset_info['min_length_in_tokens'] and \\\n",
    "                                                        x['prompt_len'] < dataset_info['max_length_in_tokens'])\n",
    "\n",
    "    # select prompts between pre-configured threshold lengths and are in the selected language\n",
    "    if 'language' in df.columns:\n",
    "        df_filtered = df[(df.language == dataset_info['language']) & (df.prompt_len_in_range)]\n",
    "    else:\n",
    "        df_filtered = df[df.prompt_len_in_range]\n",
    "\n",
    "    logger.info(f\"after filtering for {json.dumps(dataset_info, indent=2)}, shape of dataframe is {df_filtered.shape}\")\n",
    "    if df_filtered.shape[0] == 0:\n",
    "        logger.error(f\"did not find any prompts in the dataframe that matched the filtering criteria, exiting\")\n",
    "        return None\n",
    "    # df_filtered.head()\n",
    "\n",
    "    # Here, we construct request payloads for each row in the filtered DataFrame\n",
    "    df_filtered['request'] = df_filtered.apply(lambda r: construct_request_payload(r, config), axis=1)\n",
    "    logger.info(f\"payload request entry looks like this -> {json.dumps(df_filtered['request'].iloc[0], indent=2)}\")\n",
    "\n",
    "    # Convert the 'request' column of the filtered DataFrame to a JSON Lines string\n",
    "    json_lines_str = df_filtered['request'].to_json(orient='records', lines=True)\n",
    "\n",
    "    lang = dataset_info['language']\n",
    "    min_len = dataset_info['min_length_in_tokens']\n",
    "    max_len = dataset_info['max_length_in_tokens']\n",
    "    file_name = dataset_info['payload_file'].format(lang=lang, min=min_len, max=max_len)\n",
    "\n",
    "    prompts_path = os.path.join(DATA_DIR, config['dir_paths']['prompts_prefix'])\n",
    "\n",
    "    # defining the s3_path these prompts will go to\n",
    "    s3_file_path = os.path.join(prompts_path, file_name)\n",
    "\n",
    "    # Write the JSON Lines string to S3\n",
    "    # get the bucket name, config vars from config file\n",
    "    prefix = f\"{config['dir_paths']['prompts_prefix']}/{config['s3_read_data']['source_data_prefix']}\"\n",
    "    write_to_s3(json_lines_str, config['aws']['bucket'], DATA_DIR, prefix, file_name)\n",
    "\n",
    "    logger.info(f\"dataset of different payload file structures saved to s3://{config['aws']['bucket']}/{s3_file_path}\")\n",
    "    return f\"s3://{config['aws']['bucket']}/{s3_file_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = ((df, d, config) for d in config['datasets']['filters'])\n",
    "\n",
    "# This results in the creation of payload files for each dataset\n",
    "paths: List = list(itertools.starmap(create_dataset_payload_file, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join([p for p in paths if p]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
