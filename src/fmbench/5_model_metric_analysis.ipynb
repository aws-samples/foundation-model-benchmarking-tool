{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics Analysis \n",
    "---------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "### This part of our solution design includes the chunk of taking the metrics generated and creating visualizations from it for further analysis to make decisions more quicker, efficient, and cost optimal.\n",
    "\n",
    "- In this file, we will go over and create side by side visualizations of different models deployed, how their inference latency is impacted based on the concurrency level, instance size and different model configurations. Using these visualizations and charts, making executive decisions, saving on time and cost becomes critical. \n",
    "\n",
    "\n",
    "- In this notebook, we will also record the error rates for each of the deployed model endpoints based on how it ran against different metrics as specified above. These visualizations will be applicable and work for any and every jumpstart and non jumpstart model if deployed correctly using the prior steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import math\n",
    "import json\n",
    "import tempfile\n",
    "import datetime\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import seaborn and other related libraries for visualizations and plotting charts\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tomark import Tomark\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from dateutil.parser import parse\n",
    "from typing import List, Optional, Dict\n",
    "import importlib.resources as pkg_resources\n",
    "from fmbench import __version__ as fmbench_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# rcParams for configuring Matplotlib settings\n",
    "from matplotlib import rcParams\n",
    "import plotly\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# figure size in inches\n",
    "rcParams['figure.figsize'] = 10, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"CONFIG_FILE={CONFIG_FILE}\")\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the associated pricing config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "pricing_file_path: str = config['pricing'] \n",
    "\n",
    "# initialize the pricing config file to None\n",
    "pricing_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files('fmbench'), 'configs')\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "pricing_module = Path(config['pricing'])\n",
    "logger.info(f\"pricing config provided for inference from this model is --> {pricing_module}\")\n",
    "pricing_file_path = os.path.join(config_dir, pricing_module)\n",
    "logger.info(f\"pricing config file path is --> {pricing_file_path}\")\n",
    "\n",
    "pricing_config = load_config(pricing_file_path)\n",
    "logger.info(f\"pricing config file recorded: {json.dumps(pricing_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug is True:\n",
    "    metrics_path_file: str = os.path.join(\"..\", \"..\", METADATA_DIR, METRICS_PATH_FNAME)\n",
    "else:\n",
    "    metrics_path_file: str = os.path.join(METADATA_DIR, METRICS_PATH_FNAME)\n",
    "logger.info(f\"cwd={os.getcwd()}, METADATA_DIR={METADATA_DIR}, METRICS_PATH_FNAME={METRICS_PATH_FNAME}, metrics_path_file={metrics_path_file}\")\n",
    "METRICS_DIR: str = Path(metrics_path_file).read_text().strip()\n",
    "logger.info(f\"metrics_path_file={metrics_path_file}, METRICS_DIR={METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join(METRICS_DIR, config[\"report\"][\"per_inference_request_file\"])\n",
    "logger.info(f\"File path containing the metrics per inference folder --> {file_path}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    file_content = get_s3_object(config['aws']['bucket'], file_path)\n",
    "    # Use pandas to read the CSV content\n",
    "    df_per_inference = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(f\"{file_path} read into dataframe of shape {df_per_inference.shape}, \"\n",
    "                f\"cols={df_per_inference.columns}\")\n",
    "    logger.info(f\"{file_path} contains results for the following endpoints={df_per_inference.endpoint_name.unique()}\")\n",
    "    logger.info(df_per_inference.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between prompt token length and inference latency for different instances and concurrency levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rename a column in the dataframe for clarity of the instance parameter of the model used\n",
    "df_per_inference = df_per_inference.rename(columns={\"instance_type\": \"instance\"})\n",
    "logger.info(f\"df_per_inference.latency.quantile ->\\n{df_per_inference.latency.quantile([0.25, 0.5, 0.75, 0.95, 0.99])}\")\n",
    "if df_per_inference.latency.quantile(0.5) < 1:\n",
    "    print(\"multiplying by 1000\")\n",
    "    latency_units = \"milliseconds\"\n",
    "    multiplier = 1000\n",
    "    step_size = 500\n",
    "    df_per_inference.latency = df_per_inference.latency*1000\n",
    "else:\n",
    "    multiplier = 10\n",
    "    step_size = 5\n",
    "    latency_units = \"seconds\"\n",
    "\n",
    "## Initializing yticks and title for the chart\n",
    "yticks: Optional[List] = None\n",
    "title: Optional[str] = None\n",
    "\n",
    "if config['report'].get('latency_vs_token_len_chart'):\n",
    "    yticks: List = config['report']['latency_vs_token_len_chart'].get('y_ticks')\n",
    "    title: str = config['report']['latency_vs_token_len_chart'].get('title')\n",
    "\n",
    "if title is None:\n",
    "    title = \"Effect of token length on inference latency\"\n",
    "\n",
    "unique_instance_types = df_per_inference.instance.unique()\n",
    "if len(unique_instance_types) == 1:\n",
    "    logger.info(f\"there is only {len(unique_instance_types)} instance type ({unique_instance_types}), \"\n",
    "                f\"not using row as instance\")\n",
    "    # This created a FacetGrid for plotting multiple scatter plots based on 'instance' and 'concurrency' categories\n",
    "    g = sns.FacetGrid(df_per_inference, col=\"concurrency\", hue=\"instance\", height=3.5, aspect=1.25, col_wrap=3)\n",
    "else:\n",
    "    logger.info(f\"there are {len(unique_instance_types)} instance types ({unique_instance_types}), \"\n",
    "                f\"using row as instance\")\n",
    "    g = sns.FacetGrid(df_per_inference, col=\"concurrency\", row=\"instance\", hue=\"instance\", height=3.5, aspect=1.25)\n",
    "\n",
    "\n",
    "## Subtitle of the facetgrid\n",
    "g.fig.suptitle(title)\n",
    "# # This will map a scatterplot to the FacetGrid for each subset of the data\n",
    "sns_plot = g.map(sns.scatterplot, \"prompt_tokens\", \"latency\")\n",
    "\n",
    "# flatten axes into a 1-d array\n",
    "axes = g.axes.flatten()\n",
    "\n",
    "# iterate through the axes\n",
    "for i, ax in enumerate(axes):\n",
    "    if latency_units == \"milliseconds\":\n",
    "        m = 1000\n",
    "    else:\n",
    "        m = 1\n",
    "    ax.axhline(config['report']['latency_budget']*m, ls='--', c='red')\n",
    "\n",
    "# Set the y-axis label for all plots\n",
    "g = g.set_ylabels(f\"Latency ({latency_units})\")\n",
    "    \n",
    "if yticks is None:\n",
    "    # Y-axis ticks based on the maximum latency value and setting them in that manner\n",
    "    yticks: List = list(range(0, (int(df_per_inference.latency.max())//multiplier+2)*multiplier, step_size))\n",
    "\n",
    "if yticks is None:\n",
    "    # Y-axis ticks based on the maximum latency value and setting them in that manner\n",
    "    yticks: List = list(range(0, (int(df_per_inference.latency.max())//multiplier+1)*multiplier, step_size))\n",
    "    logger.info(f\"yticks was not configured, setting it to yticks[:10]={yticks[:10]}, \"\n",
    "                f\"based on latency max of {df_per_inference.latency.max()}s, \"\n",
    "                f\"multiplier={multiplier}, step_size={step_size}\")\n",
    "else:\n",
    "    logger.info(f\"yticks is configured, yticks={yticks}\")\n",
    "g = g.set(yticks=yticks)\n",
    "\n",
    "# Set the x-axis label for all plots as the prompt length or tokens\n",
    "g = g.set_xlabels(\"Prompt length (tokens)\")\n",
    "\n",
    "# Create a bytes buffer to save the plot\n",
    "buffer = io.BytesIO()\n",
    "sns_plot.savefig(buffer, format='png')\n",
    "buffer.seek(0)  # Rewind buffer to the beginning\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, TOKENS_VS_LATENCY_PLOT_FNAME)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{TOKENS_VS_LATENCY_PLOT_FNAME}\")\n",
    "\n",
    "# Optionally, display the plot\n",
    "sns_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.latency.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the all metrics file path and read it to generate visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_metrics_fpath = os.path.join(METRICS_DIR, config[\"report\"][\"all_metrics_file\"])\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    logger.info(f\"going to read all metrics file from {all_metrics_fpath}\")\n",
    "    file_content = get_s3_object(BUCKET_NAME, all_metrics_fpath)\n",
    "    \n",
    "    # Use pandas to read the CSV content\n",
    "    df_all_metrics = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(f\"{all_metrics_fpath} read into dataframe of shape {df_all_metrics.shape}\")\n",
    "    df_all_metrics.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n",
    "# if the instance count is not set then set it to 1, this happens\n",
    "# in case of BYOE or Bedrock, so we want to count such a case as 1 compute unit\n",
    "\n",
    "df_all_metrics['instance_count'] = df_all_metrics['instance_count'].fillna(1)\n",
    "df_all_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## displaying all of the available columns in the all metrics dataframe\n",
    "df_all_metrics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_metrics.instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the number of experiment names within the metrics dataframe, instance types and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments = df_all_metrics.experiment_name.unique()\n",
    "instance_types = df_all_metrics.instance_type.unique()\n",
    "# model_names = df_all_metrics.ModelName.unique()\n",
    "# logger.info(f\"contains information about {len(experiments)} experiments, {len(instance_types)} instance types, {len(model_names)} models\")\n",
    "logger.info(f\"contains information about {len(experiments)} experiments, {len(instance_types)} instance types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## extract some of the columns\n",
    "relevant_cols = ['experiment_name',\n",
    "                   'payload_file',\n",
    "                     'instance_type',\n",
    "                      'instance_count', \n",
    "                       'concurrency',\n",
    "                         'error_rate',\n",
    "                           'prompt_token_count_mean',\n",
    "                             'prompt_token_throughput',\n",
    "                               'completion_token_count_mean',\n",
    "                                 'completion_token_throughput',\n",
    "                                   #'latency_mean',\n",
    "                                      'latency_p50',\n",
    "                                        'latency_p95',\n",
    "                                         'latency_p99',\n",
    "                                         #'TTFT_mean',\n",
    "                                         'TTFT_p50',\n",
    "                                         #'TTFT_p95',\n",
    "                                         'TTFT_p99',\n",
    "                                          #'TPOT_mean',\n",
    "                                           'TPOT_p50',\n",
    "                                           # 'TPOT_p95',\n",
    "                                             'TPOT_p99',\n",
    "                                          # 'TTLT_mean',\n",
    "                                          #  'TTLT_p50',\n",
    "                                          #   'TTLT_p95',\n",
    "                                          #    'TTLT_p99',\n",
    "                                              'transactions_per_minute']\n",
    "\n",
    "## initialize a group by columns to use further in generating portions of the dataframe and filtering it\n",
    "group_by_cols = ['experiment_name',\n",
    "                   'payload_file',\n",
    "                     'instance_type',\n",
    "                      'instance_count', \n",
    "                        'concurrency']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an 'experiment_counts.csv' to store metrics on experiment name, the payload file, concurrency and the total counts associated to that given experiment to visualize the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_metrics.instance_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_counts = df_all_metrics[group_by_cols].value_counts().reset_index()\n",
    "\n",
    "# Convert df_counts to CSV format\n",
    "csv_buffer = io.StringIO()\n",
    "df_counts.to_csv(csv_buffer, index=False)\n",
    "csv_data = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name and the S3 path\n",
    "COUNTS_FNAME = \"experiment_counts.csv\"\n",
    "counts_s3_path = os.path.join(METRICS_DIR, COUNTS_FNAME)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(csv_data, BUCKET_NAME, \"\", METRICS_DIR, COUNTS_FNAME)\n",
    "logger.info(f\"Counts DataFrame saved to s3://{BUCKET_NAME}/{counts_s3_path}\")\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the mean error rates for each experiment with different congifurations using the same columns of interest used in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_error_rates = df_all_metrics.groupby(group_by_cols).agg({'error_rate': 'mean'}).reset_index()\n",
    "df_error_rates = df_error_rates.round(2)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_error_rates.to_csv(csv_buffer, index=False)\n",
    "error_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name and the S3 path\n",
    "ERROR_RATES_FNAME: str = \"error_rates.csv\"\n",
    "counts_s3_path = os.path.join(METRICS_DIR, ERROR_RATES_FNAME)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(error_csv, BUCKET_NAME, \"\", METRICS_DIR, ERROR_RATES_FNAME)\n",
    "logger.info(f\"Error Counts DataFrame saved to s3://{BUCKET_NAME}/{counts_s3_path}\")\n",
    "\n",
    "df_error_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Inference error rates across different concurrency levels and instance types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_error_rates = df_error_rates.rename(columns={\"instance_type\": \"instance\", \"payload_file\": \"dataset\"})\n",
    "\n",
    "# Clean up the dataset names by removing json file extensions and prefixes\n",
    "df_error_rates.dataset = df_error_rates.dataset.map(lambda x: x.replace(\".jsonl\", \"\").replace(\"payload_\", \"\"))\n",
    "\n",
    "# this creates a facetGrid for plotting scatter plots based on 'instance' and 'dataset'\n",
    "logger.info(f\"df_error_rates --> {df_error_rates}\")\n",
    "\n",
    "df_error_rates_only_nz = df_error_rates[df_error_rates.error_rate > 0]\n",
    "logger.info(f\"there are {df_error_rates_only_nz.shape[0]} experiment runs that encountered errors\")\n",
    "if df_error_rates_only_nz.shape[0] > 0:    \n",
    "    g = sns.FacetGrid(df_error_rates_only_nz, col=\"instance\", row=\"dataset\", hue=\"instance\", height=3.5, aspect=1.25)\n",
    "    # Maps a scatterplot to the FacetGrid for each subset of the data\n",
    "    sns_plot = g.map(sns.scatterplot, \"concurrency\", \"error_rate\")\n",
    "    # flatten axes into a 1-d array\n",
    "    axes = g.axes.flatten()\n",
    "\n",
    "    # iterate through the axes\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.axhline(config['report']['error_rate_budget']*1000, ls='--', c='red')\n",
    "\n",
    "    # Create a subtitle\n",
    "    with sns.plotting_context('paper', font_scale = 1.3):\n",
    "        g.fig.suptitle(\"Inference error rates for different concurrency levels and instance types\\nOnly non-zero error rates shown.\")\n",
    "        g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\", size=8)\n",
    "\n",
    "    # Set x and y labels for this chart\n",
    "    g = g.set_ylabels(\"Error rate (failed / total inferences)\")\n",
    "    g = g.set_xlabels(\"Concurrency level\")\n",
    "    g.figure.subplots_adjust(top=.8)\n",
    "    sns_plot.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "else:\n",
    "    # create a new dummy dataframe just for plotting an empty chart\n",
    "    \n",
    "    df_error_rates_only_nz = pd.DataFrame({\"concurrency\": df_error_rates.concurrency.unique(),\n",
    "                                           \"error_rate\": [0]*len(df_error_rates.concurrency.unique())\n",
    "                                          })\n",
    "    sns_plot = sns.scatterplot(data=df_error_rates_only_nz, x=\"concurrency\", y=\"error_rate\")\n",
    "    sns_plot.set_xticks(df_error_rates_only_nz.concurrency.unique())\n",
    "    sns_plot.set(xlabel=\"Concurrency level\",\n",
    "                 ylabel=\"Error rate (failed / total inferences)\",\n",
    "                 title=\"Inference error rates for different concurrency levels and instance types\\nError rate is zero for all experiment runs.\")\n",
    "    sns_plot.axhline(config['report']['error_rate_budget']*1000, ls='--', c='red')\n",
    "    sns_plot.figure.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, ERROR_RATES_PLOT_FNAME)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{ERROR_RATES_PLOT_FNAME}\")\n",
    "\n",
    "## Display the plot \n",
    "sns_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for the df elements that have error rates above 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_error_rates_nz = df_error_rates[df_error_rates.error_rate > 0]\n",
    "df_error_rates_nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## initialize a dataframe to get the mean of the columns in consideration\n",
    "df_summary_metrics = df_all_metrics[relevant_cols].groupby(group_by_cols).mean().reset_index()\n",
    "\n",
    "# ugly way of doing this, will refactor this later (maybe)\n",
    "df_summary_metrics.fillna(PLACE_HOLDER, inplace=True)\n",
    "int_cols = ['prompt_token_count_mean', 'prompt_token_throughput', 'completion_token_count_mean', 'completion_token_throughput', 'transactions_per_minute']\n",
    "for ic in int_cols:\n",
    "    df_summary_metrics[ic] = df_summary_metrics[ic].astype(int)\n",
    "\n",
    "df_summary_metrics.replace(PLACE_HOLDER, np.nan, inplace=True)\n",
    "#df_summary_metrics.latency_p95\t= df_summary_metrics.latency_p95.round(2)\n",
    "df_summary_metrics.latency_p50 = df_summary_metrics.latency_p50.round(2)\n",
    "df_summary_metrics.latency_p95 = df_summary_metrics.latency_p95.round(2)\n",
    "df_summary_metrics.latency_p99 = df_summary_metrics.latency_p99.round(2)\n",
    "df_summary_metrics.TTFT_p50 = df_summary_metrics.TTFT_p50.round(4)\n",
    "#df_summary_metrics.TTFT_p95 = df_summary_metrics.TTFT_p95.round(4)\n",
    "df_summary_metrics.TTFT_p99 = df_summary_metrics.TTFT_p99.round(4)\n",
    "df_summary_metrics.TPOT_p50 = df_summary_metrics.TPOT_p50.round(4)\n",
    "#df_summary_metrics.TPOT_p95 = df_summary_metrics.TPOT_p95.round(4)\n",
    "df_summary_metrics.TPOT_p99 = df_summary_metrics.TPOT_p99.round(4)\n",
    "# df_summary_metrics.TTLT_p50 = df_summary_metrics.TTLT_p50.round(4)\n",
    "# df_summary_metrics.TTLT_p95 = df_summary_metrics.TTLT_p95.round(4)\n",
    "# df_summary_metrics.TTLT_p99 = df_summary_metrics.TTLT_p99.round(4)\n",
    "df_summary_metrics.error_rate = df_summary_metrics.error_rate.round(2)\n",
    "\n",
    "# drop the columns if all, TTFT, TPOT and TTLT are NaN\n",
    "time_to_token_cols_to_check = [col for col in df_summary_metrics.columns if col.startswith(('TTFT', 'TTLT', 'TPOT'))]\n",
    "for col in time_to_token_cols_to_check:\n",
    "    if df_summary_metrics[col].isna().all():\n",
    "        df_summary_metrics.drop(columns=col, inplace=True)\n",
    "logger.info(f\"Updated summary metrics cols: {df_summary_metrics.columns}\")\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics.to_csv(csv_buffer, index=False)\n",
    "summary_metrics_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name for S3 based on the original file path\n",
    "summary_file_name = all_metrics_fpath.replace(\"all_metrics\", \"all_metrics_summary\").split('/')[-1] \n",
    "summary_s3_path = os.path.join(METRICS_DIR, summary_file_name)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(summary_metrics_csv, BUCKET_NAME, \"\", METRICS_DIR, summary_file_name)\n",
    "logger.info(f\"Summary metrics DataFrame saved to s3://{BUCKET_NAME}/{summary_s3_path}\")\n",
    "\n",
    "df_summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_nz = df_summary_metrics[df_summary_metrics.error_rate == 0]\n",
    "logger.info(f\"there are {len(df_summary_metrics_nz)} entries out of {len(df_summary_metrics)} in the summary data for which error rate is 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = df_summary_metrics[df_summary_metrics.payload_file.str.contains(config['metrics']['dataset_of_interest'])]\n",
    "logger.info(f\"shape of dataframe with summary metrics for {config['metrics']['dataset_of_interest']} is {df_summary_metrics_dataset.shape}\")\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_dataset.to_csv(csv_buffer, index=False)\n",
    "metrics_dataset = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(metrics_dataset, BUCKET_NAME, \"\", METRICS_DIR, SUMMARY_METRICS_W_PRICING_FNAME)\n",
    "logger.info(f\"Summary metrics dataset saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{SUMMARY_METRICS_W_PRICING_FNAME}\")\n",
    "\n",
    "df_summary_metrics_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_metrics_for_dataset = df_all_metrics.rename(columns={\"instance_type\": \"instance\", \"payload_file\": \"dataset\"})\n",
    "df_all_metrics_for_dataset.dataset = df_all_metrics_for_dataset.dataset.map(lambda x: x.replace(\".jsonl\", \"\").replace(\"payload_\", \"\"))\n",
    "ds = config['metrics']['dataset_of_interest']\n",
    "df_all_metrics_for_dataset = df_all_metrics_for_dataset[df_all_metrics_for_dataset.dataset.str.contains(ds)]\n",
    "row_order = list(df_all_metrics_for_dataset[[\"instance\", \"latency_p95\"]].groupby(\"instance\").mean(\"latency_p95\").reset_index()[\"instance\"])\n",
    "if len(row_order) == 0:\n",
    "    logger.error(f\"seems like a missing configuration, no data found in df_all_metrics_for_dataset for ds={ds}\")\n",
    "    col_wrap = 1\n",
    "else:    \n",
    "    logger.error(f\"found {df_all_metrics_for_dataset.shape[0]} experiment runs for ds={ds}\")\n",
    "    col_wrap = 4 if len(row_order) > 4 else len(row_order)\n",
    "\n",
    "    sns_plot = sns.catplot(\n",
    "        data=df_all_metrics_for_dataset, x='concurrency', y='latency_p95',\n",
    "        col='instance', kind='box', col_wrap=col_wrap, hue=\"instance\", row_order=row_order, height=4.5, aspect=1.0\n",
    "    )\n",
    "    # flatten axes into a 1-d array\n",
    "    axes = sns_plot.axes.flatten()\n",
    "\n",
    "    # iterate through the axes\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.axhline(config['report']['latency_budget'], ls='--', c='red')\n",
    "    sns_plot._legend.remove()\n",
    "    sns_plot.fig.suptitle(f\"Effect of concurrency on inference latency for each instance type for the {ds} dataset\\n\\n\")\n",
    "    sns_plot = sns_plot.set_ylabels(\"Latency (seconds)\")\n",
    "    sns_plot = sns_plot.set_xlabels(\"Concurrency level\")\n",
    "    sns_plot.fig.subplots_adjust(top=0.9)\n",
    "\n",
    "    sns_plot.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Write the plot to S3\n",
    "    write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME)\n",
    "    logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pricing = pd.DataFrame.from_dict(pricing_config['pricing'], orient='index').reset_index()\n",
    "df_pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle pricing for token based pricing & hourly (instance type) pricing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_cost_per_txn(row: pd.Series, pricing: Dict) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    The instance type is supposed to be unique across all inference options\n",
    "    whether it is hourly pricing based instances (g5/p4 etc.) or Bedrock\n",
    "    model ids or anything else.\n",
    "    \"\"\"\n",
    "\n",
    "    cost_per_txn: Optional[float] = None\n",
    "    # check if this is instance type\n",
    "    pricing_for_this_instance = pricing['pricing']['instance_based'].get(row['instance_type'])\n",
    "    if pricing_for_this_instance:\n",
    "        # this is instance based pricing so then cost per txn is simply\n",
    "        logger.info(f\"pricing for {row['instance_type']} -> {pricing_for_this_instance}\")\n",
    "        if row['transactions_per_minute'] > 0:\n",
    "            instance_count = row.get('instance_count', 1)\n",
    "            logger.info(f\"calculate_cost_per_txn, instance_count={instance_count}\")\n",
    "            cost_per_txn = ((pricing_for_this_instance / 60) * instance_count) / row['transactions_per_minute']\n",
    "        else:\n",
    "            logger.info(f\"transactions_per_minute={row['transactions_per_minute']}, setting cost_per_txn=None\")\n",
    "            cost_per_txn = None\n",
    "            \n",
    "    else:\n",
    "        # this is token based pricing\n",
    "        token_based_pricing_this_model = pricing['pricing']['token_based'].get(row['instance_type'])\n",
    "        logger.info(f\"pricing for {row['instance_type']} -> {token_based_pricing_this_model}\")\n",
    "        if token_based_pricing_this_model:\n",
    "            input_token_cost = (row['prompt_token_count_mean']/1000) * \\\n",
    "                                token_based_pricing_this_model['input-per-1k-tokens']\n",
    "            output_token_cost = (row['completion_token_count_mean']/1000) * \\\n",
    "                                token_based_pricing_this_model['output-per-1k-tokens']\n",
    "            cost_per_txn = input_token_cost + output_token_cost\n",
    "        else:\n",
    "            logger.error(f\"no pricing information found for {row['instance_type']}\")\n",
    "    return cost_per_txn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset['price_per_txn'] = df_summary_metrics_dataset.apply(lambda r: calculate_cost_per_txn(r, pricing_config),\n",
    "                                                                               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset['price_per_token'] = (df_summary_metrics_dataset['price_per_txn'] / \n",
    "                                                (df_summary_metrics_dataset['prompt_token_count_mean'] + \n",
    "                                                 df_summary_metrics_dataset['completion_token_count_mean']))\n",
    "\n",
    "df_summary_metrics_dataset['price_per_token'] = df_summary_metrics_dataset['price_per_token'].apply(lambda x: '{:.8f}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score_run(row: pd.core.series.Series, config: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    check all criteria configured in the report section of the config\n",
    "    these are error rates, latency and cost to determine the \"score\"\n",
    "    for each run in the following way:\n",
    "    1. Set score to 0, criteria failed to false.\n",
    "    2. If the run has an error rate lte to the error rate budget,\n",
    "       add 1 to the score otherwise set set criteria failed to true.\n",
    "    3. If the run has a price per 10k txns lte to the cost budget,\n",
    "       add 1 to the score otherwise set set criteria failed to true.\n",
    "    4. If the run has p95 latency lte to the latency budget,\n",
    "       add 1 to the score otherwise set set criteria failed to true.\n",
    "    5. If criteria failed is false add the concurrency to the score.\n",
    "    The idea that we want to select runs that satisfy cost/error rate/latency criteria\n",
    "    and out of those prefer the run with the highest level of concurrency.\n",
    "    \"\"\"\n",
    "    score: float = 0\n",
    "    point_per_criterion: int = 1\n",
    "    any_criterion_failed: bool = False\n",
    "    error_rate_criterion_failed: bool = False\n",
    "    cost_per_txn_criterion_failed: bool = False\n",
    "    latency_criterion_failed: bool = False\n",
    "\n",
    "    mk_text_green = lambda k, m=1: f\"<span style='color:green'>{row[k]*m:0.2f}</span>\"\n",
    "    mk_text_red = lambda k, m=1: f\"<span style='color:red'>**{row[k]*m:0.2f}**</span>\"\n",
    "    mk_text_red2 = lambda k: f\"<span style='color:red'>**{row[k]}**</span>\"\n",
    "    \n",
    "    # error rate\n",
    "    threshold: float = config['report'].get('error_rate_budget', 0)\n",
    "    if row['error_rate'] <= threshold:\n",
    "        score += point_per_criterion\n",
    "        error_rate_text = mk_text_green('error_rate')\n",
    "    else:\n",
    "        error_rate_criterion_failed = True\n",
    "        error_rate_text = mk_text_red('error_rate')\n",
    "        logger.info(f\"score_run, experiment_name={row['experiment_name']}, setting \"\n",
    "                    f\"error_rate_criterion_failed={error_rate_criterion_failed} because \"\n",
    "                    f\"error_rate={row['error_rate']} > threshold={threshold}\")\n",
    "\n",
    "    # latency\n",
    "    threshold: float = config['report'].get('latency_budget', 10)\n",
    "    if row['latency_p95'] <= threshold:\n",
    "        # extra points for better latency\n",
    "        score += point_per_criterion + (threshold-row['latency_p95'])/threshold\n",
    "        latency_p95_text = mk_text_green('latency_p95')\n",
    "    else:\n",
    "        latency_criterion_failed = True\n",
    "        latency_p95_text = mk_text_red('latency_p95')\n",
    "        logger.info(f\"score_run, experiment_name={row['experiment_name']}, setting \"\n",
    "                    f\"latency_criterion_failed={latency_criterion_failed} because \"\n",
    "                    f\"latency_p95={row['latency_p95']} > threshold={threshold}\")\n",
    "\n",
    "    # cost_per_10k_txn_budget\n",
    "    threshold: float = config['report'].get('cost_per_10k_txn_budget', 5)\n",
    "    if row['price_per_txn'] and row['price_per_txn'] * 10000 <= threshold:\n",
    "        # extra points for better price\n",
    "        score += point_per_criterion + (threshold-(row['price_per_txn']* 10000))/threshold     \n",
    "        price_per_10k_txn_text = mk_text_green('price_per_txn', m=10000)\n",
    "    else:\n",
    "        cost_per_txn_criterion_failed = True\n",
    "        if row['price_per_txn']:\n",
    "            price_per_10k_txn_text = mk_text_red('price_per_txn', m=10000)\n",
    "            cost_per_10k_txn = row['price_per_txn'] * 10000\n",
    "        else:\n",
    "            cost_per_10k_txn = None\n",
    "            price_per_10k_txn_text = mk_text_red2('price_per_txn')\n",
    "\n",
    "        logger.info(f\"score_run, experiment_name={row['experiment_name']}, setting \"\n",
    "                    f\"cost_per_txn_criterion_failed={cost_per_txn_criterion_failed} because \"\n",
    "                    f\"cost_per_10k_txn={cost_per_10k_txn} > threshold={threshold}\")\n",
    "\n",
    "    # if all criteria passed then add points for concurrency\n",
    "    # we want to select the run with the highest concurrency amongst\n",
    "    # all runs that satisfy all criteria\n",
    "    any_criterion_failed = error_rate_criterion_failed or \\\n",
    "                           latency_criterion_failed or \\\n",
    "                           cost_per_txn_criterion_failed\n",
    "\n",
    "    if any_criterion_failed is True:\n",
    "        logger.info(f\"experiment_name={row['experiment_name']}, not adding points for \"\n",
    "                    f\"concurrency because any_criterion_failed={any_criterion_failed}\")\n",
    "    else:\n",
    "        score += row['concurrency']\n",
    "\n",
    "    score_dict = dict(score=score,\n",
    "                      any_criterion_failed=any_criterion_failed,\n",
    "                      error_rate_criterion_failed=error_rate_criterion_failed,\n",
    "                      latency_criterion_failed=latency_criterion_failed,\n",
    "                      cost_per_txn_criterion_failed=cost_per_txn_criterion_failed,\n",
    "                      error_rate_text=error_rate_text,\n",
    "                      latency_p95_text=latency_p95_text,\n",
    "                      price_per_10k_txn_text=price_per_10k_txn_text)\n",
    "    logger.info(json.dumps(row.to_dict() | score_dict, indent=2, default=str))\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset['score_dict'] = df_summary_metrics_dataset.apply(lambda row: score_run(row, config), axis=1)\n",
    "score_keys = df_summary_metrics_dataset.score_dict.iloc[0].keys()\n",
    "for k in score_keys:\n",
    "    df_summary_metrics_dataset[k] = df_summary_metrics_dataset.score_dict.map(lambda d: d[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = df_summary_metrics_dataset.sort_values(by=\"score\", ascending=False)\n",
    "file_path_df = os.path.join(METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "os.makedirs(os.path.dirname(file_path_df), exist_ok=True)\n",
    "df_summary_metrics_dataset.to_csv(file_path_df, index=False)\n",
    "summary_metrics_dataset_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(summary_metrics_dataset_csv, config['aws']['bucket'], \"\", METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME)\n",
    "logger.info(f\"Summary metrics dataset saved to s3://{config['aws']['bucket']}/{METRICS_DIR}/{SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME}\")\n",
    "\n",
    "df_summary_metrics_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select the best option overall and for each instance type\n",
    "df_summary_metrics_dataset_overall = df_summary_metrics_dataset[df_summary_metrics_dataset.score == df_summary_metrics_dataset.score.max()]\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_dataset_overall.to_csv(csv_buffer, index=False)\n",
    "metrics_overall_data = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(metrics_overall_data, BUCKET_NAME, \"\", METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME)\n",
    "\n",
    "df_summary_metrics_dataset_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset_overall = df_summary_metrics_dataset_overall.round(6)\n",
    "df_summary_metrics_dataset_overall.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = df_summary_metrics_dataset.dropna()\n",
    "idx = df_summary_metrics_dataset.groupby(['instance_type']).score.idxmax()\n",
    "logger.info(f\"shape of df_summary_metrics_dataset={df_summary_metrics_dataset.shape}, idx={idx}\")\n",
    "df_summary_metrics_best_option_instance_type = df_summary_metrics_dataset.loc[idx]\n",
    "logger.info(f\"shape of df_summary_metrics_best_option_instance_type={df_summary_metrics_best_option_instance_type.shape}\")\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_best_option_instance_type.to_csv(csv_buffer, index=False)\n",
    "best_option = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(best_option, BUCKET_NAME, \"\", METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME)\n",
    "\n",
    "df_summary_metrics_best_option_instance_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_best_option_instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_price_per_tx = df_summary_metrics_best_option_instance_type.price_per_txn.min()\n",
    "txn_count_for_showing_cost: int = config[\"report\"][\"txn_count_for_showing_cost\"]\n",
    "\n",
    "price_tx_col_name = f\"price_per_tx_{txn_count_for_showing_cost}_txn\"\n",
    "df_summary_metrics_best_option_instance_type[price_tx_col_name] = df_summary_metrics_best_option_instance_type.price_per_txn * txn_count_for_showing_cost\n",
    "df_summary_metrics_best_option_instance_type[price_tx_col_name] = round(df_summary_metrics_best_option_instance_type[price_tx_col_name], 2)\n",
    "df_summary_metrics_best_option_instance_type = df_summary_metrics_best_option_instance_type.sort_values(by=price_tx_col_name)\n",
    "sns_plot = sns.barplot(df_summary_metrics_best_option_instance_type,\n",
    "                       y=\"instance_type\",\n",
    "                       x=price_tx_col_name,\n",
    "                       hue=\"instance_type\",\n",
    "                       orient='h')\n",
    "sns_plot.axvline(x=config['report']['cost_per_10k_txn_budget'],\n",
    "                 ls='dotted',\n",
    "                 c='red',\n",
    "                 linewidth=1) \n",
    "title: str = f\"Comparing performance of {config['general']['model_name']} for {config['metrics']['dataset_of_interest']} dataset\"\n",
    "sns_plot.set(ylabel=\"\", xlabel=f\"Cost per {txn_count_for_showing_cost:,} transactions (USD)\", title=title)\n",
    "\n",
    "xticks = sns_plot.get_xticks()\n",
    "xtick_labels = sns_plot.get_xticklabels()\n",
    "#print(sns_plot.get_xticks(xticks))\n",
    "sns_plot.set_xticks(np.append(xticks, config['report']['cost_per_10k_txn_budget']))\n",
    "#print(sns_plot.get_xticks(xtick_labels))\n",
    "sns_plot.set_xticklabels(np.append(xtick_labels,\n",
    "                                   matplotlib.text.Text(config['report']['cost_per_10k_txn_budget'], 0, f\"${config['report']['cost_per_10k_txn_budget']}\\n(threshold)\")))\n",
    "print(sns_plot.get_xticklabels())\n",
    "num_instance_types = len(df_summary_metrics_dataset.instance_type.unique())\n",
    "for r in df_summary_metrics_best_option_instance_type.iterrows():\n",
    "    y = r[1]['instance_type']\n",
    "    if num_instance_types == 1:\n",
    "        v_shift = config[\"report\"][\"v_shift_w_single_instance\"]\n",
    "    else:\n",
    "        v_shift = config[\"report\"][\"v_shift_w_gt_one_instance\"]\n",
    "\n",
    "    print(f\"v_shift={v_shift}\")    \n",
    "    x = r[1][price_tx_col_name] + v_shift + 1\n",
    "    text = f\"{r[1]['transactions_per_minute']} txn/min,\\nconcurrency={r[1]['concurrency']},\\n{r[1]['latency_p95']}s per txn\"\n",
    "    print(f\"x={x}, y={y}, text={text}\")\n",
    "    sns_plot.text(x, y, text, \n",
    "       fontsize = 8,          # Size\n",
    "       #fontstyle = \"oblique\",  # Style\n",
    "       color = \"red\",          # Color\n",
    "       ha = \"left\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "\n",
    "business_summary_plot_fpath: str = os.path.join(METRICS_DIR, BUSINESS_SUMMARY_PLOT_FNAME2)\n",
    "sns_plot.figure.savefig(buffer, format='png', bbox_inches='tight')\n",
    "buffer.seek(0)\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", \"\", business_summary_plot_fpath)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{business_summary_plot_fpath}\")\n",
    "\n",
    "# Display the plot \n",
    "sns_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset['cost_per_10k_txn'] = df_summary_metrics_dataset.price_per_txn * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARTS_PER_ROW: int = 2\n",
    "fig = px.scatter(df_summary_metrics_dataset,\n",
    "                 x=\"latency_p95\",\n",
    "                 y=\"cost_per_10k_txn\",\n",
    "                 size=\"transactions_per_minute\",\n",
    "                 color=\"instance_type\", #\",\n",
    "                 facet_col=\"concurrency\",\n",
    "                 #symbol =\"concurrency\",\n",
    "                 facet_col_wrap=CHARTS_PER_ROW,\n",
    "                 #text=\"instance_type\",\n",
    "                 log_y=False,\n",
    "                 #size_max=10,\n",
    "                 category_orders={\"concurrency\": sorted(list(df_summary_metrics_dataset.concurrency))},\n",
    "                 width=1000, height=600)\n",
    "\n",
    "fig.for_each_yaxis(lambda y: y.update(title = ''))\n",
    "fig.add_annotation(x=-0.05,y=0.5,\n",
    "                   text=\"Cost per 10,000 requests (USD)\", textangle=-90,\n",
    "                   xref=\"paper\", yref=\"paper\")\n",
    "\n",
    "fig.update_xaxes(title=\"\")\n",
    "fig.add_annotation(\n",
    "    showarrow=False,\n",
    "    xanchor='center',\n",
    "    xref='paper', \n",
    "    x=0.5, \n",
    "    yref='paper',\n",
    "    y=-0.1,\n",
    "    text='p95 latency (seconds)'\n",
    ")\n",
    "\n",
    "fig.update_layout(legend={'title_text':''})\n",
    "\n",
    "fig.add_vline(x=config['report']['latency_budget'], line_width=1, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.add_hline(y=config['report']['cost_per_10k_txn_budget'], line_width=1, line_dash=\"dash\", line_color=\"red\")\n",
    "\n",
    "fig.add_shape(\n",
    "        type=\"rect\",\n",
    "        x0=0,\n",
    "        y0=0,\n",
    "        x1=config['report']['latency_budget'],\n",
    "        y1=config['report']['cost_per_10k_txn_budget'],\n",
    "        fillcolor=\"pink\",\n",
    "        line_width=0,\n",
    "        layer=\"below\",\n",
    "        row='all',\n",
    "        col='all',\n",
    "        exclude_empty_subplots=True\n",
    "    )\n",
    "\n",
    "title: str = f\"{config['general']['model_name']} price|performance on the {config['metrics']['dataset_of_interest']} dataset\"\n",
    "#subtitle: str = \"Hover over bubbles in the shaded quadrant to see best best price|performance options.\"\n",
    "subtitle: str = \"Bubbles in shaded quadrant represent experiments that satisfy price|performance costraints.<br>Larger the bubble the greater the transactions/minute.\"\n",
    "\n",
    "fig.update_layout(title=dict(\n",
    "                      text=f\"{title}<br>{subtitle}\",\n",
    "                      x=0.5,\n",
    "                      y=0.95,\n",
    "                      xanchor='center',\n",
    "                      yanchor='top'))\n",
    "\n",
    "fig.update_layout(margin=dict(t=100))\n",
    "fig.update_xaxes(rangemode=\"nonnegative\")\n",
    "# fig.update_layout(legend=dict(\n",
    "#     yanchor=\"bottom\",\n",
    "#     y=-0.425,\n",
    "#     xanchor=\"left\",\n",
    "#     x=0.01\n",
    "# ))\n",
    "\n",
    "# now is the tricky part to add annotation for the best price performance\n",
    "# we have to identify the correct facet for the best entry and then correct\n",
    "# row and col index for that facet\n",
    "# we first identify the best entry as a json dictionary\n",
    "# to find the index we sort all the concurrency levens in asc order\n",
    "# then find the row col index by iterating but we are iterating from 1,1\n",
    "# which is at the top left because that is how we sett it visually\n",
    "# but the facets row col counter starts at bottom left so the col counter is\n",
    "# correct but the row counter needs to be inverted\n",
    "best_price_perf = df_summary_metrics_dataset_overall.to_dict(orient='records')[0]\n",
    "best_price_perf_x = best_price_perf['instance_type']\n",
    "best_price_perf_y = best_price_perf['price_per_txn']*10000\n",
    "best_price_perf_avg_latency = best_price_perf['latency_p95']\n",
    "best_price_perf_cost_per_10k = round(best_price_perf['price_per_txn']*10000, 2)\n",
    "best_price_perf_instance_type = best_price_perf['instance_type']\n",
    "best_price_perf_instance_count = best_price_perf['instance_count']\n",
    "best_price_perf_tpm = best_price_perf['transactions_per_minute']\n",
    "concurrencies = np.sort(df_summary_metrics_dataset.concurrency.unique())\n",
    "\n",
    "concurrencies = list(df_summary_metrics_dataset.concurrency.unique())\n",
    "\n",
    "c = 1\n",
    "total_rows = math.ceil(len(concurrencies)/CHARTS_PER_ROW)\n",
    "r = total_rows\n",
    "for i, v in enumerate(np.sort(concurrencies)):\n",
    "    logger.info(f\"iter {i}, r={r}, c={c}, v={v}, \"\n",
    "                f\"best_price_perf_concurrency={best_price_perf['concurrency']}\")\n",
    "    if v == best_price_perf['concurrency']: #best_price_perf['concurrency']:\n",
    "        logger.info(f\"iter={i}, match found, r={r}, c={c}\")\n",
    "        break\n",
    "    if c == CHARTS_PER_ROW:\n",
    "        r -= 1\n",
    "        c = 1\n",
    "        logger.info(f\"iter {i}, reset r and c indexes, r={r}, c={c}\")\n",
    "    else:\n",
    "        c += 1\n",
    "        logger.info(f\"iter {i}, incrementing c, c={c}\")\n",
    "    \n",
    "best_facet_row = r\n",
    "best_facet_col = c\n",
    "logger.info(f\"facet row col for best_price_perf, r={r}, total_rows={total_rows}, \"\n",
    "            f\"best_facet_row={best_facet_row}, best_facet_col={best_facet_col}\")\n",
    "\n",
    "fig.add_annotation(\n",
    "        x=best_price_perf_avg_latency,\n",
    "        y=best_price_perf_cost_per_10k,\n",
    "        text=\"best price|performance*\",\n",
    "        row=best_facet_row,\n",
    "        col=best_facet_col,\n",
    "        showarrow=True,\n",
    "        arrowhead=1,\n",
    "        exclude_empty_subplots=True)\n",
    "\n",
    "instance_count_str = f\"{best_price_perf_instance_count} instances of \" if best_price_perf_instance_count > 1 else \"\"\n",
    "fig.add_annotation(\n",
    "    showarrow=False,\n",
    "    xanchor='left',\n",
    "    xref='paper', \n",
    "    x=0, \n",
    "    yref='paper',\n",
    "    y=-0.15,\n",
    "    text=f\"*<b>best price|performance</b>: {instance_count_str}{best_price_perf_instance_type}, p95 latency {best_price_perf_avg_latency}s, 10k txn cost ${best_price_perf_cost_per_10k}, transactions/minute {best_price_perf_tpm}.\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_summary_plot_fpath: str = os.path.join(METRICS_DIR, BUSINESS_SUMMARY_PLOT_FNAME)\n",
    "\n",
    "local_path: str = os.path.join(tempfile.tempdir, BUSINESS_SUMMARY_PLOT_FNAME)\n",
    "pio.write_image(fig, local_path) \n",
    "upload_file_to_s3(BUCKET_NAME, local_path, business_summary_plot_fpath)\n",
    "logger.info(f\"writing business summary image from {local_path} to s3://{BUCKET_NAME}/{business_summary_plot_fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path: str = os.path.join(tempfile.tempdir, BUSINESS_SUMMARY_PLOT_FNAME.split('.')[0] + \".html\")\n",
    "fig.write_html(local_path)\n",
    "business_summary_html_plot_fpath = business_summary_plot_fpath.split('.')[0] + \".html\"\n",
    "upload_file_to_s3(BUCKET_NAME, local_path, business_summary_html_plot_fpath)\n",
    "logger.info(f\"writing business summary image from {local_path} to s3://{BUCKET_NAME}/{business_summary_html_plot_fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df_summary_metrics_best_option_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cost_csv_content_fpath = os.path.join(METRICS_DIR, SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE)\n",
    "logger.info(f\"the cost information can be found in the csv file here -> {cost_csv_content_fpath}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    cost_content = get_s3_object(BUCKET_NAME, cost_csv_content_fpath)\n",
    "\n",
    "    # Use pandas to read the CSV content\n",
    "    df_cost_metrics = pd.read_csv(io.StringIO(cost_content))\n",
    "    logger.info(f\"{cost_csv_content_fpath} read into dataframe of shape {df_cost_metrics.shape}\")\n",
    "    df_cost_metrics.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "# df_cost_metrics.fillna('', inplace=True)\n",
    "\n",
    "df_cost_metrics.head()\n",
    "\n",
    "# Convert df_cost_metrics to Markdown table\n",
    "cost_mkdn_table = Tomark.table(df_cost_metrics.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_metrics_summarized_fpath = os.path.join(METRICS_DIR, ENDPOINT_METRICS_SUMMARIZED_FNAME)\n",
    "logger.info(f\"the ep metrics summarized information can be found in the csv file here -> {ep_metrics_summarized_fpath}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    cost_content = get_s3_object(BUCKET_NAME, ep_metrics_summarized_fpath)\n",
    "\n",
    "    # Use pandas to read the CSV content\n",
    "    df_ep_metrics_summarized = pd.read_csv(io.StringIO(cost_content))\n",
    "    df_ep_metrics_summarized = df_ep_metrics_summarized.round(2)\n",
    "    logger.info(f\"{ep_metrics_summarized_fpath} read into dataframe of shape {df_ep_metrics_summarized.shape}\")\n",
    "    df_ep_metrics_summarized.head()\n",
    "    # Convert df_cost_metrics to Markdown table\n",
    "    endpoint_metrics_summarized_table = Tomark.table(df_ep_metrics_summarized.to_dict(orient='records'))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "    endpoint_metrics_summarized_table = \"_No endpoint metrics data is available_.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUSINESS_SUMMARY: str = \"\"\"We did performance benchmarking for `{model_name}` on \"{instance_types}\" on multiple datasets and based on the test results the best price performance for dataset `{ds}` is provided by the `{selected_instance_type}`.\n",
    "\n",
    "{mkdn_table}\n",
    "\n",
    "The price performance comparison for different instance types is presented below. An interactive version of this chart is available [here]({business_summary_plot_interactive_fpath}).\n",
    "\n",
    "![Price performance comparison]({business_summary_plot_fpath})\n",
    "\n",
    "### Latency Metrics Analysis\n",
    "\n",
    "The following table provides token latency metrics including the overall latency, Time To First Token (TTFT), and Time Per Output Token (TPOT) for the `{ds}` dataset.\n",
    "\n",
    "{latency_metrics_table}\n",
    "\n",
    "### Failed experiments\n",
    "\n",
    "{failed_experiment_text}\n",
    "\n",
    "{failed_experiment_table}\n",
    "\n",
    "### Endpoint metrics\n",
    "\n",
    "The following table provides endpoint utilization and invocation metrics.\n",
    "\n",
    "{endpoint_metrics_summarized_table}\n",
    "\n",
    "### Configuration\n",
    "\n",
    "The configuration used for these tests is available in the [`config`](#configuration-file) file.\n",
    "\n",
    "### Experiment cost\n",
    "\n",
    "The cost to run each experiment is provided in the table below. The total cost for running all experiments is {total_cost_as_str}.\n",
    "\n",
    "\n",
    "\n",
    "{cost_table}\n",
    "\n",
    "\"\"\"\n",
    "transposed_list = []\n",
    "logger.info(f\"df_summary_metrics_dataset_overall is: {df_summary_metrics_dataset_overall.columns}\")\n",
    "logger.info(f\"df_summary_metrics_dataset instance types: {df_summary_metrics_dataset.instance_type}\")\n",
    "best_instance_type_info = df_summary_metrics_dataset_overall.to_dict(orient='records')[0]\n",
    "cols_to_delete = [\"score\",\n",
    "                  \"score_dict\",\n",
    "                  \"any_criterion_failed\",\n",
    "                  \"latency_criterion_failed\",\n",
    "                  \"cost_per_txn_criterion_failed\",\n",
    "                  \"error_rate_criterion_failed\",\n",
    "                  \"error_rate_text\",\n",
    "                  \"latency_p95_text\",\n",
    "                  \"price_per_10k_txn_text\"]\n",
    "\n",
    "for c in cols_to_delete:\n",
    "    del best_instance_type_info[c]\n",
    "\n",
    "# we want to convert all the latency/TTFT/TTLT/TPOT columns into one single column\n",
    "# for e.g. latency mean|p50|p95|p99: 0.044 | 0.45 | 0.6 | 0.789\n",
    "latency_prefixes = ['latency_', 'TTFT_', 'TPOT_']\n",
    "latency_suffixes = [\"p50\", \"p95\", \"p99\"]\n",
    "latency_cols = []\n",
    "new_dict = {}\n",
    "for c in latency_prefixes:\n",
    "    # remove the underscore from the end\n",
    "    new_col = c[:-1] + ' '\n",
    "    new_val = ''\n",
    "    for s in latency_suffixes:\n",
    "        col_to_delete = c + s\n",
    "        # check if this key exists, it may not exist for some metric\n",
    "        # for example we dont keep p95 for TPOT only p50 and p99\n",
    "        if best_instance_type_info.get(col_to_delete) is None:\n",
    "            continue\n",
    "        new_col += s + \", \"\n",
    "        val = best_instance_type_info[col_to_delete]\n",
    "        # somewhere in the code mean has been rounded to 6\n",
    "        # cant find where, round it to 4\n",
    "        if s == \"mean\":\n",
    "            val = round(val, 4)\n",
    "        new_val += str(val) + \", \"\n",
    "        logger.info(f\"c={c}, s={s}, col_to_delete={col_to_delete}, new_val={new_val}\")\n",
    "        latency_cols.append(col_to_delete)\n",
    "    # remove the underscore from the end\n",
    "    new_col = new_col[:-2]\n",
    "    new_val = new_val[:-2]\n",
    "    logger.info(f\"new_col={new_col}, new_val={new_val}\")\n",
    "    new_dict[new_col] = new_val\n",
    "\n",
    "# we dont want a colum each for each combination of latency metric and suffix\n",
    "# we want all latency statistical summary metrics in a single row for ease of display\n",
    "# so now that we have it that way in a new dict so first delete those columns from\n",
    "# the old dictionary\n",
    "for c in latency_cols:\n",
    "    del best_instance_type_info[c]\n",
    "# now merge the new dict with the old dict\n",
    "best_instance_type_info = best_instance_type_info | new_dict\n",
    "#logger.info(f\"new_dict={new_dict}\")\n",
    "for k, v in best_instance_type_info.items():\n",
    "    transposed_list.append({\"Information\": k, \"Value\": v})\n",
    "mkdn_table = Tomark.table(transposed_list)\n",
    "\n",
    "\n",
    "# Define the prefixes and columns for latency-related metrics\n",
    "latency_index_cols = ['experiment_name', 'instance_type', 'concurrency']\n",
    "df_summary_latency_metrics_dataset = df_summary_metrics_dataset[latency_index_cols + latency_cols]\n",
    "latency_metrics_table = \"_No token latency metrics data is available. To get token latency metrics enable streaming responses by setting `stream: True` in experiment configuration_.\"\n",
    "if 'TTFT_p50' in df_summary_latency_metrics_dataset.columns:\n",
    "    if all(df_summary_latency_metrics_dataset.TTFT_p50.isnull()):\n",
    "        logger.info(f\"all TTFT metrics are null, so no token latency metrics are available\")        \n",
    "    else:\n",
    "        latency_metrics_table = Tomark.table(df_summary_latency_metrics_dataset.to_dict(orient='records'))\n",
    "else:\n",
    "    logger.info(f\"TTFT metrics fields are not available, looks like no experiment had streaming enabled\")        \n",
    "    \n",
    "    \n",
    "experiments_w_some_criterion_failed = df_summary_metrics_dataset.any_criterion_failed.sum()\n",
    "perf_criteria_text = f\"\"\"`Latency` < `{config['report']['latency_budget']}s`, `cost per 10k transactions`: `${config['report']['cost_per_10k_txn_budget']}`, `error rate`: `{config['report']['error_rate_budget']}`\"\"\"\n",
    "if experiments_w_some_criterion_failed > 0:\n",
    "    logger.info(f\"a total of {experiments_w_some_criterion_failed} experiment run(s) failed at least one criterion\")\n",
    "    failed_experiment_text = f\"\"\"There were a total of {experiments_w_some_criterion_failed} experiment run(s) that failed at least one configured performance criteria: {perf_criteria_text}. See table below.    \n",
    "    \"\"\"\n",
    "    failed_experiment_table = Tomark.table(df_summary_metrics_dataset[df_summary_metrics_dataset.any_criterion_failed == True][['experiment_name',\n",
    "                                         'payload_file',\n",
    "                                         'concurrency',\n",
    "                                         'error_rate_text',\n",
    "                                         'latency_p95_text',\n",
    "                                         'price_per_10k_txn_text']].to_dict('records'))\n",
    "else:\n",
    "    failed_experiment_text = f\"\"\"All experiments satisfied the configured performance criteria: {perf_criteria_text}.\"\"\"\n",
    "    failed_experiment_table = \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plural = \"s\" if len(df_summary_metrics.instance_type.unique()) > 1 else \"\"\n",
    "instance_types_md = \", \".join([f\"`{it}`\" for it in df_summary_metrics.instance_type.unique()])\n",
    "selected_instance_type: str = df_summary_metrics_dataset_overall.to_dict(orient='records')[0]['instance_type']\n",
    "ds: str = config['metrics']['dataset_of_interest']\n",
    "\n",
    "business_summary: str = BUSINESS_SUMMARY.format(model_name=config['general']['model_name'],\n",
    "                                                instance_types=instance_types_md,\n",
    "                                                plural=plural,\n",
    "                                                ds=ds,\n",
    "                                                selected_instance_type=selected_instance_type,\n",
    "                                                mkdn_table=\"\\n\" + mkdn_table + \"\\n\",\n",
    "                                                failed_experiment_text=\"\\n\" + failed_experiment_text + \"\\n\",\n",
    "                                                failed_experiment_table=\"\\n\" + failed_experiment_table + \"\\n\",\n",
    "                                                endpoint_metrics_summarized_table=endpoint_metrics_summarized_table,\n",
    "                                                latency_metrics_table=\"\\n\" + latency_metrics_table + \"\\n\",\n",
    "                                                cfg_file_path=os.path.basename(CONFIG_FILE),\n",
    "                                                business_summary_plot_fpath=BUSINESS_SUMMARY_PLOT_FNAME,\n",
    "                                                business_summary_plot_interactive_fpath=os.path.basename(business_summary_html_plot_fpath),\n",
    "                                                cost_table=cost_mkdn_table,\n",
    "                                                total_cost_as_str=f\"${df_cost_metrics.cost.sum():.2f}\")\n",
    "business_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "dttm = str(datetime.utcnow())\n",
    "datasets_used = \", \".join([f\"`{d}`\" for d in config['s3_read_data']['source_data_files']])\n",
    "title = config['report'].get('title',\n",
    "                             f\"Results for performance benchmarking {config['general']['model_name']}\")\n",
    "overall_results_md = OVERALL_RESULTS_MD.format(title=title,\n",
    "                                               dttm=dttm,\n",
    "                                               fmbench_version=fmbench_version,\n",
    "                                               business_summary=business_summary, \n",
    "                                               datasets=datasets_used)\n",
    "results_group_cols: List[str] = ['instance_type', 'payload_file']\n",
    "result_rows: List[str] = []\n",
    "latency_budget: int = config['report'].get('latency_budget',\n",
    "                                           LATENCY_BUDGET)\n",
    "\n",
    "for row in df_summary_metrics[results_group_cols].drop_duplicates().iterrows():\n",
    "    instance_type = row[1]['instance_type']\n",
    "    dataset = row[1]['payload_file']\n",
    "    df_summary_metrics_nz_subset = df_summary_metrics_nz[(df_summary_metrics_nz.instance_type == instance_type) &\n",
    "                                                          (df_summary_metrics_nz.payload_file == dataset) &\n",
    "                                                           (df_summary_metrics_nz.latency_p95 <= latency_budget)]\n",
    "    num_results = df_summary_metrics_nz_subset.shape[0]\n",
    "    result_row: Optional[str] = None\n",
    "    if num_results > 0:\n",
    "        logger.info(f\"there are {num_results} options to choose the best option from for instance_type={instance_type}, dataset={dataset}\")\n",
    "        df_summary_metrics_nz_subset_selected = df_summary_metrics_nz_subset[df_summary_metrics_nz_subset.concurrency == df_summary_metrics_nz_subset.concurrency.max()]\n",
    "        best = df_summary_metrics_nz_subset_selected.to_dict(orient='records')[0]\n",
    "        best_instance_type= best['instance_type']\n",
    "        # logger.info(best)\n",
    "        result_desc = RESULT_DESC.format(latency_budget=latency_budget,\n",
    "                           instance_type=best_instance_type,\n",
    "                           dataset=dataset,\n",
    "                           concurrency=best['concurrency'],\n",
    "                           latency_median=best['latency_p50'],\n",
    "                           prompt_size=int(best['prompt_token_count_mean']),\n",
    "                           completion_size=int(best['completion_token_count_mean']),\n",
    "                           tpm=int(best['transactions_per_minute']))     \n",
    "\n",
    "    else:\n",
    "        logger.info(f\"there are NO options to choose from for instance_type={instance_type}, dataset={dataset}\")\n",
    "        best_instance_type= instance_type\n",
    "        result_desc = RESULT_FAILURE_DESC.format(latency_budget=latency_budget,\n",
    "                                                 instance_type=instance_type,\n",
    "                                                 dataset=dataset)\n",
    "    result_row: str = RESULT_ROW.format(instance_type=best_instance_type,\n",
    "                                        dataset=dataset,\n",
    "                                        desc=result_desc)\n",
    "    result_rows.append(result_row)\n",
    "\n",
    "overall_results_md += \"\\n\".join(result_rows)\n",
    "\n",
    "OVERALL_RESULTS_PLOTS_MD: str = \"\"\"\n",
    "\n",
    "## Plots\n",
    "\n",
    "The following plots provide insights into the results from the different experiments run.\n",
    "\n",
    "![{plot1_text}]({plot1_fname})\n",
    "\n",
    "![{plot2_text}]({plot2_fname})\n",
    "\n",
    "![{plot3_text}]({plot3_fname})\n",
    "\"\"\"\n",
    "\n",
    "overall_results_plots_md: str = OVERALL_RESULTS_PLOTS_MD.format(plot1_text=ERROR_RATES_PLOT_TEXT, \n",
    "                                                                plot1_fname=ERROR_RATES_PLOT_FNAME,\n",
    "                                                                plot2_text=TOKENS_VS_LATENCY_PLOT_TEXT, \n",
    "                                                                plot2_fname=TOKENS_VS_LATENCY_PLOT_FNAME,\n",
    "                                                                plot3_text=CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT, \n",
    "                                                                plot3_fname=CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME)\n",
    "\n",
    "overall_results_md += overall_results_plots_md\n",
    "\n",
    "# dump the config file at the end so that there is a record of what \n",
    "# we tested with\n",
    "# write the config file in the results directory for record sake\n",
    "import yaml\n",
    "class VerboseSafeDumper(yaml.SafeDumper):\n",
    "    def ignore_aliases(self, data):\n",
    "        return True\n",
    "\n",
    "config_yml = yaml.dump(config, Dumper=VerboseSafeDumper)\n",
    "config_dump = f\"\"\"\\n\\n\n",
    "## Configuration file\n",
    "```.bash\n",
    "{config_yml}\n",
    "```\n",
    "\"\"\"\n",
    "overall_results_md += config_dump\n",
    "\n",
    "\n",
    "fpath: str = os.path.join(METRICS_DIR, RESULTS_DESC_MD_FNAME)\n",
    "logger.info(f\"writing final markdown to {METRICS_DIR}\")\n",
    "Path(fpath).write_text(overall_results_md)\n",
    "logger.info(overall_results_md)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(overall_results_md, BUCKET_NAME, \"\", METRICS_DIR, RESULTS_DESC_MD_FNAME)\n",
    "logger.info(f\"results.md file saved to to s3://{BUCKET_NAME}/{METRICS_DIR}/{RESULTS_DESC_MD_FNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save all the metrics and report files locally\n",
    "import time\n",
    "import shutil\n",
    "shutil.rmtree(RESULTS_DIR, ignore_errors=True)\n",
    "time.sleep(10)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=False)\n",
    "\n",
    "# config_yml = yaml.dump(config, default_flow_style=False)\n",
    "fpath: str = os.path.join(RESULTS_DIR, os.path.basename(CONFIG_FILE))\n",
    "logger.info(f\"saving config yaml in file {fpath}\")\n",
    "Path(fpath).write_text(config_yml)\n",
    "\n",
    "logger.info(f\"going to download all metrics and reports from s3 into {RESULTS_DIR} directory\")\n",
    "download_multiple_files_from_s3(BUCKET_NAME, METRICS_DIR, RESULTS_DIR)\n",
    "import glob\n",
    "result_files = glob.glob(os.path.join(RESULTS_DIR, \"**\"), recursive=True)\n",
    "logger.info(\"\\n\".join([f for f in result_files]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import docker\n",
    "import subprocess\n",
    "docker_running = False\n",
    "generate_html_report = config['report'].get('generate_html_report', True)\n",
    "\n",
    "if generate_html_report is True:\n",
    "    try:\n",
    "        docker_client = docker.DockerClient()\n",
    "        docker_running = docker_client.ping()\n",
    "    except Exception as e:\n",
    "        print(f\"seems like docker is not installed or not running, exception={e}\")\n",
    "\n",
    "    if docker_running is True:\n",
    "        report_md_path = f\"{os.getcwd()}/{RESULTS_DIR}\"\n",
    "        report_html_path = os.path.join(report_md_path, f\"{config['general']['name']}.html\")\n",
    "        cmd = f\"docker run --rm -v {report_md_path}:/public -w /public -u $(id -u):$(id -g) registry.gitlab.com/quarto-forge/docker/quarto quarto render report.md --to html --self-contained\"\n",
    "        logger.info(f\"going to create self-conained html report with cmd=\\\"{cmd}\\\"\")\n",
    "        process = subprocess.Popen(cmd,\n",
    "                                   stdout=subprocess.PIPE, \n",
    "                                   stderr=subprocess.PIPE,\n",
    "                                   text=True,\n",
    "                                   shell=True)\n",
    "        std_out, std_err = process.communicate()\n",
    "        logger.info(std_out.strip())\n",
    "        logger.info(std_err)\n",
    "\n",
    "    else:\n",
    "        logger.error(f\"docker is not available, not going to create self-conained html report\")\n",
    "else:\n",
    "    logger.info(f\"generate_html_report={generate_html_report}, skipping html report generation\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
