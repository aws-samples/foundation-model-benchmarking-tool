{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics Analysis\n",
    "\n",
    "---\n",
    "\n",
    "_This notebook works best with the conda_python3 kernel on a ml.t3.medium machine_.\n",
    "\n",
    "### This part of our solution design includes the chunk of taking the metrics generated and creating visualizations from it for further analysis to make decisions more quicker, efficient, and cost optimal.\n",
    "\n",
    "- In this file, we will go over and create side by side visualizations of different models deployed, how their inference latency is impacted based on the concurrency level, instance size and different model configurations. Using these visualizations and charts, making executive decisions, saving on time and cost becomes critical.\n",
    "\n",
    "- In this notebook, we will also record the error rates for each of the deployed model endpoints based on how it ran against different metrics as specified above. These visualizations will be applicable and work for any and every jumpstart and non jumpstart model if deployed correctly using the prior steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import tempfile\n",
    "import datetime\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import seaborn and other related libraries for visualizations and plotting charts\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tomark import Tomark\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from dateutil.parser import parse\n",
    "from typing import List, Optional, Dict\n",
    "import importlib.resources as pkg_resources\n",
    "from fmbench import __version__ as fmbench_version\n",
    "from fmbench.scripts.pricing import load_and_update_pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# rcParams for configuring Matplotlib settings\n",
    "from matplotlib import rcParams\n",
    "import plotly\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# figure size in inches\n",
    "rcParams[\"figure.figsize\"] = 10, 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"CONFIG_FILE={CONFIG_FILE}\")\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the associated pricing config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "pricing_file_path: str = config[\"pricing\"]\n",
    "\n",
    "# initialize the pricing config file to None\n",
    "pricing_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files(\"fmbench\"), \"configs\")\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "pricing_module = Path(config[\"pricing\"])\n",
    "logger.info(\n",
    "    f\"pricing config provided for inference from this model is --> {pricing_module}\"\n",
    ")\n",
    "pricing_file_path = os.path.join(config_dir, pricing_module)\n",
    "logger.info(f\"pricing config file path is --> {pricing_file_path}\")\n",
    "\n",
    "instance_list = [\n",
    "    experiment.get(\"instance_type\")\n",
    "    for experiment in config.get(\"experiments\", [])\n",
    "    if experiment.get(\"instance_type\")\n",
    "]\n",
    "\n",
    "\n",
    "# Print the extracted instance types\n",
    "logger.info(f\"Extracted instances from the main config --> {instance_list}\")\n",
    "\n",
    "pricing_config = load_and_update_pricing(\n",
    "    pricing_file_path, PRICING_FALLBACK_YAML_PATH, instance_list\n",
    ")\n",
    "logger.info(f\"pricing config file recorded: {json.dumps(pricing_config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug is True:\n",
    "    metrics_path_file: str = os.path.join(\"..\", \"..\", METADATA_DIR, METRICS_PATH_FNAME)\n",
    "else:\n",
    "    metrics_path_file: str = os.path.join(METADATA_DIR, METRICS_PATH_FNAME)\n",
    "logger.info(\n",
    "    f\"cwd={os.getcwd()}, METADATA_DIR={METADATA_DIR}, METRICS_PATH_FNAME={METRICS_PATH_FNAME}, metrics_path_file={metrics_path_file}\"\n",
    ")\n",
    "METRICS_DIR: str = Path(metrics_path_file).read_text().strip()\n",
    "logger.info(f\"metrics_path_file={metrics_path_file}, METRICS_DIR={METRICS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join(METRICS_DIR, config[\"report\"][\"per_inference_request_file\"])\n",
    "logger.info(f\"File path containing the metrics per inference folder --> {file_path}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    file_content = get_s3_object(config[\"aws\"][\"bucket\"], file_path)\n",
    "    # Use pandas to read the CSV content\n",
    "    df_per_inference = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(\n",
    "        f\"{file_path} read into dataframe of shape {df_per_inference.shape}, \"\n",
    "        f\"cols={df_per_inference.columns}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"{file_path} contains results for the following endpoints={df_per_inference.endpoint_name.unique()}\"\n",
    "    )\n",
    "    logger.info(df_per_inference.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between prompt token length and inference latency for different instances and concurrency levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rename a column in the dataframe for clarity of the instance parameter of the model used\n",
    "df_per_inference = df_per_inference.rename(columns={\"instance_type\": \"instance\"})\n",
    "logger.info(\n",
    "    f\"df_per_inference.latency.quantile ->\\n{df_per_inference.latency.quantile([0.25, 0.5, 0.75, 0.95, 0.99])}\"\n",
    ")\n",
    "if df_per_inference.latency.quantile(0.5) < 1:\n",
    "    print(\"multiplying by 1000\")\n",
    "    latency_units = \"milliseconds\"\n",
    "    multiplier = 1000\n",
    "    step_size = 500\n",
    "    df_per_inference.latency = df_per_inference.latency * 1000\n",
    "else:\n",
    "    multiplier = 10\n",
    "    step_size = 5\n",
    "    latency_units = \"seconds\"\n",
    "\n",
    "## Initializing yticks and title for the chart\n",
    "yticks: Optional[List] = None\n",
    "title: Optional[str] = None\n",
    "\n",
    "if config[\"report\"].get(\"latency_vs_token_len_chart\"):\n",
    "    yticks: List = config[\"report\"][\"latency_vs_token_len_chart\"].get(\"y_ticks\")\n",
    "    title: str = config[\"report\"][\"latency_vs_token_len_chart\"].get(\"title\")\n",
    "\n",
    "if title is None:\n",
    "    title = \"Effect of token length on inference latency\"\n",
    "\n",
    "unique_instance_types = df_per_inference.instance.unique()\n",
    "if len(unique_instance_types) == 1:\n",
    "    logger.info(\n",
    "        f\"there is only {len(unique_instance_types)} instance type ({unique_instance_types}), \"\n",
    "        f\"not using row as instance\"\n",
    "    )\n",
    "    # This created a FacetGrid for plotting multiple scatter plots based on 'instance' and 'concurrency' categories\n",
    "    g = sns.FacetGrid(\n",
    "        df_per_inference,\n",
    "        col=\"concurrency\",\n",
    "        hue=\"instance\",\n",
    "        height=3.5,\n",
    "        aspect=1.25,\n",
    "        col_wrap=3,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\n",
    "        f\"there are {len(unique_instance_types)} instance types ({unique_instance_types}), \"\n",
    "        f\"using row as instance\"\n",
    "    )\n",
    "    g = sns.FacetGrid(\n",
    "        df_per_inference,\n",
    "        col=\"concurrency\",\n",
    "        row=\"instance\",\n",
    "        hue=\"instance\",\n",
    "        height=3.5,\n",
    "        aspect=1.25,\n",
    "    )\n",
    "\n",
    "\n",
    "## Subtitle of the facetgrid\n",
    "g.fig.suptitle(title)\n",
    "# # This will map a scatterplot to the FacetGrid for each subset of the data\n",
    "sns_plot = g.map(sns.scatterplot, \"prompt_tokens\", \"latency\")\n",
    "\n",
    "# flatten axes into a 1-d array\n",
    "axes = g.axes.flatten()\n",
    "\n",
    "# iterate through the axes\n",
    "for i, ax in enumerate(axes):\n",
    "    if latency_units == \"milliseconds\":\n",
    "        m = 1000\n",
    "    else:\n",
    "        m = 1\n",
    "    ax.axhline(config[\"report\"][\"latency_budget\"] * m, ls=\"--\", c=\"red\")\n",
    "\n",
    "# Set the y-axis label for all plots\n",
    "g = g.set_ylabels(f\"Latency ({latency_units})\")\n",
    "\n",
    "if yticks is None:\n",
    "    # Y-axis ticks based on the maximum latency value and setting them in that manner\n",
    "    yticks: List = list(\n",
    "        range(\n",
    "            0,\n",
    "            (int(df_per_inference.latency.max()) // multiplier + 2) * multiplier,\n",
    "            step_size,\n",
    "        )\n",
    "    )\n",
    "\n",
    "if yticks is None:\n",
    "    # Y-axis ticks based on the maximum latency value and setting them in that manner\n",
    "    yticks: List = list(\n",
    "        range(\n",
    "            0,\n",
    "            (int(df_per_inference.latency.max()) // multiplier + 1) * multiplier,\n",
    "            step_size,\n",
    "        )\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"yticks was not configured, setting it to yticks[:10]={yticks[:10]}, \"\n",
    "        f\"based on latency max of {df_per_inference.latency.max()}s, \"\n",
    "        f\"multiplier={multiplier}, step_size={step_size}\"\n",
    "    )\n",
    "else:\n",
    "    logger.info(f\"yticks is configured, yticks={yticks}\")\n",
    "g = g.set(yticks=yticks)\n",
    "\n",
    "# Set the x-axis label for all plots as the prompt length or tokens\n",
    "g = g.set_xlabels(\"Prompt length (tokens)\")\n",
    "\n",
    "# Create a bytes buffer to save the plot\n",
    "buffer = io.BytesIO()\n",
    "sns_plot.savefig(buffer, format=\"png\")\n",
    "buffer.seek(0)  # Rewind buffer to the beginning\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(\n",
    "    buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, TOKENS_VS_LATENCY_PLOT_FNAME\n",
    ")\n",
    "logger.info(\n",
    "    f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{TOKENS_VS_LATENCY_PLOT_FNAME}\"\n",
    ")\n",
    "\n",
    "# Optionally, display the plot\n",
    "sns_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.latency.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the all metrics file path and read it to generate visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_metrics_fpath = os.path.join(METRICS_DIR, config[\"report\"][\"all_metrics_file\"])\n",
    "# Read the file from S3\n",
    "try:\n",
    "    logger.info(f\"going to read all metrics file from {all_metrics_fpath}\")\n",
    "    file_content = get_s3_object(BUCKET_NAME, all_metrics_fpath)\n",
    "\n",
    "    # Use pandas to read the CSV content\n",
    "    df_all_metrics = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(\n",
    "        f\"{all_metrics_fpath} read into dataframe of shape {df_all_metrics.shape}\"\n",
    "    )\n",
    "    df_all_metrics.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n",
    "# if the instance count is not set then set it to 1, this happens\n",
    "# in case of BYOE or Bedrock, so we want to count such a case as 1 compute unit\n",
    "\n",
    "df_all_metrics[\"instance_count\"] = df_all_metrics[\"instance_count\"].fillna(1)\n",
    "df_all_metrics.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## displaying all of the available columns in the all metrics dataframe\n",
    "df_all_metrics.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_metrics.instance_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the number of experiment names within the metrics dataframe, instance types and models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments = df_all_metrics.experiment_name.unique()\n",
    "instance_types = df_all_metrics.instance_type.unique()\n",
    "# model_names = df_all_metrics.ModelName.unique()\n",
    "# logger.info(f\"contains information about {len(experiments)} experiments, {len(instance_types)} instance types, {len(model_names)} models\")\n",
    "logger.info(\n",
    "    f\"contains information about {len(experiments)} experiments, {len(instance_types)} instance types\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## extract some of the columns\n",
    "relevant_cols = [\n",
    "    \"experiment_name\",\n",
    "    \"payload_file\",\n",
    "    \"instance_type\",\n",
    "    \"instance_count\",\n",
    "    \"concurrency\",\n",
    "    \"error_rate\",\n",
    "    \"prompt_token_count_mean\",\n",
    "    \"prompt_token_throughput\",\n",
    "    \"completion_token_count_mean\",\n",
    "    \"completion_token_throughput\",\n",
    "    \"latency_p50\",\n",
    "    \"latency_p95\",\n",
    "    \"latency_p99\",\n",
    "    \"TTFT_p50\",\n",
    "    \"TTFT_p99\",\n",
    "    \"TPOT_p50\",\n",
    "    \"TPOT_p99\",\n",
    "    \"transactions_per_minute\",\n",
    "]\n",
    "\n",
    "## initialize a group by columns to use further in generating portions of the dataframe and filtering it\n",
    "group_by_cols = [\n",
    "    \"experiment_name\",\n",
    "    \"payload_file\",\n",
    "    \"instance_type\",\n",
    "    \"instance_count\",\n",
    "    \"concurrency\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an 'experiment_counts.csv' to store metrics on experiment name, the payload file, concurrency and the total counts associated to that given experiment to visualize the distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_metrics.instance_type.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_counts = df_all_metrics[group_by_cols].value_counts().reset_index()\n",
    "\n",
    "# Convert df_counts to CSV format\n",
    "csv_buffer = io.StringIO()\n",
    "df_counts.to_csv(csv_buffer, index=False)\n",
    "csv_data = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name and the S3 path\n",
    "COUNTS_FNAME = \"experiment_counts.csv\"\n",
    "counts_s3_path = os.path.join(METRICS_DIR, COUNTS_FNAME)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(csv_data, BUCKET_NAME, \"\", METRICS_DIR, COUNTS_FNAME)\n",
    "logger.info(f\"Counts DataFrame saved to s3://{BUCKET_NAME}/{counts_s3_path}\")\n",
    "\n",
    "df_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the mean error rates for each experiment with different congifurations using the same columns of interest used in the cell above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_error_rates = (\n",
    "    df_all_metrics.groupby(group_by_cols).agg({\"error_rate\": \"mean\"}).reset_index()\n",
    ")\n",
    "df_error_rates = df_error_rates.round(2)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_error_rates.to_csv(csv_buffer, index=False)\n",
    "error_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name and the S3 path\n",
    "ERROR_RATES_FNAME: str = \"error_rates.csv\"\n",
    "counts_s3_path = os.path.join(METRICS_DIR, ERROR_RATES_FNAME)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(error_csv, BUCKET_NAME, \"\", METRICS_DIR, ERROR_RATES_FNAME)\n",
    "logger.info(f\"Error Counts DataFrame saved to s3://{BUCKET_NAME}/{counts_s3_path}\")\n",
    "\n",
    "df_error_rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Inference error rates across different concurrency levels and instance types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_error_rates = df_error_rates.rename(\n",
    "    columns={\"instance_type\": \"instance\", \"payload_file\": \"dataset\"}\n",
    ")\n",
    "\n",
    "# Clean up the dataset names by removing json file extensions and prefixes\n",
    "df_error_rates.dataset = df_error_rates.dataset.map(\n",
    "    lambda x: x.replace(\".jsonl\", \"\").replace(\"payload_\", \"\")\n",
    ")\n",
    "\n",
    "# this creates a facetGrid for plotting scatter plots based on 'instance' and 'dataset'\n",
    "logger.info(f\"df_error_rates --> {df_error_rates}\")\n",
    "\n",
    "df_error_rates_only_nz = df_error_rates[df_error_rates.error_rate > 0]\n",
    "logger.info(\n",
    "    f\"there are {df_error_rates_only_nz.shape[0]} experiment runs that encountered errors\"\n",
    ")\n",
    "if df_error_rates_only_nz.shape[0] > 0:\n",
    "    g = sns.FacetGrid(\n",
    "        df_error_rates_only_nz,\n",
    "        col=\"instance\",\n",
    "        row=\"dataset\",\n",
    "        hue=\"instance\",\n",
    "        height=3.5,\n",
    "        aspect=1.25,\n",
    "    )\n",
    "    # Maps a scatterplot to the FacetGrid for each subset of the data\n",
    "    sns_plot = g.map(sns.scatterplot, \"concurrency\", \"error_rate\")\n",
    "    # flatten axes into a 1-d array\n",
    "    axes = g.axes.flatten()\n",
    "\n",
    "    # iterate through the axes\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.axhline(config[\"report\"][\"error_rate_budget\"] * 1000, ls=\"--\", c=\"red\")\n",
    "\n",
    "    # Create a subtitle\n",
    "    with sns.plotting_context(\"paper\", font_scale=1.3):\n",
    "        g.fig.suptitle(\n",
    "            \"Inference error rates for different concurrency levels and instance types\\nOnly non-zero error rates shown.\"\n",
    "        )\n",
    "        g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\", size=8)\n",
    "\n",
    "    # Set x and y labels for this chart\n",
    "    g = g.set_ylabels(\"Error rate (failed / total inferences)\")\n",
    "    g = g.set_xlabels(\"Concurrency level\")\n",
    "    g.figure.subplots_adjust(top=0.8)\n",
    "    sns_plot.savefig(buffer, format=\"png\")\n",
    "    buffer.seek(0)\n",
    "else:\n",
    "    # create a new dummy dataframe just for plotting an empty chart\n",
    "\n",
    "    df_error_rates_only_nz = pd.DataFrame(\n",
    "        {\n",
    "            \"concurrency\": df_error_rates.concurrency.unique(),\n",
    "            \"error_rate\": [0] * len(df_error_rates.concurrency.unique()),\n",
    "        }\n",
    "    )\n",
    "    sns_plot = sns.scatterplot(\n",
    "        data=df_error_rates_only_nz, x=\"concurrency\", y=\"error_rate\"\n",
    "    )\n",
    "    sns_plot.set_xticks(df_error_rates_only_nz.concurrency.unique())\n",
    "    sns_plot.set(\n",
    "        xlabel=\"Concurrency level\",\n",
    "        ylabel=\"Error rate (failed / total inferences)\",\n",
    "        title=\"Inference error rates for different concurrency levels and instance types\\nError rate is zero for all experiment runs.\",\n",
    "    )\n",
    "    sns_plot.axhline(config[\"report\"][\"error_rate_budget\"] * 1000, ls=\"--\", c=\"red\")\n",
    "    sns_plot.figure.savefig(buffer, format=\"png\")\n",
    "    buffer.seek(0)\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, ERROR_RATES_PLOT_FNAME)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{ERROR_RATES_PLOT_FNAME}\")\n",
    "\n",
    "## Display the plot\n",
    "sns_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for the df elements that have error rates above 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_error_rates_nz = df_error_rates[df_error_rates.error_rate > 0]\n",
    "df_error_rates_nz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## initialize a dataframe to get the mean of the columns in consideration\n",
    "df_summary_metrics = (\n",
    "    df_all_metrics[relevant_cols].groupby(group_by_cols).mean().reset_index()\n",
    ")\n",
    "\n",
    "# ugly way of doing this, will refactor this later (maybe)\n",
    "df_summary_metrics.fillna(PLACE_HOLDER, inplace=True)\n",
    "int_cols = [\n",
    "    \"prompt_token_count_mean\",\n",
    "    \"prompt_token_throughput\",\n",
    "    \"completion_token_count_mean\",\n",
    "    \"completion_token_throughput\",\n",
    "    \"transactions_per_minute\",\n",
    "]\n",
    "for ic in int_cols:\n",
    "    df_summary_metrics[ic] = df_summary_metrics[ic].astype(int)\n",
    "\n",
    "df_summary_metrics.replace(PLACE_HOLDER, np.nan, inplace=True)\n",
    "# df_summary_metrics.latency_p95\t= df_summary_metrics.latency_p95.round(2)\n",
    "df_summary_metrics.latency_p50 = df_summary_metrics.latency_p50.round(2)\n",
    "df_summary_metrics.latency_p95 = df_summary_metrics.latency_p95.round(2)\n",
    "df_summary_metrics.latency_p99 = df_summary_metrics.latency_p99.round(2)\n",
    "df_summary_metrics.TTFT_p50 = df_summary_metrics.TTFT_p50.round(4)\n",
    "# df_summary_metrics.TTFT_p95 = df_summary_metrics.TTFT_p95.round(4)\n",
    "df_summary_metrics.TTFT_p99 = df_summary_metrics.TTFT_p99.round(4)\n",
    "df_summary_metrics.TPOT_p50 = df_summary_metrics.TPOT_p50.round(4)\n",
    "# df_summary_metrics.TPOT_p95 = df_summary_metrics.TPOT_p95.round(4)\n",
    "df_summary_metrics.TPOT_p99 = df_summary_metrics.TPOT_p99.round(4)\n",
    "# df_summary_metrics.TTLT_p50 = df_summary_metrics.TTLT_p50.round(4)\n",
    "# df_summary_metrics.TTLT_p95 = df_summary_metrics.TTLT_p95.round(4)\n",
    "# df_summary_metrics.TTLT_p99 = df_summary_metrics.TTLT_p99.round(4)\n",
    "df_summary_metrics.error_rate = df_summary_metrics.error_rate.round(2)\n",
    "\n",
    "# drop the columns if all, TTFT, TPOT and TTLT are NaN\n",
    "time_to_token_cols_to_check = [\n",
    "    col\n",
    "    for col in df_summary_metrics.columns\n",
    "    if col.startswith((\"TTFT\", \"TTLT\", \"TPOT\"))\n",
    "]\n",
    "for col in time_to_token_cols_to_check:\n",
    "    if df_summary_metrics[col].isna().all():\n",
    "        df_summary_metrics.drop(columns=col, inplace=True)\n",
    "logger.info(f\"Updated summary metrics cols: {df_summary_metrics.columns}\")\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics.to_csv(csv_buffer, index=False)\n",
    "summary_metrics_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name for S3 based on the original file path\n",
    "summary_file_name = all_metrics_fpath.replace(\n",
    "    \"all_metrics\", \"all_metrics_summary\"\n",
    ").split(\"/\")[-1]\n",
    "summary_s3_path = os.path.join(METRICS_DIR, summary_file_name)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(summary_metrics_csv, BUCKET_NAME, \"\", METRICS_DIR, summary_file_name)\n",
    "logger.info(f\"Summary metrics DataFrame saved to s3://{BUCKET_NAME}/{summary_s3_path}\")\n",
    "\n",
    "df_summary_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_nz = df_summary_metrics[df_summary_metrics.error_rate == 0]\n",
    "logger.info(\n",
    "    f\"there are {len(df_summary_metrics_nz)} entries out of {len(df_summary_metrics)} in the summary data for which error rate is 0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = df_summary_metrics[\n",
    "    df_summary_metrics.payload_file.str.contains(\n",
    "        config[\"metrics\"][\"dataset_of_interest\"]\n",
    "    )\n",
    "]\n",
    "logger.info(\n",
    "    f\"shape of dataframe with summary metrics for {config['metrics']['dataset_of_interest']} is {df_summary_metrics_dataset.shape}\"\n",
    ")\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_dataset.to_csv(csv_buffer, index=False)\n",
    "metrics_dataset = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(\n",
    "    metrics_dataset, BUCKET_NAME, \"\", METRICS_DIR, SUMMARY_METRICS_W_PRICING_FNAME\n",
    ")\n",
    "logger.info(\n",
    "    f\"Summary metrics dataset saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{SUMMARY_METRICS_W_PRICING_FNAME}\"\n",
    ")\n",
    "\n",
    "df_summary_metrics_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_metrics_for_dataset = df_all_metrics.rename(\n",
    "    columns={\"instance_type\": \"instance\", \"payload_file\": \"dataset\"}\n",
    ")\n",
    "df_all_metrics_for_dataset.dataset = df_all_metrics_for_dataset.dataset.map(\n",
    "    lambda x: x.replace(\".jsonl\", \"\").replace(\"payload_\", \"\")\n",
    ")\n",
    "ds = config[\"metrics\"][\"dataset_of_interest\"]\n",
    "df_all_metrics_for_dataset = df_all_metrics_for_dataset[\n",
    "    df_all_metrics_for_dataset.dataset.str.contains(ds)\n",
    "]\n",
    "row_order = list(\n",
    "    df_all_metrics_for_dataset[[\"instance\", \"latency_p95\"]]\n",
    "    .groupby(\"instance\")\n",
    "    .mean(\"latency_p95\")\n",
    "    .reset_index()[\"instance\"]\n",
    ")\n",
    "if len(row_order) == 0:\n",
    "    logger.error(\n",
    "        f\"seems like a missing configuration, no data found in df_all_metrics_for_dataset for ds={ds}\"\n",
    "    )\n",
    "    col_wrap = 1\n",
    "else:\n",
    "    logger.error(\n",
    "        f\"found {df_all_metrics_for_dataset.shape[0]} experiment runs for ds={ds}\"\n",
    "    )\n",
    "    col_wrap = 4 if len(row_order) > 4 else len(row_order)\n",
    "\n",
    "    sns_plot = sns.catplot(\n",
    "        data=df_all_metrics_for_dataset,\n",
    "        x=\"concurrency\",\n",
    "        y=\"latency_p95\",\n",
    "        col=\"instance\",\n",
    "        kind=\"box\",\n",
    "        col_wrap=col_wrap,\n",
    "        hue=\"instance\",\n",
    "        row_order=row_order,\n",
    "        height=4.5,\n",
    "        aspect=1.0,\n",
    "    )\n",
    "    # flatten axes into a 1-d array\n",
    "    axes = sns_plot.axes.flatten()\n",
    "\n",
    "    # iterate through the axes\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.axhline(config[\"report\"][\"latency_budget\"], ls=\"--\", c=\"red\")\n",
    "    sns_plot._legend.remove()\n",
    "    sns_plot.fig.suptitle(\n",
    "        f\"Effect of concurrency on inference latency for each instance type for the {ds} dataset\\n\\n\"\n",
    "    )\n",
    "    sns_plot = sns_plot.set_ylabels(\"Latency (seconds)\")\n",
    "    sns_plot = sns_plot.set_xlabels(\"Concurrency level\")\n",
    "    sns_plot.fig.subplots_adjust(top=0.9)\n",
    "\n",
    "    sns_plot.savefig(buffer, format=\"png\")\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Write the plot to S3\n",
    "    write_to_s3(\n",
    "        buffer.getvalue(),\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pricing = pd.DataFrame.from_dict(\n",
    "    pricing_config[\"pricing\"], orient=\"index\"\n",
    ").reset_index()\n",
    "df_pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle pricing for token based pricing & hourly (instance type) pricing models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_cost_per_txn(row: pd.Series, pricing: Dict) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    The instance type is supposed to be unique across all inference options\n",
    "    whether it is hourly pricing based instances (g5/p4 etc.) or Bedrock\n",
    "    model ids or anything else.\n",
    "    \"\"\"\n",
    "\n",
    "    cost_per_txn: Optional[float] = None\n",
    "    # check if this is instance type\n",
    "    pricing_for_this_instance = pricing[\"pricing\"][\"instance_based\"].get(\n",
    "        row[\"instance_type\"]\n",
    "    )\n",
    "    if pricing_for_this_instance:\n",
    "        # this is instance based pricing so then cost per txn is simply\n",
    "        logger.info(\n",
    "            f\"pricing for {row['instance_type']} -> {pricing_for_this_instance}\"\n",
    "        )\n",
    "        if row[\"transactions_per_minute\"] > 0:\n",
    "            instance_count = row.get(\"instance_count\", 1)\n",
    "            logger.info(f\"calculate_cost_per_txn, instance_count={instance_count}\")\n",
    "            cost_per_txn = ((pricing_for_this_instance / 60) * instance_count) / row[\n",
    "                \"transactions_per_minute\"\n",
    "            ]\n",
    "        else:\n",
    "            logger.info(\n",
    "                f\"transactions_per_minute={row['transactions_per_minute']}, setting cost_per_txn=None\"\n",
    "            )\n",
    "            cost_per_txn = None\n",
    "\n",
    "    else:\n",
    "        # this is token based pricing\n",
    "        token_based_pricing_this_model = pricing[\"pricing\"][\"token_based\"].get(\n",
    "            row[\"instance_type\"]\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"pricing for {row['instance_type']} -> {token_based_pricing_this_model}\"\n",
    "        )\n",
    "        if token_based_pricing_this_model:\n",
    "            input_token_cost = (\n",
    "                row[\"prompt_token_count_mean\"] / 1000\n",
    "            ) * token_based_pricing_this_model[\"input-per-1k-tokens\"]\n",
    "            output_token_cost = (\n",
    "                row[\"completion_token_count_mean\"] / 1000\n",
    "            ) * token_based_pricing_this_model[\"output-per-1k-tokens\"]\n",
    "            cost_per_txn = input_token_cost + output_token_cost\n",
    "        else:\n",
    "            logger.error(f\"no pricing information found for {row['instance_type']}\")\n",
    "    return cost_per_txn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset[\"price_per_txn\"] = df_summary_metrics_dataset.apply(\n",
    "    lambda r: calculate_cost_per_txn(r, pricing_config), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset[\"price_per_token\"] = df_summary_metrics_dataset[\n",
    "    \"price_per_txn\"\n",
    "] / (\n",
    "    df_summary_metrics_dataset[\"prompt_token_count_mean\"]\n",
    "    + df_summary_metrics_dataset[\"completion_token_count_mean\"]\n",
    ")\n",
    "\n",
    "df_summary_metrics_dataset[\"price_per_token\"] = df_summary_metrics_dataset[\n",
    "    \"price_per_token\"\n",
    "].apply(lambda x: \"{:.8f}\".format(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score_run(row: pd.core.series.Series, config: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    check all criteria configured in the report section of the config\n",
    "    these are error rates, latency and cost to determine the \"score\"\n",
    "    for each run in the following way:\n",
    "    1. Set score to 0, criteria failed to false.\n",
    "    2. If the run has an error rate lte to the error rate budget,\n",
    "       add 1 to the score otherwise set set criteria failed to true.\n",
    "    3. If the run has a price per 10k txns lte to the cost budget,\n",
    "       add 1 to the score otherwise set set criteria failed to true.\n",
    "    4. If the run has p95 latency lte to the latency budget,\n",
    "       add 1 to the score otherwise set set criteria failed to true.\n",
    "    5. If criteria failed is false add the concurrency to the score.\n",
    "    The idea that we want to select runs that satisfy cost/error rate/latency criteria\n",
    "    and out of those prefer the run with the highest level of concurrency.\n",
    "    \"\"\"\n",
    "    score: float = 0\n",
    "    point_per_criterion: int = 1\n",
    "    any_criterion_failed: bool = False\n",
    "    error_rate_criterion_failed: bool = False\n",
    "    cost_per_txn_criterion_failed: bool = False\n",
    "    latency_criterion_failed: bool = False\n",
    "\n",
    "    mk_text_green = lambda k, m=1: f\"<span style='color:green'>{row[k]*m:0.2f}</span>\"\n",
    "    mk_text_red = lambda k, m=1: f\"<span style='color:red'>**{row[k]*m:0.2f}**</span>\"\n",
    "    mk_text_red2 = lambda k: f\"<span style='color:red'>**{row[k]}**</span>\"\n",
    "    latency_latitude: float = config[\"report\"].get(\"latency_latitude\", 0.01)\n",
    "    # error rate\n",
    "    threshold: float = config[\"report\"].get(\"error_rate_budget\", 0)\n",
    "    if row[\"error_rate\"] <= threshold:\n",
    "        score += point_per_criterion\n",
    "        error_rate_text = mk_text_green(\"error_rate\")\n",
    "    else:\n",
    "        error_rate_criterion_failed = True\n",
    "        error_rate_text = mk_text_red(\"error_rate\")\n",
    "        logger.info(\n",
    "            f\"score_run, experiment_name={row['experiment_name']}, setting \"\n",
    "            f\"error_rate_criterion_failed={error_rate_criterion_failed} because \"\n",
    "            f\"error_rate={row['error_rate']} > threshold={threshold}\"\n",
    "        )\n",
    "\n",
    "    # latency\n",
    "    threshold: float = config[\"report\"].get(\"latency_budget\", 10)\n",
    "    if row[\"latency_p95\"] <= (1 + latency_latitude) * threshold:\n",
    "        # extra points for better latency\n",
    "        score += point_per_criterion + (threshold - row[\"latency_p95\"]) / threshold\n",
    "        latency_p95_text = mk_text_green(\"latency_p95\")\n",
    "    else:\n",
    "        latency_criterion_failed = True\n",
    "        latency_p95_text = mk_text_red(\"latency_p95\")\n",
    "        logger.info(\n",
    "            f\"score_run, experiment_name={row['experiment_name']}, setting \"\n",
    "            f\"latency_criterion_failed={latency_criterion_failed} because \"\n",
    "            f\"latency_p95={row['latency_p95']} > threshold={threshold}\"\n",
    "        )\n",
    "\n",
    "    # cost_per_10k_txn_budget\n",
    "    threshold: float = config[\"report\"].get(\"cost_per_10k_txn_budget\", 5)\n",
    "    if row[\"price_per_txn\"] and row[\"price_per_txn\"] * 10000 <= threshold:\n",
    "        # extra points for better price\n",
    "        score += (\n",
    "            point_per_criterion\n",
    "            + (threshold - (row[\"price_per_txn\"] * 10000)) / threshold\n",
    "        )\n",
    "        price_per_10k_txn_text = mk_text_green(\"price_per_txn\", m=10000)\n",
    "    else:\n",
    "        cost_per_txn_criterion_failed = True\n",
    "        if row[\"price_per_txn\"]:\n",
    "            price_per_10k_txn_text = mk_text_red(\"price_per_txn\", m=10000)\n",
    "            cost_per_10k_txn = row[\"price_per_txn\"] * 10000\n",
    "        else:\n",
    "            cost_per_10k_txn = None\n",
    "            price_per_10k_txn_text = mk_text_red2(\"price_per_txn\")\n",
    "\n",
    "        logger.info(\n",
    "            f\"score_run, experiment_name={row['experiment_name']}, setting \"\n",
    "            f\"cost_per_txn_criterion_failed={cost_per_txn_criterion_failed} because \"\n",
    "            f\"cost_per_10k_txn={cost_per_10k_txn} > threshold={threshold}\"\n",
    "        )\n",
    "\n",
    "    # if all criteria passed then add points for concurrency\n",
    "    # we want to select the run with the highest concurrency amongst\n",
    "    # all runs that satisfy all criteria\n",
    "    any_criterion_failed = (\n",
    "        error_rate_criterion_failed\n",
    "        or latency_criterion_failed\n",
    "        or cost_per_txn_criterion_failed\n",
    "    )\n",
    "\n",
    "    if any_criterion_failed is True:\n",
    "        logger.info(\n",
    "            f\"experiment_name={row['experiment_name']}, not adding points for \"\n",
    "            f\"concurrency because any_criterion_failed={any_criterion_failed}\"\n",
    "        )\n",
    "    else:\n",
    "        score += row[\"concurrency\"]\n",
    "\n",
    "    score_dict = dict(\n",
    "        score=score,\n",
    "        any_criterion_failed=any_criterion_failed,\n",
    "        error_rate_criterion_failed=error_rate_criterion_failed,\n",
    "        latency_criterion_failed=latency_criterion_failed,\n",
    "        cost_per_txn_criterion_failed=cost_per_txn_criterion_failed,\n",
    "        error_rate_text=error_rate_text,\n",
    "        latency_p95_text=latency_p95_text,\n",
    "        price_per_10k_txn_text=price_per_10k_txn_text,\n",
    "    )\n",
    "    logger.info(json.dumps(row.to_dict() | score_dict, indent=2, default=str))\n",
    "    return score_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset[\"score_dict\"] = df_summary_metrics_dataset.apply(\n",
    "    lambda row: score_run(row, config), axis=1\n",
    ")\n",
    "score_keys = df_summary_metrics_dataset.score_dict.iloc[0].keys()\n",
    "for k in score_keys:\n",
    "    df_summary_metrics_dataset[k] = df_summary_metrics_dataset.score_dict.map(\n",
    "        lambda d: d[k]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = df_summary_metrics_dataset.sort_values(\n",
    "    by=\"score\", ascending=False\n",
    ")\n",
    "file_path_df = os.path.join(METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "os.makedirs(os.path.dirname(file_path_df), exist_ok=True)\n",
    "df_summary_metrics_dataset.to_csv(file_path_df, index=False)\n",
    "summary_metrics_dataset_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(\n",
    "    summary_metrics_dataset_csv,\n",
    "    config[\"aws\"][\"bucket\"],\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME,\n",
    ")\n",
    "logger.info(\n",
    "    f\"Summary metrics dataset saved to s3://{config['aws']['bucket']}/{METRICS_DIR}/{SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME}\"\n",
    ")\n",
    "\n",
    "df_summary_metrics_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select the best option overall and for each instance type\n",
    "df_summary_metrics_dataset_overall = df_summary_metrics_dataset[\n",
    "    df_summary_metrics_dataset.score == df_summary_metrics_dataset.score.max()\n",
    "]\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_dataset_overall.to_csv(csv_buffer, index=False)\n",
    "metrics_overall_data = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(\n",
    "    metrics_overall_data,\n",
    "    BUCKET_NAME,\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME,\n",
    ")\n",
    "\n",
    "df_summary_metrics_dataset_overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset_overall = df_summary_metrics_dataset_overall.round(6)\n",
    "df_summary_metrics_dataset_overall.to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = df_summary_metrics_dataset.dropna()\n",
    "idx = df_summary_metrics_dataset.groupby([\"instance_type\"]).score.idxmax()\n",
    "logger.info(\n",
    "    f\"shape of df_summary_metrics_dataset={df_summary_metrics_dataset.shape}, idx={idx}\"\n",
    ")\n",
    "df_summary_metrics_best_option_instance_type = df_summary_metrics_dataset.loc[idx]\n",
    "logger.info(\n",
    "    f\"shape of df_summary_metrics_best_option_instance_type={df_summary_metrics_best_option_instance_type.shape}\"\n",
    ")\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_best_option_instance_type.to_csv(csv_buffer, index=False)\n",
    "best_option = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(\n",
    "    best_option,\n",
    "    BUCKET_NAME,\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME,\n",
    ")\n",
    "\n",
    "df_summary_metrics_best_option_instance_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_best_option_instance_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_price_per_tx = df_summary_metrics_best_option_instance_type.price_per_txn.min()\n",
    "txn_count_for_showing_cost: int = config[\"report\"][\"txn_count_for_showing_cost\"]\n",
    "\n",
    "price_tx_col_name = f\"price_per_tx_{txn_count_for_showing_cost}_txn\"\n",
    "df_summary_metrics_best_option_instance_type[price_tx_col_name] = (\n",
    "    df_summary_metrics_best_option_instance_type.price_per_txn\n",
    "    * txn_count_for_showing_cost\n",
    ")\n",
    "df_summary_metrics_best_option_instance_type[price_tx_col_name] = round(\n",
    "    df_summary_metrics_best_option_instance_type[price_tx_col_name], 2\n",
    ")\n",
    "df_summary_metrics_best_option_instance_type = (\n",
    "    df_summary_metrics_best_option_instance_type.sort_values(by=price_tx_col_name)\n",
    ")\n",
    "sns_plot = sns.barplot(\n",
    "    df_summary_metrics_best_option_instance_type,\n",
    "    y=\"instance_type\",\n",
    "    x=price_tx_col_name,\n",
    "    hue=\"instance_type\",\n",
    "    orient=\"h\",\n",
    ")\n",
    "sns_plot.axvline(\n",
    "    x=config[\"report\"][\"cost_per_10k_txn_budget\"], ls=\"dotted\", c=\"red\", linewidth=1\n",
    ")\n",
    "title: str = (\n",
    "    f\"Comparing performance of {config['general']['model_name']} for {config['metrics']['dataset_of_interest']} dataset\"\n",
    ")\n",
    "sns_plot.set(\n",
    "    ylabel=\"\",\n",
    "    xlabel=f\"Cost per {txn_count_for_showing_cost:,} transactions (USD)\",\n",
    "    title=title,\n",
    ")\n",
    "\n",
    "xticks = sns_plot.get_xticks()\n",
    "xtick_labels = sns_plot.get_xticklabels()\n",
    "# print(sns_plot.get_xticks(xticks))\n",
    "sns_plot.set_xticks(np.append(xticks, config[\"report\"][\"cost_per_10k_txn_budget\"]))\n",
    "# print(sns_plot.get_xticks(xtick_labels))\n",
    "sns_plot.set_xticklabels(\n",
    "    np.append(\n",
    "        xtick_labels,\n",
    "        matplotlib.text.Text(\n",
    "            config[\"report\"][\"cost_per_10k_txn_budget\"],\n",
    "            0,\n",
    "            f\"${config['report']['cost_per_10k_txn_budget']}\\n(threshold)\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "print(sns_plot.get_xticklabels())\n",
    "num_instance_types = len(df_summary_metrics_dataset.instance_type.unique())\n",
    "for r in df_summary_metrics_best_option_instance_type.iterrows():\n",
    "    y = r[1][\"instance_type\"]\n",
    "    if num_instance_types == 1:\n",
    "        v_shift = config[\"report\"][\"v_shift_w_single_instance\"]\n",
    "    else:\n",
    "        v_shift = config[\"report\"][\"v_shift_w_gt_one_instance\"]\n",
    "\n",
    "    print(f\"v_shift={v_shift}\")\n",
    "    x = r[1][price_tx_col_name] + v_shift + 1\n",
    "    text = f\"{r[1]['transactions_per_minute']} txn/min,\\nconcurrency={r[1]['concurrency']},\\n{r[1]['latency_p95']}s per txn\"\n",
    "    print(f\"x={x}, y={y}, text={text}\")\n",
    "    sns_plot.text(\n",
    "        x,\n",
    "        y,\n",
    "        text,\n",
    "        fontsize=8,  # Size\n",
    "        # fontstyle = \"oblique\",  # Style\n",
    "        color=\"red\",  # Color\n",
    "        ha=\"left\",  # Horizontal alignment\n",
    "        va=\"center\",\n",
    "    )  # Vertical alignment\n",
    "\n",
    "business_summary_plot_fpath: str = os.path.join(\n",
    "    METRICS_DIR, BUSINESS_SUMMARY_PLOT_FNAME2\n",
    ")\n",
    "sns_plot.figure.savefig(buffer, format=\"png\", bbox_inches=\"tight\")\n",
    "buffer.seek(0)\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", \"\", business_summary_plot_fpath)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{business_summary_plot_fpath}\")\n",
    "\n",
    "# Display the plot\n",
    "sns_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset[\"cost_per_10k_txn\"] = (\n",
    "    df_summary_metrics_dataset.price_per_txn * 10000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHARTS_PER_ROW: int = 2\n",
    "fig = px.scatter(\n",
    "    df_summary_metrics_dataset,\n",
    "    x=\"latency_p95\",\n",
    "    y=\"cost_per_10k_txn\",\n",
    "    size=\"transactions_per_minute\",\n",
    "    color=\"instance_type\",  # \",\n",
    "    facet_col=\"concurrency\",\n",
    "    # symbol =\"concurrency\",\n",
    "    facet_col_wrap=CHARTS_PER_ROW,\n",
    "    # text=\"instance_type\",\n",
    "    log_y=False,\n",
    "    # size_max=10,\n",
    "    category_orders={\n",
    "        \"concurrency\": sorted(list(df_summary_metrics_dataset.concurrency))\n",
    "    },\n",
    "    width=1000,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig.for_each_yaxis(lambda y: y.update(title=\"\"))\n",
    "fig.add_annotation(\n",
    "    x=-0.05,\n",
    "    y=0.5,\n",
    "    text=\"Cost per 10,000 requests (USD)\",\n",
    "    textangle=-90,\n",
    "    xref=\"paper\",\n",
    "    yref=\"paper\",\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title=\"\")\n",
    "fig.add_annotation(\n",
    "    showarrow=False,\n",
    "    xanchor=\"center\",\n",
    "    xref=\"paper\",\n",
    "    x=0.5,\n",
    "    yref=\"paper\",\n",
    "    y=-0.1,\n",
    "    text=\"p95 latency (seconds)\",\n",
    ")\n",
    "\n",
    "fig.update_layout(legend={\"title_text\": \"\"})\n",
    "\n",
    "fig.add_vline(\n",
    "    x=config[\"report\"][\"latency_budget\"],\n",
    "    line_width=1,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    ")\n",
    "fig.add_hline(\n",
    "    y=config[\"report\"][\"cost_per_10k_txn_budget\"],\n",
    "    line_width=1,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    ")\n",
    "\n",
    "fig.add_shape(\n",
    "    type=\"rect\",\n",
    "    x0=0,\n",
    "    y0=0,\n",
    "    x1=config[\"report\"][\"latency_budget\"],\n",
    "    y1=config[\"report\"][\"cost_per_10k_txn_budget\"],\n",
    "    fillcolor=\"pink\",\n",
    "    line_width=0,\n",
    "    layer=\"below\",\n",
    "    row=\"all\",\n",
    "    col=\"all\",\n",
    "    exclude_empty_subplots=True,\n",
    ")\n",
    "\n",
    "title: str = (\n",
    "    f\"{config['general']['model_name']} price|performance on the {config['metrics']['dataset_of_interest']} dataset\"\n",
    ")\n",
    "# subtitle: str = \"Hover over bubbles in the shaded quadrant to see best best price|performance options.\"\n",
    "subtitle: str = (\n",
    "    \"Bubbles in shaded quadrant represent experiments that satisfy price|performance costraints.<br>Larger the bubble the greater the transactions/minute.\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=f\"{title}<br>{subtitle}\", x=0.5, y=0.95, xanchor=\"center\", yanchor=\"top\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(margin=dict(t=100))\n",
    "fig.update_xaxes(rangemode=\"nonnegative\")\n",
    "# fig.update_layout(legend=dict(\n",
    "#     yanchor=\"bottom\",\n",
    "#     y=-0.425,\n",
    "#     xanchor=\"left\",\n",
    "#     x=0.01\n",
    "# ))\n",
    "\n",
    "# now is the tricky part to add annotation for the best price performance\n",
    "# we have to identify the correct facet for the best entry and then correct\n",
    "# row and col index for that facet\n",
    "# we first identify the best entry as a json dictionary\n",
    "# to find the index we sort all the concurrency levens in asc order\n",
    "# then find the row col index by iterating but we are iterating from 1,1\n",
    "# which is at the top left because that is how we sett it visually\n",
    "# but the facets row col counter starts at bottom left so the col counter is\n",
    "# correct but the row counter needs to be inverted\n",
    "best_price_perf = df_summary_metrics_dataset_overall.to_dict(orient=\"records\")[0]\n",
    "logger.info(f\"best_price_perf={best_price_perf}\")\n",
    "best_price_perf_x = best_price_perf[\"instance_type\"]\n",
    "best_price_perf_y = best_price_perf[\"price_per_txn\"] * 10000\n",
    "best_price_perf_avg_latency = best_price_perf[\"latency_p95\"]\n",
    "best_price_perf_cost_per_10k = round(best_price_perf[\"price_per_txn\"] * 10000, 2)\n",
    "best_price_perf_instance_type = best_price_perf[\"instance_type\"]\n",
    "best_price_perf_instance_count = best_price_perf[\"instance_count\"]\n",
    "best_price_perf_tpm = best_price_perf[\"transactions_per_minute\"]\n",
    "best_completion_token_count_mean = int(best_price_perf[\"completion_token_count_mean\"])\n",
    "best_prompt_token_count_mean = int(best_price_perf[\"prompt_token_count_mean\"])\n",
    "concurrencies = np.sort(df_summary_metrics_dataset.concurrency.unique())\n",
    "\n",
    "concurrencies = list(df_summary_metrics_dataset.concurrency.unique())\n",
    "\n",
    "c = 1\n",
    "total_rows = math.ceil(len(concurrencies) / CHARTS_PER_ROW)\n",
    "r = total_rows\n",
    "for i, v in enumerate(np.sort(concurrencies)):\n",
    "    logger.info(\n",
    "        f\"iter {i}, r={r}, c={c}, v={v}, \"\n",
    "        f\"best_price_perf_concurrency={best_price_perf['concurrency']}\"\n",
    "    )\n",
    "    if v == best_price_perf[\"concurrency\"]:  # best_price_perf['concurrency']:\n",
    "        logger.info(f\"iter={i}, match found, r={r}, c={c}\")\n",
    "        break\n",
    "    if c == CHARTS_PER_ROW:\n",
    "        r -= 1\n",
    "        c = 1\n",
    "        logger.info(f\"iter {i}, reset r and c indexes, r={r}, c={c}\")\n",
    "    else:\n",
    "        c += 1\n",
    "        logger.info(f\"iter {i}, incrementing c, c={c}\")\n",
    "\n",
    "best_facet_row = r\n",
    "best_facet_col = c\n",
    "logger.info(\n",
    "    f\"facet row col for best_price_perf, r={r}, total_rows={total_rows}, \"\n",
    "    f\"best_facet_row={best_facet_row}, best_facet_col={best_facet_col}\"\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=best_price_perf_avg_latency,\n",
    "    y=best_price_perf_cost_per_10k,\n",
    "    text=\"best price|performance*\",\n",
    "    row=best_facet_row,\n",
    "    col=best_facet_col,\n",
    "    showarrow=True,\n",
    "    arrowhead=1,\n",
    "    exclude_empty_subplots=True,\n",
    ")\n",
    "\n",
    "instance_count_str = (\n",
    "    f\"{best_price_perf_instance_count} instances of \"\n",
    "    if best_price_perf_instance_count > 1\n",
    "    else \"\"\n",
    ")\n",
    "fig.add_annotation(\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    xref=\"paper\",\n",
    "    x=0,\n",
    "    yref=\"paper\",\n",
    "    y=-0.15,\n",
    "    text=f\"*<b>best price|performance</b>: {instance_count_str}{best_price_perf_instance_type}, p95 latency {best_price_perf_avg_latency}s, 10k txn cost ${best_price_perf_cost_per_10k}, transactions/minute {best_price_perf_tpm}, completion tokens {best_completion_token_count_mean}, prompt tokens {best_prompt_token_count_mean}.\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "business_summary_plot_fpath: str = os.path.join(\n",
    "    METRICS_DIR, BUSINESS_SUMMARY_PLOT_FNAME\n",
    ")\n",
    "\n",
    "local_path: str = os.path.join(tempfile.tempdir, BUSINESS_SUMMARY_PLOT_FNAME)\n",
    "pio.write_image(fig, local_path)\n",
    "upload_file_to_s3(BUCKET_NAME, local_path, business_summary_plot_fpath)\n",
    "logger.info(\n",
    "    f\"writing business summary image from {local_path} to s3://{BUCKET_NAME}/{business_summary_plot_fpath}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_path: str = os.path.join(\n",
    "    tempfile.tempdir, BUSINESS_SUMMARY_PLOT_FNAME.split(\".\")[0] + \".html\"\n",
    ")\n",
    "fig.write_html(local_path)\n",
    "business_summary_html_plot_fpath = business_summary_plot_fpath.split(\".\")[0] + \".html\"\n",
    "upload_file_to_s3(BUCKET_NAME, local_path, business_summary_html_plot_fpath)\n",
    "logger.info(\n",
    "    f\"writing business summary image from {local_path} to s3://{BUCKET_NAME}/{business_summary_html_plot_fpath}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df_summary_metrics_best_option_instance_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cost_csv_content_fpath = os.path.join(\n",
    "    METRICS_DIR, SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE\n",
    ")\n",
    "logger.info(\n",
    "    f\"the cost information can be found in the csv file here -> {cost_csv_content_fpath}\"\n",
    ")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    cost_content = get_s3_object(BUCKET_NAME, cost_csv_content_fpath)\n",
    "\n",
    "    # Use pandas to read the CSV content\n",
    "    df_cost_metrics = pd.read_csv(io.StringIO(cost_content))\n",
    "    logger.info(\n",
    "        f\"{cost_csv_content_fpath} read into dataframe of shape {df_cost_metrics.shape}\"\n",
    "    )\n",
    "    df_cost_metrics.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "# df_cost_metrics.fillna('', inplace=True)\n",
    "\n",
    "df_cost_metrics.head()\n",
    "\n",
    "# Convert df_cost_metrics to Markdown table\n",
    "cost_mkdn_table = Tomark.table(df_cost_metrics.to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the experiment cost for evaluations if any. This table contains the\n",
    "# total input and output tokens processed by each judge model and the associated\n",
    "# total cost for running each model\n",
    "evals_cost_csv_content_fpath = os.path.join(METRICS_DIR, EVAL_COST_PER_JUDGE_MODEL)\n",
    "logger.info(\n",
    "    f\"the cost information for evaluations can be found in the csv file here -> {evals_cost_csv_content_fpath}\"\n",
    ")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    evals_cost_content = get_s3_object(BUCKET_NAME, evals_cost_csv_content_fpath)\n",
    "\n",
    "    # Use pandas to read the CSV content\n",
    "    eval_df_cost_metrics = pd.read_csv(io.StringIO(evals_cost_content))\n",
    "    logger.info(\n",
    "        f\"{evals_cost_csv_content_fpath} read into dataframe of shape {eval_df_cost_metrics.shape}\"\n",
    "    )\n",
    "    evals_cost_mkdn_table = Tomark.table(eval_df_cost_metrics.to_dict(orient=\"records\"))\n",
    "    eval_df_cost_metrics.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading evaluation costs from S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ep_metrics_summarized_fpath = os.path.join(\n",
    "    METRICS_DIR, ENDPOINT_METRICS_SUMMARIZED_FNAME\n",
    ")\n",
    "logger.info(\n",
    "    f\"the ep metrics summarized information can be found in the csv file here -> {ep_metrics_summarized_fpath}\"\n",
    ")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    cost_content = get_s3_object(BUCKET_NAME, ep_metrics_summarized_fpath)\n",
    "\n",
    "    # Use pandas to read the CSV content\n",
    "    df_ep_metrics_summarized = pd.read_csv(io.StringIO(cost_content))\n",
    "    df_ep_metrics_summarized = df_ep_metrics_summarized.round(2)\n",
    "    logger.info(\n",
    "        f\"{ep_metrics_summarized_fpath} read into dataframe of shape {df_ep_metrics_summarized.shape}\"\n",
    "    )\n",
    "    df_ep_metrics_summarized.head()\n",
    "    # Convert df_cost_metrics to Markdown table\n",
    "    endpoint_metrics_summarized_table = Tomark.table(\n",
    "        df_ep_metrics_summarized.to_dict(orient=\"records\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "    endpoint_metrics_summarized_table = \"_No endpoint metrics data is available_.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get evaluation data and generate visualizations\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the metrics generation step, we use the data gathered from the evaluation step (if any) and use it to generate as follows:\n",
    "\n",
    "1. Overall accuracy chart: This chart shows the overall accuracy as per majority voting on all candidate models evaluated by the panel of LLM evaluators. This gives users a high level business judgement as to the overall accuracy metrics for each model on the entire dataset.\n",
    "\n",
    "1. Accuracy per candidate model per payload file: This chart shows a more granular view into the accuracy metrics per candidate model per payload file. Users can analyze this chart to view and spot trends within the accuracy trajectory as the prompt size increases or decreases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if the merged long df exists that contains the variable and accuracy value\n",
    "# per payload file per candidate model to generate the per payload accuracy chart\n",
    "per_model_per_payload_accuracy_counts_fpath = os.path.join(\n",
    "    METRICS_DIR, PER_PAYLOAD_MODEL_ACCURACY_MAJORITY_VOTING\n",
    ")\n",
    "try:\n",
    "    per_payload_content = get_s3_object(\n",
    "        BUCKET_NAME, per_model_per_payload_accuracy_counts_fpath, decode=\"True\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"{per_model_per_payload_accuracy_counts_fpath} not found in {BUCKET_NAME}\"\n",
    "    )\n",
    "    per_payload_content = None\n",
    "\n",
    "if per_payload_content is None:\n",
    "    model_evaluation_text_per_payload_accuracy: str = (\n",
    "        \"_Per payload per model accuracy data is not available_.\"\n",
    "    )\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "    df_per_payload_evals = pd.read_csv(StringIO(per_payload_content))\n",
    "    merged_accuracy_df_long_df = df_per_payload_evals.melt(\n",
    "        id_vars=[\"candidate_model\", \"payload_file\"]\n",
    "    )\n",
    "    merged_accuracy_df_long_df.head()\n",
    "    df_per_payload_evals.groupby(\"candidate_model\").mean(\n",
    "        numeric_only=True\n",
    "    ).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Accuracy Trajectory Across Payload Sizes\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the evaluation step, the accuracy measure for each candidate model across each judge is calculated across different variations of payload sizes. This gives users more insights into accuracy measure for their desired payload size for their specific use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt and completion token counts to include in subtitle ofr the following two plots\n",
    "min_prompt_tokens: Optional[int] = int(df_summary_metrics.prompt_token_count_mean.min())\n",
    "max_prompt_tokens: Optional[int] = int(df_summary_metrics.prompt_token_count_mean.max())\n",
    "min_completion_tokens: Optional[int] = int(\n",
    "    df_summary_metrics.completion_token_count_mean.min()\n",
    ")\n",
    "max_completion_tokens: Optional[int] = int(\n",
    "    df_summary_metrics.completion_token_count_mean.max()\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"as read from df_summary_stats, min_prompt_tokens={min_prompt_tokens}, max_prompt_tokens={max_prompt_tokens}, \"\n",
    "    f\"min_completion_tokens={min_completion_tokens}, max_completion_tokens={max_completion_tokens}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if model evals were enabled or not, if not then we just provide a canned message\n",
    "majority_vote_per_model_accuracy_metrics_fpath = os.path.join(\n",
    "    METRICS_DIR, PER_MODEL_ACCURACY_PER_EVAL_JUDGE\n",
    ")\n",
    "try:\n",
    "    content = get_s3_object(\n",
    "        BUCKET_NAME, majority_vote_per_model_accuracy_metrics_fpath, decode=\"True\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"{majority_vote_per_model_accuracy_metrics_fpath} not found in {BUCKET_NAME}\"\n",
    "    )\n",
    "    content = None\n",
    "\n",
    "if content is None:\n",
    "    model_evaluation_text: str = \"_Model evaluation data is not available_.\"\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "    df_evals = pd.read_csv(StringIO(content))\n",
    "    num_judges = len(df_evals.judge_model_id.unique())\n",
    "    model_count = len(df_evals.candidate_model.unique())\n",
    "    judge_name_list = \", \".join(list(df_evals.judge_model_id.unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mean(text):\n",
    "    import re\n",
    "\n",
    "    # Regular expression to extract the numbers\n",
    "    pattern = r\"(\\d+)-(\\d+)\"\n",
    "\n",
    "    # Use re.search to find the numbers\n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    # Extract and print the numbers if a match is found\n",
    "    if match:\n",
    "        n1 = int(match.group(1))\n",
    "        n2 = int(match.group(2))\n",
    "        return (n1 + n2) / 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "try:\n",
    "    data = merged_accuracy_df_long_df[\n",
    "        merged_accuracy_df_long_df.variable == \"majority_voting_accuracy\"\n",
    "    ]\n",
    "    data.payload_file = data.payload_file.map(\n",
    "        lambda x: x.replace(\"payload_\", \"\").replace(\".jsonl\", \"\")\n",
    "    )\n",
    "    data[\"mean_payload_size\"] = data.payload_file.map(get_mean)\n",
    "    data = data.sort_values(by=\"mean_payload_size\")\n",
    "\n",
    "    # dataset name\n",
    "    ds_name = \"LongBench\"\n",
    "\n",
    "    # Create the line + point plot\n",
    "    fig = px.line(\n",
    "        data,\n",
    "        x=\"payload_file\",\n",
    "        y=\"value\",\n",
    "        color=\"candidate_model\",\n",
    "        line_dash_sequence=[\"dash\"],  # Set line style to dashed\n",
    "        symbol=\"candidate_model\",  # Set different symbols based on candidate_model\n",
    "        markers=True,\n",
    "    )  # Show points on the line\n",
    "\n",
    "    # Update the trace for all symbols to be 'x'\n",
    "    fig.update_traces(\n",
    "        marker=dict(symbol=\"x\", size=10)\n",
    "    )  # Set the marker to 'X' and adjust size\n",
    "\n",
    "    # Update layout for y-axis range\n",
    "    subtitle = f\"Prompt size: [{min_prompt_tokens} - {max_prompt_tokens}] tokens, completion size: [{min_completion_tokens} - {max_completion_tokens}] tokens\"\n",
    "    fig.update_layout(\n",
    "        title=f'Model accuracy for prompts of different sizes in the \"{ds_name}\" dataset <br>determined by a {num_judges} LLM judge panel through majority vote<br>{subtitle}',\n",
    "        yaxis=dict(\n",
    "            range=[0, 110],\n",
    "            title=\"Accuracy\",\n",
    "            ticksuffix=\"%\",\n",
    "        ),\n",
    "        xaxis_title=\"Prompt size range\",\n",
    "        legend_title=None,\n",
    "        margin=dict(t=150),\n",
    "        width=1500,  # Set the width of the plot\n",
    "        height=500,  # Set the height of the plot\n",
    "    )\n",
    "    fig.update_layout(font=dict(size=18))\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    logger.info(\n",
    "        f\"Error occured while generating per model per payload accuracy measure chart: {e}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    per_model_per_payload_accuracy_fpath: str = os.path.join(\n",
    "        METRICS_DIR, PER_PAYLOAD_FILE_ACCURACY_TRAJECTORY\n",
    "    )\n",
    "\n",
    "    local_path: str = os.path.join(\n",
    "        tempfile.tempdir, PER_PAYLOAD_FILE_ACCURACY_TRAJECTORY\n",
    "    )\n",
    "    pio.write_image(fig, local_path)\n",
    "    upload_file_to_s3(BUCKET_NAME, local_path, per_model_per_payload_accuracy_fpath)\n",
    "    logger.info(\n",
    "        f\"writing accuracy trajectory per payload file from {local_path} to s3://{BUCKET_NAME}/{per_model_per_payload_accuracy_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.info(\n",
    "        f\"Error occured while generating per model per payload accuracy measure chart: {e}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if content is not None:\n",
    "    local_path: str = os.path.join(\n",
    "        tempfile.tempdir, PER_PAYLOAD_FILE_ACCURACY_TRAJECTORY.split(\".\")[0] + \".html\"\n",
    "    )\n",
    "    fig.write_html(local_path)\n",
    "    per_model_per_payload_accuracy_html_fpath = (\n",
    "        per_model_per_payload_accuracy_fpath.split(\".\")[0] + \".html\"\n",
    "    )\n",
    "    upload_file_to_s3(\n",
    "        BUCKET_NAME, local_path, per_model_per_payload_accuracy_html_fpath\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"writing business summary image from {local_path} to s3://{BUCKET_NAME}/{per_model_per_payload_accuracy_html_fpath}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the overall majority voting accuracy per candidate model\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the evaluation step, the overall accuracy (based on all majority votes) are calculated for each of the candidate model and displayed on a bar chart. This helps gather insights into an overall business judgement as to which model satisfies the accuracy criteria (configurable within the config file).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    merged_accuracy_df_long_df.groupby([\"candidate_model\", \"variable\"])[\n",
    "        \"value\"\n",
    "    ].mean().reset_index()\n",
    "    majority_vote_accuracy_df = (\n",
    "        merged_accuracy_df_long_df.groupby([\"candidate_model\", \"variable\"])[\"value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    majority_vote_accuracy_df = majority_vote_accuracy_df.sort_values(\n",
    "        by=\"value\", ascending=False\n",
    "    )\n",
    "    data = majority_vote_accuracy_df[\n",
    "        majority_vote_accuracy_df.variable == \"majority_voting_accuracy\"\n",
    "    ]\n",
    "    data\n",
    "except Exception as e:\n",
    "    logger.info(f\"Error occured while getting the majority voting dataframe: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def findnth(haystack, needle, n):\n",
    "    parts = haystack.split(needle, n + 1)\n",
    "    if len(parts) <= n + 1:\n",
    "        return -1\n",
    "    return len(haystack) - len(parts[-1]) - len(needle)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Sample data\n",
    "    data = majority_vote_accuracy_df[\n",
    "        majority_vote_accuracy_df.variable == \"majority_voting_accuracy\"\n",
    "    ]\n",
    "    # Define the y-coordinate for the dashed line\n",
    "    threshold = 80\n",
    "    dashed_line_y = threshold\n",
    "\n",
    "    # dataset name\n",
    "    ds_name = \"LongBench\"\n",
    "\n",
    "    # models that have accuracy gte than threshold\n",
    "    selected_models = data[data.value >= threshold].candidate_model.unique()\n",
    "    num_selected_models = len(selected_models)\n",
    "    if num_selected_models > 0:\n",
    "        # if there are more than 3 models then text would not fit in one line so add\n",
    "        # a line break after the 3rd model\n",
    "        selected_models_str = (\n",
    "            \", \".join(selected_models) + f\" had accuracy >= {threshold}% \"\n",
    "        )\n",
    "        if selected_models_str.count(\",\") > 3:\n",
    "            i = findnth(selected_models_str, \",\", 2)\n",
    "            selected_models_str = list(selected_models_str)\n",
    "            selected_models_str[i] = \",<br>\"\n",
    "            selected_models_str = \"\".join(selected_models_str).replace(\"<br> \", \"<br>\")\n",
    "            selected_models_str = (\n",
    "                f\"{num_selected_models} models, namely: \" + selected_models_str\n",
    "            )\n",
    "    else:\n",
    "        selected_models_str = f\"No candidate model(s) had accuracy >= the accuracy threshold of {threshold}%\"\n",
    "\n",
    "    # Create the bar chart with Plotly Express\n",
    "    subtitle = f\"Prompt size: [{min_prompt_tokens} - {max_prompt_tokens}] tokens, completion size: [{min_completion_tokens} - {max_completion_tokens}] tokens\"\n",
    "    fig = px.bar(\n",
    "        data,\n",
    "        x=\"candidate_model\",\n",
    "        y=\"value\",\n",
    "        # color='candidate_model',\n",
    "        title=f'Model accuracy on \"{ds_name}\" dataset determined by a {num_judges} LLM judge panel through majority vote<br>{selected_models_str}<br>{subtitle}',\n",
    "        color=\"value\",\n",
    "        color_continuous_scale=\"Blues\",\n",
    "    )\n",
    "\n",
    "    # Add a dashed horizontal line\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=-0.5,\n",
    "        y0=dashed_line_y,\n",
    "        x1=len(data[\"candidate_model\"]) - 0.5,\n",
    "        y1=dashed_line_y,\n",
    "        line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "        xref=\"x\",\n",
    "        yref=\"y\",\n",
    "    )\n",
    "\n",
    "    # Add an annotation to the horizontal line\n",
    "    fig.add_annotation(\n",
    "        x=len(data[\"candidate_model\"]) - 1,  # Position at the end of the line\n",
    "        y=dashed_line_y,\n",
    "        text=f\"Accuracy threshold {threshold}%\",  # The text to display\n",
    "        showarrow=False,\n",
    "        yshift=10,  # Slightly move the annotation above the line\n",
    "        font=dict(color=\"red\", size=15),  # Customize the text style\n",
    "        align=\"right\",\n",
    "    )\n",
    "\n",
    "    # Add annotations on top of each bar\n",
    "    for i, row in data.iterrows():\n",
    "        fig.add_annotation(\n",
    "            x=row[\"candidate_model\"],  # X position (category)\n",
    "            y=row[\"value\"],  # Y position (value)\n",
    "            text=f\"{round(row['value'], 2)}%\",  # Annotation text\n",
    "            showarrow=False,  # No arrow, just the text\n",
    "            yshift=10,  # Shift text upward so it's above the bar\n",
    "        )\n",
    "\n",
    "    # Update layout for better readability based on the number of candidate models in the evaluation process\n",
    "    if model_count <= 2:\n",
    "        height = 500\n",
    "    else:\n",
    "        height = 750\n",
    "\n",
    "    # Update layout for y-axis range\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(\n",
    "            range=[0, 100],\n",
    "            title=\"Accuracy\",\n",
    "            ticksuffix=\"%\",\n",
    "        ),\n",
    "        xaxis_title=\"\",\n",
    "        width=1500,  # Set the width of the plot\n",
    "        height=height,  # Set the height of the plot\n",
    "    )\n",
    "\n",
    "    fig.update_layout(font=dict(size=18), margin=dict(t=200))\n",
    "\n",
    "    fig.update_layout(showlegend=False, coloraxis_showscale=False)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    logger.info(f\"Error occured while generating the overall accuracy chart: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    overall_model_majority_voting_accuracy_fpath: str = os.path.join(\n",
    "        METRICS_DIR, OVERALL_CANDIDATE_MODEL_MAJORITY_VOTING_ACCURACY\n",
    "    )\n",
    "\n",
    "    local_path: str = os.path.join(\n",
    "        tempfile.tempdir, OVERALL_CANDIDATE_MODEL_MAJORITY_VOTING_ACCURACY\n",
    "    )\n",
    "    pio.write_image(fig, local_path)\n",
    "    upload_file_to_s3(\n",
    "        BUCKET_NAME, local_path, overall_model_majority_voting_accuracy_fpath\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"writing the overall candidate model accuracy (with majority voting) from {local_path} to s3://{BUCKET_NAME}/{overall_model_majority_voting_accuracy_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.info(\n",
    "        f\"Error occured while writing the overall accuracy chart to s3://{BUCKET_NAME}/{overall_model_majority_voting_accuracy_fpath}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if content is not None:\n",
    "    local_path: str = os.path.join(\n",
    "        tempfile.tempdir,\n",
    "        OVERALL_CANDIDATE_MODEL_MAJORITY_VOTING_ACCURACY.split(\".\")[0] + \".html\",\n",
    "    )\n",
    "    fig.write_html(local_path)\n",
    "    overall_model_majority_voting_accuracy_html_fpath = (\n",
    "        overall_model_majority_voting_accuracy_fpath.split(\".\")[0] + \".html\"\n",
    "    )\n",
    "    upload_file_to_s3(\n",
    "        BUCKET_NAME, local_path, overall_model_majority_voting_accuracy_html_fpath\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"writing business summary image from {local_path} to s3://{BUCKET_NAME}/{overall_model_majority_voting_accuracy_html_fpath}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert df_per_payload_evals to Markdown table\n",
    "    df_temp = (\n",
    "        df_per_payload_evals.groupby(\"candidate_model\")\n",
    "        .mean(numeric_only=True)\n",
    "        .reset_index()\n",
    "        .round(2)\n",
    "    )\n",
    "    # most accuract model first\n",
    "    df_temp = df_temp.sort_values(by=\"majority_voting_accuracy\", ascending=False)\n",
    "    # the first column of this dataframe is the candidate model name and the rest all\n",
    "    # are accuracy metrics columns so we want to append a % to all this columns\n",
    "    df_temp.iloc[:, 1:] = df_temp.iloc[:, 1:].astype(str) + \"%\"\n",
    "    model_eval_accuracy_mkdn_table = Tomark.table(df_temp.to_dict(orient=\"records\"))\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not get the model eval accuracy breakdown for each judge model and the overall majority voting results: {e}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation text\n",
    "if content is not None:\n",
    "    model_evaluation_text: str = f\"\"\"\n",
    "Model evaluations were performed by a panel of {num_judges} LLM judges: {judge_name_list}. Model outputs were compared with ground truth available in the dataset by the judge models. The following charts provide the results of model evaluations.\n",
    "\n",
    "{model_eval_accuracy_mkdn_table}\n",
    "\n",
    "![Overall model accuracy]({OVERALL_CANDIDATE_MODEL_MAJORITY_VOTING_ACCURACY})\n",
    "View an interactive version of the overall accuracy chart [here]({os.path.basename(overall_model_majority_voting_accuracy_html_fpath)})\n",
    "\n",
    "![Model accuracy trend across prompt sizes]({PER_PAYLOAD_FILE_ACCURACY_TRAJECTORY})\n",
    "View an interactive version of the accuracy trajectory chart [here]({os.path.basename(per_model_per_payload_accuracy_html_fpath)})\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUSINESS_SUMMARY: str = \"\"\"We did performance benchmarking for `{model_name}` on \"{instance_types}\" on multiple datasets and based on the test results the best price performance for dataset `{ds}` is provided by the `{selected_instance_type}`.\n",
    "\n",
    "{mkdn_table}\n",
    "\n",
    "The price performance comparison for different instance types is presented below. An interactive version of this chart is available [here]({business_summary_plot_interactive_fpath}).\n",
    "\n",
    "![Price performance comparison]({business_summary_plot_fpath})\n",
    "\n",
    "### Latency Metrics Analysis\n",
    "\n",
    "The following table provides token latency metrics including the overall latency, Time To First Token (TTFT), and Time Per Output Token (TPOT) for the `{ds}` dataset.\n",
    "\n",
    "{latency_metrics_table}\n",
    "\n",
    "### Failed experiments\n",
    "\n",
    "{failed_experiment_text}\n",
    "\n",
    "{failed_experiment_table}\n",
    "\n",
    "### Model evaluations\n",
    "\n",
    "{model_evaluation_text}\n",
    "\n",
    "### Endpoint metrics\n",
    "\n",
    "The following table provides endpoint utilization and invocation metrics.\n",
    "\n",
    "{endpoint_metrics_summarized_table}\n",
    "\n",
    "### Configuration\n",
    "\n",
    "The configuration used for these tests is available in the [`config`](#configuration-file) file.\n",
    "\n",
    "### Experiment cost\n",
    "\n",
    "#### Model Benchmarking Cost\n",
    "\n",
    "The cost to run each experiment is provided in the table below. The total cost for running all experiments is {total_cost_as_str}.\n",
    "\n",
    "\n",
    "{cost_table}\n",
    "\n",
    "#### Model Evaluation Cost\n",
    "\n",
    "The cost to evaluate {num_instances} candidate models using {num_judges} LLM evaluators is {total_evalaution_cost_as_str}.\n",
    "\n",
    "{evals_cost_mkdn_table}\n",
    "\n",
    "The total cost incurred for **model benchmarking and evaluations**: {total_eval_benchmarking_cost}.\n",
    "\"\"\"\n",
    "transposed_list = []\n",
    "logger.info(\n",
    "    f\"df_summary_metrics_dataset_overall is: {df_summary_metrics_dataset_overall.columns}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"df_summary_metrics_dataset instance types: {df_summary_metrics_dataset.instance_type}\"\n",
    ")\n",
    "best_instance_type_info = df_summary_metrics_dataset_overall.to_dict(orient=\"records\")[\n",
    "    0\n",
    "]\n",
    "cols_to_delete = [\n",
    "    \"score\",\n",
    "    \"score_dict\",\n",
    "    \"any_criterion_failed\",\n",
    "    \"latency_criterion_failed\",\n",
    "    \"cost_per_txn_criterion_failed\",\n",
    "    \"error_rate_criterion_failed\",\n",
    "    \"error_rate_text\",\n",
    "    \"latency_p95_text\",\n",
    "    \"price_per_10k_txn_text\",\n",
    "]\n",
    "\n",
    "for c in cols_to_delete:\n",
    "    del best_instance_type_info[c]\n",
    "\n",
    "# we want to convert all the latency/TTFT/TTLT/TPOT columns into one single column\n",
    "# for e.g. latency mean|p50|p95|p99: 0.044 | 0.45 | 0.6 | 0.789\n",
    "latency_prefixes = [\"latency_\", \"TTFT_\", \"TPOT_\"]\n",
    "latency_suffixes = [\"p50\", \"p95\", \"p99\"]\n",
    "latency_cols = []\n",
    "new_dict = {}\n",
    "for c in latency_prefixes:\n",
    "    # remove the underscore from the end\n",
    "    new_col = c[:-1] + \" \"\n",
    "    new_val = \"\"\n",
    "    # check for if that latency column has any values\n",
    "    has_values: bool = False\n",
    "    for s in latency_suffixes:\n",
    "        col_to_delete = c + s\n",
    "        # check if this key exists, it may not exist for some metric\n",
    "        # for example we dont keep p95 for TPOT only p50 and p99\n",
    "        if best_instance_type_info.get(col_to_delete) is None:\n",
    "            continue\n",
    "        has_values = True\n",
    "        new_col += s + \", \"\n",
    "        val = best_instance_type_info[col_to_delete]\n",
    "        # somewhere in the code mean has been rounded to 6\n",
    "        # cant find where, round it to 4\n",
    "        if s == \"mean\":\n",
    "            val = round(val, 4)\n",
    "        new_val += str(val) + \", \"\n",
    "        logger.info(f\"c={c}, s={s}, col_to_delete={col_to_delete}, new_val={new_val}\")\n",
    "        latency_cols.append(col_to_delete)\n",
    "    # remove the underscore from the end\n",
    "    new_col = new_col[:-2]\n",
    "    new_val = new_val[:-2]\n",
    "    logger.info(f\"new_col={new_col}, new_val={new_val}\")\n",
    "    if has_values:\n",
    "        new_dict[new_col] = new_val\n",
    "\n",
    "# we dont want a colum each for each combination of latency metric and suffix\n",
    "# we want all latency statistical summary metrics in a single row for ease of display\n",
    "# so now that we have it that way in a new dict so first delete those columns from\n",
    "# the old dictionary\n",
    "for c in latency_cols:\n",
    "    del best_instance_type_info[c]\n",
    "# now merge the new dict with the old dict\n",
    "best_instance_type_info = best_instance_type_info | new_dict\n",
    "# logger.info(f\"new_dict={new_dict}\")\n",
    "for k, v in best_instance_type_info.items():\n",
    "    transposed_list.append({\"Information\": k, \"Value\": v})\n",
    "mkdn_table = Tomark.table(transposed_list)\n",
    "\n",
    "\n",
    "# Define the prefixes and columns for latency-related metrics\n",
    "latency_index_cols = [\"experiment_name\", \"instance_type\", \"concurrency\"]\n",
    "df_summary_latency_metrics_dataset = df_summary_metrics_dataset[\n",
    "    latency_index_cols + latency_cols\n",
    "]\n",
    "latency_metrics_table = \"_No token latency metrics data is available. To get token latency metrics enable streaming responses by setting `stream: True` in experiment configuration_.\"\n",
    "if \"TTFT_p50\" in df_summary_latency_metrics_dataset.columns:\n",
    "    if all(df_summary_latency_metrics_dataset.TTFT_p50.isnull()):\n",
    "        logger.info(\n",
    "            f\"all TTFT metrics are null, so no token latency metrics are available\"\n",
    "        )\n",
    "    else:\n",
    "        latency_metrics_table = Tomark.table(\n",
    "            df_summary_latency_metrics_dataset.to_dict(orient=\"records\")\n",
    "        )\n",
    "else:\n",
    "    logger.info(\n",
    "        f\"TTFT metrics fields are not available, looks like no experiment had streaming enabled\"\n",
    "    )\n",
    "\n",
    "\n",
    "experiments_w_some_criterion_failed = (\n",
    "    df_summary_metrics_dataset.any_criterion_failed.sum()\n",
    ")\n",
    "perf_criteria_text = f\"\"\"`Latency` < `{config['report']['latency_budget']}s`, `cost per 10k transactions`: `${config['report']['cost_per_10k_txn_budget']}`, `error rate`: `{config['report']['error_rate_budget']}`\"\"\"\n",
    "if experiments_w_some_criterion_failed > 0:\n",
    "    logger.info(\n",
    "        f\"a total of {experiments_w_some_criterion_failed} experiment run(s) failed at least one criterion\"\n",
    "    )\n",
    "    failed_experiment_text = f\"\"\"There were a total of {experiments_w_some_criterion_failed} experiment run(s) that failed at least one configured performance criteria: {perf_criteria_text}. See table below.    \n",
    "    \"\"\"\n",
    "    failed_experiment_table = Tomark.table(\n",
    "        df_summary_metrics_dataset[\n",
    "            df_summary_metrics_dataset.any_criterion_failed == True\n",
    "        ][\n",
    "            [\n",
    "                \"experiment_name\",\n",
    "                \"payload_file\",\n",
    "                \"concurrency\",\n",
    "                \"error_rate_text\",\n",
    "                \"latency_p95_text\",\n",
    "                \"price_per_10k_txn_text\",\n",
    "            ]\n",
    "        ].to_dict(\n",
    "            \"records\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    failed_experiment_text = f\"\"\"All experiments satisfied the configured performance criteria: {perf_criteria_text}.\"\"\"\n",
    "    failed_experiment_table = \"\"\n",
    "\n",
    "\n",
    "plural = \"s\" if len(df_summary_metrics.instance_type.unique()) > 1 else \"\"\n",
    "instance_types_md = \", \".join(\n",
    "    [f\"`{it}`\" for it in df_summary_metrics.instance_type.unique()]\n",
    ")\n",
    "num_instance_types = len(df_summary_metrics.instance_type.unique())\n",
    "selected_instance_type: str = df_summary_metrics_dataset_overall.to_dict(\n",
    "    orient=\"records\"\n",
    ")[0][\"instance_type\"]\n",
    "ds: str = config[\"metrics\"][\"dataset_of_interest\"]\n",
    "\n",
    "num_judges_value = num_judges if \"num_judges\" in locals() else \"No\"\n",
    "eval_df_cost_metrics = eval_df_cost_metrics if \"eval_df_cost_metrics\" in locals() else 0\n",
    "evals_cost_mkdn_table = (\n",
    "    evals_cost_mkdn_table if \"evals_cost_mkdn_table\" in locals() else \"\"\n",
    ")\n",
    "\n",
    "total_evalaution_cost_as_str = (\n",
    "    f\"${eval_df_cost_metrics.total_cost.sum():.4f}\"\n",
    "    if isinstance(eval_df_cost_metrics, pd.DataFrame)\n",
    "    else 0\n",
    ")\n",
    "\n",
    "total_eval_benchmarking_cost = (\n",
    "    f\"${df_cost_metrics.cost.sum() + eval_df_cost_metrics.total_cost.sum():.4f}\"\n",
    "    if isinstance(eval_df_cost_metrics, pd.DataFrame)\n",
    "    else f\"${df_cost_metrics.cost.sum():.2f}\"\n",
    ")\n",
    "\n",
    "business_summary = BUSINESS_SUMMARY.format(\n",
    "    model_name=config[\"general\"][\"model_name\"],\n",
    "    instance_types=instance_types_md,\n",
    "    num_instances=num_instance_types,\n",
    "    num_judges=num_judges_value,\n",
    "    plural=plural,\n",
    "    ds=ds,\n",
    "    selected_instance_type=selected_instance_type,\n",
    "    mkdn_table=\"\\n\" + mkdn_table + \"\\n\",\n",
    "    failed_experiment_text=\"\\n\" + failed_experiment_text + \"\\n\",\n",
    "    failed_experiment_table=\"\\n\" + failed_experiment_table + \"\\n\",\n",
    "    endpoint_metrics_summarized_table=endpoint_metrics_summarized_table,\n",
    "    latency_metrics_table=\"\\n\" + latency_metrics_table + \"\\n\",\n",
    "    cfg_file_path=os.path.basename(CONFIG_FILE),\n",
    "    business_summary_plot_fpath=BUSINESS_SUMMARY_PLOT_FNAME,\n",
    "    business_summary_plot_interactive_fpath=os.path.basename(\n",
    "        business_summary_html_plot_fpath\n",
    "    ),\n",
    "    cost_table=cost_mkdn_table,\n",
    "    evals_cost_mkdn_table=evals_cost_mkdn_table,\n",
    "    model_evaluation_text=model_evaluation_text,\n",
    "    total_cost_as_str=f\"${df_cost_metrics.cost.sum():.2f}\",\n",
    "    total_evalaution_cost_as_str=total_evalaution_cost_as_str,\n",
    "    total_eval_benchmarking_cost=total_eval_benchmarking_cost,\n",
    ")\n",
    "\n",
    "business_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "dttm = str(datetime.utcnow())\n",
    "datasets_used = \", \".join(\n",
    "    [f\"`{d}`\" for d in config[\"s3_read_data\"][\"source_data_files\"]]\n",
    ")\n",
    "title = config[\"report\"].get(\n",
    "    \"title\", f\"Results for performance benchmarking {config['general']['model_name']}\"\n",
    ")\n",
    "overall_results_md = OVERALL_RESULTS_MD.format(\n",
    "    title=title,\n",
    "    dttm=dttm,\n",
    "    fmbench_version=fmbench_version,\n",
    "    business_summary=business_summary,\n",
    "    datasets=datasets_used,\n",
    ")\n",
    "results_group_cols: List[str] = [\"instance_type\", \"payload_file\"]\n",
    "result_rows: List[str] = []\n",
    "latency_budget: int = config[\"report\"].get(\"latency_budget\", LATENCY_BUDGET)\n",
    "\n",
    "for row in df_summary_metrics[results_group_cols].drop_duplicates().iterrows():\n",
    "    instance_type = row[1][\"instance_type\"]\n",
    "    dataset = row[1][\"payload_file\"]\n",
    "    df_summary_metrics_nz_subset = df_summary_metrics_nz[\n",
    "        (df_summary_metrics_nz.instance_type == instance_type)\n",
    "        & (df_summary_metrics_nz.payload_file == dataset)\n",
    "        & (df_summary_metrics_nz.latency_p95 <= latency_budget)\n",
    "    ]\n",
    "    num_results = df_summary_metrics_nz_subset.shape[0]\n",
    "    result_row: Optional[str] = None\n",
    "    if num_results > 0:\n",
    "        logger.info(\n",
    "            f\"there are {num_results} options to choose the best option from for instance_type={instance_type}, dataset={dataset}\"\n",
    "        )\n",
    "        df_summary_metrics_nz_subset_selected = df_summary_metrics_nz_subset[\n",
    "            df_summary_metrics_nz_subset.concurrency\n",
    "            == df_summary_metrics_nz_subset.concurrency.max()\n",
    "        ]\n",
    "        best = df_summary_metrics_nz_subset_selected.to_dict(orient=\"records\")[0]\n",
    "        best_instance_type = best[\"instance_type\"]\n",
    "        # logger.info(best)\n",
    "        result_desc = RESULT_DESC.format(\n",
    "            latency_budget=latency_budget,\n",
    "            instance_type=best_instance_type,\n",
    "            dataset=dataset,\n",
    "            concurrency=best[\"concurrency\"],\n",
    "            latency_median=best[\"latency_p50\"],\n",
    "            prompt_size=int(best[\"prompt_token_count_mean\"]),\n",
    "            completion_size=int(best[\"completion_token_count_mean\"]),\n",
    "            tpm=int(best[\"transactions_per_minute\"]),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"there are NO options to choose from for instance_type={instance_type}, dataset={dataset}\"\n",
    "        )\n",
    "        best_instance_type = instance_type\n",
    "        result_desc = RESULT_FAILURE_DESC.format(\n",
    "            latency_budget=latency_budget, instance_type=instance_type, dataset=dataset\n",
    "        )\n",
    "    result_row: str = RESULT_ROW.format(\n",
    "        instance_type=best_instance_type, dataset=dataset, desc=result_desc\n",
    "    )\n",
    "    result_rows.append(result_row)\n",
    "\n",
    "overall_results_md += \"\\n\".join(result_rows)\n",
    "\n",
    "OVERALL_RESULTS_PLOTS_MD: str = \"\"\"\n",
    "\n",
    "## Plots\n",
    "\n",
    "The following plots provide insights into the results from the different experiments run.\n",
    "\n",
    "![{plot1_text}]({plot1_fname})\n",
    "\n",
    "![{plot2_text}]({plot2_fname})\n",
    "\n",
    "![{plot3_text}]({plot3_fname})\n",
    "\"\"\"\n",
    "\n",
    "overall_results_plots_md: str = OVERALL_RESULTS_PLOTS_MD.format(\n",
    "    plot1_text=ERROR_RATES_PLOT_TEXT,\n",
    "    plot1_fname=ERROR_RATES_PLOT_FNAME,\n",
    "    plot2_text=TOKENS_VS_LATENCY_PLOT_TEXT,\n",
    "    plot2_fname=TOKENS_VS_LATENCY_PLOT_FNAME,\n",
    "    plot3_text=CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT,\n",
    "    plot3_fname=CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME,\n",
    ")\n",
    "\n",
    "overall_results_md += overall_results_plots_md\n",
    "\n",
    "# dump the config file at the end so that there is a record of what\n",
    "# we tested with\n",
    "# write the config file in the results directory for record sake\n",
    "import yaml\n",
    "\n",
    "\n",
    "class VerboseSafeDumper(yaml.SafeDumper):\n",
    "    def ignore_aliases(self, data):\n",
    "        return True\n",
    "\n",
    "\n",
    "config_yml = yaml.dump(config, Dumper=VerboseSafeDumper)\n",
    "config_dump = f\"\"\"\\n\\n\n",
    "## Configuration file\n",
    "```.bash\n",
    "{config_yml}\n",
    "```\n",
    "\"\"\"\n",
    "overall_results_md += config_dump\n",
    "\n",
    "\n",
    "fpath: str = os.path.join(METRICS_DIR, RESULTS_DESC_MD_FNAME)\n",
    "logger.info(f\"writing final markdown to {METRICS_DIR}\")\n",
    "Path(fpath).write_text(overall_results_md)\n",
    "logger.info(overall_results_md)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(overall_results_md, BUCKET_NAME, \"\", METRICS_DIR, RESULTS_DESC_MD_FNAME)\n",
    "logger.info(\n",
    "    f\"results.md file saved to to s3://{BUCKET_NAME}/{METRICS_DIR}/{RESULTS_DESC_MD_FNAME}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save all the metrics and report files locally\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(RESULTS_DIR, ignore_errors=True)\n",
    "time.sleep(10)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=False)\n",
    "\n",
    "# config_yml = yaml.dump(config, default_flow_style=False)\n",
    "fpath: str = os.path.join(RESULTS_DIR, os.path.basename(CONFIG_FILE))\n",
    "logger.info(f\"saving config yaml in file {fpath}\")\n",
    "Path(fpath).write_text(config_yml)\n",
    "\n",
    "logger.info(\n",
    "    f\"going to download all metrics and reports from s3 into {RESULTS_DIR} directory\"\n",
    ")\n",
    "download_multiple_files_from_s3(BUCKET_NAME, METRICS_DIR, RESULTS_DIR)\n",
    "import glob\n",
    "\n",
    "result_files = glob.glob(os.path.join(RESULTS_DIR, \"**\"), recursive=True)\n",
    "logger.info(\"\\n\".join([f for f in result_files]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import docker\n",
    "import subprocess\n",
    "\n",
    "docker_running = False\n",
    "generate_html_report = config[\"report\"].get(\"generate_html_report\", True)\n",
    "\n",
    "if generate_html_report is True:\n",
    "    try:\n",
    "        docker_client = docker.DockerClient()\n",
    "        docker_running = docker_client.ping()\n",
    "    except Exception as e:\n",
    "        print(f\"seems like docker is not installed or not running, exception={e}\")\n",
    "\n",
    "    if docker_running is True:\n",
    "        report_md_path = f\"{os.getcwd()}/{RESULTS_DIR}\"\n",
    "        report_html_path = os.path.join(\n",
    "            report_md_path, f\"{config['general']['name']}.html\"\n",
    "        )\n",
    "        cmd = f\"docker run --rm -v {report_md_path}:/public -w /public -u $(id -u):$(id -g) registry.gitlab.com/quarto-forge/docker/quarto quarto render report.md --to html --self-contained\"\n",
    "        logger.info(f'going to create self-conained html report with cmd=\"{cmd}\"')\n",
    "        process = subprocess.Popen(\n",
    "            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True\n",
    "        )\n",
    "        std_out, std_err = process.communicate()\n",
    "        logger.info(std_out.strip())\n",
    "        logger.info(std_err)\n",
    "\n",
    "    else:\n",
    "        logger.error(\n",
    "            f\"docker is not available, not going to create self-conained html report\"\n",
    "        )\n",
    "else:\n",
    "    logger.info(\n",
    "        f\"generate_html_report={generate_html_report}, skipping html report generation\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
