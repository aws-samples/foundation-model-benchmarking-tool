{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training scripts for Models to measure performance\n",
    "---------------------\n",
    "\n",
    "This notebook initiates the training script to execute and measures metrics such as time to train, loss values, instance utilization metrics, etc. \n",
    "\n",
    "The training pipeline consists of the following stages:\n",
    "\n",
    "1. Configuration Retrieval: Experiment configurations, including training scripts, hyperparameters, and datasets, are fetched from a central repository or file.\n",
    "\n",
    "1. Training Execution: The training script is executed based on the configuration. This script:\n",
    "\n",
    "1. Prepares the dataset.\n",
    "\n",
    "1. Sets up the model and tokenizer.\n",
    "\n",
    "1. Initiates training using the optimum.neuron.NeuronTrainer.\n",
    "\n",
    "1. Saves the trained model and logs metrics.\n",
    "\n",
    "1. Metrics Recording: Metrics such as loss, training time, and validation accuracy are captured and stored.\n",
    "\n",
    "All the data and metrics are stored in the `report.html` that is created. User can configure variables for target total loss value as well as the time to train in the config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import importlib.util\n",
    "import fmbench.scripts\n",
    "from pathlib import Path\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from fmbench.scripts import constants\n",
    "from typing import Dict, List, Optional\n",
    "from sagemaker import get_execution_role\n",
    "import importlib.resources as pkg_resources\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.exceptions import NoCredentialsError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a logger to log all messages while the code runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the config.yml file\n",
    "------\n",
    "\n",
    "The config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations, and model configurations like the version of the model, the endpoint name, model_id that needs to be deployed. Configurations also support the gives instance type to be used, for example: \"ml.g5.24xlarge\", the image uri, whether or not to deploy this given model, followed by an inference script \"jumpstart.py\" which supports the inference script for jumpstart models to deploy the model in this deploy notebook. The experiment configuration also has the training script parameter, that is run as a part of the benchmarking test, where the loss value, time to train and other metrics are recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Load the config.yml file referring to the globals.py file\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "\n",
    "## configure the aws region and execution role\n",
    "aws_region = config['aws']['region']\n",
    "\n",
    "\n",
    "# try:\n",
    "#     sagemaker_execution_role = get_execution_role()\n",
    "#     config['aws']['sagemaker_execution_role'] = sagemaker_execution_role\n",
    "#     logger.info(f\"determined SageMaker exeuction role from get_execution_role\")\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"could not determine SageMaker execution role, error={e}\")\n",
    "#     logger.info(f\"going to look for execution role in config file..\")\n",
    "#     sagemaker_execution_role = config['aws'].get('sagemaker_execution_role')\n",
    "#     if sagemaker_execution_role is not None:\n",
    "#         logger.info(f\"found SageMaker execution role in config file..\")\n",
    "\n",
    "logger.info(f\"aws_region={aws_region}, execution_role={config['aws']['sagemaker_execution_role']}\")\n",
    "logger.info(f\"config={json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_training(experiment_config: Dict, aws_region: str, role_arn: str) -> Optional[Dict]:\n",
    "    \"\"\"Function to run training for a model\"\"\"\n",
    "    # Log the training details\n",
    "    logger.info(f\"going to train {experiment_config}, in {aws_region} with {role_arn}\")\n",
    "    training_result: Optional[Dict] = None\n",
    "\n",
    "    # Check if training is enabled in the config; skip if not\n",
    "    run_training = experiment_config.get('run_training', False)\n",
    "    if run_training is False:\n",
    "        logger.info(f\"skipping training of {experiment_config['model_id']} because run_training={run_training}\")\n",
    "        training_result = dict(\n",
    "            model_id=experiment_config['model_id'],\n",
    "            experiment_name=experiment_config['name'],\n",
    "            instance_type=experiment_config.get('instance_type'),\n",
    "            instance_count=experiment_config.get('instance_count'),\n",
    "            trained=False\n",
    "        )\n",
    "        return training_result\n",
    "\n",
    "    # Get the scripts directory\n",
    "    scripts_dir = Path(pkg_resources.files('fmbench'), 'scripts')\n",
    "    logger.info(f\"Using fmbench.scripts directory: {scripts_dir}\")\n",
    "\n",
    "    try:\n",
    "        # Import and run the training script\n",
    "        module_name = Path(experiment_config['training_script']).stem\n",
    "        logger.info(f\"script provided for training this model is --> {module_name}\")\n",
    "        training_script_path = scripts_dir / f\"{module_name}.py\"\n",
    "        logger.info(f\"script path is --> {training_script_path}\")\n",
    "\n",
    "        if not training_script_path.exists():\n",
    "            logger.error(f\"Training script {training_script_path} not found.\")\n",
    "            return None\n",
    "\n",
    "        logger.info(f\"Training using local code: {training_script_path}\")\n",
    "\n",
    "        # Import the training module\n",
    "        spec = importlib.util.spec_from_file_location(module_name, str(training_script_path))\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules[module_name] = module\n",
    "        spec.loader.exec_module(module)\n",
    "\n",
    "        # Run training and measure time\n",
    "        st = time.perf_counter()\n",
    "        training_result = module.train(experiment_config, role_arn)\n",
    "        elapsed_time = time.perf_counter() - st\n",
    "        \n",
    "        logger.info(f\"time taken to train model_id={experiment_config['model_id']} via \"\n",
    "                    f\"{training_script_path} is {elapsed_time:0.2f}\")\n",
    "        \n",
    "        # Add training time to result\n",
    "        if training_result:\n",
    "            training_result['training_time'] = elapsed_time\n",
    "            training_result['trained'] = True\n",
    "            \n",
    "        return training_result\n",
    "\n",
    "    except Exception as error:\n",
    "        logger.error(f\"An error occurred during training: {error}\")\n",
    "        return training_result\n",
    "\n",
    "async def async_run_training(experiment_config: Dict, role_arn: str, aws_region: str) -> Dict:\n",
    "    \"\"\"Asynchronous wrapper function to allow concurrent training requests\"\"\"\n",
    "    return await asyncio.to_thread(run_training, experiment_config, role_arn, aws_region)\n",
    "\n",
    "async def async_train_all_models(config: Dict) -> List[Dict]:\n",
    "    \"\"\"Final asynchronous function to train all models concurrently\"\"\"\n",
    "    # Extract experiments from the config\n",
    "    experiments: List[Dict] = config['experiments']\n",
    "    n: int = 4  # max concurrency to avoid throttling\n",
    "    \n",
    "    # Check for non-reentrant training scripts\n",
    "    non_reentrant_training_scripts = config.get('non_reentrant_training_scripts', [])\n",
    "    non_reentrant_scripts_present = [e['training_script'] for e in experiments \n",
    "                                   if e['training_script'] in non_reentrant_training_scripts]\n",
    "    \n",
    "    if len(non_reentrant_scripts_present) > 1:\n",
    "        logger.info(f\"non_reentrant_training_scripts_present={len(non_reentrant_scripts_present)}, \"\n",
    "                    f\"going to train models serially\")\n",
    "        n = 1\n",
    "\n",
    "    # Split experiments into smaller batches for concurrent training\n",
    "    experiments_splitted = [experiments[i * n:(i + 1) * n] for i in range((len(experiments) + n - 1) // n)]\n",
    "    results = []\n",
    "    \n",
    "    for exp_list in experiments_splitted:\n",
    "        # Run training in batches\n",
    "        result = await asyncio.gather(*[\n",
    "            async_run_training(\n",
    "                m,\n",
    "                config['aws']['region'],\n",
    "                config['aws']['sagemaker_execution_role']\n",
    "            ) for m in exp_list\n",
    "        ])\n",
    "        # Collect results from each batch\n",
    "        results.extend(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Start timer\n",
    "s = time.perf_counter()\n",
    "\n",
    "# Run all training jobs\n",
    "training_results = await async_train_all_models(config)\n",
    "\n",
    "# Calculate total time\n",
    "elapsed_async = time.perf_counter() - s\n",
    "logger.info(f\"Training results: {training_results}\")\n",
    "logger.info(f\"All training completed in {elapsed_async:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
