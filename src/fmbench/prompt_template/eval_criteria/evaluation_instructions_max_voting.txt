1. Evaluate whether the candidate model output answers the question correctly compared to the ground truth provided.

2. Your response should be a JSON containing two elements: "verdict" and "explanation". The "verdict" field should mention whether the candidate model output is "correct" or "incorrect". The "explanation" field should provide the reason for the verdict based on the evaluation of the candidate model output against the ground truth.

3. First, check if the candidate model response correctly answers the question like the ground truth. If so, then
the verdict correct. If not, then the verdict is incorrect. Don't check if the answer is correct based on the context. Always
check if it is correct based on the ground truth. If not, then the verdict is incorrect.

4. Use the ground truth as the ultimate source of truth. If the candidate model response says it cannot find the answer in the context, 
or if the answer is not in the context, then that is incorrect. Do not evaluate that in your verdict or explanation as being correct. Your role is to check if the 
model response is correct based on only the ground truth, not the context.

5. If the ground truth is a comma-separated list with multiple options for responses, the verdict is correct if the candidate model output aligns with any of the options in the comma separated list. It
does not have to match all, but if it aligns with one of the options, then the verdict is correct. If the ground truth contains
a single option for a response, then only compare the candidate model response with that to check if the candidate model response
is correct or not.

6. The candidate model output does not have to use the exact wording of the ground truth but should correctly answer the question in a semantically equivalent manner. If there is more content in the candidate model response than required but it answers the question correctly and aligns with the ground truth, the verdict is "correct". Otherwise, it is "incorrect".

7. Absence of Information: If the candidate model output says that the context does not contain the answer and but the ground truth contains the answer, the verdict is "incorrect".

8. Incorrect Answers: If the candidate model output answers the question incorrectly or says that it cannot answer or does not know the answer, the verdict is "incorrect".

9. Correct Semantic Matching: If the candidate model output matches the ground truth, not necessarily in wordings, but in providing the correct answer, the verdict is "correct".

10. Proper JSON Format: Ensure your response is in a proper JSON format containing the "verdict" and "explanation" fields without any pre-filler words.

11. Last Question Consideration: If there are multiple questions within the context, refer to the last question for evaluation.

12. Synonym and Paraphrase Matching: If the candidate model output uses synonyms or paraphrases different from the ground truth but still correctly answers the question in the same way as the ground truth, the verdict should be "correct".

13. Additional Context: If the candidate model output includes unnecessary context or information not given in the ground truth but successfully concludes the correct answer as the ground truth, the verdict is "correct".

14. If the ground truth does not contain the entire answer to the question, and the candidate model response correctly has the complete answer to the question, then the 
verdict is correct. If the candidate model response says that it cannot find the answer or does not give any answer to the question, whereas the ground truth does, then 
the verdict is incorrect.

15. You do not have to check if the model response is correct based on the context, but only based on the
ground truth. The ground truth is the source of truth for whether the candidate model response is correct
or incorrect.

16. Do not complicate the evaluation. Take a deep breath, and following all instructions, determine if the candidate
model output is correct based on the ground truth provided. Do not mix up the ground truth and candidate model output.
The candidate model output needs to be evaluated and not the ground truth.