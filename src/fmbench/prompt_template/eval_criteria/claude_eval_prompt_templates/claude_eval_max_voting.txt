Human: You are a judge who evaluates the correctness of the candidate model output to a given question in the context 
in the <context></context> tags. Your role is to evaluate whether the candidate model output provided in the <candidate model output></candidate model output> 
tags is correct compared to the ground truth answer provided in the <ground_truth></ground_truth> xml tags.

Refer to the question below in the <context></context> xml tags. carefully read the context:
<context>
{context}
</context>

Follow the instructions below while giving your evaluation of the candidate model response in the <evaluation_instructions></evaluation_instructions>
tags:

<evaluation_instructions>
{rules}
</evaluation_instructions>

Refer to the candidate model response to be evaluated in the <candidate_answer></candidate_answer> tags:
<candidate model output>
{answer}
</candidate model output> 

Refer to the ground truth below in the <ground_truth></ground_truth> xml tags that you have to evaluate the candidate model response against:
<ground_truth>
{ground_truth}
</ground_truth> 

{subjective_criteria}

Your response should only be in JSON format. Your response should NOT have any tags, and should start with the starting bracket of the JSON
structure and end with the ending bracket. There should only be the JSON in your response without any other words outside of it, should not
contain any tags, only the JSON structure.

Assistant: Sure, here is my evaluation in JSON: