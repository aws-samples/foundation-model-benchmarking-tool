{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on all deployed endpoints: Various combinations of payloads, concurrency levels, model configurations\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of our solution design includes running inferences on all deployed model endpoints (with different configurations, concurrency levels and payload sizes). This notebook runs inferences in a manner that is calls endpoints concurrently and asychronously to generate responses and record metrics. Here are some of the key components:\n",
    "\n",
    "- **Accessing the deployed endpoints**, creating a predictor object for these endpoints to call them during inference time.\n",
    "\n",
    "- **Functions to define metrics**: This notebook sets stage for metrics to be recorded during the time of invocation of all these models for benchmarking purposes.\n",
    "\n",
    "- **Running Actual Inferences**: Once the metrics are defined, we set a blocker function that is responsible for creating inference on a single payload called get_inference. We then run a series of asynchronous functions that can be viewed in the code (link above), to create asychronous inferefences on the deployed models. The way we send requests are by creating combinations: this means creating combinations of payloads of different sizes that can be viewed in the config.yml file, with different concurrency levels (in this case we first go through all patches of payloads with a concurrency level of 1, then 2, and then 4). You can set this to your desired value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import uuid\n",
    "import copy\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import botocore\n",
    "import itertools\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import * \n",
    "from fmbench import defaults\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker.predictor import Predictor\n",
    "import importlib.resources as pkg_resources\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from fmbench.scripts.bedrock_predictor import BedrockPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Config.yml file that contains information that is used across this benchmarking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pricing config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "pricing_file_path: str = config['pricing'] \n",
    "\n",
    "# initialize the pricing config file to None\n",
    "pricing_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files('fmbench'), 'configs')\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "pricing_module = Path(config['pricing'])\n",
    "logger.info(f\"pricing config provided for inference from this model is --> {pricing_module}\")\n",
    "pricing_file_path = os.path.join(config_dir, pricing_module)\n",
    "logger.info(f\"pricing config file path is --> {pricing_file_path}\")\n",
    "\n",
    "pricing_config = load_config(pricing_file_path)\n",
    "logger.info(f\"pricing config file recorded: {json.dumps(pricing_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## getting access to the s3 bucket where endpoints.json for different models resides\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the deployed model endpoints from the endpoints.json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Refer to the file path for the endpoint\n",
    "# getting the endpoint as an s3 object from the deployed path\n",
    "try:\n",
    "    endpoint_info_list = json.loads(get_s3_object(config['aws']['bucket'], ENDPOINT_LIST_PATH))\n",
    "    logger.info(f\"found information for {len(endpoint_info_list)} endpoints in bucket={config['aws']['bucket']}, key={ENDPOINT_LIST_PATH}\")\n",
    "    logger.info(json.dumps(endpoint_info_list, indent=2))\n",
    "# if there is no endpoint information, assume that the user is using a bedrock model and send in the endpoint info list as an empty list\n",
    "except FileNotFoundError:\n",
    "    logger.warning(f\"Key {ENDPOINT_LIST_PATH} not found in bucket {config['aws']['bucket']}. Using an empty list for endpoints for bedrock models/bring your own externalized endpoints.\")\n",
    "    endpoint_info_list = []\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "        logger.warning(f\"Key {ENDPOINT_LIST_PATH} not found in bucket {config['aws']['bucket']}. Using an empty list for endpoints for bedrock models/bring your own externalized endpoints.\")\n",
    "        endpoint_info_list = []\n",
    "    # raise an error if the model is not deployed as an endpoint via sagemaker/eks or bedrock\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List down the endpoint names that have been deployed\n",
    "endpoint_name_list = [e['endpoint']['EndpointName'] for e in endpoint_info_list]\n",
    "\n",
    "# endpoint information \n",
    "logger.info(f\"there are {len(endpoint_name_list)} deployed endpoint(s), endpoint_name_list->{endpoint_name_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions to define and calculate metrics during the time of invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_sum(vals: List) -> Union[int, float]:\n",
    "    return sum(filter(None, vals))\n",
    "\n",
    "\n",
    "def safe_div(n: Union[int, float], d: Union[int, float]) -> Optional[Union[int, float]]:\n",
    "    return n/d if d else None\n",
    "\n",
    "\n",
    "def stat_summaries(responses: List[Dict],\n",
    "                   metric_name: str,\n",
    "                   successes: int) -> Tuple[float]:\n",
    "    metric_vals = [r[metric_name] for r in responses]\n",
    "    metric_vals_not_none = list(filter(None, metric_vals))\n",
    "    if metric_vals_not_none != []:\n",
    "        metric_mean = safe_div(safe_sum(metric_vals_not_none), successes)\n",
    "        metric_p50, metric_p95, metric_p99 = np.percentile(metric_vals_not_none,\n",
    "                                                              [50, 95, 99])\n",
    "    else:\n",
    "        metric_p50, metric_p95, metric_p99, metric_mean = None, None, None, None\n",
    "    return metric_p50, metric_p95, metric_p99, metric_mean\n",
    "\n",
    "\n",
    "# Represents the function to calculate all of the metrics at the time of inference\n",
    "def calculate_metrics(responses,\n",
    "                      chunk,\n",
    "                      elapsed_async,\n",
    "                      experiment_name,\n",
    "                      concurrency, payload_file) -> Dict:\n",
    "\n",
    "    # calculate errors based on the completion status of the inference prompt\n",
    "    errors = [r for r in responses if r['completion'] is None or r['completion_tokens'] is None]\n",
    "\n",
    "    # Calculate the difference as the successes \n",
    "    successes = len(chunk) - len(errors)\n",
    "\n",
    "    # Count all of the prompts token count during inference\n",
    "    all_prompts_token_count = safe_sum([r['prompt_tokens'] for r in responses])\n",
    "    prompt_token_throughput = round(all_prompts_token_count / elapsed_async, 2)\n",
    "    prompt_token_count_mean = safe_div(all_prompts_token_count, successes)\n",
    "    all_completions_token_count = safe_sum([r['completion_tokens'] for r in responses])\n",
    "    completion_token_throughput = round(all_completions_token_count / elapsed_async, 2)\n",
    "    completion_token_count_mean = safe_div(all_completions_token_count, successes)\n",
    "    transactions_per_second = round(successes / elapsed_async, 2)\n",
    "    transactions_per_minute = int(transactions_per_second * 60)\n",
    "\n",
    "    # calculate the latency, TTFT, TTLT and TPOT mean utilizing the safe_sum function defined above\n",
    "    latency_p50, latency_p95, latency_p99, latency_mean = stat_summaries(responses,\n",
    "                                                                         'latency',\n",
    "                                                                         successes)\n",
    "\n",
    "    ttft_p50, ttft_p95, ttft_p99, ttft_mean = stat_summaries(responses,\n",
    "                                                             'time_to_first_token',\n",
    "                                                             successes)\n",
    "\n",
    "    tpot_p50, tpot_p95, tpot_p99, tpot_mean = stat_summaries(responses,\n",
    "                                                             'time_per_output_token',\n",
    "                                                             successes)\n",
    "    \n",
    "    ttlt_p50, ttlt_p95, ttlt_p99, ttlt_mean = stat_summaries(responses,\n",
    "                                                             'time_to_last_token',\n",
    "                                                             successes)\n",
    "\n",
    "    # Function returns all these values at the time of the invocations\n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'concurrency': concurrency,\n",
    "        'payload_file': payload_file,\n",
    "        'errors': errors,\n",
    "        'successes': successes,\n",
    "        'error_rate': len(errors)/len(chunk),\n",
    "        'all_prompts_token_count': all_prompts_token_count,\n",
    "        'prompt_token_count_mean': prompt_token_count_mean,\n",
    "        'prompt_token_throughput': prompt_token_throughput,\n",
    "        'all_completions_token_count': all_completions_token_count,\n",
    "        'completion_token_count_mean': completion_token_count_mean,\n",
    "        'completion_token_throughput': completion_token_throughput,\n",
    "        'transactions': len(chunk),\n",
    "        'transactions_per_second': transactions_per_second,\n",
    "        'transactions_per_minute': transactions_per_minute,\n",
    "        #'latency_mean': latency_mean,\n",
    "        'latency_p50': latency_p50,\n",
    "        'latency_p95': latency_p95,\n",
    "        'latency_p99': latency_p99,\n",
    "        #'TTFT_mean': ttft_mean,\n",
    "        'TTFT_p50': ttft_p50,\n",
    "        #'TTFT_p95': ttft_p95,\n",
    "        'TTFT_p99': ttft_p99,\n",
    "        #'TPOT_mean': tpot_mean,\n",
    "        'TPOT_p50': tpot_p50,\n",
    "        #'TPOT_p95': tpot_p95,\n",
    "        'TPOT_p99': tpot_p99,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a blocking function and a series of asynchronous concurrent model prompt invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_metrics(endpoint_name=None,\n",
    "                prompt=None,\n",
    "                ground_truth=None,\n",
    "                question=None,\n",
    "                payload_file=None,\n",
    "                inference_params=None,\n",
    "                completion=None,\n",
    "                prompt_tokens=None,\n",
    "                completion_tokens=None,\n",
    "                latency=None,\n",
    "                time_to_first_token=None,\n",
    "                time_per_output_token=None,\n",
    "                time_to_last_token=None,\n",
    "                uuid=None) -> Dict:\n",
    "    return dict(endpoint_name=endpoint_name,\n",
    "                prompt=prompt,\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                payload_file=payload_file,\n",
    "                **inference_params,\n",
    "                completion=completion,\n",
    "                prompt_tokens=prompt_tokens,\n",
    "                completion_tokens=completion_tokens,\n",
    "                latency=latency,\n",
    "                time_to_first_token=time_to_first_token,\n",
    "                time_per_output_token=time_per_output_token,\n",
    "                time_to_last_token=time_to_last_token,\n",
    "                uuid=uuid)\n",
    "\n",
    "\n",
    "# function to get inference\n",
    "def get_inference(predictor, payload, payload_file) -> Dict:\n",
    "    try:\n",
    "        # get inference\n",
    "        request_uuid = uuid.uuid4().hex\n",
    "        logger.info(f\"get_inference, sending request with uuid={request_uuid}\")\n",
    "        resp = predictor.get_prediction(payload)\n",
    "        logger.info(f\"get_inference, response for uuid={request_uuid}, resp={resp}\")\n",
    "        # handle the case when ground truth is either provided or not provided as a \n",
    "        # part of the dataset\n",
    "        ground_truth = payload.get('ground_truth', None)\n",
    "        question = payload.get('question', None)\n",
    "        if ground_truth is not None:\n",
    "            if isinstance(ground_truth, list):\n",
    "                ground_truth = ','.join(ground_truth)\n",
    "        else:\n",
    "            ground_truth = None\n",
    "        # Set metrics and logging for both cases\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               # the open source long bench dataset has ground truth responses in a list. In this case, \n",
    "                               # use all of the elements in the list as a source of ground truth and check for whether the \n",
    "                               # answer matches any. If the ground truth is not a list, then assuming it being a string, we\n",
    "                               # use the string as the ground truth\n",
    "                               ground_truth,\n",
    "                               question,\n",
    "                               payload_file,\n",
    "                               predictor.inference_parameters,\n",
    "                               resp['response_json'].get(\"generated_text\", \"\"),\n",
    "                               resp['prompt_tokens'],\n",
    "                               resp['completion_tokens'],\n",
    "                               resp['latency'],\n",
    "                               resp['time_to_first_token'],\n",
    "                               resp['time_per_output_token'],\n",
    "                               resp['time_to_last_token'],\n",
    "                               request_uuid)\n",
    "\n",
    "        # log the output of the prediction\n",
    "        logger.info(f\"get_inference, done, uuid={request_uuid}, endpoint={predictor.endpoint_name}, \"\n",
    "                    f\"prompt_tokens={resp['prompt_tokens']}, completion_tokens={resp['completion_tokens']}, \"\n",
    "                    f\"latency={resp['latency']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"get_inference, uuid={request_uuid}, error occurred with {predictor.endpoint_name}, exception={str(e)}\")\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               predictor.inference_parameters,\n",
    "                               None,\n",
    "                               None,\n",
    "                               None,\n",
    "                               None,\n",
    "                               None,\n",
    "                               None,\n",
    "                               None,\n",
    "                               None,\n",
    "                               None,\n",
    "                               None,\n",
    "                               None)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a series of asynchronous functions to invoke and run inferences concurrently and asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Represents a function to start invoking models in separate thread asynchronously\n",
    "# for the blocker function\n",
    "async def async_get_inference(predictor, payload: Dict, payload_file: str) -> Dict:\n",
    "    return await asyncio.to_thread(get_inference, predictor, payload, payload_file)\n",
    "\n",
    "\n",
    "# Gathers all of the tasks and sets of the concurrent calling of the asychronous \n",
    "# invocations\n",
    "async def async_get_all_inferences(predictor, payload_list: List, payload_file: str) -> List:\n",
    "    logger.info(f\"async_get_all_inferences, length of payload_list={len(payload_list)}\")\n",
    "    return await asyncio.gather(*[async_get_inference(predictor, payload, payload_file) \\\n",
    "                                  for payload in payload_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "    \n",
    "# This function runs the asynchronous function series above together\n",
    "# for different experiments and concurrency levels.\n",
    "async def run_inferences(predictor: sagemaker.base_predictor.Predictor, \n",
    "                         chunk: List,\n",
    "                         experiment: Dict,\n",
    "                         concurrency: int,\n",
    "                         payload_file: str) -> Tuple[List, Dict]:\n",
    "    logger.info(f\"processing chunk with concurrency={concurrency}\")\n",
    "    s = time.perf_counter()\n",
    "    responses = await async_get_all_inferences(predictor, chunk, payload_file)\n",
    "    elapsed_async = time.perf_counter() - s\n",
    "\n",
    "    # Add more metadata about this experiment\n",
    "    for r in responses:\n",
    "        r['experiment_name'] = experiment['name']\n",
    "        r['concurrency'] = concurrency\n",
    "\n",
    "    metrics = calculate_metrics(responses, chunk, elapsed_async, experiment['name'], concurrency, payload_file)\n",
    "    return responses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fmbench.scripts.fmbench_predictor import FMBenchPredictor\n",
    "\n",
    "\n",
    "# Function to create the predictors from the experiment we are iterating over\n",
    "def create_predictor_for_experiment(experiment: Dict,\n",
    "                                    config: Dict,\n",
    "                                    endpoint_info_list: List) -> Optional[FMBenchPredictor]:\n",
    "    # initialize inference spec to none\n",
    "    inference_spec = None\n",
    "    ep_info = [e for e in endpoint_info_list if e['experiment_name'] == experiment['name']]\n",
    "    logger.info(f\"endpoint info found is: {ep_info}\")\n",
    "\n",
    "    # if the endpoint info list is not empty, the deployed model \n",
    "    # is a sagemaker endpoint and contains the endpoint config as a dict\n",
    "    if ep_info != []:\n",
    "        # get the endpoint name from the dict created\n",
    "        ep_name = ep_info[0]['endpoint']['EndpointName']\n",
    "\n",
    "        # get the inference spec based on the model type\n",
    "        inference_spec = experiment.get(\"inference_spec\")\n",
    "        logger.info(f\"experiment name={experiment['name']}, ep_name={ep_name}, \"\n",
    "                    f\"inference_spec={inference_spec}\")\n",
    "    # case to handle if the model is not a sagemaker endpoint and coming from\n",
    "    # the bedrock predictor file\n",
    "    else:\n",
    "        ep_name = experiment['ep_name']\n",
    "        # re initializing the inference spec in the case of bring your own endpoint/rest predictor for access to inference parameters\n",
    "        inference_spec = experiment.get(\"inference_spec\") \n",
    "        logger.info(f\"seems like an external endpoint, no endpoint info found, going to use \"\n",
    "                    f\"ep_name={ep_name}\")\n",
    "\n",
    "    # create predictor objects\n",
    "    # Proceed with deployment as before\n",
    "    # Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "    scripts_dir = Path(pkg_resources.files('fmbench'), 'scripts')\n",
    "    logger.info(f\"Using fmbench.scripts directory: {scripts_dir}\")\n",
    "\n",
    "    # Ensure the scripts directory exists\n",
    "    scripts_dir.mkdir(parents=True, exist_ok=True)\n",
    "    module_name = Path(experiment['inference_script']).stem\n",
    "    logger.info(f\"script provided for inference from this model={module_name}\")\n",
    "    script_path = scripts_dir / f\"{module_name}.py\"\n",
    "    logger.info(f\"script path={script_path}\")\n",
    "\n",
    "    # Check and proceed with local script\n",
    "    if not script_path.exists():\n",
    "        logger.error(f\"script {script_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"deploying using local code: {script_path}\")\n",
    "\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(script_path))\n",
    "    inference_module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = inference_module\n",
    "    spec.loader.exec_module(inference_module)\n",
    "    # create a predictor from each endpoint in experiments\n",
    "    metadata: Optional[Dict] = None\n",
    "    if ep_info != []:\n",
    "        if ep_info[0].get('endpoint'):\n",
    "            production_variants = ep_info[0].get('endpoint').get(\"ProductionVariants\")\n",
    "            if production_variants is not None:\n",
    "                variant_name = production_variants[0].get(\"VariantName\")\n",
    "                metadata = dict(variant_name=variant_name)\n",
    "                logger.info(f\"ep_name={ep_name}, variant_name={variant_name}\")\n",
    "        use_messages_api_format = experiment.get('use_messages_api_format')\n",
    "        if use_messages_api_format:\n",
    "            if metadata is None:\n",
    "                metadata = dict(use_messages_api_format=use_messages_api_format)\n",
    "            else:\n",
    "                metadata['use_messages_api_format'] = use_messages_api_format\n",
    "    logger.info(f\"ep_name={ep_name}, metadata={metadata}\")\n",
    "    return inference_module.create_predictor(ep_name, inference_spec, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here, we will process combinations of concurrency levels, the payload files\n",
    "# and then loop through the different combinations to make payloads splitted \n",
    "# in terms of the concurrency metric and how we can run it and make inference\n",
    "\n",
    "def create_payload_dict(jline: str, experiment: Dict) -> Dict:\n",
    "    payload: Dict = json.loads(jline)\n",
    "    return payload\n",
    "\n",
    "\n",
    "def create_combinations(experiment: Dict) -> List[Tuple]:\n",
    "    combinations_data = []\n",
    "\n",
    "    # Repeat for each concurrency level\n",
    "    combinations = list(itertools.product(experiment['concurrency_levels'],\n",
    "                                          experiment['payload_files']))\n",
    "    logger.info(f\"there are {len(combinations)} combinations of {combinations} to run\")\n",
    "\n",
    "    MAX_REQ_CNT = config['datasets'].get('max_iters_per_combination', defaults.MAX_REQ_CNT)\n",
    "    MIN_REQ_CNT = config['datasets'].get('min_iters_per_combination', defaults.MIN_REQ_CNT)\n",
    "    logger.info(f\"setting the max iterations per combination to: {MAX_REQ_CNT}, min iterations per combination to: {MIN_REQ_CNT}\")\n",
    "\n",
    "    for concurrency, payload_file in combinations:\n",
    "        # Construct the full S3 file path\n",
    "        s3_file_path = f\"{PROMPTS_DIR}/{config['s3_read_data']['source_data_prefix']}/{payload_file}\"\n",
    "        logger.info(f\"s3 path where the payload files are being read from -> {s3_file_path}\")\n",
    "\n",
    "        # Read the payload file from S3\n",
    "        try:\n",
    "            # response = s3_client.get_object(Bucket=config['aws']['bucket'], Key=s3_file_path)\n",
    "            # payload_file_content = response['Body'].read().decode('utf-8')\n",
    "            payload_file_content = get_s3_object(bucket=config['aws']['bucket'], key=s3_file_path)\n",
    "\n",
    "            # Create a payload list by processing each line\n",
    "            payload_list = [create_payload_dict(jline, experiment)\n",
    "                            for jline in payload_file_content.splitlines()]\n",
    "\n",
    "            fp: str = f\"s3://{config['aws']['bucket']}/{s3_file_path}\"\n",
    "            logger.info(f\"read from {fp}, contains {len(payload_list)} lines\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file from S3: {e}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"creating combinations for concurrency={concurrency}, \\\n",
    "                      payload_file={payload_file}, payload_list length={len(payload_list)}\")\n",
    "\n",
    "        n = concurrency\n",
    "\n",
    "        pl_list_len = len(payload_list)\n",
    "        if pl_list_len < n:\n",
    "            nr_elements_to_add = n - pl_list_len\n",
    "\n",
    "            for i in range(nr_elements_to_add):\n",
    "                payload_list.append(payload_list[i % pl_list_len])\n",
    "\n",
    "        # Split the original list into sublists which contain the number of\n",
    "        # requests we want to send concurrently\n",
    "        payload_list_splitted = [payload_list[i * n:(i + 1) * n]\n",
    "                                 for i in range((len(payload_list) + n - 1) // n)]\n",
    "\n",
    "        for p in payload_list_splitted:\n",
    "            p_ori_len = len(p)\n",
    "            if p_ori_len < n:\n",
    "                elements_to_add = n - p_ori_len\n",
    "                for i in range(elements_to_add):\n",
    "                    p.append(p[i % p_ori_len])\n",
    "\n",
    "        # Only keep lists that have at least concurrency number of elements\n",
    "        len_before = len(payload_list_splitted)\n",
    "        payload_list_splitted = [p for p in payload_list_splitted if len(p) == concurrency]\n",
    "\n",
    "        payload_list_splitted_len = len(payload_list_splitted)\n",
    "\n",
    "        if payload_list_splitted_len > MAX_REQ_CNT:\n",
    "            payload_list_splitted = payload_list_splitted[0:MAX_REQ_CNT]\n",
    "\n",
    "        if payload_list_splitted_len < MIN_REQ_CNT:\n",
    "            nr_list_element_to_add = MIN_REQ_CNT - payload_list_splitted_len\n",
    "\n",
    "            for i in range(nr_list_element_to_add):\n",
    "                payload_list_splitted.append(payload_list_splitted[i % payload_list_splitted_len])\n",
    "\n",
    "        logger.info(f\"after only retaining chunks of length {concurrency}, \"\n",
    "                    f\"we have {len(payload_list_splitted)} chunks, previously we had {len_before} chunks\")\n",
    "        combinations_data.append((concurrency, payload_file, payload_list_splitted))\n",
    "    logger.info(f\"there are {len(combinations)} for {experiment}\")\n",
    "    return combinations_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for each experiment\n",
    "#   - for each endpoint and concurrency in an experiment\n",
    "\n",
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "\n",
    "_ = list(map(clear_dir, [METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]))\n",
    "\n",
    "# Initializing the experiment run cost to 0\n",
    "exp_cost: Optional[float] = None\n",
    "\n",
    "# To keep track of the experiment durations and the time it takes for \n",
    "# the model endpoint to be in service to calculate cost association\n",
    "experiment_durations: List[float] = []\n",
    "\n",
    "# start the timer before the start of inferences\n",
    "current_time = datetime.now(timezone.utc)\n",
    "logger.info(f\"current time recorded while running this experiment is {current_time} \")\n",
    "\n",
    "num_experiments: int = len(config['experiments'])\n",
    "\n",
    "# dataframe list to hold metrics for each endpoint\n",
    "df_ep_metrics_list = []\n",
    "# list for holding predictors and run start and end timestamp\n",
    "# because cloud watch metrics are available after a 1-minute delay\n",
    "predictors_and_metrics_timestamp_list = []\n",
    "all_responses_list: List[Dict] = []\n",
    "for e_idx, experiment in enumerate(config['experiments']):\n",
    "    # Start timer for the experiment   \n",
    "    experiment_start_time = time.perf_counter()\n",
    "    predictor = create_predictor_for_experiment(experiment, config, endpoint_info_list)\n",
    "\n",
    "    if predictor is None:\n",
    "        logger.error(f\"predictor could not be created for experiment={experiment}, moving to next...\")\n",
    "        continue\n",
    "\n",
    "    combination_data = create_combinations(experiment)\n",
    "\n",
    "    prompt_tokens_total: int = 0\n",
    "    completion_tokens_total: int = 0\n",
    "    for concurrency, payload_file, split_payload in combination_data:\n",
    "        # track time at minute boundaries\n",
    "        experiment_at_concurrency_start_dttm = datetime.utcnow().replace(second=0, microsecond=0)\n",
    "        for chunk_index, chunk in enumerate(split_payload):\n",
    "            logger.info(f\"experiment_index={e_idx+1}/{num_experiments}, \"\n",
    "                        f\"concurrency={concurrency}, payload_file={payload_file}, \"\n",
    "                        f\"chunk_index={chunk_index+1}/{len(split_payload)}\")\n",
    "\n",
    "            # set concurrency level in Python asyncio so that the number of threads\n",
    "            # is set to same as concurrency level otherwise number of threads defaults\n",
    "            # to number of processors*5 (see\n",
    "            # https://stackoverflow.com/questions/75885213/how-to-increase-asyncio-thread-limits-in-an-existing-co-routine)\n",
    "            loop = asyncio.get_running_loop()\n",
    "            loop.set_default_executor(ThreadPoolExecutor(max_workers=concurrency))\n",
    "            responses, metrics = await run_inferences(predictor,\n",
    "                                                      chunk,\n",
    "                                                      experiment,\n",
    "                                                      concurrency, payload_file)\n",
    "            if metrics:\n",
    "                logger.info(f\"metrics={json.dumps(metrics, indent=2, default=str)}\")\n",
    "                prompt_tokens_total += metrics.get('all_prompts_token_count', 0)\n",
    "                completion_tokens_total += metrics.get('all_completions_token_count', 0)\n",
    "                metrics_json = json.dumps(metrics, indent=2)\n",
    "                metrics_file_name = f\"{time.time()}.json\"\n",
    "                metrics_s3_path = os.path.join(METRICS_PER_CHUNK_DIR,\n",
    "                                               metrics_file_name)\n",
    "                write_to_s3(metrics_json,\n",
    "                            config['aws']['bucket'],\n",
    "                            \"\",\n",
    "                            METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "\n",
    "            if responses:\n",
    "                save_s3_list = []\n",
    "                all_responses_list.extend(responses)\n",
    "\n",
    "                for r in responses:\n",
    "                    response_json = json.dumps(r, indent=2)\n",
    "                    response_file_name = f\"{time.time()}.json\"\n",
    "                    response_s3_path = os.path.join(METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "                    save_s3_list.append((response_json,\n",
    "                                config['aws']['bucket'],\n",
    "                                \"\",\n",
    "                                METRICS_PER_INFERENCE_DIR,\n",
    "                                response_file_name)\n",
    "                                )\n",
    "                write_multiple_to_s3(save_s3_list)\n",
    "\n",
    "        # save endpoint metrics\n",
    "        experiment_at_concurrency_end_dttm = datetime.utcnow().replace(second=0, microsecond=0)\n",
    "        # if the endtime and start time are in the same minute then move the endtime to the next\n",
    "        # minute otherwise cloudwatch would return an empty resonse\n",
    "        time_delta_in_seconds = (experiment_at_concurrency_end_dttm - experiment_at_concurrency_start_dttm).seconds\n",
    "        if time_delta_in_seconds < 60:\n",
    "            experiment_at_concurrency_end_dttm += timedelta(seconds=60)\n",
    "\n",
    "        predictors_and_metrics_timestamp_list.append((predictor,\n",
    "                                                      experiment_at_concurrency_start_dttm,\n",
    "                                                      experiment_at_concurrency_end_dttm,\n",
    "                                                      concurrency,\n",
    "                                                      experiment['instance_type']))\n",
    "\n",
    "    # Experiment done, stopping the timer for this given experiment\n",
    "    experiment_end_time = time.perf_counter()\n",
    "\n",
    "    # calculating the duration of this given endpoint inference time\n",
    "    experiment_duration = experiment_end_time - experiment_start_time\n",
    "    logger.info(f\"the {experiment['name']} ran for {experiment_duration} seconds\")\n",
    "\n",
    "    # calculating the per second cost for this instance type\n",
    "    exp_instance_type: str = experiment['instance_type']\n",
    "\n",
    "    # cost for this given exp\n",
    "    logger.info(f\"metrics json is: {metrics}\")\n",
    "\n",
    "    # calculate the cost of run for both sagemaker and external models\n",
    "    # use the pricing config file here to get the pricing\n",
    "    exp_cost = predictor.calculate_cost(exp_instance_type,\n",
    "                                        experiment.get('instance_count'),\n",
    "                                        pricing_config,\n",
    "                                        experiment_duration,\n",
    "                                        prompt_tokens_total,\n",
    "                                        completion_tokens_total)\n",
    "    logger.info(f\"the cost for running {experiment['name']} running on \"\n",
    "                f\"{exp_instance_type} for {experiment_duration}s is ${exp_cost}\")\n",
    "\n",
    "    experiment_durations.append({\n",
    "        'experiment_name': experiment['name'],\n",
    "        'instance_type': exp_instance_type,\n",
    "        'instance_count': experiment.get('instance_count'),\n",
    "        'duration_in_seconds': f\"{experiment_duration:.2f}\",\n",
    "        'cost': f\"{exp_cost:.6f}\",\n",
    "    })\n",
    "\n",
    "    logger.info(f\"experiment={e_idx+1}/{num_experiments}, name={experiment['name']}, \"\n",
    "                f\"duration={experiment_duration:.6f} seconds, exp_cost={exp_cost:.6f}, done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add a 1-minute sleep to be able to grab the CW metrics from the last run\n",
    "sleep_time: int = 60\n",
    "logger.info(f\"going to sleep for {sleep_time}s before querying metrics from the endpoint\")\n",
    "time.sleep(sleep_time)\n",
    "logger.info(f\"after sleep for {sleep_time}s before querying metrics from the endpoint\")\n",
    "\n",
    "for predictor, \\\n",
    "    experiment_at_concurrency_start_dttm, \\\n",
    "    experiment_at_concurrency_end_dttm, \\\n",
    "    concurrency, \\\n",
    "    instance_type in predictors_and_metrics_timestamp_list:\n",
    "    # save endpoint metrics\n",
    "    df_ep_metrics = predictor.get_metrics(experiment_at_concurrency_start_dttm,\n",
    "                                          experiment_at_concurrency_end_dttm)\n",
    "    if df_ep_metrics is not None:\n",
    "        # we want concurrency after timestamp, endpoint name\n",
    "        df_ep_metrics.insert(loc=2,\n",
    "                             column='instance_type',\n",
    "                             value=instance_type)\n",
    "        df_ep_metrics.insert(loc=3,\n",
    "                             column='concurrency',\n",
    "                             value=concurrency)\n",
    "        df_ep_metrics_list.append(df_ep_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# after all experiments are done, concatenate all the per experiment metrics and save to a single dataframe\n",
    "df_all_ep_metrics: Optional[pd.DataFrame] = None\n",
    "if len(df_ep_metrics_list) > 0:\n",
    "    df_all_ep_metrics = pd.concat(df_ep_metrics_list)\n",
    "    csv_ep_metrics = io.StringIO()\n",
    "    df_all_ep_metrics.to_csv(csv_ep_metrics, index=False)\n",
    "    csv_ep_metrics = csv_ep_metrics.getvalue()\n",
    "    logger.info(f\"shape of all df_all_ep_metrics is {df_all_ep_metrics.shape}\")\n",
    "    logger.info(df_all_ep_metrics.head())\n",
    "    write_to_s3(csv_ep_metrics,\n",
    "                config['aws']['bucket'],\n",
    "                \"\",\n",
    "                METRICS_DIR,\n",
    "                ENDPOINT_METRICS_FNAME)\n",
    "    fpath: str = f\"s3://{config['aws']['bucket']}/{METRICS_DIR}/{ENDPOINT_METRICS_FNAME}\"\n",
    "    logger.info(f\"all endpoint metrics saved to {fpath}\")\n",
    "else:\n",
    "    logger.error(f\"length of df_ep_metrics is {df_ep_metrics}, nothing to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if df_all_ep_metrics is not None:\n",
    "    # we want to delete the first row of each endpoint metric because it may not \n",
    "    # have started at a minute boundary and therefore would be an entry for an\n",
    "    # incomplete minute , unless, it is the only entry for that endpoint in which\n",
    "    # case we have no choice but leave it as is\n",
    "    groups = df_all_ep_metrics.groupby('EndpointName').filter(lambda x: len(x) > 1)\n",
    "\n",
    "    # mark the first row of these groups\n",
    "    groups['is_first'] = groups.groupby('EndpointName').cumcount() == 0\n",
    "\n",
    "    # filter out the first row of these groups\n",
    "    logger.info(f\"shape of df_ep_metrics before removing first row for each endpoint = {df_all_ep_metrics.shape}\") \n",
    "    df_ep_metrics = df_all_ep_metrics[~df_all_ep_metrics.index.isin(groups[groups['is_first']].index)]\n",
    "    logger.info(f\"shape of df_ep_metrics after removing first row for each endpoint = {df_all_ep_metrics.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if df_all_ep_metrics is not None:\n",
    "    # Convert from micro to milli\n",
    "    df_all_ep_metrics['ModelLatency'] = df_all_ep_metrics['ModelLatency'] / 1000\n",
    "\n",
    "    # Define the metric columns\n",
    "    aggregate_metric_cols = {\n",
    "        \"CPUUtilization\": \"mean\",\n",
    "        \"DiskUtilization\": \"mean\",\n",
    "        \"GPUMemoryUtilization\": \"mean\",\n",
    "        \"GPUUtilization\": \"mean\",\n",
    "        \"MemoryUtilization\": \"mean\",\n",
    "        \"ModelLatency\": \"mean\",\n",
    "        \"InvocationsPerInstance\": \"sum\",\n",
    "        \"Invocations\": \"sum\",\n",
    "        \"Invocation5XXErrors\": \"sum\",\n",
    "        \"Invocation4XXErrors\": \"sum\"\n",
    "    }\n",
    "\n",
    "    # Filter out the columns that are not present in the DataFrame\n",
    "    existing_columns = {col: func for col, func in aggregate_metric_cols.items() if col in df_all_ep_metrics.columns}\n",
    "    logger.info(f\"Found the following metric columns: {existing_columns}, going to summarize the results.\")\n",
    "\n",
    "    # Summarize for each endpoint and concurrency level\n",
    "    df_ep_metrics_summarized = df_all_ep_metrics.groupby(by=[\"EndpointName\", \"instance_type\", \"concurrency\"]).agg(existing_columns).reset_index()\n",
    "\n",
    "    logger.info(f\"df_ep_metrics_summarized = {df_ep_metrics_summarized}\")\n",
    "    csv_ep_metrics = io.StringIO()\n",
    "    df_ep_metrics_summarized.to_csv(csv_ep_metrics, index=False)\n",
    "    csv_ep_metrics = csv_ep_metrics.getvalue()\n",
    "    logger.info(f\"shape of all df_ep_metrics_summarized is {df_ep_metrics_summarized.shape}\")\n",
    "    logger.info(df_ep_metrics_summarized.head())\n",
    "    write_to_s3(csv_ep_metrics,\n",
    "                config['aws']['bucket'],\n",
    "                \"\",\n",
    "                METRICS_DIR,\n",
    "                ENDPOINT_METRICS_SUMMARIZED_FNAME)\n",
    "    fpath: str = f\"s3://{config['aws']['bucket']}/{METRICS_DIR}/{ENDPOINT_METRICS_SUMMARIZED_FNAME}\"\n",
    "    logger.info(f\"all endpoint metrics summarized saved to {fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After all experiments are done, summarize and optionally save experiment durations along with costs\n",
    "df_durations = pd.DataFrame(experiment_durations)\n",
    "logger.info(f\"experiment durations: {df_durations}\")\n",
    "\n",
    "# Convert the DataFrame to CSV and write it to S3 or wherever you prefer\n",
    "csv_buffer_cost = io.StringIO()\n",
    "df_durations.to_csv(csv_buffer_cost, index=False)\n",
    "experiment_associated_cost = csv_buffer_cost.getvalue()\n",
    "\n",
    "# Assuming write_to_s3() is already defined and configured correctly\n",
    "write_to_s3(experiment_associated_cost, config['aws']['bucket'],\n",
    "            \"\",\n",
    "            METRICS_DIR,\n",
    "            SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE)\n",
    "fpath: str = f\"s3://{config['aws']['bucket']}/{METRICS_DIR}/{SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE}\"\n",
    "logger.info(f\"summary for cost of instance per endpoint per run saved to {fpath}\")\n",
    "\n",
    "logger.info(f\"total cost of all experiments: ${sum(df_durations.cost.astype(float))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # List .json files in the specified S3 directory\n",
    "# s3_files = list_s3_files(config['aws']['bucket'], METRICS_PER_INFERENCE_DIR)\n",
    "\n",
    "# # Read and parse each JSON file from S3\n",
    "# json_list = list(map(lambda key: json.loads(get_s3_object(config['aws']['bucket'], key)), \\\n",
    "#                      s3_files))\n",
    "\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df_responses = pd.DataFrame(all_responses_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all responses\")\n",
    "df_responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(config['aws']['bucket'], METRICS_PER_CHUNK_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = list(map(lambda key: json.loads(get_s3_object(config['aws']['bucket'], key)), \\\n",
    "                     s3_files))\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_metrics.shape} from all responses\")\n",
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_experiments = pd.json_normalize(config['experiments'])\n",
    "df_experiments = df_experiments.rename(columns={\"name\": \"experiment_name\"})\n",
    "if 'deploy' not in df_experiments.columns:\n",
    "    logger.info(\"deploy not setup for any experiment, setting it to None for all experiments\")\n",
    "    df_experiments['deploy'] = None\n",
    "\n",
    "df_experiments_skip_deploy = df_experiments[df_experiments.deploy != True]\n",
    "logger.info(f\"df_experiments shape={df_experiments.shape}, \"\n",
    "            f\"df_experiments_skip_deploy shape={df_experiments_skip_deploy.shape}\")\n",
    "logger.info(df_experiments_skip_deploy.head())\n",
    "logger.info(f\"df_experiments_skip_deploy={df_experiments_skip_deploy[['experiment_name', 'instance_type']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if the endpoint list contains elements, utilize the sagemaker endpoint configuration properties\n",
    "if endpoint_info_list:\n",
    "    df_endpoints = pd.json_normalize(endpoint_info_list)\n",
    "    if 'endpoint_config.ProductionVariants' in df_endpoints.columns and df_endpoints['endpoint_config.ProductionVariants'].notna().all():\n",
    "        df_endpoints['instance_type'] = df_endpoints['endpoint_config.ProductionVariants'].map(lambda x: x[0]['InstanceType'])\n",
    "        df_endpoints['instance_count'] = df_endpoints['endpoint_config.ProductionVariants'].map(lambda x: x[0]['InitialInstanceCount'])\n",
    "    else:\n",
    "        df_endpoints['instance_type'] = df_endpoints['instance_type']\n",
    "        df_endpoints['instance_count'] = df_endpoints['instance_count']\n",
    "    cols_for_env = [c for c in df_endpoints.columns if 'Environment' in c]\n",
    "    logger.info(f\"cols_for_env={cols_for_env}\")\n",
    "    cols_of_interest = ['experiment_name',\n",
    "                        'instance_type',\n",
    "                        'instance_count',\n",
    "                        'endpoint.EndpointName',\n",
    "                        'model_config.ModelName',\n",
    "                        'model_config.PrimaryContainer.Image',\n",
    "                        'model_config.PrimaryContainer.ModelDataSource.S3DataSource.S3Uri']\n",
    "    cols_of_interest.extend(cols_for_env)\n",
    "\n",
    "    cols_of_interest = [c for c in cols_of_interest if c in df_endpoints.columns]\n",
    "    df_endpoints = df_endpoints[cols_of_interest]\n",
    "    cols_of_interest_renamed = [c.split('.')[-1] for c in cols_of_interest]\n",
    "    df_endpoints.columns = cols_of_interest_renamed\n",
    "\n",
    "    # Check if 'experiment_name' column exists in both DataFrames\n",
    "    logger.info(\"columns in df_responses:\", df_responses.columns)\n",
    "\n",
    "# if the endpoint list is empty, create columns specific to the bedrock/other supported \n",
    "# models, which includes the name of the endpoint, experiment name, model name, etc\n",
    "else:\n",
    "    # Create an empty DataFrame with the desired columns\n",
    "    logger.info(\"the endpoint_info_list is empty, creating an empty dataframe\")\n",
    "    df_endpoints = pd.DataFrame(columns=['experiment_name',\n",
    "                                         'instance_type',\n",
    "                                         'instance_count',\n",
    "                                         'EndpointName',\n",
    "                                         'ModelName',\n",
    "                                         'Image',\n",
    "                                         'S3Uri'])\n",
    "\n",
    "# at this point we have either a df_endpoints dataframe filled with SageMaker endpoint info\n",
    "# or an empty dataframe if none of the experiments deployed any models on an endpoint\n",
    "# for example if we were using Bedrock...so we now want to add any missing experiment and\n",
    "# instance type information into the endpoint dataframe so that the rest of the analysis\n",
    "# can proceed in the same way for both SageMaker and non-SageMaker deployments\n",
    "if len(df_experiments_skip_deploy) != 0:\n",
    "    logger.info(f\"adding {len(df_experiments_skip_deploy)} \"\n",
    "                f\"experiments to df_endpoints from df_experiments_skip_deploy\")\n",
    "    df_endpoints = pd.concat([df_endpoints,\n",
    "                              df_experiments_skip_deploy[['experiment_name',\n",
    "                                                          'instance_type',\n",
    "                                                          'instance_count']]])\n",
    "logger.info(f\"df_endpoints shape={df_endpoints.shape}, df_endpoints={df_endpoints}\")\n",
    "logger.info(f\"df_endpoints has {len(df_endpoints.experiment_name.unique())} experiments, \"\n",
    "            f\"{df_endpoints.experiment_name.unique}\")\n",
    "\n",
    "logger.info(f\"df_responses shape={df_responses.shape}, df_endpoints={df_responses}\")\n",
    "logger.info(f\"df_responses has {len(df_responses.experiment_name.unique())} experiments, \"\n",
    "            f\"{df_responses.experiment_name.unique()}\")\n",
    "# Merge operation\n",
    "df_results = pd.merge(left=df_responses,\n",
    "                      right=df_endpoints,\n",
    "                      how='left',\n",
    "                      left_on='experiment_name',\n",
    "                      right_on='experiment_name')\n",
    "logger.info(f\"df_results shape={df_results.shape}, df_results={df_results}\")\n",
    "logger.info(f\"df_results has {len(df_results.experiment_name.unique())} experiments, \"\n",
    "            f\"{df_results.experiment_name.unique()}\")\n",
    "for e, experiment in enumerate(config['experiments']):\n",
    "    experiment_name = experiment['name']\n",
    "    instance_type = experiment['instance_type']\n",
    "    instance_count = experiment['instance_count']\n",
    "\n",
    "    logger.info(f\"index {e+1}, experiment_name={experiment_name}, instance type={instance_type}\")\n",
    "    # Update the instance_type column in df_results where the EndpointName matches\n",
    "    df_results.loc[df_results['experiment_name'] == experiment_name, 'instance_type'] = instance_type\n",
    "    df_results.loc[df_results['experiment_name'] == experiment_name, 'instance_count'] = instance_count\n",
    "\n",
    "# Inspect the result\n",
    "logger.info(f\"after adding experiment info, df_results shape={df_results.shape}, df_results={df_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert df_results to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_results.to_csv(csv_buffer, index=False)\n",
    "csv_data_results = csv_buffer.getvalue()\n",
    "results_file_name = config['report']['per_inference_request_file'].format(datetime=date_time)\n",
    "results_s3_path = os.path.join(METRICS_DIR, results_file_name)\n",
    "logger.info(f\"results s3 path for per inference csv --> {results_s3_path}\")\n",
    "write_to_s3(csv_data_results, config['aws']['bucket'], \"\", METRICS_DIR, results_file_name)\n",
    "logger.info(f\"saved results dataframe of shape={df_results.shape} in s3://{BUCKET_NAME}/{results_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure the metadata directory exists\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "\n",
    "# Path for the metrics_path.txt file\n",
    "metrics_path_file = os.path.join(METADATA_DIR, 'metrics_path.txt')\n",
    "logger.info(f\"the metrics metadata path is saved here --> {metrics_path_file}\")\n",
    "\n",
    "# Write the METRICS_DIR to metrics_path.txt\n",
    "with open(metrics_path_file, 'w') as file:\n",
    "    file.write(METRICS_DIR)\n",
    "\n",
    "# Write this data to S3\n",
    "write_to_s3(METRICS_DIR, config['aws']['bucket'], \"\", DATA_DIR, 'metrics_path.txt')\n",
    "\n",
    "logger.info(f\"the information on the defined path for results on these metrics are given in this --> {METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"df_metrics cols = {df_metrics.columns}\")\n",
    "logger.info(f\"df_endpoints cols = {df_endpoints.columns}\")\n",
    "df_metrics = pd.merge(left=df_metrics,\n",
    "                      right=df_endpoints,\n",
    "                      how='left',\n",
    "                      left_on='experiment_name',\n",
    "                      right_on='experiment_name')\n",
    "logger.info(df_metrics)\n",
    "# Convert df_metrics to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_metrics.to_csv(csv_buffer, index=False)\n",
    "csv_data_metrics = csv_buffer.getvalue()\n",
    "metrics_file_name = config['report']['all_metrics_file'].format(datetime=date_time)\n",
    "metrics_s3_path = os.path.join(METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"results s3 path for metrics csv --> {metrics_s3_path}\")\n",
    "write_to_s3(csv_data_metrics, config['aws']['bucket'], \"\", METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in s3://{config['aws']['bucket']}/{metrics_s3_path}\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
