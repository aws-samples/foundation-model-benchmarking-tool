{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate candidate models using Majority Voting with PoLL (Panel of LLM Evaluators), gather findings on quantitative metrics (such as _Cosine Similarity, levenshtein distance, and token set ratio_)\n",
    "\n",
    "---\n",
    "\n",
    "_This notebook works best with the conda_python3 kernel on a ml.t3.medium machine_.\n",
    "\n",
    "#### This step of the solution evaluates the quality of responses generated by the candidate models that have to be benchmarked/evaluated. It does so by performing the steps below:\n",
    "\n",
    "- **Fetches the inference request file that contains all results from the inference step**: The inference results file that contains all inferences from each candidate model is fetched along with the associated metrics such as ground truth (if any), source payload file, concurrency level, etc.\n",
    "\n",
    "- **Generates quantitative metrics for evaluation**: This step calculates quantitative metrics to measure similarity and accuracy, using the _Cosine Similarity, levenshtein distance, and token set ratio_ metrics. Cosine similarity is a metric used to measure how similar two vectors are, regardless of their size. Levenshtein distance is a string metric for measuring the difference between two sequences. The Token Set Ratio algorithm tokenizes both input strings, removes duplicate tokens, and calculates the similarity score. This helps in getting a quantitative overall score to get an overall accuracy/similarity score for each model based on its responses. These quantitative metrics can be used for further downstream tasks to measure the trends, patterns and behaviours of different models on the same dataset or the same models hosted on different serving stacks across various AWS Generative AI services.\n",
    "- **Use a _Panel of LLM Evaluators_ approach to get subjective evaluations**: Refer to this [paper](https://arxiv.org/pdf/2404.18796). We use the following ways to evaluate the responses from the `candidate models` (models used to generate inferences)\n",
    "\n",
    "  - **Majority Voting**: When a dataset provides a ground truth, FMBench uses a technique called `Majority Voting`. Here, we use PoLL, _or a panel of LLM evaluators_, from different model families to evaluate each candidate model's response based on whether it generates a `correct` or an `incorrect` answer simply based on its comparison with the ground truth. Using models from different model families as a PoLL, increases it's ability to match a human level evaluation, makes the evaluation process more streamlined, consistent across all the responses, and reduces the latency and cost of evaluating the candidate models over time. The intra model bias during the evaluation process is also eliminated since more than a single model acts as a panel evaluator. FMBench uses [the majority voting evaluation instructions](prompt_template/eval_criteria/evaluation_instructions_majority_vote.txt) that are fed into the prompt templates supplied to different judge models to evalaute responses at runtime.\n",
    "\n",
    "  **Evaluation Process Flow**\n",
    "\n",
    "  1. First, all the quantitative metrics are calculated, such as the _Cosine Similarity, levenshtein similarity, and token set ratio_ for each candidate model. Once all the quantitative metrics are calculated, all the responses from each candidate model on each payload file is sent through a _Panel of LLM Evaluators_. This panel uses the ground truth for each sample from the data and checks the correctness of every candidate model output across the entire dataset. An example of a prompt that is used to perform this evaluation is [here](prompt_template/eval_criteria/claude_eval_prompt_templates/claude_eval_majority_vote.txt). Every LLM evaluator uses its specific prompt template that is populated with the question, context, ground truth, candidate model response and evaluation instructions at runtime. The evaluation instructions that are used for majority voting can be viewed [here](prompt_template/eval_criteria/evaluation_instructions_majority_vote.txt). Every LLM evaluator compares the model output to the given ground truth and gives a verdict as a binary decision ([`correct`/`incorrect`]) based on the correctness of the candidate model response and an explanation for that given verdict. For the purpose of this evaluation, FMBench uses 3 models as LLM evaluators but users/customers can use less or more.\n",
    "\n",
    "  1. Next, all LLM evaluations of the candidate models are sent through another evaluation layer. This layer performs a check on the evaluations done using the _Cosine Similarity Score_. This evaluation step reinforces the correctness or incorrectness of the evaluation made by the panel of LLM evaluators. This categorizes each evaluation into 3 sub categories:\n",
    "\n",
    "     1. If an LLM evaluator evaluates a candidate model response as `incorrect`, then we check if the _Cosine similarity score_ of that response is less than or equal to the `incorrect_verdict_cosine_similarity_threshold`. If so, then we define the evaluation done as correctly incorrect and move to the next.\n",
    "\n",
    "     1. If an LLM evaluator evaluates a candidate model response as `correct`, then we check if the _Cosine similarity score_ of that response is greater than or equal to the `correct_verdict_cosine_similarity_threshold`. If so, then we define the evaluation done as correctly correct and move to the next.\n",
    "\n",
    "     1. If an LLM evaluator evaluates a candidate model response as `correct` or `incorrect`, but none or either of the Cosine Similarity Thresholds are not met, those responses are labelled as `needs_further_human_or_LLM_evaluation`. For responses that are initially `incorrect` and do not satisfy the cosine similarity threshold are categorized as incorrect anyways.\n",
    "\n",
    "     ![](img/llm_eval_flowchart.png)\n",
    "\n",
    "**_All evaluations are generated in a JSON format for further downstream analytics on the evaluation results_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import ray\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import yaml\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "import plotly.io as pio\n",
    "from pathlib import Path\n",
    "from statistics import mode\n",
    "import plotly.express as px\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from numpy.linalg import norm\n",
    "from litellm import completion\n",
    "from typing import List, Optional, Dict\n",
    "import importlib.resources as pkg_resources\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fmbench.scripts.pricing import load_and_update_pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger to get logs\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the ray service to run async calls in parallel to bedrock easily\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"CONFIG_FILE={CONFIG_FILE}\")\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the associated pricing config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "pricing_file_path: str = config[\"pricing\"]\n",
    "\n",
    "# initialize the pricing config file to None\n",
    "pricing_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files(\"fmbench\"), \"configs\")\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "pricing_module = Path(config[\"pricing\"])\n",
    "logger.info(\n",
    "    f\"pricing config provided for inference from this model is --> {pricing_module}\"\n",
    ")\n",
    "pricing_file_path = os.path.join(config_dir, pricing_module)\n",
    "logger.info(f\"pricing config file path is --> {pricing_file_path}\")\n",
    "\n",
    "instance_list = [\n",
    "    experiment.get(\"instance_type\")\n",
    "    for experiment in config.get(\"experiments\", [])\n",
    "    if experiment.get(\"instance_type\")\n",
    "]\n",
    "\n",
    "\n",
    "# Print the extracted instance types\n",
    "logger.info(f\"Extracted instances from the main config --> {instance_list}\")\n",
    "\n",
    "pricing_config = load_and_update_pricing(\n",
    "    pricing_file_path, PRICING_FALLBACK_YAML_PATH, instance_list\n",
    ")\n",
    "logger.info(f\"pricing config file recorded: {json.dumps(pricing_config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model evaluation information\n",
    "\n",
    "---\n",
    "\n",
    "The common model configuration file contains information about which evaluation strategy to use (`majority voting`),\n",
    "the ground truth column if provided by the user in the config file, which FMs on Bedrock to use as LLM as evaluators,\n",
    "the prompt templates used by each LLM evaluator for Majority voting, the quantitative metric thresholds for an evaluation to be correct/incorrect,\n",
    "directory paths, inference parameters and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "model_eval_fpath: str = config[\"model_evaluations\"]\n",
    "\n",
    "# initialize the pricing config file to None\n",
    "eval_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files(\"fmbench\"), \"configs\")\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "eval_module = Path(config[\"model_evaluations\"])\n",
    "logger.info(f\"eval config provided for evaluation --> {eval_module}\")\n",
    "eval_file_path = os.path.join(config_dir, eval_module)\n",
    "logger.info(f\"eval config file path is --> {eval_file_path}\")\n",
    "\n",
    "# eval_config = load_config(eval_file_path).format(method_name=config['method_name'])\n",
    "with open(eval_file_path, \"r\") as file:\n",
    "    model_eval_info = file.read()\n",
    "    # load the preliminary unformatted config file to fetch the method name and plug it into\n",
    "    # the prompt template file names\n",
    "    model_eval_info_config = yaml.safe_load(model_eval_info)\n",
    "    model_eval_formatted_content = model_eval_info.format(\n",
    "        ground_truth=config[\"datasets\"].get(\"ground_truth_col_key\", None),\n",
    "        method_name=model_eval_info_config[\"model_evaluations\"][\n",
    "            \"PoLL_Composition_and_Voting\"\n",
    "        ].get(\"method\", None),\n",
    "        question=config[\"datasets\"].get(\"question_col_key\", None),\n",
    "    )\n",
    "    eval_config = yaml.safe_load(model_eval_formatted_content)\n",
    "\n",
    "# view all information that will be used in the evaluation process, which includes the ground truth\n",
    "# in the dataset, the evaluation method (Majority voting) and associated information\n",
    "logger.info(f\"eval config file recorded: {json.dumps(eval_config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug is True:\n",
    "    metrics_path_file: str = os.path.join(\"..\", \"..\", METADATA_DIR, METRICS_PATH_FNAME)\n",
    "else:\n",
    "    metrics_path_file: str = os.path.join(METADATA_DIR, METRICS_PATH_FNAME)\n",
    "logger.info(\n",
    "    f\"cwd={os.getcwd()}, METADATA_DIR={METADATA_DIR}, METRICS_PATH_FNAME={METRICS_PATH_FNAME}, metrics_path_file={metrics_path_file}\"\n",
    ")\n",
    "METRICS_DIR: str = Path(metrics_path_file).read_text().strip()\n",
    "logger.info(f\"metrics_path_file={metrics_path_file}, METRICS_DIR={METRICS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path: str = os.path.join(\n",
    "    METRICS_DIR, config[\"report\"][\"per_inference_request_file\"]\n",
    ")\n",
    "logger.info(f\"File path containing the metrics per inference folder --> {file_path}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    file_content = get_s3_object(config[\"aws\"][\"bucket\"], file_path)\n",
    "    # Use pandas to read the CSV content\n",
    "    df_per_inference = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(\n",
    "        f\"{file_path} read into dataframe of shape {df_per_inference.shape}, \"\n",
    "        f\"cols={df_per_inference.columns}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"{file_path} contains results for the following endpoints={df_per_inference.endpoint_name.unique()}\"\n",
    "    )\n",
    "    logger.info(df_per_inference.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates in the inference file caused due to higher concurrency levels\n",
    "\n",
    "Calculate the accuracy on a unique set of data for each candidate model. If a given candidate model ran inferences on multiple concurrency levels for benchmarking purposes, FMBench uses only the unique set of prompts used per candidate model to get a measure of accuracy. This in turn reduces the time and cost to get model evaluations through the panel of LLM evalautors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"Inferences recorded from {len(df_per_inference.endpoint_name.unique())} endpoints.\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Shape of the inference file before removing duplicate inferences per candidate model: {df_per_inference.shape}\"\n",
    ")\n",
    "df_per_inference = df_per_inference.drop_duplicates(\n",
    "    [\"endpoint_name\", \"prompt\"], keep=\"last\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Shape of the inference file after removing duplicate inferences per candidate model: {df_per_inference.shape}\"\n",
    ")\n",
    "df_per_inference.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"Going to be using this inference file to generate evaluations on -> {df_per_inference.head()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between prompt token length and inference latency for different instances and concurrency levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"Information on the inference file being used for evaluations: {df_per_inference.latency.describe()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"Total number of inferences to evaluate from candidate models: {df_per_inference.shape[0]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the `sentence-transformers/all-mpnet-base-v2` embeddings model to calculate the _Cosine Similarity_ scores\n",
    "\n",
    "---\n",
    "\n",
    "This portion of the evaluation step does as follows:\n",
    "\n",
    "1. Uses the `sentence-transformers/all-mpnet-base-v2` model from Hugging Face. This is a sentence-transformers model. It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "1. Use the embeddings model to get quantitative metrics from the inferences. This helps to get a similarity score between the ground truth answers from a dataset if any are given and the actual responses from the model received during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the quantitiative evaluation information from the config file, such as the embeddings model\n",
    "# to be used\n",
    "embeddings_model_quantitative_info: Dict = eval_config[\"model_evaluations\"][\n",
    "    \"quantitative_eval_info\"\n",
    "]\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    This function loads the sentence-transformers model based on the provided model ID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = None\n",
    "        model_id = embeddings_model_quantitative_info[\"embeddings_model_id\"].get(\n",
    "            \"model_id\", None\n",
    "        )\n",
    "        if model_id:\n",
    "            model = SentenceTransformer(model_id)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Model ID is not provided or invalid in the configuration.\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"The SentenceTransformer embeddings model could not be loaded: {e}\"\n",
    "        )\n",
    "        model = None\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the embeddings model to calculate the cosine similarity scores\n",
    "model = load_model()\n",
    "logger.info(\n",
    "    f\"Embeddings model info which will be used to calculate the cosine similarity scores for Majority Voting Eval: {model}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between two texts. In this case,\n",
    "    the cosine similarity is the comparison between the ground truth in the given dataset\n",
    "    and the candidate model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cosine_similarity_score: float = None\n",
    "        # returns the embedding for a given text using the sentence-transformers model.\n",
    "        A = model.encode([text1])[0]\n",
    "        B = model.encode([text2])[0]\n",
    "        cosine_similarity_score = dot(A, B) / (norm(A) * norm(B))\n",
    "        logger.info(\n",
    "            f\"Calculating the cosine similarity score, current score: {cosine_similarity_score}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cosine similarity was not calculated at this iteration: {e}\")\n",
    "        cosine_similarity_score = None\n",
    "    return cosine_similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the method that is being used to evaluate the content (which is either Majority voting)\n",
    "model_eval_subjective_info: List[Dict] = eval_config[\"model_evaluations\"][\n",
    "    \"subjective_eval_info\"\n",
    "]\n",
    "method_name: str = eval_config[\"model_evaluations\"][\"PoLL_Composition_and_Voting\"].get(\n",
    "    \"method\", None\n",
    ")\n",
    "logger.info(\n",
    "    f\"The evaluation method FMBench is going to use to evaluate different model responses: {method_name}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"judge panel being used to evaluate model responses: {model_eval_subjective_info.get('judge_panel_list', None)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the quantitative metrics if evaluation is set to Majority voting\n",
    "logger.info(\n",
    "    f\"Valid ground truth column found in the inference file: {eval_config['model_evaluations'].get('ground_truth_col')}, calculating cosine similarity scores\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"~Creating embeddings and calculating cosine similarity scores for of all candidate model responses now. This might take a 1-2 minutes~\"\n",
    ")\n",
    "ground_truth_col_name: Optional[str] = config[\"datasets\"].get(\n",
    "    \"ground_truth_col_key\", None\n",
    ")\n",
    "\n",
    "# Check for ground truth column and raise an exception if not found\n",
    "if ground_truth_col_name is None:\n",
    "    raise ValueError(\n",
    "        f\"Expected a valid ground truth column name in the config file information, got {ground_truth_col_name}. Cannot continue.\"\n",
    "    )\n",
    "\n",
    "# If we reach this point, we know the ground truth column exists\n",
    "df_per_inference[\"cosine_similarity_score\"] = df_per_inference.apply(\n",
    "    lambda row: calculate_cosine_similarity(row[\"completion\"], row[\"ground_truth\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "logger.info(f\"Calculated the cosine similarity score: {df_per_inference.head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Evaluations: Hierarchical Flow\n",
    "\n",
    "---\n",
    "\n",
    "1. Check for the lexical match/similarity between the ground truth and the answer using one main quantitative metrics: \\_Cosine similarity score.\n",
    "\n",
    "1. Parse each candidate model response through a panel of LLM evaluators to determine the accuracy of that model across the entire dataset.\n",
    "\n",
    "1. Send all evaluations from the LLM evaluators through a final evaluation layer to check if the evaluation is correctly or incorrectly made with the help of quantitative metric thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the all_metrics path to send the evaluation metrics to\n",
    "all_metrics_fpath: str = os.path.join(METRICS_DIR, config[\"report\"][\"all_metrics_file\"])\n",
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_cosine_similarity_scores_csv = csv_buffer.getvalue()\n",
    "inference_cosine_similarity_scores_s3_path = os.path.join(\n",
    "    METRICS_DIR, PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES\n",
    ")  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(\n",
    "    df_per_inference_with_cosine_similarity_scores_csv,\n",
    "    BUCKET_NAME,\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES,\n",
    ")\n",
    "logger.info(\n",
    "    f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{inference_cosine_similarity_scores_s3_path}\"\n",
    ")\n",
    "df_per_inference.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use _Panel of LLM Evaluators_ to get Subjective Evaluations on various evaluation criteria\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we run evaluations on all candidate models using a panel of LLM evaluators. We use a main evaluation method: `Majority Voting`. To eliminate intra-model bias, we address this by scoring answer correctness based not on a single judge, but instead on a panel composed of multiple evaluator models.\n",
    "\n",
    "1. **Majority Voting**: We use the PoLL to evaluate candidate model responses by checking its correctness compared to a provided ground truth answer in the dataset. We prompt each PoLL to evaluate and give the response in a JSON structure, giving a verdict on whether the response is correct or incorrect based on its comparison with the ground truth, and an explanation as to why that is. With all verdicts and responses in JSON, we can perform downstream tasks such as:\n",
    "\n",
    "   1. Calculate the overall accuracy of each model using the correct versus the (correct + incorrect) responses\n",
    "\n",
    "   1. Calculate the `error rate` or frequency or incorrect responses\n",
    "\n",
    "   1. Categorize the errors based on the explanations provided by the evaluators. Common categories might include misunderstanding the question, incomplete answers, factual inaccuracies\n",
    "\n",
    "   1. Summary of overall correct/incorrect, and the best model based on the PoLL. Rank the models on Correctness versus Incorrectness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the qualitative/subjective evaluation information from the config file to evaluate answers from different\n",
    "# endpoints on various criteria\n",
    "model_eval_subjective_info: Dict = eval_config[\"model_evaluations\"][\n",
    "    \"subjective_eval_info\"\n",
    "]\n",
    "eval_criteria_list = model_eval_subjective_info.get(\"eval_criteria\", None)\n",
    "logger.info(\n",
    "    f\"available llm as a judge evaluation information to use: {json.dumps(model_eval_subjective_info, indent=2)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the inference parameters that the LLM judge panel will use while evaluating model candidate responses\n",
    "INFERENCE_PARAMETERS_LLM_PANEL: Dict = eval_config[\"model_evaluations\"][\n",
    "    \"subjective_eval_info\"\n",
    "].get(\"inference_parameters\", None)\n",
    "logger.info(\n",
    "    f\"Inference parameters that LLM evaluators will use: {INFERENCE_PARAMETERS_LLM_PANEL}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llm_evaluation(model_id: str, prompt: str):\n",
    "    \"\"\"\n",
    "    Get inference using LiteLLM. This function is called by each evaluator on the panel of\n",
    "    llm evaluators to get a response on a given prompt. This is in the case of where there is\n",
    "    Majority voting enabled\n",
    "    \"\"\"\n",
    "    # represents the service name\n",
    "    logger.info(f\"get_inference, model_id={model_id}\")\n",
    "    service_name: str = \"bedrock\"\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name\n",
    "    # initialize the response dict\n",
    "    ret = dict(\n",
    "        exception=None,\n",
    "        prompt=prompt,\n",
    "        completion=None,\n",
    "        completion_token_count=None,\n",
    "        prompt_token_count=None,\n",
    "        input_token_cost=None,\n",
    "        output_token_cost=None,\n",
    "        total_cost=None,\n",
    "        model_id=model_id,\n",
    "    )\n",
    "    body = ret[\"prompt\"]\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region\n",
    "    try:\n",
    "        # Represents calling the litellm completion/messaging api utilizing the completion/embeddings API\n",
    "        print(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(\n",
    "            model=bedrock_model,\n",
    "            messages=[{\"content\": body, \"role\": \"user\"}],\n",
    "            temperature=INFERENCE_PARAMETERS_LLM_PANEL.get(\"temperature\", 0.1),\n",
    "            max_tokens=INFERENCE_PARAMETERS_LLM_PANEL.get(\"max_tokens\", 100),\n",
    "            caching=INFERENCE_PARAMETERS_LLM_PANEL.get(\"caching\", False),\n",
    "        )\n",
    "        print(f\"response: {response}\")\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens\n",
    "        ret[\"prompt_token_count\"] = response.usage.prompt_tokens\n",
    "        ret[\"completion_token_count\"] = response.usage.completion_tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret[\"exception\"] = e\n",
    "    logger.info(f\"completion: {ret['completion']}\")\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_filename(s):\n",
    "    \"\"\"\n",
    "    convert a string to another string that can be used as a filename\n",
    "    i.e. remove white space and non-word chars\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"None\"\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "    # Replace all runs of whitespace with a single dash\n",
    "    s = re.sub(r\"\\s+\", \"-\", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_as_json(x: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Convert a string into a dictionary. Remove any\n",
    "    stray whitespaces which could break the json parsing\n",
    "    \"\"\"\n",
    "    d: Optional[Dict] = None\n",
    "    try:\n",
    "        x = x.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        d = json.loads(x)\n",
    "    except Exception as e:\n",
    "        print(f\"parse_as_json, error parsing string as json, string={x}\")\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.rename(\n",
    "    columns={\"completion\": \"candidate_model_response\"}, inplace=True\n",
    ")\n",
    "df_per_inference.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the evaluation prompt payloads\n",
    "\n",
    "---\n",
    "\n",
    "Here, the evaluation prompt template is used by the LLM judge to evaluate the answers on different criteria.\n",
    "This prompt template function uses a set of rules, prompt template, the answer, and ground truth (if any) in the\n",
    "evaluation solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_eval_prompts(\n",
    "    eval_template: str,\n",
    "    answer: str,\n",
    "    rules: str,\n",
    "    ground_truth: Optional[str],\n",
    "    question: Optional[str],\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prepares the evaluation prompts by preparing the standard eval prompt template\n",
    "    with the rules of a given subjective criteria, context, answer and ground truth (if any ground truth is provided)\n",
    "    This function prepares prompt payloads for both evaluation criteria: Majority voting. In the\n",
    "    case of Majority voting, there is no subjective criteria that is inputted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        processed_eval_template: Optional[str] = None\n",
    "        processed_eval_template = eval_template.format(\n",
    "            rules=rules, answer=answer, ground_truth=ground_truth, question=question\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error encountered while generating the evaluation prompt template: {e}\"\n",
    "        )\n",
    "        processed_eval_template = None\n",
    "    return processed_eval_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "\n",
    "# create the metrics directory that stores all of the json files containing evaluations from all Panel of LLM evaluators\n",
    "METRICS_PER_POLL_EVAL_DIR: str = os.path.join(\n",
    "    METRICS_DIR, METRICS_PER_POLL_EVAL_DIR_NAME\n",
    ")\n",
    "_ = list(map(clear_dir, [METRICS_PER_POLL_EVAL_DIR]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the pricing information for the instance type\n",
    "bedrock_pricing = pricing_config[\"pricing\"][\"token_based\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_candidate_model_name(model_name: str) -> str:\n",
    "    # the candidate model is actually an endpoint name, so remove the timestamp and -endpoint from\n",
    "    # string so \"Meta-Llama-3-1-8B-Instruct-g5-2024-08-17-01-25-45-284-endpoint\" would become\n",
    "    # \"Meta-Llama-3-1-8B-Instruct-g5\", and no change would happen for Bedrock models as they dont\n",
    "    # contain timestamp, for example anthropic.claude-3-opus-20240229-v1:0 would remain unchanged\n",
    "\n",
    "    # regex to match the timestamp and the endpoint part\n",
    "    regex = r\"-\\d{4}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{3}.*$\"\n",
    "    # removing the matched part\n",
    "    model_name_normalized = re.sub(regex, \"\", model_name)\n",
    "    return model_name_normalized\n",
    "\n",
    "\n",
    "def run_panel_of_llm_evals(\n",
    "    i: int, total: int, row: Dict, model_id: str, eval_method_name: str, uuid: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Runs the evaluation for one row\n",
    "    The eval prompt is already available in the row dictionary\n",
    "    and we simply want to run the inference against the judge model.\n",
    "    The results are returned in a new dictionary that contains the model\n",
    "    response and some fields from the original dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # initialize the response dictionary that contains the pricing information\n",
    "        # along with other metrics. If there is any error encountered, this response\n",
    "        # dictionary is returned as is\n",
    "        resp = dict(\n",
    "            exception=None,\n",
    "            completion=None,\n",
    "            completion_token_count=None,\n",
    "            prompt_token_count=None,\n",
    "            input_token_cost=None,\n",
    "            output_token_cost=None,\n",
    "            total_cost=None,\n",
    "            model_id=model_id,\n",
    "            candidate_model_response=row[\"candidate_model_response\"],\n",
    "            candidate_model=None,\n",
    "            payload_file=row[\"payload_file\"],\n",
    "            cosine_similarity_score=row[\"cosine_similarity_score\"],\n",
    "            ground_truth=row[\"ground_truth\"],\n",
    "            question=row[\"question\"] if \"question\" in row else None,\n",
    "        )\n",
    "        # save all the responses from the model in a dictionary\n",
    "        resp: Optional[Dict] = None\n",
    "        candidate_model = normalize_candidate_model_name(row[\"endpoint_name\"])\n",
    "        logger.info(\n",
    "            f\"run_eval, row {i}/{total}, judge_model_id={model_id}, candidate model={candidate_model}\"\n",
    "        )\n",
    "        # create the payload for model inference\n",
    "        prompt = row[f\"{model_id}_{method_name}_eval_prompt\"]\n",
    "        # generate the evaluation on the data using the model judge\n",
    "        resp = get_llm_evaluation(model_id, prompt)\n",
    "        # assign the completion from the candidate model to the `candidate_model_response`,\n",
    "        # and the actual evaluation will be contained in a field called `completion`\n",
    "        resp[\"candidate_model_response\"] = row[\"candidate_model_response\"]\n",
    "        resp[\"candidate_model\"] = candidate_model\n",
    "        resp[\"payload_file\"] = row[\"payload_file\"]\n",
    "        resp[\"cosine_similarity_score\"] = row[\"cosine_similarity_score\"]\n",
    "        # Calculate cost based on the number of input and output tokens\n",
    "        model_pricing = bedrock_pricing.get(model_id, None)\n",
    "        if model_pricing:\n",
    "            resp[\"input_token_cost\"] = (\n",
    "                resp[\"prompt_token_count\"] / 1000.0\n",
    "            ) * model_pricing[\"input-per-1k-tokens\"]\n",
    "            resp[\"output_token_cost\"] = (\n",
    "                resp[\"completion_token_count\"] / 1000.0\n",
    "            ) * model_pricing[\"output-per-1k-tokens\"]\n",
    "            resp[\"total_cost\"] = resp[\"input_token_cost\"] + resp[\"output_token_cost\"]\n",
    "            logger.info(\n",
    "                f\"instance_type={model_id}, prompt_tokens={resp['prompt_token_count']}, \"\n",
    "                f\"input_token_cost={resp['input_token_cost']}, output_token_cost={resp['completion_token_count']}, \"\n",
    "                f\"output_token_cost={resp['output_token_cost']}, total cost={resp['total_cost']}\"\n",
    "            )\n",
    "        else:\n",
    "            logger.error(\n",
    "                f'model pricing for \"{model_id}\" not found, '\n",
    "                f\"cannot calculate experiment cost\"\n",
    "            )\n",
    "        # if there is a ground truth (in case of Majority voting) or\n",
    "        # criteria name (in case of average pooline), include those in the json response\n",
    "        resp[\"ground_truth\"] = row[\"ground_truth\"]\n",
    "        if \"question\" in row:\n",
    "            resp[\"question\"] = row[\"question\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encountered while running evaluation: {e}\")\n",
    "        resp[\"exception\"] = str(e)\n",
    "    return resp\n",
    "\n",
    "\n",
    "# we use Ray to parallize\n",
    "@ray.remote\n",
    "def async_run_eval(\n",
    "    i: int, total: int, row: Dict, model_id: str, eval_method_name: str, uuid: str\n",
    ") -> Dict:\n",
    "    print(\n",
    "        f\"async_run_eval, i={i}, total={total}, judge_model_info={model_id}, eval_method: {eval_method_name}, uuid: {uuid}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"async_run_eval, i={i}, total={total}, judge_model_info={model_id}, eval_method: {eval_method_name}, uuid: {uuid}\"\n",
    "    )\n",
    "    return run_panel_of_llm_evals(i, total, row, model_id, eval_method_name, uuid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "df_per_inference_list = json.loads(df_per_inference.to_json(orient=\"records\"))\n",
    "logger.info(\n",
    "    f\"Total number of candidate models going to be evaluated: {len(df_per_inference_list)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare evaluation prompt templates\n",
    "\n",
    "---\n",
    "\n",
    "This portion of the step prepares the evaluation prompt templates that are used in the evaluation process of using `Majority Voting` using the PoLL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_eval_subjective_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"Number of judges being used for this model evaluation: {len(model_eval_subjective_info.get('judge_panel_list', None))}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Inference Parameters that are going to be used by the judge panels while evaluating candidate models: {model_eval_subjective_info.get('inference_parameters', None)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare prompt payloads\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the step, FMBench iterates through each of the row containing the model response and prepares the corresponding prompt payloads. In this step, the prompt template for a given evaluation method is used. For Majority voting, a standard prompt template is used with evaluation instructions and candidate model responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "model_eval_dir: Optional[str] = eval_config[\"model_evaluations\"][\"model_eval_dir\"]\n",
    "eval_prompts_dir: str = Path(\n",
    "    pkg_resources.files(\"fmbench\"),\n",
    "    f\"{config['s3_read_data']['prompt_template_dir']}/{model_eval_dir.get('eval_prompts_dir', None)}\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Iterate through each LLM as a judge and each evaluation criterion\n",
    "    for llm_info in model_eval_subjective_info.get(\"judge_panel_list\", []):\n",
    "        model_id: str = llm_info[\"model_id\"]\n",
    "        method_name: str = eval_config[\"model_evaluations\"][\n",
    "            \"PoLL_Composition_and_Voting\"\n",
    "        ].get(\"method\", None)\n",
    "        eval_prompt_template_fname: str = (\n",
    "            f\"{llm_info.get('eval_prompt_template_name', None)}.txt\"\n",
    "        )\n",
    "\n",
    "        # Use the evaluation prompt template path to read in the standard prompt template that\n",
    "        # is used in the creation of prompt payloads\n",
    "        eval_prompt_template_dir = llm_info.get(\"eval_prompt_template_dir\", None)\n",
    "        eval_prompt_template_path = os.path.join(\n",
    "            eval_prompts_dir, eval_prompt_template_dir, eval_prompt_template_fname\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"evaluation prompt template file path being used for {model_id}: {eval_prompt_template_path}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"evaluation prompt template file name: {eval_prompt_template_fname}\"\n",
    "        )\n",
    "        eval_prompt_template = Path(eval_prompt_template_path).read_text()\n",
    "        logger.info(f\"Evaluation prompt template being used: {eval_prompt_template}\")\n",
    "\n",
    "        # There is a standard instructions file for both Majority voting on how to evaluate the\n",
    "        # model responses (whether it should be a binary decision or rating on a scale of 1-5)\n",
    "        eval_instructions_fname = next(\n",
    "            (\n",
    "                rule\n",
    "                for rule in model_eval_dir.get(\"eval_instructions_files\", None)\n",
    "                if method_name in rule\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        rules = Path(\n",
    "            os.path.join(eval_prompts_dir, eval_instructions_fname)\n",
    "        ).read_text()\n",
    "        logger.info(f\"rules: {rules}\")\n",
    "        column_name = f\"{model_id}_{method_name}_eval_prompt\"\n",
    "        df_per_inference[column_name] = df_per_inference.apply(\n",
    "            lambda r: prepare_eval_prompts(\n",
    "                eval_prompt_template,\n",
    "                r[\"candidate_model_response\"],\n",
    "                rules,\n",
    "                r[\"ground_truth\"],\n",
    "                r[\"question\"],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error occurred in the creation of prompt payloads: {e}\")\n",
    "    df_per_inference = None\n",
    "\n",
    "df_per_inference.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_eval_prompt_payloads = csv_buffer.getvalue()\n",
    "eval_prompt_payloads_for_inference = os.path.join(\n",
    "    METRICS_DIR, PROCESSED_EVAL_PROMPT_PAYLOADS\n",
    ")  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(\n",
    "    df_per_inference_with_eval_prompt_payloads,\n",
    "    BUCKET_NAME,\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    PROCESSED_EVAL_PROMPT_PAYLOADS,\n",
    ")\n",
    "logger.info(\n",
    "    f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{eval_prompt_payloads_for_inference}\"\n",
    ")\n",
    "df_per_inference.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "eval_records_list = json.loads(df_per_inference.to_json(orient=\"records\"))\n",
    "logger.info(f\"Total number evaluations to be done: {len(eval_records_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the hierarchy of Model Evaluations\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the step, FMBench performs the following actions:\n",
    "\n",
    "1. For `Majority Voting` - We suppose that a ground truth already exists in the dataset. We first calculate quantitative metrics.\n",
    "\n",
    "1. We use the LLM panel of judges (in this case 3 judges), to give a verdict on whether the `answer` from the candidate models during inference is `correct` or `incorrect`. The panel of LLM judges also gives an explanation as to why it evaluated a candidate model response as correct or incorrect.\n",
    "\n",
    "1. Each model response is given in a JSON structure which is further used for downstream analytics, to decide the comparision of evaluation results between different model candidates and more.\n",
    "\n",
    "1. The evaluations are sent through a final layer to decide if an evaluation made using an LLM evaluator is made correctly/incorrectly.\n",
    "\n",
    "**_This step takes a couple of minutes to complete based on the size of the dataset and the judge models. Model completion time depends on the PoLL models being used. `Llama3-70b`, `Cohere command-r-v1` and `claude 3 Sonnet` were used for this example_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the llm as a judge panel list\n",
    "judge_panel_list: List[Dict] = model_eval_subjective_info.get(\"judge_panel_list\", None)\n",
    "logger.info(\n",
    "    f\"The judge panel list contains {len(judge_panel_list)} judges. Their information: {judge_panel_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"~Panel of LLM evaluators are going to start evaluating responses. This might take a couple of minutes depending on the size of the dataset and candidate model responses~\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_quantitative_eval_enabled: bool = eval_config[\"model_evaluations\"][\n",
    "    \"PoLL_Composition_and_Voting\"\n",
    "].get(\"use_quantitative_metrics\", False)\n",
    "logger.info(\n",
    "    f\"Are quantitative metrics going to be used to make a final eval decision: {is_quantitative_eval_enabled}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the evaluation process\n",
    "\n",
    "---\n",
    "\n",
    "This process loops through the evaluation prompt payloads that are prepared. For Majority voting, a JSON containing 2 elements is generated: \"verdict\" of whether the given answer is correct or incorrect and an \"explanation\".\n",
    "\n",
    "Responses from either evaluation processes are sent for further downstream processes to determine the most accurate\n",
    "and subjectively correct model based on domain specific use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n: int = model_eval_subjective_info.get(\"run_parallel_inference_count\", 5)\n",
    "list_of_lists = [\n",
    "    eval_records_list[i * n : (i + 1) * n]\n",
    "    for i in range((len(eval_records_list) + n - 1) // n)\n",
    "]\n",
    "resp_list = []\n",
    "erroneous_count: int = 0\n",
    "st: float = time.perf_counter()\n",
    "\n",
    "# Iterate over the judge panel and sublists\n",
    "for judge_panelist_info in judge_panel_list:\n",
    "    logger.info(\n",
    "        f\"============Running inference for judge panelist {judge_panelist_info['model_id']} for {method_name} ============\"\n",
    "    )\n",
    "    for idx, sublist in enumerate(list_of_lists):\n",
    "        model_id: str = judge_panelist_info[\"model_id\"]\n",
    "        logger.info(\n",
    "            f\"Getting inference for list {idx + 1}/{len(list_of_lists)}, size of list={len(sublist)}\"\n",
    "        )\n",
    "        try:\n",
    "            resp_list.extend(\n",
    "                ray.get(\n",
    "                    [\n",
    "                        async_run_eval.remote(\n",
    "                            i + 1,\n",
    "                            len(sublist),\n",
    "                            record,\n",
    "                            model_id,\n",
    "                            method_name,\n",
    "                            record[\"uuid\"],\n",
    "                        )\n",
    "                        for i, record in enumerate(sublist)\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing list {idx + 1}/{len(list_of_lists)}: {e}\")\n",
    "            erroneous_count += 1\n",
    "    # Sleep for two seconds before moving on to the next model\n",
    "    logger.info(\n",
    "        f\"~Sleeping for one second before the next Panel of LLM evaluates the responses~\"\n",
    "    )\n",
    "    time.sleep(1)\n",
    "\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"Total elapsed time for inference: {elapsed_time:.2f} seconds\")\n",
    "logger.info(f\"Total erroneous lists: {erroneous_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send all Panel of LLM evaluator responses to S3 as `JSON` files\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect all of the panel of LLM evals and send them all as JSON files to S3\n",
    "if resp_list:\n",
    "    save_s3_list = []\n",
    "    try:\n",
    "        for resp in resp_list:\n",
    "            if resp:\n",
    "                llm_eval_response = json.dumps(resp, indent=2)\n",
    "                candidate_model_id = resp.get(\"candidate_model\", None)\n",
    "                if candidate_model_id:  # Ensure candidate_model_id is not None\n",
    "                    # Extract a few words from the poll eval response to append to the file name\n",
    "                    response_excerpt = \" \".join(\n",
    "                        resp.get(\"candidate_model_response\", \"\").split()[:5]\n",
    "                    )\n",
    "                    sanitized_response_excerpt = \"\".join(\n",
    "                        [c if c.isalnum() else \"_\" for c in response_excerpt]\n",
    "                    )\n",
    "                    llm_eval_json_fname = f\"{candidate_model_id}_{time.time()}_{sanitized_response_excerpt}.json\"\n",
    "                    response_s3_path = os.path.join(\n",
    "                        METRICS_PER_POLL_EVAL_DIR, llm_eval_json_fname\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"Sending model eval result files to s3 path prefix: {response_s3_path}\"\n",
    "                    )\n",
    "                    save_s3_list.append(\n",
    "                        (\n",
    "                            llm_eval_response,\n",
    "                            config[\"aws\"][\"bucket\"],\n",
    "                            \"\",\n",
    "                            METRICS_PER_POLL_EVAL_DIR,\n",
    "                            llm_eval_json_fname,\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"candidate_model_id is None, skipping this response.\"\n",
    "                    )\n",
    "            else:\n",
    "                logger.warning(\"Response is None, skipping this entry.\")\n",
    "        if save_s3_list:\n",
    "            # Split the save_s3_list into smaller batches to get\n",
    "            # rid of the cannot write to s3 bucket - request rate was hitting maximum threshold\n",
    "            batch_size: int = 50\n",
    "            delay: float = 1\n",
    "            for i in range(0, len(save_s3_list), batch_size):\n",
    "                batch = save_s3_list[i : i + batch_size]\n",
    "                # write a batch of evaluation result files to s3\n",
    "                write_multiple_to_s3(batch)\n",
    "                time.sleep(delay)  # Delay between batches\n",
    "        else:\n",
    "            logger.error(\"No valid responses to write to S3.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing or writing to S3: {e}\")\n",
    "else:\n",
    "    logger.info(\"No responses to write to S3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save All Results: Perform downstream analytical tasks on each PoLL evaluation result\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the evaluation step:\n",
    "\n",
    "1. We compile all metrics gathered from the Majority Voting experiment, and send them as `CSV`, `txt` files to s3.\n",
    "\n",
    "1. These metrics include: Quantitative metrics and binary decision scores (for Majority Voting).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the results list into a dataframe for easy analytics\n",
    "df_eval_results = pd.DataFrame(resp_list)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how=\"all\")\n",
    "# the exception, judge model id, prompt token count, will be NaN for the verdicts decided\n",
    "# using the lexical match and not moved forward to the panel of LLM evaluators\n",
    "df_eval_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parse out the completion from LLM as a judge and column bind\n",
    "# the fields of the dictionary to the original results dataframe\n",
    "df_eval_results_only = (\n",
    "    df_eval_results[\"completion\"].apply(parse_as_json).apply(pd.Series)\n",
    ")\n",
    "df_eval_results_only.dropna(axis=1, how=\"all\")\n",
    "df_eval_results = pd.concat([df_eval_results, df_eval_results_only], axis=1)\n",
    "df_eval_results.rename(columns={\"model_id\": \"judge_model_id\"}, inplace=True)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how=\"all\")\n",
    "df_eval_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new column and assign the original verdict to this column\n",
    "df_eval_results[\"original_verdict\"] = df_eval_results[\"verdict\"]\n",
    "df_eval_results.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the correctness of LLM Evaluators using quantitative metrics\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the evaluation step, we perform the following steps:\n",
    "\n",
    "1. Evaluate whether the LLM evaluators sent in the correct evaluations using another layer of checks with _Cosine Similarity Score_.\n",
    "\n",
    "1. If the verdicts decided by the LLM evaluators (`correct` or `incorrect`) do not meet the respective cosine similarity thresholds, then they are sent into another file for further analysis for human or another LLM evaluation loop.\n",
    "\n",
    "There are two possible cases for this evaluation:\n",
    "\n",
    "1. **Incorrect Verdicts**: If the verdict from the judge model is incorrect, then check if the cosine similarity of that\n",
    "   incorrectly identified verdict is less than the `incorrect_verdict_cosine_similarity_threshold`. If so, then it is\n",
    "   finally sent in as is into the dataframe. If the LLM evaluator defines a verdict as incorrect but if it has a higher cosine\n",
    "   similarity than the incorrect cosine similarity threshold, then it is marked for \"needing further evaluation using a human\" or\n",
    "   another LLM evalution.\n",
    "\n",
    "2. **Correct Verdicts**: If the verdict from the judge model is correct and if it exceeds the correctness cosine similarity threshold,\n",
    "   then the model is evaluated as correct and sent in for further downstream analytics. For the correct verdicts identified by the judge models\n",
    "   that do not meet the correctness cosine similarity threshold, are defined as \"needed further human/LLM evaluation\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantitative_verdict_cosine_similarity_decision(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Given an LLM evaluator response, this function checks for whether a verdict provided by an LLM evaluator\n",
    "    is correctly evaluated using a cosine similarity metric threshold for correct and incorrect verdicts. These\n",
    "    are the two cases that this function handles for each evaluation done using LLM as evaluators:\n",
    "\n",
    "    1. Incorrect Verdicts: If the verdict from the judge model is incorrect, then check if the cosine similarity of that\n",
    "    incorrectly identified verdict is less than the `incorrect_verdict_cosine_similarity_threshold`. If so, then it is\n",
    "    finally sent in as is into the dataframe. If the LLM evaluator defines a verdict as incorrect but if it has a higher cosine\n",
    "    similarity than the incorrect cosine similarity threshold, then it is marked for \"needing further evaluation using a human\" or\n",
    "    another LLM evalution.\n",
    "\n",
    "    2. Correct Verdicts: If the verdict from the judge model is correct and if it exceeds the correctness cosine similarity threshold,\n",
    "    then the model is evaluated as correct and sent in for further downstream analytics. For the correct verdicts identified by the judge models\n",
    "    that do not meet the correctness cosine similarity threshold, are defined as \"needed further human/LLM evaluation\".\n",
    "\n",
    "    This function is used if the evaluation method being used is Majority voting, specifically in the case\n",
    "    of when ground truth is provided.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This is a boolean value that is returned defining whether a given verdict is valid based on\n",
    "        # the comparison of its respective cosine similarity score and cosine similarity threshold for correctness/incorrectness\n",
    "        is_eval_done_correctly: Optional[bool] = None\n",
    "        correct_cosine_similarity_threshold: Optional[float] = None\n",
    "        incorrect_cosine_similarity_threshold: Optional[float] = None\n",
    "\n",
    "        # Check if the evaluation method is Majority voting and if the customer has enabled\n",
    "        # evaluation decisions to also be made by quantitative metric thresholds\n",
    "        if is_quantitative_eval_enabled:\n",
    "            # Retrieve the information that is going to be used to check for whether a verdict is\n",
    "            # incorrectly identified as correct or incorrect\n",
    "            judge_model_id: str = row[\"judge_model_id\"]\n",
    "            verdict: str = row[\"verdict\"]\n",
    "            explanation: str = row[\"explanation\"]\n",
    "            cosine_similarity_score: float = row[\"cosine_similarity_score\"]\n",
    "\n",
    "            # Get the correctness and incorrectness cosine similarity threshold scores\n",
    "            correct_cosine_similarity_threshold = eval_config[\"model_evaluations\"][\n",
    "                \"quantitative_eval_info\"\n",
    "            ].get(\"correct_verdict_cosine_similarity_threshold\", None)\n",
    "            incorrect_cosine_similarity_threshold = eval_config[\"model_evaluations\"][\n",
    "                \"quantitative_eval_info\"\n",
    "            ].get(\"incorrect_verdict_cosine_similarity_threshold\", None)\n",
    "\n",
    "            # If the verdict is correct and is greater than or equal to the correct cosine similarity threshold, then\n",
    "            # the verdict is correct. If not, the verdict is identified to need further evaluation\n",
    "\n",
    "            # include the original verdict here\n",
    "            if verdict == \"correct\":\n",
    "                if cosine_similarity_score >= correct_cosine_similarity_threshold:\n",
    "                    row[\"explanation\"] = (\n",
    "                        f\"{explanation} Cosine similarity is {cosine_similarity_score}, which does meets and is above the threshold of {correct_cosine_similarity_threshold}.\"\n",
    "                    )\n",
    "                    is_eval_done_correctly = True\n",
    "                else:\n",
    "                    row[\"verdict\"] = \"needs_further_human_or_LLM_evaluation\"\n",
    "                    row[\"explanation\"] = (\n",
    "                        f\"{explanation} Cosine similarity is {cosine_similarity_score}, which does not meet the threshold of {correct_cosine_similarity_threshold}. Evaluate it further to determine the correct answer.\"\n",
    "                    )\n",
    "                    is_eval_done_correctly = False\n",
    "\n",
    "            # If the verdict is incorrect and is less than or equal to the incorrect cosine similarity threshold, then\n",
    "            # the verdict is correctly identified as incorrect. If not, the verdict is identified to need further evaluation\n",
    "            elif verdict == \"incorrect\":\n",
    "                if cosine_similarity_score <= incorrect_cosine_similarity_threshold:\n",
    "                    row[\"explanation\"] = (\n",
    "                        f\"{explanation} Cosine similarity is {cosine_similarity_score}, which does is below the threshold of {incorrect_cosine_similarity_threshold}.\"\n",
    "                    )\n",
    "                    is_eval_done_correctly = True\n",
    "                else:\n",
    "                    row[\"verdict\"] = \"needs_further_human_or_LLM_evaluation\"\n",
    "                    # if the verdict needs further evaluation but was incorrect originally, then reset the verdict to incorrect\n",
    "                    if row[\"verdict\"] == \"needs_further_human_or_LLM_evaluation\":\n",
    "                        row[\"verdict\"] = \"incorrect\"\n",
    "                        row[\"explanation\"] = (\n",
    "                            f\"{explanation} Cosine Similarity of {cosine_similarity_score} >= {incorrect_cosine_similarity_threshold} incorrect cosine similarity threshold, does not meet threshold.\"\n",
    "                        )\n",
    "                        is_eval_done_correctly = True\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Error in quantitative_verdict_cosine_similarity_decision: {str(e)}\"\n",
    "        )\n",
    "        is_eval_done_correctly = None\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the layer of another evaluation filter on the dataframe containing all LLM as evaluator results\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if df_eval_results is not None:\n",
    "    df_eval_results = df_eval_results.apply(\n",
    "        lambda r: quantitative_verdict_cosine_similarity_decision(r), axis=1\n",
    "    )\n",
    "df_eval_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send the raw results as a csv file to the S3 bucket\n",
    "csv_buffer = io.StringIO()\n",
    "df_eval_results.to_csv(csv_buffer, index=False)\n",
    "eval_llm_as_a_judge_results = csv_buffer.getvalue()\n",
    "eval_results_csv_fpath = os.path.join(\n",
    "    METRICS_DIR, MODEL_EVAL_COMPLETIONS_CSV\n",
    ")  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(\n",
    "    eval_llm_as_a_judge_results,\n",
    "    BUCKET_NAME,\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    MODEL_EVAL_COMPLETIONS_CSV,\n",
    ")\n",
    "logger.info(\n",
    "    f\"Per PoLL model responses saved as a csv to s3://{BUCKET_NAME}/{eval_results_csv_fpath}\"\n",
    ")\n",
    "df_eval_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"Total number of evaluations that are done using different panel of LLM evaluators: {df_eval_results.shape[0]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate evaluation cost per LLM evaluator per candidate model\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the evaluation step, the evaluation cost is calculated. The cost for the input and output tokens processed per LLM evaluator for each evaluation for each candidate model is summed up to give a total cost for evaluating the dataset using each evaluator. The total cost is added up in the final model metrics step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_cost_df = (\n",
    "    df_eval_results.groupby(\"judge_model_id\")[\n",
    "        [\"total_cost\", \"prompt_token_count\", \"completion_token_count\"]\n",
    "    ]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "eval_cost_df = eval_cost_df.sort_values(\"total_cost\", ascending=False)\n",
    "eval_cost_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send the cost calculation for running each evaluator to s3. This CSV file contains the total cost (which is the\n",
    "# summation of the input and output tokens across all evaluations across all candidate models), the total prompt token counts\n",
    "# and the total completion token counts across the entire dataset\n",
    "try:\n",
    "    eval_cost_df = (\n",
    "        df_eval_results.groupby(\"judge_model_id\")[\n",
    "            [\"total_cost\", \"prompt_token_count\", \"completion_token_count\"]\n",
    "        ]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    eval_cost_df = eval_cost_df.sort_values(\"total_cost\", ascending=False)\n",
    "    eval_cost_df[\"total_cost\"] = round(eval_cost_df[\"total_cost\"], 4)\n",
    "    csv_buffer = io.StringIO()\n",
    "    eval_cost_df.to_csv(csv_buffer, index=False)\n",
    "    eval_cost_df_responses = csv_buffer.getvalue()\n",
    "    eval_cost_df_responses_fpath = os.path.join(METRICS_DIR, EVAL_COST_PER_JUDGE_MODEL)\n",
    "    write_to_s3(\n",
    "        eval_cost_df_responses, BUCKET_NAME, \"\", METRICS_DIR, EVAL_COST_PER_JUDGE_MODEL\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Cost calculations for running each LLM evaluator to evaluate candidate models is sent to s3://{BUCKET_NAME}/{eval_cost_df_responses_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not calculate the total cost for running each LLM evaluator to evaluate candidate models: {e}\"\n",
    "    )\n",
    "\n",
    "if eval_cost_df is not None:\n",
    "    eval_cost_df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Voting Results: Send the incorrect and correct responses to S3 separately in `CSV` files for downstream analytics for each model judge\n",
    "\n",
    "---\n",
    "\n",
    "In this portion of the step, we will send the model responses as CSV, txt files to s3 for further downstream processing and report generations\n",
    "\n",
    "1. We calculate the majority vote done using the verdicts from each panel of LLM judges\n",
    "\n",
    "1. Calculate the majority vote accuracy ranking for each candidate model, i.e., which candidate model ranked at the top using majority correct votes from panel of LLM evaluators and so on.\n",
    "\n",
    "1. Generate metrics on a final `candidate_model_accuracy` table containing insights into accuracy of a model per judge per candidate model as well as accuracy of that given model across all judges as per majority vote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For Majority Voting - all responses from the panel of LLM as evaluators are sent\n",
    "# to s3 as a csv file\n",
    "try:\n",
    "    logger.info(\n",
    "        f\"Method name is {method_name}, sending the correct and incorrect verdicts to s3\"\n",
    "    )\n",
    "    verdict_types: List[str] = [\n",
    "        \"incorrect\",\n",
    "        \"correct\",\n",
    "        \"needs_further_human_or_LLM_evaluation\",\n",
    "    ]\n",
    "    all_llm_eval_responses_df: Optional[pd.DataFrame] = None\n",
    "    # iterate through each of the verdict tupe and save each verdict type responses from each evaluator in different\n",
    "    # csv files. For example, a csv files containing only incorrect verdicts from all model judges, whereas another\n",
    "    # csv file containing only the correct verdicts.\n",
    "    for verdict in verdict_types:\n",
    "        df_verdicts = df_eval_results[df_eval_results[\"verdict\"] == verdict]\n",
    "        all_llm_eval_responses_df = pd.concat(\n",
    "            [all_llm_eval_responses_df, df_verdicts], ignore_index=True\n",
    "        )\n",
    "        if not df_verdicts.empty:\n",
    "            csv_buffer = io.StringIO()\n",
    "            df_verdicts.to_csv(csv_buffer, index=False)\n",
    "            verdict_responses = csv_buffer.getvalue()\n",
    "            verdict_file = (\n",
    "                INCORRECT_VERDICT_RESPONSES_FILE\n",
    "                if verdict == \"incorrect\"\n",
    "                else (\n",
    "                    CORRECT_VERDICT_RESPONSES_FILE\n",
    "                    if verdict == \"correct\"\n",
    "                    else NEEDS_FURTHER_EVAL_FILE\n",
    "                )\n",
    "            )\n",
    "            verdict_responses_fpath = os.path.join(METRICS_DIR, verdict_file)\n",
    "            write_to_s3(verdict_responses, BUCKET_NAME, \"\", METRICS_DIR, verdict_file)\n",
    "            logger.info(\n",
    "                f\"{verdict.capitalize()} verdict responses sent to s3://{BUCKET_NAME}/{verdict_responses_fpath}\"\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Number of {verdict} responses in total: {df_verdicts.shape[0]}\"\n",
    "            )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error encountered while writing the evaluation responses to s3: {e}\")\n",
    "    all_llm_eval_responses_df = None\n",
    "\n",
    "all_llm_eval_responses_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the number of unique judges\n",
    "num_judge_models: int = len(all_llm_eval_responses_df.judge_model_id.unique())\n",
    "logger.info(f\"there are {num_judge_models} LLM judge models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For Majority Voting - send all incorrect and correct verdicts as txt files to s3 for readability purposes\n",
    "try:\n",
    "    logger.info(\n",
    "        f\"Method name is {method_name}, sending the correct and incorrect verdicts to s3\"\n",
    "    )\n",
    "    verdict_types: List[str] = [\n",
    "        \"incorrect\",\n",
    "        \"correct\",\n",
    "        \"needs_further_human_or_LLM_evaluation\",\n",
    "    ]\n",
    "    judge_model_ids = df_eval_results[\"judge_model_id\"].unique()\n",
    "    # save each judge model's correct and incorrect verdict files as txt files\n",
    "    # for downstream analytics and readability purposes\n",
    "    for judge_model_id in judge_model_ids:\n",
    "        for verdict in verdict_types:\n",
    "            df_judge_verdict = df_eval_results[\n",
    "                (df_eval_results[\"verdict\"] == verdict)\n",
    "                & (df_eval_results[\"judge_model_id\"] == judge_model_id)\n",
    "            ]\n",
    "            if not df_judge_verdict.empty:\n",
    "                txt_buffer = io.StringIO()\n",
    "                for index, row in df_judge_verdict.iterrows():\n",
    "                    txt_buffer.write(\n",
    "                        f\"candidate model: {row['candidate_model']}\\n\"\n",
    "                        f\"Question: {row['question']}\\n\"\n",
    "                        f\"candidate model response: {row['candidate_model_response']}\\n\"\n",
    "                        f\"ground truth: {row['ground_truth']}\\n\"\n",
    "                        f\"verdict: {row['verdict']}\\n\"\n",
    "                        f\"explanation: {row['explanation']}\\n\"\n",
    "                        f\"cosine similarity: {row['cosine_similarity_score']}\\n\\n\"\n",
    "                    )\n",
    "                judge_verdict_responses = txt_buffer.getvalue()\n",
    "                verdict_file = f\"{judge_model_id}_{verdict}_verdicts_evaluation.txt\"\n",
    "                judge_verdict_responses_fpath = os.path.join(METRICS_DIR, verdict_file)\n",
    "                write_to_s3(\n",
    "                    judge_verdict_responses, BUCKET_NAME, \"\", METRICS_DIR, verdict_file\n",
    "                )\n",
    "                logger.info(\n",
    "                    f\"{verdict.capitalize()} verdict responses for judge {judge_model_id} saved to s3://{BUCKET_NAME}/{judge_verdict_responses_fpath}\"\n",
    "                )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error encountered while writing the evaluation responses to s3: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the overall quantitate metrics of each model scored by the PoLL\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mean cosine similarity score, levenshtein distance and token set ratio\n",
    "try:\n",
    "    panel_summary_responses_df = (\n",
    "        df_eval_results.groupby([\"judge_model_id\", \"candidate_model\", \"verdict\"])\n",
    "        .agg(\n",
    "            count=(\"verdict\", \"size\"),\n",
    "            mean_cosine_similarity=(\"cosine_similarity_score\", \"mean\"),\n",
    "        )\n",
    "        .unstack(fill_value=0)\n",
    "        .stack()\n",
    "        .reset_index()\n",
    "    )\n",
    "    csv_buffer = io.StringIO()\n",
    "    panel_summary_responses_df.to_csv(csv_buffer, index=False)\n",
    "    panel_summary_responses = csv_buffer.getvalue()\n",
    "    llm_as_a_judge_per_eval_summary_fpath = os.path.join(\n",
    "        METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES\n",
    "    )\n",
    "    write_to_s3(\n",
    "        panel_summary_responses,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        LLM_JUDGE_PANEL_RESPONSE_SUMMARIES,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Summary on each eval (Majority voting) for each panel judge sent to s3://{BUCKET_NAME}/{llm_as_a_judge_per_eval_summary_fpath}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"View information on the accuracy metrics: {panel_summary_responses_df.head()}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not calculate the overall accuracy metrics for Majority Voting: {e}\"\n",
    "    )\n",
    "panel_summary_responses_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def majority_vote(row):\n",
    "    \"\"\"\n",
    "    This function calculates the majority vote based on whether the candidate model response is correct or incorrect\n",
    "    based on the vote from the panel of judges. It only returns 'correct' if there are more 'correct' votes than 'incorrect'\n",
    "    and 'NaN' values combined, and similarly for 'incorrect'. Otherwise, it returns 'no_majority_vote'.\n",
    "    \"\"\"\n",
    "    verdict_columns = [col for col in row.index if col.endswith(\"_verdict\")]\n",
    "    # find majority vote\n",
    "    verdicts = [row[c] for c in verdict_columns]\n",
    "    majority_vote = mode(verdicts)\n",
    "    return majority_vote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the majority voting pivot table along with the majority vote decision\n",
    "try:\n",
    "    majority_vote_pivoted_df = df_eval_results.pivot_table(\n",
    "        index=[\"question\", \"candidate_model\", \"payload_file\"],\n",
    "        columns=\"judge_model_id\",\n",
    "        values=[\"verdict\"],\n",
    "        aggfunc=\"first\",\n",
    "    )\n",
    "\n",
    "    majority_vote_pivoted_df.columns = [\n",
    "        f\"{judge_model}_{col}\" for col, judge_model in majority_vote_pivoted_df.columns\n",
    "    ]\n",
    "    majority_vote_pivoted_df.reset_index(inplace=True)\n",
    "    majority_vote_pivoted_df[\"majority_vote\"] = majority_vote_pivoted_df.apply(\n",
    "        majority_vote, axis=1\n",
    "    )\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_pivoted_df.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_raw_results = csv_buffer.getvalue()\n",
    "    majority_vote_raw_results_metrics_fpath = os.path.join(\n",
    "        METRICS_DIR, MAJORITY_VOTE_DF_RAW_RESULTS_FILE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        majority_vote_raw_results,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        MAJORITY_VOTE_DF_RAW_RESULTS_FILE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Majority results file containing raw results sent to s3://{BUCKET_NAME}/{majority_vote_raw_results_metrics_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate the raw responses for Majority Voting: {e}\")\n",
    "\n",
    "majority_vote_pivoted_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the total number of correct and incorrect count for the model for each payload file\n",
    "majority_vote_data_df_per_payload = pd.DataFrame()\n",
    "majority_vote_data_df_per_payload[\"correct_count\"] = majority_vote_pivoted_df.groupby(\n",
    "    [\"candidate_model\", \"payload_file\"]\n",
    ")[\"majority_vote\"].apply(lambda x: (x == \"correct\").sum())\n",
    "majority_vote_data_df_per_payload[\"incorrect_count\"] = majority_vote_pivoted_df.groupby(\n",
    "    [\"candidate_model\", \"payload_file\"]\n",
    ")[\"majority_vote\"].apply(lambda x: (x == \"incorrect\").sum())\n",
    "majority_vote_data_df_per_payload.reset_index(inplace=True)\n",
    "majority_vote_data_df_per_payload.sort_values(by=\"correct_count\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy as per the Majority Vote per Payload file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the accuracy of the model based on majority voting\n",
    "if (\n",
    "    \"correct_count\"\n",
    "    and \"incorrect_count\"\n",
    "    in majority_vote_data_df_per_payload.sort_values(\n",
    "        by=\"correct_count\", ascending=False\n",
    "    ).columns\n",
    "):\n",
    "    majority_vote_data_df_per_payload[\"majority_voting_accuracy\"] = round(\n",
    "        (\n",
    "            majority_vote_data_df_per_payload[\"correct_count\"]\n",
    "            / (\n",
    "                majority_vote_data_df_per_payload[\"correct_count\"]\n",
    "                + majority_vote_data_df_per_payload[\"incorrect_count\"]\n",
    "            )\n",
    "        )\n",
    "        * 100,\n",
    "        2,\n",
    "    )\n",
    "\n",
    "majority_vote_data_df_per_payload.sort_values(\n",
    "    by=\"majority_voting_accuracy\", ascending=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the per candidate model accuracy per judge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the df on judge model id, candidate model and verdict, and then calculate the accuracy of each judge model\n",
    "df_per_model_accuracy_counts_df = (\n",
    "    df_eval_results.groupby(\n",
    "        [\"judge_model_id\", \"candidate_model\", \"payload_file\", \"verdict\"]\n",
    "    )\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# get the accuracy for each candidate model\n",
    "df_per_model_accuracy_counts_df[\"accuracy\"] = (\n",
    "    df_per_model_accuracy_counts_df.get(\"correct\", 0)\n",
    "    / (\n",
    "        df_per_model_accuracy_counts_df.get(\"incorrect\", 0)\n",
    "        + df_per_model_accuracy_counts_df.get(\"needs_further_evaluation\", 0)\n",
    "        + df_per_model_accuracy_counts_df.get(\"correct\", 0)\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "\n",
    "df_per_model_accuracy_counts_df[\"accuracy\"] = round(\n",
    "    df_per_model_accuracy_counts_df[\"accuracy\"], 2\n",
    ")\n",
    "df_per_model_accuracy_counts_df.reset_index(inplace=True)\n",
    "df_per_model_accuracy_counts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_accuracy_df = df_per_model_accuracy_counts_df.pivot_table(\n",
    "    index=[\"candidate_model\", \"payload_file\"],\n",
    "    columns=\"judge_model_id\",\n",
    "    values=\"accuracy\",\n",
    ")\n",
    "overall_accuracy_df.reset_index(inplace=True)\n",
    "print(overall_accuracy_df.columns)\n",
    "overall_accuracy_df.columns = [\"candidate_model\", \"payload_file\"] + [\n",
    "    f\"judge_{col}_accuracy\" for col in overall_accuracy_df.columns[2:]\n",
    "]\n",
    "overall_accuracy_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge both panel voting and per model eval df to get all metrics together\n",
    "merged_accuracy_df = pd.merge(\n",
    "    overall_accuracy_df,\n",
    "    majority_vote_data_df_per_payload,\n",
    "    on=[\"candidate_model\", \"payload_file\"],\n",
    ")\n",
    "merged_accuracy_df = merged_accuracy_df.drop(\n",
    "    columns=[\"correct_count\", \"incorrect_count\"], axis=1\n",
    ")\n",
    "merged_accuracy_df = merged_accuracy_df.sort_values(\n",
    "    by=\"majority_voting_accuracy\", ascending=False\n",
    ")\n",
    "\n",
    "# Send the accuracy metrics to S3\n",
    "csv_buffer = io.StringIO()\n",
    "merged_accuracy_df.to_csv(csv_buffer, index=False)\n",
    "per_model_per_payload_accuracy_counts = csv_buffer.getvalue()\n",
    "per_model_per_payload_accuracy_counts_fpath = os.path.join(\n",
    "    METRICS_DIR, PER_PAYLOAD_MODEL_ACCURACY_MAJORITY_VOTING\n",
    ")\n",
    "\n",
    "write_to_s3(\n",
    "    per_model_per_payload_accuracy_counts,\n",
    "    BUCKET_NAME,\n",
    "    \"\",\n",
    "    METRICS_DIR,\n",
    "    PER_PAYLOAD_MODEL_ACCURACY_MAJORITY_VOTING,\n",
    ")\n",
    "logger.info(\n",
    "    f\"Per model per payload majority vote accuracy scores sent to s3://{BUCKET_NAME}/{per_model_per_payload_accuracy_counts_fpath}\"\n",
    ")\n",
    "merged_accuracy_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the majority voting accuracy per model\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the majority voting accuracy per model based on the number of correct and incorrect verdicts\n",
    "try:\n",
    "    majority_vote_data_df = pd.DataFrame()\n",
    "    majority_vote_data_df[\"correct_count\"] = majority_vote_pivoted_df.groupby(\n",
    "        \"candidate_model\"\n",
    "    )[\"majority_vote\"].apply(lambda x: (x == \"correct\").sum())\n",
    "    majority_vote_data_df[\"incorrect_count\"] = majority_vote_pivoted_df.groupby(\n",
    "        \"candidate_model\"\n",
    "    )[\"majority_vote\"].apply(lambda x: (x == \"incorrect\").sum())\n",
    "    majority_vote_data_df.reset_index(inplace=True)\n",
    "    majority_vote_data_df.sort_values(by=\"correct_count\", ascending=False)\n",
    "\n",
    "    if \"correct_count\" and \"incorrect_count\" in majority_vote_data_df.columns:\n",
    "        majority_vote_data_df[\"majority_voting_accuracy\"] = round(\n",
    "            (\n",
    "                majority_vote_data_df[\"correct_count\"]\n",
    "                / (\n",
    "                    majority_vote_data_df[\"correct_count\"]\n",
    "                    + majority_vote_data_df[\"incorrect_count\"]\n",
    "                )\n",
    "            )\n",
    "            * 100,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    majority_vote_data_df = majority_vote_data_df.sort_values(\n",
    "        by=\"majority_voting_accuracy\", ascending=False\n",
    "    )\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_data_df.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_per_model_accuracy = csv_buffer.getvalue()\n",
    "    majority_vote_per_model_accuracy_metrics_fpath = os.path.join(\n",
    "        METRICS_DIR, PER_MODEL_ACCURACY_POLL\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        majority_vote_per_model_accuracy,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        PER_MODEL_ACCURACY_POLL,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Per model PoLL accuracy sent to to s3://{BUCKET_NAME}/{majority_vote_per_model_accuracy_metrics_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate per model PoLL accuracy: {e}\")\n",
    "\n",
    "majority_vote_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the majority vote per payload file\n",
    "try:\n",
    "    # Group by payload_file and candidate_model to calculate correct and incorrect counts\n",
    "    majority_vote_payload_df = (\n",
    "        majority_vote_pivoted_df.groupby([\"payload_file\", \"candidate_model\"])[\n",
    "            \"majority_vote\"\n",
    "        ]\n",
    "        .apply(lambda x: (x == \"correct\").sum())\n",
    "        .reset_index(name=\"correct_count\")\n",
    "    )\n",
    "    majority_vote_payload_df[\"incorrect_count\"] = (\n",
    "        majority_vote_pivoted_df.groupby([\"payload_file\", \"candidate_model\"])[\n",
    "            \"majority_vote\"\n",
    "        ]\n",
    "        .apply(lambda x: (x == \"incorrect\").sum())\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if \"correct_count\" and \"incorrect_count\" in majority_vote_data_df.columns:\n",
    "        majority_vote_payload_df[\"majority_voting_accuracy\"] = round(\n",
    "            (\n",
    "                majority_vote_payload_df[\"correct_count\"]\n",
    "                / (\n",
    "                    majority_vote_payload_df[\"correct_count\"]\n",
    "                    + majority_vote_payload_df[\"incorrect_count\"]\n",
    "                )\n",
    "            )\n",
    "            * 100,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    # Sort by accuracy for better readability\n",
    "    majority_vote_payload_df = majority_vote_payload_df.sort_values(\n",
    "        by=\"majority_voting_accuracy\", ascending=False\n",
    "    )\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_payload_df.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_per_payload_accuracy = csv_buffer.getvalue()\n",
    "    majority_vote_per_payload_accuracy_metrics_fpath = os.path.join(\n",
    "        METRICS_DIR, PER_PAYLOAD_PER_MODEL_POLL_ACCURACY\n",
    "    )\n",
    "    write_to_s3(\n",
    "        majority_vote_per_payload_accuracy,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        PER_PAYLOAD_PER_MODEL_POLL_ACCURACY,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Per payload file accuracy sent to s3://{BUCKET_NAME}/{majority_vote_per_payload_accuracy_metrics_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate per payload file accuracy: {e}\")\n",
    "\n",
    "majority_vote_payload_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the per candidate model accuracy per panel of LLM evaluator\n",
    "try:\n",
    "    df_per_model_accuracy_counts_df = (\n",
    "        df_eval_results.groupby([\"judge_model_id\", \"candidate_model\", \"verdict\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    # get the accuracy for each candidate model\n",
    "    df_per_model_accuracy_counts_df[\"accuracy\"] = (\n",
    "        df_per_model_accuracy_counts_df.get(\"correct\", 0)\n",
    "        / (\n",
    "            df_per_model_accuracy_counts_df.get(\"incorrect\", 0)\n",
    "            + df_per_model_accuracy_counts_df.get(\"needs_further_evaluation\", 0)\n",
    "            + df_per_model_accuracy_counts_df.get(\"correct\", 0)\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    df_per_model_accuracy_counts_df[\"accuracy\"] = round(\n",
    "        df_per_model_accuracy_counts_df[\"accuracy\"], 2\n",
    "    )\n",
    "    df_per_model_accuracy_counts_df.reset_index(inplace=True)\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    df_per_model_accuracy_counts_df.to_csv(csv_buffer, index=False)\n",
    "    df_per_model_accuracy_counts = csv_buffer.getvalue()\n",
    "    df_per_model_accuracy_counts_metrics_fpath = os.path.join(\n",
    "        METRICS_DIR, PER_MODEL_ACCURACY_PER_EVAL_JUDGE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        df_per_model_accuracy_counts,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        PER_MODEL_ACCURACY_PER_EVAL_JUDGE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Per model accuracy per eval judge sent to s3://{BUCKET_NAME}/{df_per_model_accuracy_counts_metrics_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate per model accuracy per eval judge: {e}\")\n",
    "\n",
    "df_per_model_accuracy_counts_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the summary table\n",
    "\n",
    "---\n",
    "\n",
    "Fetch the summary table containing the per judge accuracy per candidate model and the per model accuracy based on majority vote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the per candidate model accuracy per panel of LLM evaluator\n",
    "try:\n",
    "    overall_accuracy_df = df_per_model_accuracy_counts_df.pivot_table(\n",
    "        index=\"candidate_model\", columns=\"judge_model_id\", values=\"accuracy\"\n",
    "    )\n",
    "    overall_accuracy_df.reset_index(inplace=True)\n",
    "    overall_accuracy_df.columns = [\"candidate_model\"] + [\n",
    "        f\"judge_{col}_accuracy\" for col in overall_accuracy_df.columns[1:]\n",
    "    ]\n",
    "\n",
    "    merged_accuracy_df = pd.merge(\n",
    "        overall_accuracy_df, majority_vote_data_df, on=\"candidate_model\"\n",
    "    )\n",
    "    merged_accuracy_df = merged_accuracy_df.drop(\n",
    "        columns=[\"correct_count\", \"incorrect_count\"], axis=1\n",
    "    )\n",
    "    merged_accuracy_df = merged_accuracy_df.sort_values(\n",
    "        by=\"majority_voting_accuracy\", ascending=False\n",
    "    )\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    merged_accuracy_df.to_csv(csv_buffer, index=False)\n",
    "    merged_accuracy_df_val = csv_buffer.getvalue()\n",
    "    merged_accuracy_df_metrics_fpath = os.path.join(\n",
    "        METRICS_DIR, CANDIDATE_MODEL_ACCURACY_FILE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        merged_accuracy_df_val,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        CANDIDATE_MODEL_ACCURACY_FILE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Per model accuracy per eval judge sent to s3://{BUCKET_NAME}/{merged_accuracy_df_metrics_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate per model accuracy per eval judge: {e}\")\n",
    "\n",
    "merged_accuracy_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Verdict Type: Overlap Analysis\n",
    "\n",
    "---\n",
    "\n",
    "In this portion, we check the `final verdict type`. This generates a verdict which is categorized into the following 4 main parts:\n",
    "\n",
    "1. correct_by_unanimous_decision: If all the panel of LLM judges evalaute a candidate model response as `correct`, then the final verdict is correct by unanimous decision.\n",
    "\n",
    "1. incorrect_by_unanimous_decision: If all the panel of LLM judges evalaute a candidate model response as `incorrect`, then the final verdict is incorrect by unanimous decision.\n",
    "\n",
    "1. correct_by_majority_vote_w_disagreement: If the panel of LLMs have diverse verdicts, but the majority vote is correct for a given candidate model response, then the final verdict is correct_by_majority_vote_w_disagreement.\n",
    "\n",
    "1. incorrect_by_majority_vote_w_disagreement: If the panel of LLMs have diverse verdicts, but the majority vote is incorrect for a given candidate model response, then the final verdict is incorrect_by_majority_vote_w_disagreement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_overlap_of_PoLL(row):\n",
    "    \"\"\"\n",
    "    This function checks how many judges overlapped in rating responses from the candidate model on questions\n",
    "    \"correctly\" and how many did not overlap (where one judge said correct and another said incorrect).\n",
    "    It only returns 'all_correct' if all columns ending with '_verdict' have a 'correct' value (i.e., no NaNs or incorrect votes).\n",
    "    \"\"\"\n",
    "    # Filter columns that end with '_verdict'\n",
    "    verdict_columns = [col for col in row.index if col.endswith(\"_verdict\")]\n",
    "\n",
    "    # Initialize the counts based on the filtered columns\n",
    "    correct_count = (row[verdict_columns] == \"correct\").sum()\n",
    "    incorrect_count = (row[verdict_columns] == \"incorrect\").sum()\n",
    "    nan_count = row[verdict_columns].isna().sum()\n",
    "    # check for when all models rate\n",
    "    total_judges: int = len(verdict_columns)\n",
    "\n",
    "    # Determine the overlap based on the counts\n",
    "    if correct_count == total_judges:\n",
    "        return \"correct_by_unanimous_decision\"\n",
    "    elif incorrect_count == total_judges:\n",
    "        return \"incorrect_by_unanimous_decision\"\n",
    "    elif row[\"majority_vote\"] == \"correct\":\n",
    "        return f\"correct_by_majority_vote_w_{incorrect_count+nan_count}_dissagreement\"\n",
    "    elif row[\"majority_vote\"] == \"incorrect\":\n",
    "        return f\"incorrect_by_majority_vote_w_{correct_count+nan_count}_dissagreement\"\n",
    "    else:\n",
    "        return \"no_overlaps\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    majority_vote_pivoted_df[\"verdict_type\"] = majority_vote_pivoted_df.apply(\n",
    "        check_overlap_of_PoLL, axis=1\n",
    "    )\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_pivoted_df.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_pivoted_final_verdict = csv_buffer.getvalue()\n",
    "    majority_vote_pivoted_final_verdict_fpath = os.path.join(\n",
    "        METRICS_DIR, PER_MODEL_ACCURACY_W_VERDICT_TYPE_FILE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        majority_vote_pivoted_final_verdict,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        PER_MODEL_ACCURACY_W_VERDICT_TYPE_FILE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Majority vote data and final verdicts are sent to s3://{BUCKET_NAME}/{majority_vote_pivoted_final_verdict_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate majority vote data and final verdicts: {e}\")\n",
    "\n",
    "majority_vote_pivoted_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now calculate the verdict breakdown for correct responses and verdict breakdown for\n",
    "# incorrect responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for incorrect responses\n",
    "try:\n",
    "    majority_vote_df_for_incorrect_verdict_analysis = majority_vote_pivoted_df.copy()\n",
    "    majority_vote_df_for_incorrect_verdict_analysis = (\n",
    "        majority_vote_df_for_incorrect_verdict_analysis[\n",
    "            majority_vote_df_for_incorrect_verdict_analysis.majority_vote == \"incorrect\"\n",
    "        ][\"verdict_type\"]\n",
    "        .value_counts(normalize=True)\n",
    "        .rename_axis(\"verdict_type_breakdown_for_incorrect\")\n",
    "        .reset_index(name=\"counts\")\n",
    "    )\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_df_for_incorrect_verdict_analysis.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_pivoted_df_incorrect = csv_buffer.getvalue()\n",
    "    majority_vote_pivoted_df_incorrect_fpath = os.path.join(\n",
    "        METRICS_DIR, VERDICT_TYPE_BREAKDOWN_FOR_INCORRECT_FILE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        majority_vote_pivoted_df_incorrect,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        VERDICT_TYPE_BREAKDOWN_FOR_INCORRECT_FILE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Majority vote with incorrect verdict breakdown is sent to s3://{BUCKET_NAME}/{majority_vote_pivoted_df_incorrect_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not calculate majority vote with incorrect verdict breakdown: {e}\"\n",
    "    )\n",
    "\n",
    "majority_vote_df_for_incorrect_verdict_analysis.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for correct responses\n",
    "try:\n",
    "    majority_vote_df_for_correct_verdict_analysis = majority_vote_pivoted_df.copy()\n",
    "    majority_vote_df_for_correct_verdict_analysis = (\n",
    "        majority_vote_df_for_correct_verdict_analysis[\n",
    "            majority_vote_df_for_correct_verdict_analysis.majority_vote == \"correct\"\n",
    "        ][\"verdict_type\"]\n",
    "        .value_counts(normalize=True)\n",
    "        .rename_axis(\"verdict_type_breakdown_for_correct\")\n",
    "        .reset_index(name=\"counts\")\n",
    "    )\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    majority_vote_df_for_correct_verdict_analysis.to_csv(csv_buffer, index=False)\n",
    "    majority_vote_pivoted_df_correct = csv_buffer.getvalue()\n",
    "    majority_vote_pivoted_df_correct_fpath = os.path.join(\n",
    "        METRICS_DIR, VERDICT_TYPE_BREAKDOWN_FOR_CORRECT_FILE\n",
    "    )\n",
    "\n",
    "    write_to_s3(\n",
    "        majority_vote_pivoted_df_correct,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        VERDICT_TYPE_BREAKDOWN_FOR_CORRECT_FILE,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Majority vote with correct verdict breakdown is sent to s3://{BUCKET_NAME}/{majority_vote_pivoted_df_correct_fpath}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not calculate majority vote with correct verdict breakdown: {e}\"\n",
    "    )\n",
    "\n",
    "majority_vote_df_for_correct_verdict_analysis.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send all responses from the evaluation process to S3 as a txt file for further downstream processing and readability purposes\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Write all explanations to a file and send to S3\n",
    "    explanations_txt_buffer = io.StringIO()\n",
    "    for index, row in df_eval_results.iterrows():\n",
    "        explanations_txt_buffer.write(\n",
    "            f\"candidate model: {row['candidate_model']}\\n\"\n",
    "            f\"Question: {row['question']}\\n\"\n",
    "            f\"candidate model response: {row['candidate_model_response']}\\n\"\n",
    "            f\"ground truth: {row['ground_truth']}\\n\"\n",
    "            f\"verdict: {row['verdict']}\\n\"\n",
    "            f\"explanation: {row['explanation']}\\n\"\n",
    "            f\"cosine similarity: {row['cosine_similarity_score']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    explanations_txt_file_content = explanations_txt_buffer.getvalue()\n",
    "    explanations_fpath = os.path.join(METRICS_DIR, ALL_EVALUATIONS_IN_TXT)\n",
    "    write_to_s3(\n",
    "        explanations_txt_file_content,\n",
    "        BUCKET_NAME,\n",
    "        \"\",\n",
    "        METRICS_DIR,\n",
    "        ALL_EVALUATIONS_IN_TXT,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"All text eval content from the llm judge panelists sent to s3://{BUCKET_NAME}/{explanations_fpath}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"All of the content including the candidate model responses, ground truth, evaluation are written: {explanations_txt_file_content}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Could not calculate the overall accuracy metrics for Majority Voting: {e}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
