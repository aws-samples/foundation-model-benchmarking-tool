{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics Analysis for LLaMa-2 benchmarking\n",
    "---------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "### This part of our solution design includes the chunk of taking the metrics generated and creating visualizations from it for further analysis to make decisions more quicker, efficient, and cost optimal.\n",
    "\n",
    "- In this file, we will go over and create side by side visualizations of different models deployed, how their inference latency is impacted based on the concurrency level, instance size and different model configurations. Using these visualizations and charts, making executive decisions, saving on time and cost becomes critical. \n",
    "\n",
    "\n",
    "- In this notebook, we will also record the error rates for each of the deployed model endpoints based on how it ran against different metrics as specified above. These visualizations will be applicable and work for any and every jumpstart and non jumpstart model if deployed correctly using the prior steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Import seaborn and other related libraries for visualizations and plotting charts\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from tomark import Tomark\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# rcParams for configuring Matplotlib settings\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# figure size in inches\n",
    "rcParams['figure.figsize'] = 10, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path_file: str = os.path.join(METADATA_DIR, METRICS_PATH_FNAME)\n",
    "METRICS_DIR: str = Path(metrics_path_file).read_text().strip()\n",
    "logger.info(f\"metrics_path_file={metrics_path_file}, METRICS_DIR={METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(METRICS_DIR, config[\"results\"][\"per_inference_request_file\"])\n",
    "logger.info(f\"File path containing the metrics per inference folder --> {file_path}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:    \n",
    "    file_content = get_s3_object(config['aws']['bucket'], file_path)\n",
    "    # Use pandas to read the CSV content\n",
    "    df_per_inference = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(f\"{file_path} read into dataframe of shape {df_per_inference.shape}\")\n",
    "    df_per_inference.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between prompt token length and inference latency for different instances and concurrency levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a column in the dataframe for clarity of the instance parameter of the model used\n",
    "df_per_inference = df_per_inference.rename(columns={\"instance_type\": \"instance\"})\n",
    "\n",
    "\n",
    "# This created a FacetGrid for plotting multiple scatter plots based on 'instance' and 'concurrency' categories\n",
    "g = sns.FacetGrid(df_per_inference, col=\"instance\", row=\"concurrency\", hue=\"instance\", height=3.5, aspect=1.25)\n",
    "\n",
    "## Subtitle of the facetgrid\n",
    "g.fig.suptitle(\"Effect of token length on inference latency\")\n",
    "\n",
    "# # This will map a scatterplot to the FacetGrid for each subset of the data\n",
    "sns_plot = g.map(sns.scatterplot, \"prompt_tokens\", \"latency\")\n",
    "\n",
    "# Set the y-axis label for all plots\n",
    "g = g.set_ylabels(\"Latency (seconds)\")\n",
    "\n",
    "# Y-axis ticks based on the maximum latency value and setting them in that manner\n",
    "yticks: List = list(range(0, (int(df_per_inference.latency.max())//10+2)*10, 5))\n",
    "g = g.set(yticks=yticks)\n",
    "\n",
    "# Set the x-axis label for all plots as the prompt length or tokens\n",
    "g = g.set_xlabels(\"Prompt length (tokens)\")\n",
    "\n",
    "# Create a bytes buffer to save the plot\n",
    "buffer = io.BytesIO()\n",
    "sns_plot.savefig(buffer, format='png')\n",
    "buffer.seek(0)  # Rewind buffer to the beginning\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, TOKENS_VS_LATENCY_PLOT_FNAME)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{TOKENS_VS_LATENCY_PLOT_FNAME}\")\n",
    "\n",
    "# Optionally, display the plot\n",
    "sns_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the all metrics file path and read it to generate visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_fpath = os.path.join(METRICS_DIR, config[\"results\"][\"all_metrics_file\"])\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    file_content = get_s3_object(BUCKET_NAME, all_metrics_fpath)\n",
    "\n",
    "    # Use pandas to read the CSV content\n",
    "    df_all_metrics = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(f\"{all_metrics_fpath} read into dataframe of shape {df_all_metrics.shape}\")\n",
    "    df_all_metrics.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n",
    "df_all_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## displaying all of the available columns in the all metrics dataframe\n",
    "df_all_metrics.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the number of experiment names within the metrics dataframe, instance types and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = df_all_metrics.experiment_name.unique()\n",
    "instance_types = df_all_metrics.instance_type.unique()\n",
    "model_names = df_all_metrics.ModelName.unique()\n",
    "logger.info(f\"contains information about {len(experiments)} experiments, {len(instance_types)} instance types, {len(model_names)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract some of the columns\n",
    "relevant_cols = ['experiment_name',\n",
    "                   'payload_file',\n",
    "                     'instance_type',\n",
    "                       'concurrency',\n",
    "                         'error_rate',\n",
    "                           'prompt_token_count_mean',\n",
    "                             'prompt_token_throughput',\n",
    "                               'completion_token_count_mean',\n",
    "                                 'completion_token_throughput',\n",
    "                                   'latency_mean',\n",
    "                                     'transactions_per_minute']\n",
    "\n",
    "## initialize a group by columns to use further in generating portions of the dataframe and filtering it\n",
    "group_by_cols = ['experiment_name',\n",
    "                   'payload_file',\n",
    "                     'instance_type',\n",
    "                      'concurrency']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an 'experiment_counts.csv' to store metrics on experiment name, the payload file, concurrency and the total counts associated to that given experiment to visualize the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_all_metrics[group_by_cols].value_counts().reset_index()\n",
    "\n",
    "# Convert df_counts to CSV format\n",
    "csv_buffer = io.StringIO()\n",
    "df_counts.to_csv(csv_buffer, index=False)\n",
    "csv_data = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name and the S3 path\n",
    "COUNTS_FNAME = \"experiment_counts.csv\"\n",
    "counts_s3_path = os.path.join(METRICS_DIR, COUNTS_FNAME)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(csv_data, BUCKET_NAME, \"\", METRICS_DIR, COUNTS_FNAME)\n",
    "logger.info(f\"Counts DataFrame saved to s3://{BUCKET_NAME}/{counts_s3_path}\")\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the mean error rates for each experiment with different congifurations using the same columns of interest used in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_rates = df_all_metrics.groupby(group_by_cols).agg({'error_rate': 'mean'}).reset_index()\n",
    "df_error_rates = df_error_rates.round(2)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_error_rates.to_csv(csv_buffer, index=False)\n",
    "error_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name and the S3 path\n",
    "ERROR_RATES_FNAME: str = \"error_rates.csv\"\n",
    "counts_s3_path = os.path.join(METRICS_DIR, ERROR_RATES_FNAME)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(error_csv, BUCKET_NAME, \"\", METRICS_DIR, ERROR_RATES_FNAME)\n",
    "logger.info(f\"Error Counts DataFrame saved to s3://{BUCKET_NAME}/{counts_s3_path}\")\n",
    "\n",
    "df_error_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Inference error rates across different concurrency levels and instance types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_rates = df_error_rates.rename(columns={\"instance_type\": \"instance\", \"payload_file\": \"dataset\"})\n",
    "\n",
    "# Clean up the dataset names by removing json file extensions and prefixes\n",
    "df_error_rates.dataset = df_error_rates.dataset.map(lambda x: x.replace(\".jsonl\", \"\").replace(\"payload_\", \"\"))\n",
    "\n",
    "# this creates a facetGrid for plotting scatter plots based on 'instance' and 'dataset'\n",
    "g = sns.FacetGrid(df_error_rates, col=\"instance\", row=\"dataset\", hue=\"instance\", height=3.5, aspect=1.25)\n",
    "\n",
    "# Maps a scatterplot to the FacetGrid for each subset of the data\n",
    "sns_plot = g.map(sns.scatterplot, \"concurrency\", \"error_rate\")\n",
    "\n",
    "# Create a subtitle\n",
    "g.fig.suptitle(\"Inference error rates for different concurrency levels and instance types\")\n",
    "\n",
    "## Set x and y labels for this chart\n",
    "g = g.set_ylabels(\"Error rate (failed / total inferences)\")\n",
    "g = g.set_xlabels(\"Concurrency level\")\n",
    "\n",
    "sns_plot.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, ERROR_RATES_PLOT_FNAME)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{ERROR_RATES_PLOT_FNAME}\")\n",
    "\n",
    "## Display the plot \n",
    "sns_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for the df elements that have error rates above 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_rates_nz = df_error_rates[df_error_rates.error_rate > 0]\n",
    "df_error_rates_nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a dataframe to get the mean of the columns in consideration\n",
    "df_summary_metrics = df_all_metrics[relevant_cols].groupby(group_by_cols).mean().reset_index()\n",
    "\n",
    "# ugly way of doing this, will refactor this later (maybe)\n",
    "df_summary_metrics.fillna(PLACE_HOLDER, inplace=True)\n",
    "int_cols = ['prompt_token_count_mean', 'prompt_token_throughput', 'completion_token_count_mean', 'completion_token_throughput', 'transactions_per_minute']\n",
    "for ic in int_cols:\n",
    "    df_summary_metrics[ic] = df_summary_metrics[ic].astype(int)\n",
    "\n",
    "df_summary_metrics.replace(PLACE_HOLDER, np.nan, inplace=True)\n",
    "df_summary_metrics.latency_mean\t= df_summary_metrics.latency_mean.round(2)\n",
    "df_summary_metrics.error_rate\t= df_summary_metrics.error_rate.round(2)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics.to_csv(csv_buffer, index=False)\n",
    "summary_metrics_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Define the file name for S3 based on the original file path\n",
    "summary_file_name = all_metrics_fpath.replace(\"all_metrics\", \"all_metrics_summary\").split('/')[-1] \n",
    "summary_s3_path = os.path.join(METRICS_DIR, summary_file_name)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(summary_metrics_csv, BUCKET_NAME, \"\", METRICS_DIR, summary_file_name)\n",
    "logger.info(f\"Summary metrics DataFrame saved to s3://{BUCKET_NAME}/{summary_s3_path}\")\n",
    "\n",
    "df_summary_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_nz = df_summary_metrics[df_summary_metrics.error_rate == 0]\n",
    "logger.info(f\"there are {len(df_summary_metrics_nz)} entries out of {len(df_summary_metrics)} in the summary data for which error rate is 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = df_summary_metrics[df_summary_metrics.payload_file.str.contains(config['metrics']['dataset_of_interest'])]\n",
    "logger.info(f\"shape of dataframe with summary metrics for {config['metrics']['dataset_of_interest']} is {df_summary_metrics_dataset.shape}\")\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_dataset.to_csv(csv_buffer, index=False)\n",
    "metrics_dataset = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(metrics_dataset, BUCKET_NAME, \"\", METRICS_DIR, SUMMARY_METRICS_W_PRICING_FNAME)\n",
    "logger.info(f\"Summary metrics dataset saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{SUMMARY_METRICS_W_PRICING_FNAME}\")\n",
    "\n",
    "df_summary_metrics_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_metrics_for_dataset = df_all_metrics.rename(columns={\"instance_type\": \"instance\", \"payload_file\": \"dataset\"})\n",
    "df_all_metrics_for_dataset.dataset = df_all_metrics_for_dataset.dataset.map(lambda x: x.replace(\".jsonl\", \"\").replace(\"payload_\", \"\"))\n",
    "ds = config['metrics']['dataset_of_interest']\n",
    "df_all_metrics_for_dataset = df_all_metrics_for_dataset[df_all_metrics_for_dataset.dataset.str.contains(ds)]\n",
    "# df_all_metrics_for_dataset.concurrency = df_all_metrics_for_dataset.concurrency.astype(str)\n",
    "row_order = list(df_all_metrics_for_dataset[[\"instance\", \"latency_mean\"]].groupby(\"instance\").mean(\"latency_mean\").reset_index()[\"instance\"])\n",
    "print(row_order)\n",
    "sns_plot = sns.catplot(\n",
    "    data=df_all_metrics_for_dataset, x='concurrency', y='latency_mean',\n",
    "    col='instance', kind='box', col_wrap=len(row_order), hue=\"instance\", row_order=row_order #, height=4.5, aspect=1.25\n",
    ")\n",
    "sns_plot._legend.remove()\n",
    "sns_plot.fig.suptitle(f\"Effect of concurrency on inference latency for each instance type for the {ds} dataset\\n\\n\")\n",
    "sns_plot = sns_plot.set_ylabels(\"Latency (seconds)\")\n",
    "sns_plot = sns_plot.set_xlabels(\"Concurrency level\")\n",
    "sns_plot.fig.subplots_adjust(top=0.8)\n",
    "\n",
    "sns_plot.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", METRICS_DIR, CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{METRICS_DIR}/{CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pricing = pd.DataFrame.from_dict(config['pricing'], orient='index').reset_index()\n",
    "df_pricing.columns = ['instance_type', 'price_per_hour']\n",
    "# fpath: str = os.path.join(METRICS_DIR, INSTANCE_PRICING_PER_HOUR_FNAME)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_pricing.to_csv(csv_buffer, index=False)\n",
    "df_pricing_data = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(df_pricing_data, BUCKET_NAME, \"\", METRICS_DIR, INSTANCE_PRICING_PER_HOUR_FNAME)\n",
    "\n",
    "df_pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset = pd.merge(df_summary_metrics_dataset, df_pricing, how='left')\n",
    "\n",
    "df_summary_metrics_dataset['price_per_txn'] = (df_summary_metrics_dataset['price_per_hour']/60)/df_summary_metrics_dataset['transactions_per_minute']\n",
    "price_per_tx_wt = config['metrics']['weights']['price_per_tx_wt']\n",
    "latency_wt = config['metrics']['weights']['latenct_wt']\n",
    "#df_summary_metrics_dataset['score'] = price_per_tx_wt*(1/df_summary_metrics_dataset['price_per_txn']) + latency_wt*(1/df_summary_metrics_dataset['latency_mean'])\n",
    "df_summary_metrics_dataset['score'] = 0.5*(1/df_summary_metrics_dataset['price_per_txn']) + 0.5*(1/df_summary_metrics_dataset['latency_mean'])\n",
    "\n",
    "\"\"\"\n",
    "df_summary_metrics_dataset['rank'] = (df_summary_metrics_dataset.sort_values(by=\"score\", ascending=False)\n",
    "                      .groupby(['instance_type'])['concurrency']\n",
    "                      .rank(method='first', ascending=False)\n",
    "                   )\n",
    "\"\"\"\n",
    "df_summary_metrics_dataset = df_summary_metrics_dataset.sort_values(by=\"score\", ascending=False)\n",
    "file_path_df = os.path.join(METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME)\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_dataset.to_csv(file_path_df, index=False)\n",
    "summary_metrics_dataset_csv = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(summary_metrics_dataset_csv, config['aws']['bucket'], \"\", METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME)\n",
    "logger.info(f\"Summary metrics dataset saved to s3://{config['aws']['bucket']}/{METRICS_DIR}/{SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME}\")\n",
    "\n",
    "df_summary_metrics_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select the best option overall and for each instance type\n",
    "df_summary_metrics_dataset_overall = df_summary_metrics_dataset[df_summary_metrics_dataset.score == df_summary_metrics_dataset.score.max()]\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_dataset_overall.to_csv(csv_buffer, index=False)\n",
    "metrics_overall_data = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(metrics_overall_data, BUCKET_NAME, \"\", METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME)\n",
    "\n",
    "df_summary_metrics_dataset_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_metrics_dataset_overall = df_summary_metrics_dataset_overall.round(4)\n",
    "df_summary_metrics_dataset_overall.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df_summary_metrics_dataset.groupby(['instance_type']).score.idxmax()\n",
    "logger.info(f\"shape of df_summary_metrics_dataset={df_summary_metrics_dataset.shape}, idx={idx}\")\n",
    "df_summary_metrics_best_option_instance_type = df_summary_metrics_dataset.loc[idx]\n",
    "logger.info(f\"shape of df_summary_metrics_best_option_instance_type={df_summary_metrics_best_option_instance_type.shape}\")\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df_summary_metrics_best_option_instance_type.to_csv(csv_buffer, index=False)\n",
    "best_option = csv_buffer.getvalue()\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(best_option, BUCKET_NAME, \"\", METRICS_DIR, SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME)\n",
    "\n",
    "df_summary_metrics_best_option_instance_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_price_per_tx = df_summary_metrics_best_option_instance_type.price_per_txn.min()\n",
    "count: int = 1000\n",
    "multiplier: int = 10 if int(min_price_per_tx * count) == 0 else 1\n",
    "\n",
    "price_tx_col_name = f\"price_per_tx_{count*multiplier}_txn\"\n",
    "\n",
    "df_summary_metrics_best_option_instance_type[price_tx_col_name] = df_summary_metrics_best_option_instance_type.price_per_txn * 10000\n",
    "df_summary_metrics_best_option_instance_type[price_tx_col_name] = df_summary_metrics_best_option_instance_type[price_tx_col_name].astype(int)\n",
    "df_summary_metrics_best_option_instance_type = df_summary_metrics_best_option_instance_type.sort_values(by=price_tx_col_name)\n",
    "sns_plot = sns.barplot(df_summary_metrics_best_option_instance_type, x=\"instance_type\", y=price_tx_col_name)\n",
    "title: str = f\"Comparing performance of {config['general']['model_name']} across instance types for {config['metrics']['dataset_of_interest']} dataset\"\n",
    "sns_plot.set(xlabel=\"Instance type\", ylabel=f\"Cost per {count*multiplier} transactions (USD)\", title=title)\n",
    "num_instance_types = len(df_summary_metrics_dataset.instance_type.unique())\n",
    "for r in df_summary_metrics_best_option_instance_type.iterrows():\n",
    "    x = r[1]['instance_type']\n",
    "    if num_instance_types == 1:\n",
    "        v_shift = 0.1\n",
    "    else:\n",
    "        v_shift = 5 + 5/num_instance_types\n",
    "    v_shift = 0.1 # harcoded for now until we add better logic\n",
    "    print(f\"v_shift={v_shift}\")    \n",
    "    y = r[1][price_tx_col_name] + v_shift\n",
    "    text = f\"{r[1]['transactions_per_minute']} txn/min, {r[1]['latency_mean']}s per txn\"\n",
    "    print(f\"x={x}, y={y}, text={text}\")\n",
    "    sns_plot.text(x, y, text, \n",
    "       fontsize = 8,          # Size\n",
    "       #fontstyle = \"oblique\",  # Style\n",
    "       color = \"red\",          # Color\n",
    "       ha = \"center\", # Horizontal alignment\n",
    "       va = \"center\") # Vertical alignment \n",
    "\n",
    "business_summary_plot_fpath: str = os.path.join(METRICS_DIR, BUSINESS_SUMMARY_PLOT_FNAME)\n",
    "sns_plot.figure.savefig(buffer, format='png')\n",
    "buffer.seek(0)\n",
    "\n",
    "# Write the plot to S3\n",
    "write_to_s3(buffer.getvalue(), BUCKET_NAME, \"\", \"\", business_summary_plot_fpath)\n",
    "logger.info(f\"Plot saved to s3://{BUCKET_NAME}/{business_summary_plot_fpath}\")\n",
    "\n",
    "## Display the plot \n",
    "sns_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df_summary_metrics_best_option_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_csv_content_fpath = os.path.join(METRICS_DIR, SUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE)\n",
    "logger.info(f\"the cost information can be found in the csv file here -> {cost_csv_content_fpath}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    cost_content = get_s3_object(BUCKET_NAME, cost_csv_content_fpath)\n",
    "\n",
    "    # Use pandas to read the CSV content\n",
    "    df_cost_metrics = pd.read_csv(io.StringIO(cost_content))\n",
    "    logger.info(f\"{cost_csv_content_fpath} read into dataframe of shape {df_cost_metrics.shape}\")\n",
    "    df_cost_metrics.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "# df_cost_metrics.fillna('', inplace=True)\n",
    "\n",
    "df_cost_metrics.head()\n",
    "\n",
    "# Convert df_cost_metrics to Markdown table\n",
    "cost_mkdn_table = Tomark.table(df_cost_metrics.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUSINESS_SUMMARY: str = \"\"\"We did performance benchmarking for the `{model_name}` model on \"{instance_types}\" instance{plural} on multiple datasets and based on the test results the best price performance for dataset `{ds}` is provided by the `{selected_instance_type}` instance type.  {mkdn_table}\n",
    "\n",
    "The price performance comparison for different instance types is presented below:\n",
    "\n",
    "![Price performance comparison]({business_summary_plot_fpath})\n",
    "\n",
    "The configuration used for these tests is available in the [`config`]({cfg_file_path}) file.\n",
    "\n",
    "The cost to run each experiment is provided in the table below. The total cost for running all experiments is {total_cost_as_str}.\n",
    "\n",
    "{cost_table}\n",
    "\n",
    "\"\"\"\n",
    "transposed_list = []\n",
    "best_instance_type_info = df_summary_metrics_dataset_overall.to_dict(orient='records')[0]\n",
    "del best_instance_type_info[\"score\"]\n",
    "for k,v in best_instance_type_info.items():\n",
    "    transposed_list.append({\"Information\": k, \"Value\": v})\n",
    "mkdn_table = Tomark.table(transposed_list)\n",
    "\n",
    "\n",
    "plural = \"s\" if len(df_summary_metrics.instance_type.unique()) > 1 else \"\"\n",
    "instance_types_md = \", \".join([f\"`{it}`\" for it in df_summary_metrics.instance_type.unique()])\n",
    "selected_instance_type: str = df_summary_metrics_dataset_overall.to_dict(orient='records')[0]['instance_type']\n",
    "ds: str = config['metrics']['dataset_of_interest']\n",
    "\n",
    "business_summary: str = BUSINESS_SUMMARY.format(model_name=config['general']['model_name'],\n",
    "                                              instance_types=instance_types_md,\n",
    "                                              plural=plural,\n",
    "                                              ds=ds,\n",
    "                                              selected_instance_type=selected_instance_type,\n",
    "                                              mkdn_table=\"\\n\" + mkdn_table,\n",
    "                                              cfg_file_path=os.path.basename(CONFIG_FILE),\n",
    "                                              business_summary_plot_fpath=BUSINESS_SUMMARY_PLOT_FNAME,\n",
    "                                              cost_table=cost_mkdn_table,\n",
    "                                              total_cost_as_str=f\"${df_cost_metrics.cost.sum():.2f}\"\n",
    "                                              )\n",
    "business_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "dttm = str(datetime.utcnow())\n",
    "\n",
    "overall_results_md = OVERALL_RESULTS_MD.format(dttm=dttm,\n",
    "                                               business_summary=business_summary)\n",
    "results_group_cols: List[str] = ['instance_type', 'payload_file']\n",
    "result_rows: List[str] = []\n",
    "for row in df_summary_metrics[results_group_cols].drop_duplicates().iterrows():\n",
    "    instance_type = row[1]['instance_type']\n",
    "    dataset = row[1]['payload_file']\n",
    "    df_summary_metrics_nz_subset = df_summary_metrics_nz[(df_summary_metrics_nz.instance_type == instance_type) &\n",
    "                                                          (df_summary_metrics_nz.payload_file == dataset) &\n",
    "                                                           (df_summary_metrics_nz.latency_mean <= LATENCY_BUDGET)]\n",
    "    num_results = df_summary_metrics_nz_subset.shape[0]\n",
    "    result_row: Optional[str] = None\n",
    "    if num_results > 0:\n",
    "        logger.info(f\"there are {num_results} options to choose the best option from for instance_type={instance_type}, dataset={dataset}\")\n",
    "        df_summary_metrics_nz_subset_selected = df_summary_metrics_nz_subset[df_summary_metrics_nz_subset.concurrency == df_summary_metrics_nz_subset.concurrency.max()]\n",
    "        best = df_summary_metrics_nz_subset_selected.to_dict(orient='records')[0]\n",
    "        # logger.info(best)\n",
    "        result_desc = RESULT_DESC.format(latency_budget=LATENCY_BUDGET,\n",
    "                           instance_type=best['instance_type'],\n",
    "                           dataset=dataset,\n",
    "                           concurrency=best['concurrency'],\n",
    "                           latency_mean=best['latency_mean'],\n",
    "                           prompt_size=int(best['prompt_token_count_mean']),\n",
    "                           completion_size=int(best['completion_token_count_mean']),\n",
    "                           tpm=int(best['transactions_per_minute']))     \n",
    "        \n",
    "        # logger.info(result_desc)\n",
    "    else:\n",
    "        logger.info(f\"there are NO options to choose from for instance_type={instance_type}, dataset={dataset}\")\n",
    "        result_desc = RESULT_FAILURE_DESC.format(latency_budget=LATENCY_BUDGET,\n",
    "                           instance_type=best['instance_type'],\n",
    "                           dataset=dataset)\n",
    "    result_row: str = RESULT_ROW.format(instance_type=best['instance_type'],\n",
    "                                        dataset=dataset,\n",
    "                                        desc=result_desc)\n",
    "    result_rows.append(result_row)\n",
    "        \n",
    "    \n",
    "    #logger.info(f\"instance_type={row[0]}, payload_file={row[1]}\")\n",
    "overall_results_md += \"\\n\".join(result_rows)\n",
    "\n",
    "OVERALL_RESULTS_PLOTS_MD: str = \"\"\"\n",
    "\n",
    "## Plots\n",
    "\n",
    "The following plots provide insights into the results from the different experiments run.\n",
    "\n",
    "![{plot1_text}]({plot1_fname})\n",
    "\n",
    "![{plot2_text}]({plot2_fname})\n",
    "\n",
    "![{plot3_text}]({plot3_fname})\n",
    "\"\"\"\n",
    "\n",
    "overall_results_plots_md: str = OVERALL_RESULTS_PLOTS_MD.format(plot1_text=ERROR_RATES_PLOT_TEXT, \n",
    "                                                                plot1_fname=ERROR_RATES_PLOT_FNAME,\n",
    "                                                                plot2_text=TOKENS_VS_LATENCY_PLOT_TEXT, \n",
    "                                                                plot2_fname=TOKENS_VS_LATENCY_PLOT_FNAME,\n",
    "                                                                plot3_text=CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT, \n",
    "                                                                plot3_fname=CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME)\n",
    "\n",
    "overall_results_md += overall_results_plots_md\n",
    "\n",
    "fpath: str = os.path.join(METRICS_DIR, RESULTS_DESC_MD_FNAME)\n",
    "logger.info(f\"writing final markdown to {METRICS_DIR}\")\n",
    "Path(fpath).write_text(overall_results_md)\n",
    "logger.info(overall_results_md)\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(overall_results_md, BUCKET_NAME, \"\", METRICS_DIR, RESULTS_DESC_MD_FNAME)\n",
    "logger.info(f\"results.md file saved to to s3://{BUCKET_NAME}/{METRICS_DIR}/{RESULTS_DESC_MD_FNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the metrics and report files locally\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "logger.info(f\"going to download all metrics and reports from s3 into {RESULTS_DIR} directory\")\n",
    "download_multiple_files_from_s3(BUCKET_NAME, METRICS_DIR, RESULTS_DIR)\n",
    "import glob\n",
    "result_files = glob.glob(os.path.join(RESULTS_DIR, \"**\"), recursive=True)\n",
    "logger.info(\"\\n\".join([f for f in result_files]))"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
