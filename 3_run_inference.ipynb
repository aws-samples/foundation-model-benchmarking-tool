{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on all deployed endpoints: Various combinations of payloads, concurrency levels, model configurations\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of our solution design includes running inferences on all deployed model endpoints (with different configurations, concurrency levels and payload sizes). This notebook runs inferences in a manner that is calls endpoints concurrently and asychronously to generate responses and record metrics. Here are some of the key components:\n",
    "\n",
    "- **Accessing the deployed endpoints**, creating a predictor object for these endpoints to call them during inference time.\n",
    "\n",
    "- **Functions to define metrics**: This notebook sets stage for metrics to be recorded during the time of invocation of all these models for benchmarking purposes.\n",
    "\n",
    "- **Running Actual Inferences**: Once the metrics are defined, we set a blocker function that is responsible for creating inference on a single payload called get_inference. We then run a series of asynchronous functions that can be viewed in the code (link above), to create asychronous inferefences on the deployed models. The way we send requests are by creating combinations: this means creating combinations of payloads of different sizes that can be viewed in the config.yml file, with different concurrency levels (in this case we first go through all patches of payloads with a concurrency level of 1, then 2, and then 4). You can set this to your desired value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## auto reload all of the changes made in the config/globals.py file \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /opt/homebrew/share/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/madhurpt/Library/Application Support/sagemaker/config.yaml\n",
      "CONFIG_FILE=configs/config-mistral-7b-tgi-g5.yml\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import copy\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import itertools\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from globals import *\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker.predictor import Predictor\n",
    "from utils import load_config, count_tokens, write_to_s3, read_from_s3\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from typing import Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36myaml\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36menum\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Enum\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILEPATH_FILE: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig_filepath.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# S3 client initialization\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "s3_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILE: \u001b[36mstr\u001b[39;49;00m = Path(CONFIG_FILEPATH_FILE).read_text()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCONFIG_FILE=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mCONFIG_FILE\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(CONFIG_FILE, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file:\u001b[37m\u001b[39;49;00m\n",
      "    config = yaml.safe_load(file)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "DATA_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PROMPTS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mprompts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_INFERENCE_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_inference\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_CHUNK_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_chunk\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# MODELS_DIR = os.path.join(DATA_DIR, \"models\", config['general']['name'])\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "MODELS_DIR = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + \u001b[33m\"\u001b[39;49;00m\u001b[33m/models\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## DEFINE THE S3 PATH FOR ENDPOINTS TO READ FROM DURING RUN INFERENCE\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_S3_PATH = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mMODELS_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m/endpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Use this to upload to the s3 bucket (extracted from the config file)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "BUCKET_NAME = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mbucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## S3 prefix\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PREFIX_NAME = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## SOURCE data is where your actual data resides in s3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SOURCE_DATA = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33msource_data_bucket_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Read the prompt template that the user uploads\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PROMPT_TEMPLATE_S3_PREFIX = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprompt_template\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "DATASET_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "SCRIPTS_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mscripts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "DIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, DATASET_DIR, METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]\u001b[37m\u001b[39;49;00m\n",
      "TOKENIZER_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33mllama2_tokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# TOKENIZER_DIR_S3 = config['aws']['custom_tokenizer']\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "_ = \u001b[36mlist\u001b[39;49;00m(\u001b[36mmap\u001b[39;49;00m(\u001b[34mlambda\u001b[39;49;00m x: os.makedirs(x, exist_ok=\u001b[34mTrue\u001b[39;49;00m), DIR_LIST))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_LIST_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(MODELS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mendpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "REQUEST_PAYLOAD_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(PROMPTS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mpayload.jsonl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTRUNCATE_POLICY\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m, Enum):\u001b[37m\u001b[39;49;00m\n",
      "    AT_PROMPT_TOKEN_LENGTH = \u001b[33m'\u001b[39;49;00m\u001b[33mat-prompt-token-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# misc. metrics related\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PLACE_HOLDER: \u001b[36mint\u001b[39;49;00m = -\u001b[34m1705338041\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# metric filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "COUNTS_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mexperiment_counts.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_DESC_MD_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_W_PRICING_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_w_pricing.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "INSTANCE_PRICING_PER_HOUR_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33minstance_pricing_per_hour.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_w_scores.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option_each_instance_type.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "BUSINESS_SUMMARY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mbusiness_summary.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# plot filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mError rates for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mTokens vs latency for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mtokens_vs_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconcurrency_vs_inference_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mConcurrency Vs latency for different instance type for selected dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "LATENCY_BUDGET: \u001b[36mint\u001b[39;49;00m = \u001b[34m20\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "OVERALL_RESULTS_MD: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m# Results for performance benchmarking\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m**Last modified (UTC): \u001b[39;49;00m\u001b[33m{dttm}\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Summary\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m{business_summary}\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Per instance results\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33mThe following table provides the best combinations for running inference for different sizes prompts on different instance types.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|Dataset   | Instance type   | Recommendation   |\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|---|---|---|\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Dataset=`{dataset}`, instance_type=`{instance_type}`\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULT_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThe best option for staying within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset is a `concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m`. A concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m achieves an `average latency of \u001b[39;49;00m\u001b[33m{latency_mean}\u001b[39;49;00m\u001b[33m seconds`, for an `average prompt size of \u001b[39;49;00m\u001b[33m{prompt_size}\u001b[39;49;00m\u001b[33m tokens` and `completion size of \u001b[39;49;00m\u001b[33m{completion_size}\u001b[39;49;00m\u001b[33m tokens` with `\u001b[39;49;00m\u001b[33m{tpm}\u001b[39;49;00m\u001b[33m transactions/minute`.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_ROW: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33m|`\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m`|`\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m`|\u001b[39;49;00m\u001b[33m{desc}\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_FAILURE_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThis experiment did not find any combination of concurrency level and other configuration settings that could provide a response within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Config.yml file that contains information that is used across this benchmarking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:11,543] p91528 {635462509.py:2} INFO - {\n",
      "  \"general\": {\n",
      "    \"name\": \"mistral-7b-tgi-g5-v1\",\n",
      "    \"model_name\": \"mistral7b\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::218208277580:role/service-role/AmazonSageMaker-ExecutionRole-20230807T175994\",\n",
      "    \"bucket\": \"fmbt\",\n",
      "    \"prefix\": \"data\",\n",
      "    \"source_data_bucket_prefix\": \"source_data\",\n",
      "    \"prompt_template\": \"prompt_template/prompt_template.txt\",\n",
      "    \"custom_tokenizer\": \"tokenizer\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"template_file\": \"prompt_template.txt\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\"\n",
      "  },\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1,\n",
      "      \"max_length_in_tokens\": 500,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 500,\n",
      "      \"max_length_in_tokens\": 1000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1000,\n",
      "      \"max_length_in_tokens\": 2000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 2000,\n",
      "      \"max_length_in_tokens\": 3000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 3000,\n",
      "      \"max_length_in_tokens\": 4000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 305,\n",
      "      \"max_length_in_tokens\": 3997,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    }\n",
      "  ],\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_2000-3000\",\n",
      "    \"weights\": {\n",
      "      \"price_per_tx_wt\": 0.65,\n",
      "      \"latenct_wt\": 0.35\n",
      "    }\n",
      "  },\n",
      "  \"pricing\": {\n",
      "    \"ml.g5.2xlarge\": 1.515,\n",
      "    \"ml.g5.12xlarge\": 7.09,\n",
      "    \"ml.g5.24xlarge\": 10.18,\n",
      "    \"ml.g5.48xlarge\": 20.36,\n",
      "    \"ml.inf2.24xlarge\": 7.79,\n",
      "    \"ml.inf2.48xlarge\": 15.58,\n",
      "    \"ml.p4d.24xlarge\": 37.688\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.1,\n",
      "    \"top_p\": 0.92,\n",
      "    \"top_k\": 120,\n",
      "    \"max_new_tokens\": 100,\n",
      "    \"truncate\": \"at-prompt-token-length\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"huggingface-llm-mistral-7b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"mistral7b\",\n",
      "      \"ep_name\": \"lmistral7b-g5-2xlarge\",\n",
      "      \"instance_type\": \"ml.g5.2xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\",\n",
      "        \"payload_en_1000-2000.jsonl\",\n",
      "        \"payload_en_2000-3000.jsonl\",\n",
      "        \"payload_en_3000-4000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4,\n",
      "        6,\n",
      "        8\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"8191\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"8192\",\n",
      "        \"MAX_BATCH_PREFILL_TOKENS\": \"8191\",\n",
      "        \"SM_NUM_GPUS\": \"1\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"results\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting access to the s3 bucket where endpoints.json for different models resides\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the deployed model endpoints from the endpoints.json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:11,855] p91528 {1472297992.py:2} INFO - endpoint from --> s3://fmbt/data/models/mistral-7b-tgi-g5-v1/endpoints.json\n",
      "[2024-01-26 15:57:11,856] p91528 {1472297992.py:8} INFO - Found information for 1 endpoints in the respective S3 path\n",
      "[2024-01-26 15:57:11,857] p91528 {1472297992.py:9} INFO - [\n",
      "  {\n",
      "    \"experiment_name\": \"mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "    \"endpoint\": {\n",
      "      \"EndpointName\": \"lmistral7b-g5-2xlarge-1706302351\",\n",
      "      \"EndpointArn\": \"arn:aws:sagemaker:us-east-1:218208277580:endpoint/lmistral7b-g5-2xlarge-1706302351\",\n",
      "      \"EndpointConfigName\": \"lmistral7b-g5-2xlarge-1706302351\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"AllTraffic\",\n",
      "          \"DeployedImages\": [\n",
      "            {\n",
      "              \"SpecifiedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "              \"ResolvedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb\",\n",
      "              \"ResolutionTime\": \"2024-01-26 15:52:34.019000-05:00\"\n",
      "            }\n",
      "          ],\n",
      "          \"CurrentWeight\": 1.0,\n",
      "          \"DesiredWeight\": 1.0,\n",
      "          \"CurrentInstanceCount\": 1,\n",
      "          \"DesiredInstanceCount\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"EndpointStatus\": \"InService\",\n",
      "      \"CreationTime\": \"2024-01-26 15:52:33.043000-05:00\",\n",
      "      \"LastModifiedTime\": \"2024-01-26 15:56:33.206000-05:00\",\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"7a48742f-bc99-4eed-9b1f-6664b8abd29b\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"7a48742f-bc99-4eed-9b1f-6664b8abd29b\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"809\",\n",
      "          \"date\": \"Fri, 26 Jan 2024 20:56:33 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"endpoint_config\": {\n",
      "      \"EndpointConfigName\": \"lmistral7b-g5-2xlarge-1706302351\",\n",
      "      \"EndpointConfigArn\": \"arn:aws:sagemaker:us-east-1:218208277580:endpoint-config/lmistral7b-g5-2xlarge-1706302351\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"AllTraffic\",\n",
      "          \"ModelName\": \"hf-llm-mistral-7b-2024-01-26-20-52-31-041\",\n",
      "          \"InitialInstanceCount\": 1,\n",
      "          \"InstanceType\": \"ml.g5.2xlarge\",\n",
      "          \"InitialVariantWeight\": 1.0,\n",
      "          \"ModelDataDownloadTimeoutInSeconds\": 1200,\n",
      "          \"ContainerStartupHealthCheckTimeoutInSeconds\": 1200\n",
      "        }\n",
      "      ],\n",
      "      \"CreationTime\": \"2024-01-26 15:52:32.570000-05:00\",\n",
      "      \"EnableNetworkIsolation\": false,\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"cc479efc-0434-4dba-b824-d1d64e253535\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"cc479efc-0434-4dba-b824-d1d64e253535\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"514\",\n",
      "          \"date\": \"Fri, 26 Jan 2024 20:56:33 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_config\": {\n",
      "      \"ModelName\": \"hf-llm-mistral-7b-2024-01-26-20-52-31-041\",\n",
      "      \"PrimaryContainer\": {\n",
      "        \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "        \"Mode\": \"SingleModel\",\n",
      "        \"ModelDataSource\": {\n",
      "          \"S3DataSource\": {\n",
      "            \"S3Uri\": \"s3://jumpstart-cache-prod-us-east-1/huggingface-llm/huggingface-llm-mistral-7b/artifacts/inference-prepack/v1.0.0/\",\n",
      "            \"S3DataType\": \"S3Prefix\",\n",
      "            \"CompressionType\": \"None\",\n",
      "            \"ModelAccessConfig\": {\n",
      "              \"AcceptEula\": true\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"Environment\": {\n",
      "          \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "          \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "          \"MAX_BATCH_PREFILL_TOKENS\": \"8191\",\n",
      "          \"MAX_INPUT_LENGTH\": \"8191\",\n",
      "          \"MAX_TOTAL_TOKENS\": \"8192\",\n",
      "          \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "          \"SAGEMAKER_ENV\": \"1\",\n",
      "          \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "          \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "          \"SM_NUM_GPUS\": \"1\"\n",
      "        }\n",
      "      },\n",
      "      \"ExecutionRoleArn\": \"arn:aws:iam::218208277580:role/service-role/AmazonSageMaker-ExecutionRole-20230807T175994\",\n",
      "      \"CreationTime\": \"2024-01-26 15:52:31.867000-05:00\",\n",
      "      \"ModelArn\": \"arn:aws:sagemaker:us-east-1:218208277580:model/hf-llm-mistral-7b-2024-01-26-20-52-31-041\",\n",
      "      \"EnableNetworkIsolation\": true,\n",
      "      \"DeploymentRecommendation\": {\n",
      "        \"RecommendationStatus\": \"COMPLETED\",\n",
      "        \"RealTimeInferenceRecommendations\": []\n",
      "      },\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"1c2fee7e-d0f1-4a63-9d36-c05a5283237a\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"1c2fee7e-d0f1-4a63-9d36-c05a5283237a\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"1168\",\n",
      "          \"date\": \"Fri, 26 Jan 2024 20:56:33 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "endpoint_info_str = read_from_s3(BUCKET_NAME, ENDPOINT_S3_PATH)\n",
    "logger.info(f\"endpoint from --> s3://{BUCKET_NAME}/{ENDPOINT_S3_PATH}\")\n",
    "\n",
    "# Process the retrieved content\n",
    "if endpoint_info_str:\n",
    "    try:\n",
    "        endpoint_info_list = json.loads(endpoint_info_str)\n",
    "        logger.info(f\"Found information for {len(endpoint_info_list)} endpoints in the respective S3 path\")\n",
    "        logger.info(json.dumps(endpoint_info_list, indent=2))\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error parsing JSON from S3: {e}\")\n",
    "else:\n",
    "    logger.error(\"Error reading from S3 or no data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:11,890] p91528 {1455142584.py:3} INFO - there are 1 deployed endpoint(s), endpoint_name_list->['lmistral7b-g5-2xlarge-1706302351']\n"
     ]
    }
   ],
   "source": [
    "# List down the endpoint names that have been deployed\n",
    "endpoint_name_list = [e['endpoint']['EndpointName'] for e in endpoint_info_list]\n",
    "logger.info(f\"there are {len(endpoint_name_list)} deployed endpoint(s), endpoint_name_list->{endpoint_name_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating predictor objects from the deployed endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:12,019] p91528 {3970465137.py:15} INFO - [<sagemaker.base_predictor.Predictor object at 0x16abb2490>]\n"
     ]
    }
   ],
   "source": [
    "# create predictor objects\n",
    "\n",
    "## create a sagemaker predictor for these endpoints\n",
    "def create_predictor(endpoint_name: str) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "    # Create a SageMaker Predictor object\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "        serializer=JSONSerializer()\n",
    "    )\n",
    "    return predictor\n",
    "\n",
    "## Display the list of predictor objects that have been deployed ready for inferencing from\n",
    "predictor_list: List = [create_predictor(ep) for ep in endpoint_name_list]\n",
    "logger.info(predictor_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions to define and calculate metrics during the time of invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_sum(l: List) -> Union[int, float]:\n",
    "    return sum(filter(None, l))\n",
    "\n",
    "def safe_div(n: Union[int, float], d: Union[int, float]) -> Optional[Union[int, float]]:\n",
    "    return n/d if d else None\n",
    "\n",
    "## Represents the function to calculate all of the metrics at the time of inference\n",
    "def calculate_metrics(responses, chunk, elapsed_async, experiment_name, concurrency, payload_file) -> Dict:\n",
    "    \n",
    "    ## calculate errors based on the completion status of the inference prompt\n",
    "    errors = [r for r in responses if r['completion'] is None]\n",
    "    \n",
    "    ## Calculate the difference as the successes \n",
    "    successes = len(chunk) - len(errors)\n",
    "    \n",
    "    ## Count all of the prompts token count during inference\n",
    "    all_prompts_token_count = safe_sum([r['prompt_tokens'] for r in responses])\n",
    "    prompt_token_throughput = round(all_prompts_token_count / elapsed_async, 2)\n",
    "    prompt_token_count_mean = safe_div(all_prompts_token_count, successes)\n",
    "    all_completions_token_count = safe_sum([r['completion_tokens'] for r in responses])\n",
    "    completion_token_throughput = round(all_completions_token_count / elapsed_async, 2)\n",
    "    completion_token_count_mean = safe_div(all_completions_token_count, successes)\n",
    "    transactions_per_second = round(successes / elapsed_async, 2)\n",
    "    transactions_per_minute = int(transactions_per_second * 60)\n",
    "    \n",
    "    ## calculate the latency mean utilizing the safe_sum function defined above\n",
    "    latency_mean = safe_div(safe_sum([r['latency'] for r in responses]), successes)\n",
    "    \n",
    "    ## Function returns all these values at the time of the invocations\n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'concurrency': concurrency,\n",
    "        'payload_file': payload_file,\n",
    "        'errors': errors,\n",
    "        'successes': successes,\n",
    "        'error_rate': len(errors)/len(chunk),\n",
    "        'all_prompts_token_count': all_prompts_token_count,\n",
    "        'prompt_token_count_mean': prompt_token_count_mean,\n",
    "        'prompt_token_throughput': prompt_token_throughput,\n",
    "        'all_completions_token_count': all_completions_token_count,\n",
    "        'completion_token_count_mean': completion_token_count_mean,\n",
    "        'completion_token_throughput': completion_token_throughput,\n",
    "        'transactions': len(chunk),\n",
    "        'transactions_per_second': transactions_per_second,\n",
    "        'transactions_per_minute': transactions_per_minute,\n",
    "        'latency_mean': latency_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a blocker function and a series of asynchronous concurrent model prompt invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_metrics(endpoint_name=None,\n",
    "                    prompt=None,\n",
    "                    inference_params=None,\n",
    "                    completion=None,\n",
    "                    prompt_tokens=None,\n",
    "                    completion_tokens=None,\n",
    "                    latency=None) -> Dict:\n",
    "    return dict(endpoint_name=endpoint_name,                \n",
    "                prompt=prompt,\n",
    "                **inference_params,\n",
    "                completion=completion,\n",
    "                prompt_tokens=prompt_tokens,\n",
    "                completion_tokens=completion_tokens,\n",
    "                latency=latency)\n",
    "\n",
    "def get_inference(predictor, payload) -> Dict:\n",
    "    \n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "    latency = 0\n",
    "\n",
    "    try:\n",
    "        prompt_tokens = count_tokens(payload['inputs'])\n",
    "        logger.info(f\"get_inference, endpoint={predictor.endpoint_name}, prompt_tokens={prompt_tokens}\")\n",
    "\n",
    "        # get inference\n",
    "        st = time.perf_counter()        \n",
    "        response = predictor.predict(payload)        \n",
    "        latency = time.perf_counter() - st\n",
    "\n",
    "        if isinstance(response, bytes):\n",
    "            response = response.decode('utf-8')\n",
    "        response_json = json.loads(response)\n",
    "        if isinstance(response_json, list):\n",
    "            response_json = response_json[0]\n",
    "\n",
    "        completion = response_json.get(\"generated_text\", \"\")\n",
    "        completion_tokens = count_tokens(completion)\n",
    "\n",
    "        # Set metrics and logging for both cases\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               completion,\n",
    "                               prompt_tokens,\n",
    "                               completion_tokens,\n",
    "                               latency)\n",
    "        # logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, response={json.dumps(response, indent=2)}, latency={latency:.2f}\")\n",
    "        logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, completion_tokens={completion_tokens}, latency={latency:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error occurred with {predictor.endpoint_name}, exception={str(e)}\")\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               None,\n",
    "                               prompt_tokens,\n",
    "                               None,\n",
    "                               None)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a series of asynchronous functions to invoke and run inferences concurrently and asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents a function to start invoking models in separate thread asynchronously for the blocker function\n",
    "async def async_get_inference(predictor, payload: Dict) -> Dict:\n",
    "    return await asyncio.to_thread(get_inference, predictor, payload)\n",
    "\n",
    "## Gathers all of the tasks and sets of the concurrent calling of the asychronous invocations\n",
    "async def async_get_all_inferences(predictor, payload_list: List) -> List:\n",
    "    return await asyncio.gather(*[async_get_inference(predictor, payload) for payload in payload_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This function runs the asychronous function series above together for different experiments and concurrency levels.\n",
    "async def run_inferences(predictor: sagemaker.base_predictor.Predictor, chunk: List, experiment: Dict, concurrency: int, payload_file: str) -> Tuple[List, Dict]:\n",
    "    logger.info(f\"Processing chunk with concurrency={concurrency}\")\n",
    "    s = time.perf_counter()\n",
    "    responses = await async_get_all_inferences(predictor, chunk)\n",
    "    elapsed_async = time.perf_counter() - s\n",
    "\n",
    "    # Add more metadata about this experiment\n",
    "    for r in responses:\n",
    "        r['experiment_name'] = experiment['name']\n",
    "        r['concurrency'] = concurrency\n",
    "\n",
    "    metrics = calculate_metrics(responses, chunk, elapsed_async, experiment['name'], concurrency, payload_file)\n",
    "    return responses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create the predictors from the experiment we are iterating over\n",
    "def create_predictor_for_experiment(experiment: str, config: Dict, endpoint_info_list: List) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "\n",
    "    ## Here, we set the index and then iterate through the experiments\n",
    "    e_idx = config['experiments'].index(experiment) + 1\n",
    "\n",
    "    ## Iterate through the endpoint information to fetch the endpoint name\n",
    "    ep_info = [e for e in endpoint_info_list if e['experiment_name'] == experiment['name']]\n",
    "    if not ep_info:\n",
    "        logger.error(f\"endpoint for experiment={experiment['name']} not found, skipping\")\n",
    "        return None\n",
    "    ep_name = ep_info[0]['endpoint']['EndpointName']\n",
    "    logger.info(f\"experiment={e_idx}, name={experiment['name']}, ep_name={ep_name}\")\n",
    "\n",
    "    # create a predictor from each endpoint in experiments\n",
    "    return create_predictor(ep_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here, we will process combinations of concurrency levels, the payload files and then loop through the \n",
    "## different combinations to make payloads splitted in terms of the concurrency metric and how we can run \n",
    "## it and make inference\n",
    "\n",
    "def create_payload_dict(jline: str, experiment: Dict) -> Dict:\n",
    "    payload: Dict = json.loads(jline)\n",
    "    if experiment.get('remove_truncate', False) is True:\n",
    "        if payload['parameters'].get('truncate'):\n",
    "            del payload['parameters']['truncate']\n",
    "    return payload\n",
    "    \n",
    "    \n",
    "def create_combinations(experiment: Dict) -> List[Tuple]:\n",
    "    combinations_data = []\n",
    "\n",
    "    # Repeat for each concurrency level\n",
    "    combinations = list(itertools.product(experiment['concurrency_levels'], experiment['payload_files']))\n",
    "    logger.info(f\"there are {len(combinations)} combinations of {combinations} to run\")\n",
    "\n",
    "    for concurrency, payload_file in combinations:\n",
    "        # Construct the full S3 file path\n",
    "        s3_file_path = os.path.join(PROMPTS_DIR, payload_file)\n",
    "        logger.info(f\"s3 path where the payload files are being read from -> {s3_file_path}\")\n",
    "\n",
    "        # Read the payload file from S3\n",
    "        try:\n",
    "            response = s3_client.get_object(Bucket=BUCKET_NAME, Key=s3_file_path)\n",
    "            payload_file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "            # Create a payload list by processing each line\n",
    "            payload_list = [create_payload_dict(jline, experiment) for jline in payload_file_content.splitlines()]\n",
    "            logger.info(f\"read from s3://{BUCKET_NAME}/{s3_file_path}, contains {len(payload_list)} lines\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file from S3: {e}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"creating combinations for concurrency={concurrency}, payload_file={payload_file}, payload_list length={len(payload_list)}\")\n",
    "        \n",
    "        n = concurrency\n",
    "        \n",
    "        if len(payload_list) < n:\n",
    "            elements_to_add = n - len(payload_list)\n",
    "            element_to_replicate = payload_list[0]\n",
    "            # payload_list = payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "        # Split the original list into sublists which contain the number of requests we want to send concurrently        \n",
    "        payload_list_splitted = [payload_list[i * n:(i + 1) * n] for i in range((len(payload_list) + n - 1) // n )]  \n",
    "        \n",
    "        for p in payload_list_splitted:\n",
    "            if len(p) < n:\n",
    "                elements_to_add = n - len(p)\n",
    "                element_to_replicate = p[0]\n",
    "                # p = p.extend([element_to_replicate]*elements_to_add)\n",
    "                p.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "\n",
    "        # Only keep lists that have at least concurrency number of elements\n",
    "        len_before = len(payload_list_splitted)\n",
    "        payload_list_splitted = [p for p in payload_list_splitted if len(p) == concurrency]\n",
    "        logger.info(f\"after only retaining chunks of length {concurrency}, we have {len(payload_list_splitted)} chunks, previously we had {len_before} chunks\")\n",
    "        combinations_data.append((concurrency, payload_file, payload_list_splitted))\n",
    "    logger.info(f\"there are {len(combinations)} for {experiment}\")\n",
    "    return combinations_data\n",
    "\n",
    "# process_combinations(experiment, predictor, PROMPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:12,248] p91528 {663335596.py:13} INFO - experiment=1, name=mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0, ep_name=lmistral7b-g5-2xlarge-1706302351\n",
      "[2024-01-26 15:57:12,262] p91528 {2948183873.py:18} INFO - there are 25 combinations of [(1, 'payload_en_1-500.jsonl'), (1, 'payload_en_500-1000.jsonl'), (1, 'payload_en_1000-2000.jsonl'), (1, 'payload_en_2000-3000.jsonl'), (1, 'payload_en_3000-4000.jsonl'), (2, 'payload_en_1-500.jsonl'), (2, 'payload_en_500-1000.jsonl'), (2, 'payload_en_1000-2000.jsonl'), (2, 'payload_en_2000-3000.jsonl'), (2, 'payload_en_3000-4000.jsonl'), (4, 'payload_en_1-500.jsonl'), (4, 'payload_en_500-1000.jsonl'), (4, 'payload_en_1000-2000.jsonl'), (4, 'payload_en_2000-3000.jsonl'), (4, 'payload_en_3000-4000.jsonl'), (6, 'payload_en_1-500.jsonl'), (6, 'payload_en_500-1000.jsonl'), (6, 'payload_en_1000-2000.jsonl'), (6, 'payload_en_2000-3000.jsonl'), (6, 'payload_en_3000-4000.jsonl'), (8, 'payload_en_1-500.jsonl'), (8, 'payload_en_500-1000.jsonl'), (8, 'payload_en_1000-2000.jsonl'), (8, 'payload_en_2000-3000.jsonl'), (8, 'payload_en_3000-4000.jsonl')] to run\n",
      "[2024-01-26 15:57:12,263] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:12,432] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-26 15:57:12,432] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=1, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-26 15:57:12,432] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 15:57:12,432] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-26 15:57:12,482] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-26 15:57:12,482] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=1, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-26 15:57:12,482] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 15:57:12,483] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1000-2000.jsonl\n",
      "[2024-01-26 15:57:12,657] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-26 15:57:12,657] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=1, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-26 15:57:12,658] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 1, we have 15 chunks, previously we had 15 chunks\n",
      "[2024-01-26 15:57:12,658] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_2000-3000.jsonl\n",
      "[2024-01-26 15:57:12,796] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-26 15:57:12,796] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=1, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-26 15:57:12,796] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 1, we have 32 chunks, previously we had 32 chunks\n",
      "[2024-01-26 15:57:12,797] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_3000-4000.jsonl\n",
      "[2024-01-26 15:57:12,961] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-26 15:57:12,962] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=1, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-26 15:57:12,962] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 1, we have 57 chunks, previously we had 57 chunks\n",
      "[2024-01-26 15:57:12,962] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n",
      "[2024-01-26 15:57:13,063] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-26 15:57:13,063] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=2, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-26 15:57:13,064] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 2, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 15:57:13,064] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-26 15:57:13,127] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-26 15:57:13,127] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=2, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-26 15:57:13,127] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 2, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 15:57:13,128] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1000-2000.jsonl\n",
      "[2024-01-26 15:57:13,181] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-26 15:57:13,181] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=2, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-26 15:57:13,181] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 2, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-26 15:57:13,182] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_2000-3000.jsonl\n",
      "[2024-01-26 15:57:13,294] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-26 15:57:13,295] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=2, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-26 15:57:13,295] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 2, we have 16 chunks, previously we had 16 chunks\n",
      "[2024-01-26 15:57:13,295] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_3000-4000.jsonl\n",
      "[2024-01-26 15:57:13,426] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-26 15:57:13,427] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=2, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-26 15:57:13,427] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 2, we have 29 chunks, previously we had 29 chunks\n",
      "[2024-01-26 15:57:13,427] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n",
      "[2024-01-26 15:57:13,484] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-26 15:57:13,484] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=4, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-26 15:57:13,484] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 4, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 15:57:13,484] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-26 15:57:13,531] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-26 15:57:13,532] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=4, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-26 15:57:13,532] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 4, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 15:57:13,533] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1000-2000.jsonl\n",
      "[2024-01-26 15:57:13,584] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-26 15:57:13,585] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=4, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-26 15:57:13,587] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 4, we have 4 chunks, previously we had 4 chunks\n",
      "[2024-01-26 15:57:13,588] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_2000-3000.jsonl\n",
      "[2024-01-26 15:57:13,686] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-26 15:57:13,687] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=4, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-26 15:57:13,690] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 4, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-26 15:57:13,693] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_3000-4000.jsonl\n",
      "[2024-01-26 15:57:13,846] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-26 15:57:13,846] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=4, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-26 15:57:13,847] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 4, we have 15 chunks, previously we had 15 chunks\n",
      "[2024-01-26 15:57:13,849] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n",
      "[2024-01-26 15:57:13,898] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-26 15:57:13,899] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=6, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-26 15:57:13,899] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 6, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 15:57:13,900] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-26 15:57:13,983] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-26 15:57:13,983] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=6, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-26 15:57:13,984] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 6, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 15:57:13,984] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1000-2000.jsonl\n",
      "[2024-01-26 15:57:14,040] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-26 15:57:14,040] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=6, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-26 15:57:14,041] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 6, we have 3 chunks, previously we had 3 chunks\n",
      "[2024-01-26 15:57:14,041] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_2000-3000.jsonl\n",
      "[2024-01-26 15:57:14,106] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-26 15:57:14,107] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=6, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-26 15:57:14,107] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 6, we have 6 chunks, previously we had 6 chunks\n",
      "[2024-01-26 15:57:14,107] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_3000-4000.jsonl\n",
      "[2024-01-26 15:57:14,239] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-26 15:57:14,239] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=6, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-26 15:57:14,240] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 6, we have 10 chunks, previously we had 10 chunks\n",
      "[2024-01-26 15:57:14,240] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n",
      "[2024-01-26 15:57:14,292] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-26 15:57:14,292] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=8, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-26 15:57:14,292] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 8, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 15:57:14,293] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-26 15:57:14,339] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-26 15:57:14,339] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=8, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-26 15:57:14,339] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 8, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 15:57:14,340] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1000-2000.jsonl\n",
      "[2024-01-26 15:57:14,406] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-26 15:57:14,407] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=8, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-26 15:57:14,407] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 8, we have 2 chunks, previously we had 2 chunks\n",
      "[2024-01-26 15:57:14,407] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_2000-3000.jsonl\n",
      "[2024-01-26 15:57:14,478] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-26 15:57:14,478] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=8, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-26 15:57:14,479] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 8, we have 4 chunks, previously we had 4 chunks\n",
      "[2024-01-26 15:57:14,479] p91528 {2948183873.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_3000-4000.jsonl\n",
      "[2024-01-26 15:57:14,585] p91528 {2948183873.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-26 15:57:14,586] p91528 {2948183873.py:38} INFO - creating combinations for concurrency=8, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-26 15:57:14,586] p91528 {2948183873.py:72} INFO - after only retaining chunks of length 8, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-26 15:57:14,586] p91528 {2948183873.py:74} INFO - there are 25 for {'name': 'mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'huggingface-llm-mistral-7b', 'model_version': '*', 'model_name': 'mistral7b', 'ep_name': 'lmistral7b-g5-2xlarge', 'instance_type': 'ml.g5.2xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl', 'payload_en_500-1000.jsonl', 'payload_en_1000-2000.jsonl', 'payload_en_2000-3000.jsonl', 'payload_en_3000-4000.jsonl'], 'concurrency_levels': [1, 2, 4, 6, 8], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '8191', 'MAX_TOTAL_TOKENS': '8192', 'MAX_BATCH_PREFILL_TOKENS': '8191', 'SM_NUM_GPUS': '1', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}\n",
      "[2024-01-26 15:57:14,587] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 15:57:14,587] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:14,693] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 15:57:18,426] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=3.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302638.428124.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:18,937] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 15:57:18,938] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:18,945] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302638.681707.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:19,753] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=0.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302639.755789.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:20,269] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/15\n",
      "[2024-01-26 15:57:20,270] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:20,275] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302639.996379.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:24,375] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=69, latency=4.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302644.377101.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:24,902] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/15\n",
      "[2024-01-26 15:57:24,903] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:24,914] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302644.616034.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:29,363] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=4.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302649.3651452.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:29,959] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/15\n",
      "[2024-01-26 15:57:29,959] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:29,963] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302649.653891.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:33,953] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=3.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302653.956031.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:34,439] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/15\n",
      "[2024-01-26 15:57:34,440] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:34,445] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302654.194429.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:36,447] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=39, latency=2.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302656.447826.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:36,959] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/15\n",
      "[2024-01-26 15:57:36,959] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:36,964] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302656.671932.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:41,083] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=4.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302661.084521.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:41,553] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/15\n",
      "[2024-01-26 15:57:41,553] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:41,561] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302661.311859.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:42,976] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=24, latency=1.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302662.977522.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:43,526] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=7/15\n",
      "[2024-01-26 15:57:43,526] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:43,533] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302663.23711.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:44,502] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=11, latency=0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302664.5037491.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:45,107] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=8/15\n",
      "[2024-01-26 15:57:45,107] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:45,113] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302664.767772.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:49,367] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=96, latency=4.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302669.368408.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:49,890] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=9/15\n",
      "[2024-01-26 15:57:49,891] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:49,902] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302669.598779.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:54,232] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302674.233114.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:54,819] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=10/15\n",
      "[2024-01-26 15:57:54,819] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:54,825] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302674.511789.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:59,015] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302679.0160031.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:57:59,477] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=11/15\n",
      "[2024-01-26 15:57:59,477] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:57:59,484] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302679.251046.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:03,795] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=4.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302683.7958899.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:04,273] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=12/15\n",
      "[2024-01-26 15:58:04,274] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:04,279] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302684.0138261.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:08,392] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302688.393214.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:08,859] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=13/15\n",
      "[2024-01-26 15:58:08,859] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:08,873] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302688.624109.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:13,260] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302693.261578.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:13,836] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=14/15\n",
      "[2024-01-26 15:58:13,836] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:13,843] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302693.536362.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:18,216] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302698.2177129.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:18,777] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=15/15\n",
      "[2024-01-26 15:58:18,778] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:18,784] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302698.454186.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:19,646] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302699.6476128.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:20,210] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/32\n",
      "[2024-01-26 15:58:20,211] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:20,229] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302699.9067352.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:25,108] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=4.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302705.10882.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:25,588] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/32\n",
      "[2024-01-26 15:58:25,588] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:25,598] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302705.364162.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:30,719] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302710.7214148.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:31,417] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/32\n",
      "[2024-01-26 15:58:31,417] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:31,423] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302710.995753.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:36,057] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302716.057808.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:36,716] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/32\n",
      "[2024-01-26 15:58:36,719] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:36,743] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302716.394301.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:41,637] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=96, latency=4.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302721.637975.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:42,188] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/32\n",
      "[2024-01-26 15:58:42,188] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:42,204] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302721.85962.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:46,938] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=95, latency=4.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302726.939769.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:47,486] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/32\n",
      "[2024-01-26 15:58:47,486] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:47,493] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302727.1673028.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:52,050] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=4.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302732.050942.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:52,681] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=7/32\n",
      "[2024-01-26 15:58:52,681] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:52,692] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302732.322208.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:57,600] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=103, latency=4.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302737.601426.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:58:58,142] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=8/32\n",
      "[2024-01-26 15:58:58,142] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:58:58,150] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302737.8257082.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:00,260] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=36, latency=2.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302740.261344.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:00,775] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=9/32\n",
      "[2024-01-26 15:59:00,776] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:00,785] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302740.5156538.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:05,638] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=4.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302745.6398249.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:06,181] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=10/32\n",
      "[2024-01-26 15:59:06,181] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:06,191] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302745.86558.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:10,953] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302750.954525.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:11,575] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=11/32\n",
      "[2024-01-26 15:59:11,575] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:11,582] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302751.209279.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:13,495] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=34, latency=1.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302753.496573.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:14,043] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=12/32\n",
      "[2024-01-26 15:59:14,043] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:14,053] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302753.744332.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:18,952] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=77, latency=4.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302758.953084.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:19,494] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=13/32\n",
      "[2024-01-26 15:59:19,494] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:19,504] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302759.172349.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:21,553] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=32, latency=2.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302761.553962.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:22,207] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=14/32\n",
      "[2024-01-26 15:59:22,207] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:22,215] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302761.8153532.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:25,887] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=71, latency=3.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302765.8902988.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:26,455] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=15/32\n",
      "[2024-01-26 15:59:26,455] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:26,463] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302766.1244018.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:31,168] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=4.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302771.170997.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:31,794] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=16/32\n",
      "[2024-01-26 15:59:31,795] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:31,813] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302771.415282.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:36,449] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302776.45154.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:36,961] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=17/32\n",
      "[2024-01-26 15:59:36,961] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:36,970] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302776.6894138.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:38,025] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=12, latency=1.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302778.027807.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:38,606] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=18/32\n",
      "[2024-01-26 15:59:38,607] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:38,627] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302778.265142.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:43,335] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=4.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302783.3367648.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:43,990] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=19/32\n",
      "[2024-01-26 15:59:43,991] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:44,009] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302783.611986.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:44,873] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302784.875665.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:45,435] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=20/32\n",
      "[2024-01-26 15:59:45,436] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:45,455] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302785.111949.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:50,184] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302790.187519.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:50,794] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=21/32\n",
      "[2024-01-26 15:59:50,795] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:50,816] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302790.430807.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:55,683] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302795.686893.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 15:59:56,342] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=22/32\n",
      "[2024-01-26 15:59:56,343] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 15:59:56,367] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302795.9688032.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:01,267] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302801.269886.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:02,094] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=23/32\n",
      "[2024-01-26 16:00:02,095] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:02,118] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302801.669454.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:06,976] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302806.979458.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:07,571] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=24/32\n",
      "[2024-01-26 16:00:07,572] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:07,591] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302807.225832.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:12,426] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302812.42817.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:13,098] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=25/32\n",
      "[2024-01-26 16:00:13,099] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:13,116] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302812.7381818.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:17,601] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302817.603986.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:18,193] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=26/32\n",
      "[2024-01-26 16:00:18,194] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:18,212] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302817.854073.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:22,810] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302822.8130429.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:23,439] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=27/32\n",
      "[2024-01-26 16:00:23,440] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:23,460] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302823.0711942.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:28,284] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=4.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302828.28596.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:28,911] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=28/32\n",
      "[2024-01-26 16:00:28,911] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:28,931] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302828.569557.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:33,829] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302833.8323882.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:34,495] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=29/32\n",
      "[2024-01-26 16:00:34,496] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:34,519] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302834.074173.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:39,283] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302839.285572.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:39,898] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=30/32\n",
      "[2024-01-26 16:00:39,900] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:39,920] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302839.510453.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:40,670] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=4, latency=0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302840.671992.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:41,281] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=31/32\n",
      "[2024-01-26 16:00:41,282] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:41,302] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302840.921731.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:46,136] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=4.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302846.13871.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:46,708] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=32/32\n",
      "[2024-01-26 16:00:46,708] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:46,716] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302846.424299.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:51,359] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302851.3617349.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:52,019] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/57\n",
      "[2024-01-26 16:00:52,020] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:52,036] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302851.62643.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:57,080] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302857.082331.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:00:57,689] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/57\n",
      "[2024-01-26 16:00:57,690] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:00:57,716] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302857.3366332.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:03,323] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302863.325108.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:04,008] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/57\n",
      "[2024-01-26 16:01:04,009] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:04,038] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302863.633625.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:05,713] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=17, latency=1.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302865.7161531.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:06,358] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/57\n",
      "[2024-01-26 16:01:06,359] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:06,384] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302865.9566212.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:12,005] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302872.007861.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:12,690] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/57\n",
      "[2024-01-26 16:01:12,691] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:12,717] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302872.243865.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:18,087] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=91, latency=5.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302878.090278.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:18,697] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/57\n",
      "[2024-01-26 16:01:18,697] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:18,718] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302878.3731809.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:20,717] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=27, latency=2.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302880.720545.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:21,323] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=7/57\n",
      "[2024-01-26 16:01:21,324] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:21,350] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302880.978553.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:26,790] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302886.792972.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:27,444] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=8/57\n",
      "[2024-01-26 16:01:27,445] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:27,466] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302887.0347579.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:29,945] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=38, latency=2.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302889.9480839.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:30,541] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=9/57\n",
      "[2024-01-26 16:01:30,542] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:30,567] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302890.191128.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:32,083] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=12, latency=1.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302892.087488.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:32,616] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=10/57\n",
      "[2024-01-26 16:01:32,617] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:32,639] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302892.341919.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:34,840] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=27, latency=2.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302894.8419058.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:35,551] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=11/57\n",
      "[2024-01-26 16:01:35,552] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:35,573] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302895.1302779.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:40,863] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302900.866613.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:41,464] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=12/57\n",
      "[2024-01-26 16:01:41,465] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:41,489] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302901.105226.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:45,772] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=74, latency=4.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302905.774872.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:46,410] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=13/57\n",
      "[2024-01-26 16:01:46,411] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:46,436] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302906.022442.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:51,807] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=5.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302911.809841.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:52,498] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=14/57\n",
      "[2024-01-26 16:01:52,499] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:52,522] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302912.111905.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:57,877] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=5.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302917.879206.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:01:58,425] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=15/57\n",
      "[2024-01-26 16:01:58,425] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:01:58,446] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302918.116363.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:03,647] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=94, latency=5.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302923.6494348.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:04,245] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=16/57\n",
      "[2024-01-26 16:02:04,246] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:04,266] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302923.894729.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:09,416] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302929.418217.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:10,047] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=17/57\n",
      "[2024-01-26 16:02:10,048] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:10,074] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302929.6992939.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:15,533] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=5.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302935.535815.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:16,167] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=18/57\n",
      "[2024-01-26 16:02:16,168] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:16,194] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302935.7726111.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:17,651] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=1.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302937.654336.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:18,174] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=19/57\n",
      "[2024-01-26 16:02:18,174] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:18,193] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302937.9053361.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:23,489] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302943.491353.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:24,214] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=20/57\n",
      "[2024-01-26 16:02:24,215] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:24,251] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302943.806178.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:29,876] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302949.879245.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:30,614] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=21/57\n",
      "[2024-01-26 16:02:30,615] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:30,639] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302950.2185159.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:34,855] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=80, latency=4.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302954.857244.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:35,439] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=22/57\n",
      "[2024-01-26 16:02:35,440] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:35,469] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302955.103743.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:40,991] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302960.99323.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:41,641] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=23/57\n",
      "[2024-01-26 16:02:41,642] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:41,667] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302961.280758.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:46,959] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302966.9620428.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:47,485] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=24/57\n",
      "[2024-01-26 16:02:47,485] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:47,507] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302967.194294.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:53,378] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=5.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302973.3802822.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:54,052] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=25/57\n",
      "[2024-01-26 16:02:54,053] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:54,077] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302973.6263602.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:59,211] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302979.213707.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:02:59,857] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=26/57\n",
      "[2024-01-26 16:02:59,858] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:02:59,886] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302979.51168.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:02,018] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=23, latency=2.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302982.020986.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:02,687] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=27/57\n",
      "[2024-01-26 16:03:02,688] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:02,716] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302982.260725.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:08,149] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=5.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302988.152478.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:08,781] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=28/57\n",
      "[2024-01-26 16:03:08,782] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:08,795] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302988.39449.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:13,964] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=5.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302993.966861.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:14,592] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=29/57\n",
      "[2024-01-26 16:03:14,593] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:14,622] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706302994.2241259.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:20,064] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303000.0674088.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:20,610] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=30/57\n",
      "[2024-01-26 16:03:20,610] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:20,627] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303000.325682.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:25,942] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=5.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303005.9448478.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:26,519] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=31/57\n",
      "[2024-01-26 16:03:26,520] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:26,545] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303006.182934.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:31,641] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303011.643404.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:32,323] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=32/57\n",
      "[2024-01-26 16:03:32,324] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:32,338] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303011.9564521.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:33,635] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=1.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303013.638363.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:34,172] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=33/57\n",
      "[2024-01-26 16:03:34,173] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:34,188] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303013.879767.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:35,573] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=1.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303015.5758002.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:36,187] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=34/57\n",
      "[2024-01-26 16:03:36,188] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:36,210] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303015.81884.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:37,344] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=1.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303017.3468428.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:37,989] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=35/57\n",
      "[2024-01-26 16:03:37,990] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:38,013] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303017.5893939.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:39,149] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=1.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303019.151497.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:39,654] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=36/57\n",
      "[2024-01-26 16:03:39,654] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:39,671] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303019.395096.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:44,787] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=103, latency=5.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303024.790997.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:45,501] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=37/57\n",
      "[2024-01-26 16:03:45,502] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:45,530] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303025.060252.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:46,800] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=1.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303026.802355.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:47,514] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=38/57\n",
      "[2024-01-26 16:03:47,514] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:47,537] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303027.1075141.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:52,668] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303032.670969.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:53,167] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=39/57\n",
      "[2024-01-26 16:03:53,167] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:53,187] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303032.91228.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:58,485] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=5.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303038.4884539.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:03:59,235] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=40/57\n",
      "[2024-01-26 16:03:59,235] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:03:59,259] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303038.7883022.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:04,732] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=5.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303044.734884.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:05,325] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=41/57\n",
      "[2024-01-26 16:04:05,325] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:05,350] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303045.0027862.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:06,613] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=1.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303046.614752.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:07,169] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=42/57\n",
      "[2024-01-26 16:04:07,169] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:07,189] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303046.855417.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:12,405] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303052.406757.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:13,012] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=43/57\n",
      "[2024-01-26 16:04:13,012] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:13,036] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303052.6871512.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:14,322] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=1.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303054.32445.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:14,963] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=44/57\n",
      "[2024-01-26 16:04:14,964] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:14,988] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303054.6151412.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:16,340] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=1.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303056.341589.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:16,983] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=45/57\n",
      "[2024-01-26 16:04:16,984] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:17,010] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303056.571315.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:18,228] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=1.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303058.2296479.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:18,864] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=46/57\n",
      "[2024-01-26 16:04:18,865] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:18,890] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303058.488419.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:20,155] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=1.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303060.156672.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:20,861] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=47/57\n",
      "[2024-01-26 16:04:20,862] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:20,888] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303060.439945.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:22,113] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=1.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303062.114924.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:22,827] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=48/57\n",
      "[2024-01-26 16:04:22,827] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:22,839] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303062.4163609.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:23,919] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=1.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303063.919775.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:24,617] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=49/57\n",
      "[2024-01-26 16:04:24,617] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:24,737] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303064.150505.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:30,239] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=5.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303070.24422.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:31,002] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=50/57\n",
      "[2024-01-26 16:04:31,003] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:31,030] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303070.563413.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:32,402] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=1.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303072.40541.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:33,048] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=51/57\n",
      "[2024-01-26 16:04:33,049] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:33,086] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303072.6946769.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:38,458] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=5.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303078.461049.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:39,141] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=52/57\n",
      "[2024-01-26 16:04:39,142] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:39,178] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303078.7352362.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:44,397] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303084.398207.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:44,921] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=53/57\n",
      "[2024-01-26 16:04:44,921] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:44,935] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303084.64764.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:50,314] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303090.315256.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:50,929] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=54/57\n",
      "[2024-01-26 16:04:50,929] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:50,941] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303090.591603.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:52,191] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=1.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303092.192445.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:52,870] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=55/57\n",
      "[2024-01-26 16:04:52,870] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:52,882] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303092.4505038.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:57,939] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=104, latency=5.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303097.941131.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:04:58,553] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=56/57\n",
      "[2024-01-26 16:04:58,553] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:04:58,569] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303098.183543.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:00,360] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=12, latency=1.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303100.3634088.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:01,011] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=57/57\n",
      "[2024-01-26 16:05:01,011] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 16:05:01,032] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303100.6237.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:02,433] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=1.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303102.434187.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:03,142] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 16:05:03,142] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:03,147] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:05:03,147] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303102.766988.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:06,948] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=3.80\n",
      "[2024-01-26 16:05:07,023] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=3.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303107.024126.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303107.302206.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:07,823] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 16:05:07,823] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:07,829] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:05:07,831] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303107.550327.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:08,951] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=1.12\n",
      "[2024-01-26 16:05:11,984] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303111.985153.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303112.261305.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:12,766] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/8\n",
      "[2024-01-26 16:05:12,766] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:12,771] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1339\n",
      "[2024-01-26 16:05:12,772] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303112.517952.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:17,739] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=69, latency=4.97\n",
      "[2024-01-26 16:05:17,739] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=4.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303117.74057.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303118.0298529.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:18,596] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/8\n",
      "[2024-01-26 16:05:18,596] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:18,601] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1154\n",
      "[2024-01-26 16:05:18,602] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303118.294558.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:21,157] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=39, latency=2.55\n",
      "[2024-01-26 16:05:23,143] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303123.1443.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303123.5588439.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:24,146] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/8\n",
      "[2024-01-26 16:05:24,146] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:24,151] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1397\n",
      "[2024-01-26 16:05:24,152] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303123.846025.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:25,955] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=24, latency=1.80\n",
      "[2024-01-26 16:05:28,779] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=4.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303128.780271.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303129.00959.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:29,621] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/8\n",
      "[2024-01-26 16:05:29,621] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:29,626] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1598\n",
      "[2024-01-26 16:05:29,628] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303129.324238.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:30,958] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=11, latency=1.33\n",
      "[2024-01-26 16:05:34,250] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=96, latency=4.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303134.251116.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303134.6069918.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:35,225] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/8\n",
      "[2024-01-26 16:05:35,226] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:35,232] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1539\n",
      "[2024-01-26 16:05:35,232] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303134.9145448.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:40,105] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.87\n",
      "[2024-01-26 16:05:40,107] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303140.108872.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303140.338206.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:40,959] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/8\n",
      "[2024-01-26 16:05:40,960] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:40,967] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1421\n",
      "[2024-01-26 16:05:40,969] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303140.650378.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:45,806] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=4.84\n",
      "[2024-01-26 16:05:45,808] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303145.811285.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303146.062985.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:46,567] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=7/8\n",
      "[2024-01-26 16:05:46,569] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:46,592] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1918\n",
      "[2024-01-26 16:05:46,592] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303146.3069062.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:51,623] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.03\n",
      "[2024-01-26 16:05:51,623] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303151.6295629.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303151.9610891.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:52,667] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=8/8\n",
      "[2024-01-26 16:05:52,668] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:52,689] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1939\n",
      "[2024-01-26 16:05:52,690] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303152.293788.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:54,074] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=1.38\n",
      "[2024-01-26 16:05:54,074] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=1.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303154.078698.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303154.340248.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:05:55,007] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/16\n",
      "[2024-01-26 16:05:55,008] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:05:55,017] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2637\n",
      "[2024-01-26 16:05:55,021] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303154.688905.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:00,902] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.88\n",
      "[2024-01-26 16:06:00,902] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=5.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303160.9035761.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303161.126084.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:01,799] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/16\n",
      "[2024-01-26 16:06:01,799] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:01,805] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2148\n",
      "[2024-01-26 16:06:01,807] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303161.458304.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:06,021] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=66, latency=4.21\n",
      "[2024-01-26 16:06:07,328] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303167.330417.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303167.6400902.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:08,373] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/16\n",
      "[2024-01-26 16:06:08,374] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:08,386] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2404\n",
      "[2024-01-26 16:06:08,388] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303167.9958541.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:13,881] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.49\n",
      "[2024-01-26 16:06:13,881] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=95, latency=5.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303173.884174.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303174.112385.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:14,585] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/16\n",
      "[2024-01-26 16:06:14,585] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:14,593] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2369\n",
      "[2024-01-26 16:06:14,595] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303174.3520799.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:20,275] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=96, latency=5.68\n",
      "[2024-01-26 16:06:20,275] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=103, latency=5.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303180.278629.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303180.555378.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:21,377] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/16\n",
      "[2024-01-26 16:06:21,378] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:21,386] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2541\n",
      "[2024-01-26 16:06:21,387] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303180.951895.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:27,028] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=5.64\n",
      "[2024-01-26 16:06:27,028] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303187.0318332.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303187.3096378.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:28,051] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/16\n",
      "[2024-01-26 16:06:28,051] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:28,058] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2186\n",
      "[2024-01-26 16:06:28,061] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303187.700472.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:30,791] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=34, latency=2.73\n",
      "[2024-01-26 16:06:33,571] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=77, latency=5.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303193.574018.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303193.828347.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:34,576] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=7/16\n",
      "[2024-01-26 16:06:34,577] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:34,591] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2686\n",
      "[2024-01-26 16:06:34,592] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303194.1778638.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:37,270] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=31, latency=2.68\n",
      "[2024-01-26 16:06:37,311] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=32, latency=2.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303197.3127449.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303197.61797.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:38,139] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=8/16\n",
      "[2024-01-26 16:06:38,140] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:38,154] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2321\n",
      "[2024-01-26 16:06:38,154] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303197.889769.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:43,616] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.46\n",
      "[2024-01-26 16:06:43,616] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=5.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303203.6214368.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303203.8603551.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:44,691] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=9/16\n",
      "[2024-01-26 16:06:44,692] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:44,713] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2428\n",
      "[2024-01-26 16:06:44,716] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303204.290669.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:46,380] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=12, latency=1.67\n",
      "[2024-01-26 16:06:50,023] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303210.024794.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303210.270507.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:50,996] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=10/16\n",
      "[2024-01-26 16:06:50,997] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:51,012] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2101\n",
      "[2024-01-26 16:06:51,014] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303210.619921.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:52,385] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=1.37\n",
      "[2024-01-26 16:06:56,140] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=5.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303216.142463.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303216.445046.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:57,138] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=11/16\n",
      "[2024-01-26 16:06:57,139] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:57,157] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2711\n",
      "[2024-01-26 16:06:57,159] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303216.77911.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:58,731] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=1.57\n",
      "[2024-01-26 16:06:58,941] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=1.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303218.943943.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303219.1916502.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:06:59,896] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=12/16\n",
      "[2024-01-26 16:06:59,897] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:06:59,914] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2691\n",
      "[2024-01-26 16:06:59,914] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303219.544616.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:05,625] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=5.71\n",
      "[2024-01-26 16:07:05,626] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=5.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303225.630322.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303225.917002.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:06,638] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=13/16\n",
      "[2024-01-26 16:07:06,639] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:07:06,653] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2062\n",
      "[2024-01-26 16:07:06,656] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303226.2742999.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:08,060] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=1.40\n",
      "[2024-01-26 16:07:11,696] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303231.6989639.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303232.0023239.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:12,731] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=14/16\n",
      "[2024-01-26 16:07:12,732] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:07:12,747] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2608\n",
      "[2024-01-26 16:07:12,749] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303232.374449.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:18,484] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.73\n",
      "[2024-01-26 16:07:18,484] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303238.490898.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303238.738333.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:19,471] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=15/16\n",
      "[2024-01-26 16:07:19,472] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:07:19,487] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2333\n",
      "[2024-01-26 16:07:19,488] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303239.088743.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:21,065] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=1.57\n",
      "[2024-01-26 16:07:24,746] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=77, latency=5.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303244.747381.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303245.007705.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:25,687] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=16/16\n",
      "[2024-01-26 16:07:25,687] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:07:25,695] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2358\n",
      "[2024-01-26 16:07:25,697] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303245.351437.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:31,294] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=5.60\n",
      "[2024-01-26 16:07:31,295] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=5.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303251.2987661.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303251.583775.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:32,361] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/29\n",
      "[2024-01-26 16:07:32,361] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:07:32,376] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3896\n",
      "[2024-01-26 16:07:32,383] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303251.961992.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:36,987] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=55, latency=4.57\n",
      "[2024-01-26 16:07:38,874] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303258.8782258.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303259.151764.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:39,952] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/29\n",
      "[2024-01-26 16:07:39,953] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:07:39,982] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3450\n",
      "[2024-01-26 16:07:39,984] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303259.546409.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:42,578] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.59\n",
      "[2024-01-26 16:07:46,349] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303266.3509068.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303266.6643121.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:47,542] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/29\n",
      "[2024-01-26 16:07:47,543] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:07:47,567] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3144\n",
      "[2024-01-26 16:07:47,569] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303267.06603.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:50,651] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=27, latency=3.08\n",
      "[2024-01-26 16:07:53,741] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=91, latency=6.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303273.744912.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303274.01415.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:07:54,905] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/29\n",
      "[2024-01-26 16:07:54,906] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:07:54,928] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3014\n",
      "[2024-01-26 16:07:54,933] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303274.442602.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:01,311] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.38\n",
      "[2024-01-26 16:08:01,311] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303281.317508.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303281.591184.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:02,416] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/29\n",
      "[2024-01-26 16:08:02,417] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:08:02,598] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3891\n",
      "[2024-01-26 16:08:02,599] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303281.999106.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:05,026] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=12, latency=2.43\n",
      "[2024-01-26 16:08:05,786] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=27, latency=3.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303285.786874.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303286.0490282.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:06,816] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/29\n",
      "[2024-01-26 16:08:06,817] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:08:06,828] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3419\n",
      "[2024-01-26 16:08:06,828] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303286.4204779.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:13,143] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=6.31\n",
      "[2024-01-26 16:08:13,144] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=6.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303293.1449332.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303293.404927.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:14,031] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=7/29\n",
      "[2024-01-26 16:08:14,031] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:08:14,043] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3436\n",
      "[2024-01-26 16:08:14,043] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303293.6703749.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:16,904] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=23, latency=2.86\n",
      "[2024-01-26 16:08:20,280] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=6.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303300.281173.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303300.5070982.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:21,018] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=8/29\n",
      "[2024-01-26 16:08:21,019] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:08:21,029] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3087\n",
      "[2024-01-26 16:08:21,031] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303300.774514.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:27,217] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.18\n",
      "[2024-01-26 16:08:27,217] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=6.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303307.218474.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303307.475521.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:28,176] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=9/29\n",
      "[2024-01-26 16:08:28,176] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:08:28,186] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3680\n",
      "[2024-01-26 16:08:28,188] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303307.850335.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:30,535] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=2.35\n",
      "[2024-01-26 16:08:34,653] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=6.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303314.654725.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303314.887545.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:35,680] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=10/29\n",
      "[2024-01-26 16:08:35,681] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:08:35,691] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3463\n",
      "[2024-01-26 16:08:35,693] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303315.315504.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:42,333] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=6.64\n",
      "[2024-01-26 16:08:42,333] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303322.336.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303322.576689.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:43,382] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=11/29\n",
      "[2024-01-26 16:08:43,383] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:08:43,402] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3050\n",
      "[2024-01-26 16:08:43,406] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303323.04644.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:49,803] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.40\n",
      "[2024-01-26 16:08:49,805] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303329.8067589.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303330.053686.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:50,883] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=12/29\n",
      "[2024-01-26 16:08:50,884] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:08:50,913] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3410\n",
      "[2024-01-26 16:08:50,917] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303330.465012.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:57,600] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.68\n",
      "[2024-01-26 16:08:57,601] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=96, latency=6.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303337.60329.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303337.9276118.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:08:58,729] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=13/29\n",
      "[2024-01-26 16:08:58,730] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:08:58,748] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3171\n",
      "[2024-01-26 16:08:58,751] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303338.3385348.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:01,949] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=29, latency=3.20\n",
      "[2024-01-26 16:09:04,947] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=6.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303344.949783.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303345.2073731.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:05,913] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=14/29\n",
      "[2024-01-26 16:09:05,914] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:09:05,930] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3098\n",
      "[2024-01-26 16:09:05,938] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303345.561712.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:12,368] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=6.44\n",
      "[2024-01-26 16:09:12,368] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=6.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303352.370964.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303352.64539.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:13,438] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=15/29\n",
      "[2024-01-26 16:09:13,438] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:09:13,453] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3400\n",
      "[2024-01-26 16:09:13,455] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303353.077984.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:19,922] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=6.47\n",
      "[2024-01-26 16:09:19,922] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303359.9260921.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303360.1776.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:20,861] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=16/29\n",
      "[2024-01-26 16:09:20,862] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:09:20,881] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3098\n",
      "[2024-01-26 16:09:20,887] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303360.4875689.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:23,016] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=2.13\n",
      "[2024-01-26 16:09:26,976] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303366.979311.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303367.238861.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:27,830] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=17/29\n",
      "[2024-01-26 16:09:27,831] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:09:27,852] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3132\n",
      "[2024-01-26 16:09:27,858] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303367.518374.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:30,041] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=2.17\n",
      "[2024-01-26 16:09:34,073] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303374.074846.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303374.296221.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:34,802] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=18/29\n",
      "[2024-01-26 16:09:34,802] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:09:34,830] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3030\n",
      "[2024-01-26 16:09:34,831] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303374.558645.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:40,916] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=6.06\n",
      "[2024-01-26 16:09:40,916] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=103, latency=6.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303380.919981.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303381.183562.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:41,955] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=19/29\n",
      "[2024-01-26 16:09:41,955] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:09:41,986] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3208\n",
      "[2024-01-26 16:09:41,989] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303381.552706.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:44,023] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.03\n",
      "[2024-01-26 16:09:48,106] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303388.109548.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303388.405219.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:49,263] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=20/29\n",
      "[2024-01-26 16:09:49,264] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:09:49,285] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3475\n",
      "[2024-01-26 16:09:49,286] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303388.824702.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:55,784] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=6.50\n",
      "[2024-01-26 16:09:55,784] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=6.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303395.788579.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303396.049952.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:56,849] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=21/29\n",
      "[2024-01-26 16:09:56,850] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:09:56,876] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3302\n",
      "[2024-01-26 16:09:56,880] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303396.422942.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:09:58,958] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=2.08\n",
      "[2024-01-26 16:10:03,055] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303403.0609431.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303403.355268.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:04,138] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=22/29\n",
      "[2024-01-26 16:10:04,138] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:10:04,164] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3537\n",
      "[2024-01-26 16:10:04,165] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303403.745495.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:06,330] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.16\n",
      "[2024-01-26 16:10:06,422] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=2.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303406.424046.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303406.671903.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:07,479] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=23/29\n",
      "[2024-01-26 16:10:07,480] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:10:07,507] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3165\n",
      "[2024-01-26 16:10:07,510] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303407.102884.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:09,558] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=2.05\n",
      "[2024-01-26 16:10:09,558] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=2.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303409.561954.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303409.819305.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:10,602] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=24/29\n",
      "[2024-01-26 16:10:10,603] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:10:10,630] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3363\n",
      "[2024-01-26 16:10:10,631] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303410.20416.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:12,489] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=1.86\n",
      "[2024-01-26 16:10:16,676] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=6.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303416.679639.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303416.944912.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:17,739] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=25/29\n",
      "[2024-01-26 16:10:17,741] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:10:17,771] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3882\n",
      "[2024-01-26 16:10:17,772] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303417.335275.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:19,888] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=2.12\n",
      "[2024-01-26 16:10:19,979] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303419.981676.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303420.2755089.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:21,142] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=26/29\n",
      "[2024-01-26 16:10:21,143] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:10:21,171] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3330\n",
      "[2024-01-26 16:10:21,172] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303420.745861.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:27,634] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=6.46\n",
      "[2024-01-26 16:10:27,636] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=104, latency=6.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303427.643556.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303427.909067.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:28,775] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=27/29\n",
      "[2024-01-26 16:10:28,777] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:10:28,808] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3497\n",
      "[2024-01-26 16:10:28,810] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303428.345824.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:35,200] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=6.39\n",
      "[2024-01-26 16:10:35,201] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303435.2045991.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303435.5117629.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:36,300] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=28/29\n",
      "[2024-01-26 16:10:36,301] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:10:36,327] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3003\n",
      "[2024-01-26 16:10:36,331] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303435.899664.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:38,243] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=1.91\n",
      "[2024-01-26 16:10:42,389] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=104, latency=6.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303442.392054.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303442.6799161.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:43,508] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=29/29\n",
      "[2024-01-26 16:10:43,509] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 16:10:43,538] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:10:43,541] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303443.079618.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:45,757] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.22\n",
      "[2024-01-26 16:10:45,758] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303445.759991.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303446.007006.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:46,809] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 16:10:46,810] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:10:46,817] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:10:46,817] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:10:46,817] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:10:46,818] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303446.430986.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:48,809] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=39, latency=1.99\n",
      "[2024-01-26 16:10:50,812] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=3.99\n",
      "[2024-01-26 16:10:50,851] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.03\n",
      "[2024-01-26 16:10:50,852] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=4.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303450.8534122.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303451.1092732.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303451.337108.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303451.55365.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:52,020] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 16:10:52,021] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:10:52,026] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:10:52,026] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:10:52,029] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:10:52,030] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303451.776086.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:53,563] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=1.53\n",
      "[2024-01-26 16:10:56,918] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.88\n",
      "[2024-01-26 16:10:56,918] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.89\n",
      "[2024-01-26 16:10:56,918] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303456.921258.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303457.193086.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303457.475924.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303457.735842.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:10:58,321] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/4\n",
      "[2024-01-26 16:10:58,322] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:10:58,331] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1339\n",
      "[2024-01-26 16:10:58,331] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1932\n",
      "[2024-01-26 16:10:58,332] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1154\n",
      "[2024-01-26 16:10:58,333] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303458.0579772.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:01,529] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=39, latency=3.19\n",
      "[2024-01-26 16:11:03,979] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=5.65\n",
      "[2024-01-26 16:11:03,981] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=5.65\n",
      "[2024-01-26 16:11:03,981] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=5.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303463.983859.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303464.206987.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303464.564018.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303464.8102741.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:05,302] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/4\n",
      "[2024-01-26 16:11:05,302] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:11:05,314] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1397\n",
      "[2024-01-26 16:11:05,316] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1373\n",
      "[2024-01-26 16:11:05,316] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1746\n",
      "[2024-01-26 16:11:05,317] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303465.034882.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:07,365] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=11, latency=2.05\n",
      "[2024-01-26 16:11:07,816] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=24, latency=2.50\n",
      "[2024-01-26 16:11:10,828] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=96, latency=5.51\n",
      "[2024-01-26 16:11:10,828] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=5.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303470.8298628.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303471.150114.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303471.469229.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303471.814021.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:12,400] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/4\n",
      "[2024-01-26 16:11:12,400] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:11:12,406] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1539\n",
      "[2024-01-26 16:11:12,408] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1695\n",
      "[2024-01-26 16:11:12,408] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1421\n",
      "[2024-01-26 16:11:12,408] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303472.0978491.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:18,059] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.65\n",
      "[2024-01-26 16:11:18,060] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=5.65\n",
      "[2024-01-26 16:11:18,061] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=5.65\n",
      "[2024-01-26 16:11:18,061] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=5.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303478.065763.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303478.3183591.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303478.619297.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303478.905204.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:19,482] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/4\n",
      "[2024-01-26 16:11:19,482] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:11:19,496] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1918\n",
      "[2024-01-26 16:11:19,499] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1910\n",
      "[2024-01-26 16:11:19,500] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1939\n",
      "[2024-01-26 16:11:19,502] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303479.200105.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:21,724] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=2.21\n",
      "[2024-01-26 16:11:25,488] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=104, latency=5.95\n",
      "[2024-01-26 16:11:25,488] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=5.99\n",
      "[2024-01-26 16:11:25,488] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303485.490607.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303485.717334.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303486.0193448.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303486.3845701.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:26,980] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/8\n",
      "[2024-01-26 16:11:26,980] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:11:26,992] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2148\n",
      "[2024-01-26 16:11:26,992] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2637\n",
      "[2024-01-26 16:11:26,995] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3000\n",
      "[2024-01-26 16:11:26,995] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303486.681531.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:32,964] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=52, latency=5.97\n",
      "[2024-01-26 16:11:34,127] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=7.13\n",
      "[2024-01-26 16:11:34,127] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=7.13\n",
      "[2024-01-26 16:11:34,128] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=7.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303494.130046.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303494.367022.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303494.797081.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303495.1405299.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:35,786] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/8\n",
      "[2024-01-26 16:11:35,787] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:11:35,795] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2404\n",
      "[2024-01-26 16:11:35,796] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2150\n",
      "[2024-01-26 16:11:35,799] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2369\n",
      "[2024-01-26 16:11:35,799] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303495.4458518.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:42,650] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=96, latency=6.85\n",
      "[2024-01-26 16:11:42,650] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=95, latency=6.85\n",
      "[2024-01-26 16:11:42,651] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=6.85\n",
      "[2024-01-26 16:11:42,652] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=103, latency=6.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303502.655783.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303502.925761.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303503.245688.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303503.59159.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:44,307] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/8\n",
      "[2024-01-26 16:11:44,307] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:11:44,316] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2186\n",
      "[2024-01-26 16:11:44,316] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2541\n",
      "[2024-01-26 16:11:44,316] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2675\n",
      "[2024-01-26 16:11:44,318] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303503.958315.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:48,324] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=34, latency=4.01\n",
      "[2024-01-26 16:11:51,187] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=6.87\n",
      "[2024-01-26 16:11:51,187] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.87\n",
      "[2024-01-26 16:11:51,187] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=77, latency=6.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303511.1902988.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303511.4240582.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303511.760729.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303512.1367629.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:52,775] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/8\n",
      "[2024-01-26 16:11:52,775] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:11:52,786] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2686\n",
      "[2024-01-26 16:11:52,786] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2500\n",
      "[2024-01-26 16:11:52,787] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2321\n",
      "[2024-01-26 16:11:52,789] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303512.4158201.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:11:56,612] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=31, latency=3.82\n",
      "[2024-01-26 16:11:56,659] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=32, latency=3.87\n",
      "[2024-01-26 16:11:59,534] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=6.75\n",
      "[2024-01-26 16:11:59,548] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303519.549258.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303519.7776299.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303520.1284428.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303520.48998.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:01,193] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/8\n",
      "[2024-01-26 16:12:01,194] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:12:01,202] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2458\n",
      "[2024-01-26 16:12:01,202] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2428\n",
      "[2024-01-26 16:12:01,202] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2101\n",
      "[2024-01-26 16:12:01,205] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303520.833614.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:03,951] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=12, latency=2.75\n",
      "[2024-01-26 16:12:07,911] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.70\n",
      "[2024-01-26 16:12:07,912] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.71\n",
      "[2024-01-26 16:12:07,912] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=6.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303527.916165.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303528.197311.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303528.555031.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303528.895518.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:09,605] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/8\n",
      "[2024-01-26 16:12:09,605] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:12:09,615] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2691\n",
      "[2024-01-26 16:12:09,619] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2711\n",
      "[2024-01-26 16:12:09,628] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2771\n",
      "[2024-01-26 16:12:09,629] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303529.239479.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:12,616] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.99\n",
      "[2024-01-26 16:12:16,669] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=7.04\n",
      "[2024-01-26 16:12:16,669] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=7.04\n",
      "[2024-01-26 16:12:16,670] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=7.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303536.6737752.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303536.997117.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303537.3891158.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303537.7152798.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:18,428] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=7/8\n",
      "[2024-01-26 16:12:18,429] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:12:18,439] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2062\n",
      "[2024-01-26 16:12:18,439] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2213\n",
      "[2024-01-26 16:12:18,442] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2770\n",
      "[2024-01-26 16:12:18,443] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303538.0883222.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:25,222] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.78\n",
      "[2024-01-26 16:12:25,222] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.78\n",
      "[2024-01-26 16:12:25,222] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.78\n",
      "[2024-01-26 16:12:25,222] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=6.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303545.2259278.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303545.51624.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303545.860071.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303546.230258.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:26,918] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=8/8\n",
      "[2024-01-26 16:12:26,918] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:12:26,927] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2333\n",
      "[2024-01-26 16:12:26,927] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2564\n",
      "[2024-01-26 16:12:26,928] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2613\n",
      "[2024-01-26 16:12:26,929] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303546.590416.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:29,495] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=4, latency=2.57\n",
      "[2024-01-26 16:12:33,709] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=6.78\n",
      "[2024-01-26 16:12:33,709] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.78\n",
      "[2024-01-26 16:12:33,709] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303553.718876.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303553.98671.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303554.43891.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303554.800014.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:35,555] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/15\n",
      "[2024-01-26 16:12:35,556] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:12:35,588] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3000\n",
      "[2024-01-26 16:12:35,590] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3896\n",
      "[2024-01-26 16:12:35,593] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3450\n",
      "[2024-01-26 16:12:35,597] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303555.151107.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:40,819] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=17, latency=5.22\n",
      "[2024-01-26 16:12:41,980] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=55, latency=6.39\n",
      "[2024-01-26 16:12:43,949] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.36\n",
      "[2024-01-26 16:12:43,951] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=8.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303563.954589.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303564.270496.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303564.6238139.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303564.977529.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:45,615] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/15\n",
      "[2024-01-26 16:12:45,615] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:12:45,625] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3144\n",
      "[2024-01-26 16:12:45,627] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3482\n",
      "[2024-01-26 16:12:45,628] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3014\n",
      "[2024-01-26 16:12:45,629] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303565.3716722.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:50,263] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=27, latency=4.64\n",
      "[2024-01-26 16:12:53,667] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.04\n",
      "[2024-01-26 16:12:53,669] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=96, latency=8.04\n",
      "[2024-01-26 16:12:54,576] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=8.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303574.5782368.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303574.842398.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303575.221069.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303575.556389.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:56,307] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/15\n",
      "[2024-01-26 16:12:56,307] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:12:56,320] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3575\n",
      "[2024-01-26 16:12:56,322] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3891\n",
      "[2024-01-26 16:12:56,322] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3419\n",
      "[2024-01-26 16:12:56,325] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303575.946592.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:12:59,589] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=12, latency=3.26\n",
      "[2024-01-26 16:13:01,313] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=4.99\n",
      "[2024-01-26 16:13:01,314] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=27, latency=4.99\n",
      "[2024-01-26 16:13:04,422] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=8.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303584.4236412.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303584.6943018.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303585.0712729.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303585.451935.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:06,169] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/15\n",
      "[2024-01-26 16:13:06,170] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:13:06,187] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3436\n",
      "[2024-01-26 16:13:06,190] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3508\n",
      "[2024-01-26 16:13:06,201] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3249\n",
      "[2024-01-26 16:13:06,204] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303585.768655.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:13,312] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=60, latency=7.09\n",
      "[2024-01-26 16:13:14,257] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=8.04\n",
      "[2024-01-26 16:13:14,257] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.05\n",
      "[2024-01-26 16:13:14,258] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=8.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303594.2605572.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303594.5165062.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303594.8703842.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303595.211691.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:15,964] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/15\n",
      "[2024-01-26 16:13:15,964] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:13:15,976] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3680\n",
      "[2024-01-26 16:13:15,977] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3463\n",
      "[2024-01-26 16:13:15,977] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3950\n",
      "[2024-01-26 16:13:15,983] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303595.616072.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:24,581] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=8.60\n",
      "[2024-01-26 16:13:24,581] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.60\n",
      "[2024-01-26 16:13:24,581] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=8.60\n",
      "[2024-01-26 16:13:25,379] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=9.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303605.3805099.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303605.6884968.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303606.065744.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303606.462003.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:27,211] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/15\n",
      "[2024-01-26 16:13:27,212] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:13:27,223] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3737\n",
      "[2024-01-26 16:13:27,223] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3050\n",
      "[2024-01-26 16:13:27,225] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3410\n",
      "[2024-01-26 16:13:27,227] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303606.827179.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:30,511] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=15, latency=3.29\n",
      "[2024-01-26 16:13:33,865] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=66, latency=6.64\n",
      "[2024-01-26 16:13:35,498] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=8.27\n",
      "[2024-01-26 16:13:36,342] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=9.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303616.343383.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303616.6639118.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303617.0263.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303617.396707.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:37,966] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=7/15\n",
      "[2024-01-26 16:13:37,967] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:13:38,077] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3171\n",
      "[2024-01-26 16:13:38,085] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3098\n",
      "[2024-01-26 16:13:38,091] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3655\n",
      "[2024-01-26 16:13:38,091] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303617.659775.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:43,528] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=41, latency=5.45\n",
      "[2024-01-26 16:13:46,221] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.13\n",
      "[2024-01-26 16:13:46,222] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=8.13\n",
      "[2024-01-26 16:13:47,076] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=8.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303627.0807948.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303627.344439.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303627.715196.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303628.1126132.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:48,873] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=8/15\n",
      "[2024-01-26 16:13:48,874] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:13:48,905] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3098\n",
      "[2024-01-26 16:13:48,907] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3400\n",
      "[2024-01-26 16:13:48,910] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3704\n",
      "[2024-01-26 16:13:48,912] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303628.4927318.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:53,585] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=4.67\n",
      "[2024-01-26 16:13:57,108] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=8.20\n",
      "[2024-01-26 16:13:57,109] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=8.20\n",
      "[2024-01-26 16:13:57,110] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303637.118855.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303637.475832.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303637.91858.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303638.269793.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:13:58,899] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=9/15\n",
      "[2024-01-26 16:13:58,900] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:13:58,934] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3132\n",
      "[2024-01-26 16:13:58,936] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3971\n",
      "[2024-01-26 16:13:58,937] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3030\n",
      "[2024-01-26 16:13:58,937] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303638.563547.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:01,714] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.77\n",
      "[2024-01-26 16:14:01,813] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=2.87\n",
      "[2024-01-26 16:14:01,813] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=2.87\n",
      "[2024-01-26 16:14:06,861] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=7.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303646.863864.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303647.129801.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303647.428916.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303647.688739.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:08,455] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=10/15\n",
      "[2024-01-26 16:14:08,457] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:14:08,489] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3848\n",
      "[2024-01-26 16:14:08,493] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3208\n",
      "[2024-01-26 16:14:08,493] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3475\n",
      "[2024-01-26 16:14:08,498] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303648.0630138.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:11,384] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.89\n",
      "[2024-01-26 16:14:16,679] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=8.18\n",
      "[2024-01-26 16:14:16,679] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=8.18\n",
      "[2024-01-26 16:14:17,625] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=9.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303657.628682.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303657.944065.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303658.3932412.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303658.8069792.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:19,665] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=11/15\n",
      "[2024-01-26 16:14:19,667] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:14:19,699] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3302\n",
      "[2024-01-26 16:14:19,700] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3581\n",
      "[2024-01-26 16:14:19,703] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3537\n",
      "[2024-01-26 16:14:19,703] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303659.201508.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:22,661] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=2.95\n",
      "[2024-01-26 16:14:22,709] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.99\n",
      "[2024-01-26 16:14:24,018] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=4.30\n",
      "[2024-01-26 16:14:27,645] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=7.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303667.6471.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303667.915412.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303668.38174.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303668.794467.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:29,620] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=12/15\n",
      "[2024-01-26 16:14:29,621] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:14:29,652] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3476\n",
      "[2024-01-26 16:14:29,653] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3235\n",
      "[2024-01-26 16:14:29,655] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3165\n",
      "[2024-01-26 16:14:29,655] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303669.202162.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:32,470] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=2.82\n",
      "[2024-01-26 16:14:32,471] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=2.81\n",
      "[2024-01-26 16:14:33,497] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=3.83\n",
      "[2024-01-26 16:14:37,285] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=7.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303677.2866619.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303677.6482282.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303678.090862.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303678.5323641.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:39,152] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=13/15\n",
      "[2024-01-26 16:14:39,153] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:14:39,187] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3330\n",
      "[2024-01-26 16:14:39,188] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3882\n",
      "[2024-01-26 16:14:39,190] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3616\n",
      "[2024-01-26 16:14:39,190] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303678.826293.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:42,079] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=2.89\n",
      "[2024-01-26 16:14:44,018] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=4.82\n",
      "[2024-01-26 16:14:47,474] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=104, latency=8.28\n",
      "[2024-01-26 16:14:47,475] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=8.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303687.483849.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303687.7476401.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303688.485163.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303688.886401.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:49,471] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=14/15\n",
      "[2024-01-26 16:14:49,473] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:14:49,505] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3566\n",
      "[2024-01-26 16:14:49,509] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3497\n",
      "[2024-01-26 16:14:49,511] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3003\n",
      "[2024-01-26 16:14:49,513] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303689.203603.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:52,273] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=2.76\n",
      "[2024-01-26 16:14:52,412] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.90\n",
      "[2024-01-26 16:14:57,581] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=8.07\n",
      "[2024-01-26 16:14:57,896] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=8.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303697.898054.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303698.2138398.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303698.605619.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303698.9747221.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:14:59,758] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=15/15\n",
      "[2024-01-26 16:14:59,759] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 16:14:59,772] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:14:59,773] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:14:59,774] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:14:59,775] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303699.375533.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:02,762] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.99\n",
      "[2024-01-26 16:15:02,763] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.99\n",
      "[2024-01-26 16:15:02,763] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.98\n",
      "[2024-01-26 16:15:04,094] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=4.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303704.0964859.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303704.372085.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303704.68224.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303704.938046.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:05,809] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 16:15:05,810] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:15:05,832] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:15:05,832] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:15:05,835] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:15:05,837] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:15:05,839] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:15:05,841] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303705.263982.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:07,800] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=38, latency=1.96\n",
      "[2024-01-26 16:15:09,995] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=4.15\n",
      "[2024-01-26 16:15:09,996] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.15\n",
      "[2024-01-26 16:15:09,996] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.15\n",
      "[2024-01-26 16:15:10,682] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.83\n",
      "[2024-01-26 16:15:10,683] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303710.686573.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303710.9381702.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303711.211592.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303711.487645.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303711.7512681.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303712.0374389.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:12,557] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 16:15:12,558] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:15:12,587] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:15:12,588] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:15:12,595] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:15:12,596] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:15:12,596] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:15:12,599] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303712.290693.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:14,592] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=1.99\n",
      "[2024-01-26 16:15:17,896] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.30\n",
      "[2024-01-26 16:15:17,897] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.30\n",
      "[2024-01-26 16:15:17,897] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.28\n",
      "[2024-01-26 16:15:17,901] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.30\n",
      "[2024-01-26 16:15:17,902] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303717.924364.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303718.283946.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303718.588539.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303718.8815172.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303719.161505.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303719.465688.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:20,076] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/3\n",
      "[2024-01-26 16:15:20,077] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:15:20,114] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1339\n",
      "[2024-01-26 16:15:20,119] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1932\n",
      "[2024-01-26 16:15:20,133] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1646\n",
      "[2024-01-26 16:15:20,135] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1397\n",
      "[2024-01-26 16:15:20,138] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1746\n",
      "[2024-01-26 16:15:20,139] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303719.782202.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:23,410] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=24, latency=3.25\n",
      "[2024-01-26 16:15:24,250] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=43, latency=4.10\n",
      "[2024-01-26 16:15:26,553] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=6.39\n",
      "[2024-01-26 16:15:26,553] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=94, latency=6.40\n",
      "[2024-01-26 16:15:26,554] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=6.43\n",
      "[2024-01-26 16:15:26,555] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=6.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303726.5668328.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303726.813538.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303727.089716.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303727.400959.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303727.666796.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303727.921078.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:28,443] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/3\n",
      "[2024-01-26 16:15:28,445] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:15:28,467] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1373\n",
      "[2024-01-26 16:15:28,469] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1598\n",
      "[2024-01-26 16:15:28,474] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1743\n",
      "[2024-01-26 16:15:28,474] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1539\n",
      "[2024-01-26 16:15:28,477] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1695\n",
      "[2024-01-26 16:15:28,477] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303728.1992059.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:31,254] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=11, latency=2.78\n",
      "[2024-01-26 16:15:34,813] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=6.33\n",
      "[2024-01-26 16:15:34,814] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=6.33\n",
      "[2024-01-26 16:15:34,814] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=6.33\n",
      "[2024-01-26 16:15:34,814] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=96, latency=6.34\n",
      "[2024-01-26 16:15:34,815] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303734.830336.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303735.111937.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303735.4307148.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303735.750134.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303736.079402.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303736.3972428.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:37,042] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/3\n",
      "[2024-01-26 16:15:37,042] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:15:37,062] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1918\n",
      "[2024-01-26 16:15:37,064] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1910\n",
      "[2024-01-26 16:15:37,068] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1939\n",
      "[2024-01-26 16:15:37,072] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1918\n",
      "[2024-01-26 16:15:37,072] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1918\n",
      "[2024-01-26 16:15:37,076] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303736.7175019.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:39,762] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=2.68\n",
      "[2024-01-26 16:15:44,000] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.92\n",
      "[2024-01-26 16:15:44,001] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=6.92\n",
      "[2024-01-26 16:15:44,002] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.93\n",
      "[2024-01-26 16:15:44,004] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=6.93\n",
      "[2024-01-26 16:15:44,747] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=7.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303744.750816.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303745.01183.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303745.330934.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303745.710271.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303746.024947.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303746.360318.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:46,969] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/6\n",
      "[2024-01-26 16:15:46,970] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:15:47,002] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3000\n",
      "[2024-01-26 16:15:47,005] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2637\n",
      "[2024-01-26 16:15:47,006] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2148\n",
      "[2024-01-26 16:15:47,007] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2715\n",
      "[2024-01-26 16:15:47,014] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2404\n",
      "[2024-01-26 16:15:47,015] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303746.693897.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:55,316] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.28\n",
      "[2024-01-26 16:15:55,317] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=95, latency=8.28\n",
      "[2024-01-26 16:15:55,319] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.29\n",
      "[2024-01-26 16:15:55,319] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=8.30\n",
      "[2024-01-26 16:15:56,147] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=9.13\n",
      "[2024-01-26 16:15:56,147] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=9.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303756.1512058.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303756.462589.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303756.7291331.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303756.99038.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303757.269367.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303757.558104.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:15:58,235] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/6\n",
      "[2024-01-26 16:15:58,236] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:15:58,277] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2369\n",
      "[2024-01-26 16:15:58,279] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2803\n",
      "[2024-01-26 16:15:58,289] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2541\n",
      "[2024-01-26 16:15:58,290] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2186\n",
      "[2024-01-26 16:15:58,294] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2775\n",
      "[2024-01-26 16:15:58,295] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303757.930182.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:03,550] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=34, latency=5.24\n",
      "[2024-01-26 16:16:03,688] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=36, latency=5.39\n",
      "[2024-01-26 16:16:06,579] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=103, latency=8.28\n",
      "[2024-01-26 16:16:06,580] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.27\n",
      "[2024-01-26 16:16:07,398] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=77, latency=9.08\n",
      "[2024-01-26 16:16:07,399] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=9.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303767.40333.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303767.664385.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303768.048053.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303768.396798.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303768.7704.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303769.1285682.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:09,776] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/6\n",
      "[2024-01-26 16:16:09,777] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:16:09,798] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2686\n",
      "[2024-01-26 16:16:09,802] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2500\n",
      "[2024-01-26 16:16:09,802] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2443\n",
      "[2024-01-26 16:16:09,811] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2321\n",
      "[2024-01-26 16:16:09,813] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2428\n",
      "[2024-01-26 16:16:09,814] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303769.4358242.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:14,731] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=12, latency=4.91\n",
      "[2024-01-26 16:16:14,866] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=32, latency=5.06\n",
      "[2024-01-26 16:16:15,173] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=39, latency=5.36\n",
      "[2024-01-26 16:16:17,768] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=7.95\n",
      "[2024-01-26 16:16:17,769] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=7.95\n",
      "[2024-01-26 16:16:18,549] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=8.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303778.5508249.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303778.8515189.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303779.230423.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303779.6140301.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303779.867339.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303780.134038.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:20,644] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/6\n",
      "[2024-01-26 16:16:20,646] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:16:20,674] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2101\n",
      "[2024-01-26 16:16:20,676] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2440\n",
      "[2024-01-26 16:16:20,683] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2711\n",
      "[2024-01-26 16:16:20,686] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2771\n",
      "[2024-01-26 16:16:20,693] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2624\n",
      "[2024-01-26 16:16:20,696] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303780.385018.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:23,303] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=2.60\n",
      "[2024-01-26 16:16:23,480] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=2.79\n",
      "[2024-01-26 16:16:28,927] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=8.21\n",
      "[2024-01-26 16:16:28,927] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=94, latency=8.22\n",
      "[2024-01-26 16:16:29,245] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=8.54\n",
      "[2024-01-26 16:16:29,245] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=8.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303789.260301.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303789.5128908.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303789.941943.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303790.3062248.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303790.678226.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303791.046968.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:31,771] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/6\n",
      "[2024-01-26 16:16:31,772] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:16:31,784] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2062\n",
      "[2024-01-26 16:16:31,787] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2213\n",
      "[2024-01-26 16:16:31,788] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2608\n",
      "[2024-01-26 16:16:31,791] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2770\n",
      "[2024-01-26 16:16:31,791] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2564\n",
      "[2024-01-26 16:16:31,794] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303791.405708.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:34,479] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.69\n",
      "[2024-01-26 16:16:39,789] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=7.99\n",
      "[2024-01-26 16:16:39,790] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=7.99\n",
      "[2024-01-26 16:16:39,790] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.00\n",
      "[2024-01-26 16:16:40,616] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=8.82\n",
      "[2024-01-26 16:16:40,616] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303800.622525.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303800.9575.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303801.3310862.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303801.7300951.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303802.106669.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303802.489254.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:43,294] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/6\n",
      "[2024-01-26 16:16:43,296] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:16:43,324] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2613\n",
      "[2024-01-26 16:16:43,326] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2358\n",
      "[2024-01-26 16:16:43,332] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2613\n",
      "[2024-01-26 16:16:43,333] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2613\n",
      "[2024-01-26 16:16:43,335] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2613\n",
      "[2024-01-26 16:16:43,336] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303802.876071.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:51,579] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=8.24\n",
      "[2024-01-26 16:16:51,579] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=8.25\n",
      "[2024-01-26 16:16:51,579] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=8.25\n",
      "[2024-01-26 16:16:51,579] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=8.24\n",
      "[2024-01-26 16:16:52,406] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=9.07\n",
      "[2024-01-26 16:16:52,408] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=9.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303812.411702.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303812.6812649.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303813.057832.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303813.451751.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303813.837534.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303814.204692.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:54,851] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/10\n",
      "[2024-01-26 16:16:54,852] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:16:54,887] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3000\n",
      "[2024-01-26 16:16:54,904] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3482\n",
      "[2024-01-26 16:16:54,896] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3896\n",
      "[2024-01-26 16:16:54,898] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3789\n",
      "[2024-01-26 16:16:54,905] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3144\n",
      "[2024-01-26 16:16:54,893] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303814.523229.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:16:58,496] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=17, latency=3.56\n",
      "[2024-01-26 16:17:03,082] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=27, latency=8.14\n",
      "[2024-01-26 16:17:04,929] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=91, latency=10.02\n",
      "[2024-01-26 16:17:04,930] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.01\n",
      "[2024-01-26 16:17:05,670] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.73\n",
      "[2024-01-26 16:17:05,672] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=10.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303825.700682.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303826.009448.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303826.36857.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303826.7600682.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303827.1780841.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303827.6058931.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:17:08,386] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/10\n",
      "[2024-01-26 16:17:08,388] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:17:08,419] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3639\n",
      "[2024-01-26 16:17:08,420] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3014\n",
      "[2024-01-26 16:17:08,431] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3891\n",
      "[2024-01-26 16:17:08,433] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3419\n",
      "[2024-01-26 16:17:08,435] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3458\n",
      "[2024-01-26 16:17:08,436] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303828.010901.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:17:14,184] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=12, latency=5.75\n",
      "[2024-01-26 16:17:16,798] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=27, latency=8.36\n",
      "[2024-01-26 16:17:18,426] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.00\n",
      "[2024-01-26 16:17:18,426] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.00\n",
      "[2024-01-26 16:17:18,426] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=9.99\n",
      "[2024-01-26 16:17:19,270] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=10.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303839.2734752.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303839.564985.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303840.015886.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303840.383939.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303840.757697.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303841.188194.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:17:22,006] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/10\n",
      "[2024-01-26 16:17:22,007] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:17:22,039] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3508\n",
      "[2024-01-26 16:17:22,043] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3436\n",
      "[2024-01-26 16:17:22,045] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3087\n",
      "[2024-01-26 16:17:22,048] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3249\n",
      "[2024-01-26 16:17:22,054] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3680\n",
      "[2024-01-26 16:17:22,055] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303841.559336.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:17:32,095] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=10.05\n",
      "[2024-01-26 16:17:32,095] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=10.05\n",
      "[2024-01-26 16:17:32,096] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=10.05\n",
      "[2024-01-26 16:17:33,016] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=10.96\n",
      "[2024-01-26 16:17:33,016] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=10.96\n",
      "[2024-01-26 16:17:33,890] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=11.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303853.8923352.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303854.14513.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303854.547349.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303854.914281.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303855.301458.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303855.6784708.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:17:36,498] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/10\n",
      "[2024-01-26 16:17:36,499] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:17:36,533] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3463\n",
      "[2024-01-26 16:17:36,545] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3996\n",
      "[2024-01-26 16:17:36,547] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3050\n",
      "[2024-01-26 16:17:36,555] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3737\n",
      "[2024-01-26 16:17:36,555] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3410\n",
      "[2024-01-26 16:17:36,560] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303856.091329.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:17:39,813] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=15, latency=3.26\n",
      "[2024-01-26 16:17:46,782] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.24\n",
      "[2024-01-26 16:17:46,783] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=10.23\n",
      "[2024-01-26 16:17:47,412] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=10.85\n",
      "[2024-01-26 16:17:47,412] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=10.85\n",
      "[2024-01-26 16:17:48,292] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=11.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303868.293865.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303868.5247948.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303868.937935.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303869.327028.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303869.711757.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303870.154163.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:17:50,747] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/10\n",
      "[2024-01-26 16:17:50,748] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:17:50,770] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3171\n",
      "[2024-01-26 16:17:50,774] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3655\n",
      "[2024-01-26 16:17:50,775] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3098\n",
      "[2024-01-26 16:17:50,779] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3662\n",
      "[2024-01-26 16:17:50,780] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3704\n",
      "[2024-01-26 16:17:50,781] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303870.440927.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:00,692] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=9.91\n",
      "[2024-01-26 16:18:00,692] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=9.91\n",
      "[2024-01-26 16:18:00,693] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=9.92\n",
      "[2024-01-26 16:18:01,595] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.81\n",
      "[2024-01-26 16:18:01,596] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=10.81\n",
      "[2024-01-26 16:18:02,437] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=11.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303882.439811.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303882.695426.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303883.096931.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303883.5065758.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303883.8215811.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303884.1835158.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:04,963] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/10\n",
      "[2024-01-26 16:18:04,964] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:18:04,987] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3098\n",
      "[2024-01-26 16:18:04,992] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3132\n",
      "[2024-01-26 16:18:04,995] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3030\n",
      "[2024-01-26 16:18:05,000] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3909\n",
      "[2024-01-26 16:18:05,001] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3135\n",
      "[2024-01-26 16:18:05,002] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303884.60898.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:07,860] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=2.83\n",
      "[2024-01-26 16:18:09,933] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=4.93\n",
      "[2024-01-26 16:18:09,934] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=4.90\n",
      "[2024-01-26 16:18:14,599] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=9.59\n",
      "[2024-01-26 16:18:14,601] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=9.56\n",
      "[2024-01-26 16:18:15,672] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=103, latency=10.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303895.6825442.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303895.921978.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303896.267484.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303896.647677.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303897.0240571.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303897.278486.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:17,802] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=7/10\n",
      "[2024-01-26 16:18:17,803] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:18:17,826] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3208\n",
      "[2024-01-26 16:18:17,829] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3475\n",
      "[2024-01-26 16:18:17,831] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3848\n",
      "[2024-01-26 16:18:17,832] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3302\n",
      "[2024-01-26 16:18:17,835] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3681\n",
      "[2024-01-26 16:18:17,836] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303897.5537012.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:20,716] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.88\n",
      "[2024-01-26 16:18:27,767] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=9.93\n",
      "[2024-01-26 16:18:27,767] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=9.94\n",
      "[2024-01-26 16:18:28,056] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=10.22\n",
      "[2024-01-26 16:18:28,056] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=10.22\n",
      "[2024-01-26 16:18:28,904] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=11.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303908.9060009.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303909.2106838.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303909.5187042.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303909.789675.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303910.070883.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303910.39579.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:31,015] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=8/10\n",
      "[2024-01-26 16:18:31,016] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:18:31,039] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3614\n",
      "[2024-01-26 16:18:31,042] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3537\n",
      "[2024-01-26 16:18:31,052] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3476\n",
      "[2024-01-26 16:18:31,054] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3165\n",
      "[2024-01-26 16:18:31,061] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3363\n",
      "[2024-01-26 16:18:31,075] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303910.700042.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:34,078] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=3.01\n",
      "[2024-01-26 16:18:35,667] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=4.58\n",
      "[2024-01-26 16:18:35,716] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=4.63\n",
      "[2024-01-26 16:18:35,988] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=4.90\n",
      "[2024-01-26 16:18:36,881] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=5.79\n",
      "[2024-01-26 16:18:37,014] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=5.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303917.016334.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303917.28212.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303917.5744529.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303917.8283038.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303918.0799801.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303918.382454.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:38,871] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=9/10\n",
      "[2024-01-26 16:18:38,873] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:18:38,906] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3882\n",
      "[2024-01-26 16:18:38,907] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3567\n",
      "[2024-01-26 16:18:38,908] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3616\n",
      "[2024-01-26 16:18:38,913] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3330\n",
      "[2024-01-26 16:18:38,913] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3497\n",
      "[2024-01-26 16:18:38,917] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303918.624875.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:41,829] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=2.92\n",
      "[2024-01-26 16:18:43,634] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=4.72\n",
      "[2024-01-26 16:18:43,870] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=4.95\n",
      "[2024-01-26 16:18:43,871] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=4.95\n",
      "[2024-01-26 16:18:45,138] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=6.22\n",
      "[2024-01-26 16:18:48,596] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=9.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303928.59888.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303928.847224.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303929.399203.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303929.819515.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303930.159275.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303930.579872.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:51,295] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=10/10\n",
      "[2024-01-26 16:18:51,296] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 16:18:51,331] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3003\n",
      "[2024-01-26 16:18:51,332] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3003\n",
      "[2024-01-26 16:18:51,332] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:18:51,333] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3783\n",
      "[2024-01-26 16:18:51,333] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3003\n",
      "[2024-01-26 16:18:51,335] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303931.013769.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:18:54,014] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=2.67\n",
      "[2024-01-26 16:18:56,144] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=4.81\n",
      "[2024-01-26 16:19:00,630] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=104, latency=9.29\n",
      "[2024-01-26 16:19:00,630] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=104, latency=9.30\n",
      "[2024-01-26 16:19:00,812] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=105, latency=9.47\n",
      "[2024-01-26 16:19:01,632] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=105, latency=10.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303941.635093.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303941.9483871.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303942.2318249.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303942.553887.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303942.86255.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303943.223665.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:03,983] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 16:19:03,983] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:19:04,008] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:19:04,018] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:19:04,018] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:19:04,020] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:19:04,021] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:19:04,023] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:19:04,027] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 16:19:04,027] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303943.615807.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:06,293] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=39, latency=2.26\n",
      "[2024-01-26 16:19:08,407] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=4.38\n",
      "[2024-01-26 16:19:08,407] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=4.36\n",
      "[2024-01-26 16:19:08,408] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=4.37\n",
      "[2024-01-26 16:19:08,409] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.35\n",
      "[2024-01-26 16:19:09,102] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=5.04\n",
      "[2024-01-26 16:19:09,103] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=5.05\n",
      "[2024-01-26 16:19:09,103] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=5.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303949.112731.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303949.384348.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303949.67732.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303949.930115.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303950.1644979.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303950.4095821.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303950.659314.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303950.908514.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:11,441] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 16:19:11,442] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:19:11,465] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:19:11,467] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:19:11,469] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:19:11,469] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:19:11,471] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:19:11,472] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:19:11,472] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 16:19:11,472] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303951.148391.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:13,930] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.45\n",
      "[2024-01-26 16:19:17,301] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.82\n",
      "[2024-01-26 16:19:17,301] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.82\n",
      "[2024-01-26 16:19:17,302] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.82\n",
      "[2024-01-26 16:19:17,302] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.82\n",
      "[2024-01-26 16:19:17,302] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.83\n",
      "[2024-01-26 16:19:17,304] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.83\n",
      "[2024-01-26 16:19:17,304] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303957.3150778.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303957.6081622.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303957.8832378.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303958.162698.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303958.445896.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303958.742337.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303959.0028942.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303959.244214.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:19,782] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/2\n",
      "[2024-01-26 16:19:19,783] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:19:19,792] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1154\n",
      "[2024-01-26 16:19:19,792] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1339\n",
      "[2024-01-26 16:19:19,794] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1932\n",
      "[2024-01-26 16:19:19,795] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1397\n",
      "[2024-01-26 16:19:19,796] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1646\n",
      "[2024-01-26 16:19:19,797] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1746\n",
      "[2024-01-26 16:19:19,797] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1598\n",
      "[2024-01-26 16:19:19,798] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303959.494893.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:24,112] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=11, latency=4.30\n",
      "[2024-01-26 16:19:24,447] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=39, latency=4.65\n",
      "[2024-01-26 16:19:24,575] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=24, latency=4.77\n",
      "[2024-01-26 16:19:26,930] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=95, latency=7.13\n",
      "[2024-01-26 16:19:26,930] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=7.13\n",
      "[2024-01-26 16:19:26,930] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=96, latency=7.12\n",
      "[2024-01-26 16:19:26,931] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=7.13\n",
      "[2024-01-26 16:19:26,931] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=69, latency=7.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303966.9512131.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303967.1819332.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303967.453145.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303967.7294898.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303968.012825.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303968.3386528.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303968.664095.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303968.952977.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:29,486] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/2\n",
      "[2024-01-26 16:19:29,487] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:19:29,516] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1743\n",
      "[2024-01-26 16:19:29,521] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1539\n",
      "[2024-01-26 16:19:29,522] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1918\n",
      "[2024-01-26 16:19:29,525] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1421\n",
      "[2024-01-26 16:19:29,525] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1695\n",
      "[2024-01-26 16:19:29,529] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1910\n",
      "[2024-01-26 16:19:29,530] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1939\n",
      "[2024-01-26 16:19:29,535] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=1743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303969.238059.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:34,096] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=4.53\n",
      "[2024-01-26 16:19:37,110] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=7.55\n",
      "[2024-01-26 16:19:37,112] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=7.56\n",
      "[2024-01-26 16:19:37,112] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=7.57\n",
      "[2024-01-26 16:19:37,113] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=7.58\n",
      "[2024-01-26 16:19:37,113] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=7.57\n",
      "[2024-01-26 16:19:37,893] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=8.33\n",
      "[2024-01-26 16:19:37,893] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=8.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303977.898761.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303978.186486.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303978.4972422.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303978.828866.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303979.149444.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303979.470361.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303979.7946272.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303980.138867.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:40,742] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/4\n",
      "[2024-01-26 16:19:40,743] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:19:40,766] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2637\n",
      "[2024-01-26 16:19:40,767] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2148\n",
      "[2024-01-26 16:19:40,768] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3000\n",
      "[2024-01-26 16:19:40,772] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2404\n",
      "[2024-01-26 16:19:40,773] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2715\n",
      "[2024-01-26 16:19:40,773] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2150\n",
      "[2024-01-26 16:19:40,774] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2803\n",
      "[2024-01-26 16:19:40,774] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303980.4176981.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:48,226] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=36, latency=7.44\n",
      "[2024-01-26 16:19:49,658] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=66, latency=8.88\n",
      "[2024-01-26 16:19:50,257] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=9.49\n",
      "[2024-01-26 16:19:50,257] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=95, latency=9.48\n",
      "[2024-01-26 16:19:50,257] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=9.48\n",
      "[2024-01-26 16:19:50,257] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=9.48\n",
      "[2024-01-26 16:19:51,108] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.33\n",
      "[2024-01-26 16:19:51,912] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=103, latency=11.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303991.916387.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303992.175541.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303992.5317671.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303992.942549.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303993.2846138.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303993.707304.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303994.1131048.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303994.54034.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:19:55,271] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/4\n",
      "[2024-01-26 16:19:55,272] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:19:55,306] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2675\n",
      "[2024-01-26 16:19:55,311] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2541\n",
      "[2024-01-26 16:19:55,320] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2186\n",
      "[2024-01-26 16:19:55,323] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2686\n",
      "[2024-01-26 16:19:55,331] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2775\n",
      "[2024-01-26 16:19:55,336] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2443\n",
      "[2024-01-26 16:19:55,338] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2321\n",
      "[2024-01-26 16:19:55,338] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706303994.878196.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:20:01,090] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=32, latency=5.74\n",
      "[2024-01-26 16:20:01,137] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=34, latency=5.79\n",
      "[2024-01-26 16:20:02,479] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=31, latency=7.12\n",
      "[2024-01-26 16:20:04,678] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=9.32\n",
      "[2024-01-26 16:20:04,678] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=9.34\n",
      "[2024-01-26 16:20:05,527] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=77, latency=10.17\n",
      "[2024-01-26 16:20:05,527] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.18\n",
      "[2024-01-26 16:20:06,299] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304006.3011599.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304006.569834.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304006.9867158.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304007.38164.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304007.715958.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304008.085942.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304008.445259.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304008.837038.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:20:09,647] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/4\n",
      "[2024-01-26 16:20:09,648] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:20:09,684] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2458\n",
      "[2024-01-26 16:20:09,684] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2101\n",
      "[2024-01-26 16:20:09,689] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2428\n",
      "[2024-01-26 16:20:09,701] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2711\n",
      "[2024-01-26 16:20:09,701] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2771\n",
      "[2024-01-26 16:20:09,705] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2440\n",
      "[2024-01-26 16:20:09,706] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2691\n",
      "[2024-01-26 16:20:09,713] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304009.2678049.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:20:12,511] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=2.79\n",
      "[2024-01-26 16:20:14,421] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=12, latency=4.70\n",
      "[2024-01-26 16:20:14,556] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=4.83\n",
      "[2024-01-26 16:20:19,042] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=9.33\n",
      "[2024-01-26 16:20:19,043] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=9.33\n",
      "[2024-01-26 16:20:19,444] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=9.71\n",
      "[2024-01-26 16:20:19,445] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=9.71\n",
      "[2024-01-26 16:20:20,233] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=10.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304020.2355819.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304020.501386.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304020.934232.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304021.2976341.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304021.697207.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304022.068117.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304022.463249.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304022.782382.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:20:23,337] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/4\n",
      "[2024-01-26 16:20:23,338] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:20:23,370] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2213\n",
      "[2024-01-26 16:20:23,371] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2062\n",
      "[2024-01-26 16:20:23,378] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2608\n",
      "[2024-01-26 16:20:23,379] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2770\n",
      "[2024-01-26 16:20:23,381] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2333\n",
      "[2024-01-26 16:20:23,381] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2564\n",
      "[2024-01-26 16:20:23,384] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2358\n",
      "[2024-01-26 16:20:23,384] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=2613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304023.054327.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:20:32,576] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=9.19\n",
      "[2024-01-26 16:20:32,577] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=9.20\n",
      "[2024-01-26 16:20:32,578] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=9.19\n",
      "[2024-01-26 16:20:32,578] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=9.19\n",
      "[2024-01-26 16:20:33,429] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.03\n",
      "[2024-01-26 16:20:33,429] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=10.04\n",
      "[2024-01-26 16:20:33,430] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=10.03\n",
      "[2024-01-26 16:20:34,257] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=10.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304034.260759.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304034.564523.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304034.9374762.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304035.267512.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304035.5591302.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304035.865556.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304036.15616.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304036.463037.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:20:37,104] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=1/8\n",
      "[2024-01-26 16:20:37,106] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:20:37,137] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3000\n",
      "[2024-01-26 16:20:37,148] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3896\n",
      "[2024-01-26 16:20:37,150] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3450\n",
      "[2024-01-26 16:20:37,151] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3789\n",
      "[2024-01-26 16:20:37,152] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3144\n",
      "[2024-01-26 16:20:37,154] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3482\n",
      "[2024-01-26 16:20:37,155] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3014\n",
      "[2024-01-26 16:20:37,159] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304036.762689.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:20:43,149] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=17, latency=5.99\n",
      "[2024-01-26 16:20:45,742] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=35, latency=8.58\n",
      "[2024-01-26 16:20:48,874] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=11.72\n",
      "[2024-01-26 16:20:48,874] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=11.72\n",
      "[2024-01-26 16:20:48,875] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=11.73\n",
      "[2024-01-26 16:20:50,677] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=91, latency=13.51\n",
      "[2024-01-26 16:20:50,677] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=13.51\n",
      "[2024-01-26 16:20:51,537] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=14.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304051.539283.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304051.840062.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304052.1976168.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304052.609499.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304052.978867.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304053.4007602.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304053.840022.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304054.1977248.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:20:54,898] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=2/8\n",
      "[2024-01-26 16:20:54,898] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:20:54,926] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3891\n",
      "[2024-01-26 16:20:54,930] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3575\n",
      "[2024-01-26 16:20:54,937] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3458\n",
      "[2024-01-26 16:20:54,937] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3419\n",
      "[2024-01-26 16:20:54,939] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3508\n",
      "[2024-01-26 16:20:54,939] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3436\n",
      "[2024-01-26 16:20:54,941] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3087\n",
      "[2024-01-26 16:20:54,942] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304054.619647.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:21:03,248] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=27, latency=8.31\n",
      "[2024-01-26 16:21:06,726] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=11.78\n",
      "[2024-01-26 16:21:06,726] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=11.79\n",
      "[2024-01-26 16:21:06,727] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=11.78\n",
      "[2024-01-26 16:21:07,624] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=12.68\n",
      "[2024-01-26 16:21:08,378] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=90, latency=13.43\n",
      "[2024-01-26 16:21:08,520] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=13.57\n",
      "[2024-01-26 16:21:09,368] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=14.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304069.370226.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304069.6741362.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304070.106157.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304070.4891148.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304070.853524.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304071.25156.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304071.640088.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304072.029485.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:21:12,820] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=3/8\n",
      "[2024-01-26 16:21:12,820] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:21:12,852] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3680\n",
      "[2024-01-26 16:21:12,875] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3463\n",
      "[2024-01-26 16:21:12,883] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3996\n",
      "[2024-01-26 16:21:12,889] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3410\n",
      "[2024-01-26 16:21:12,890] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3737\n",
      "[2024-01-26 16:21:12,891] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3979\n",
      "[2024-01-26 16:21:12,891] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3050\n",
      "[2024-01-26 16:21:12,894] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304072.445247.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:21:18,972] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=15, latency=6.06\n",
      "[2024-01-26 16:21:25,004] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=12.12\n",
      "[2024-01-26 16:21:25,004] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=12.11\n",
      "[2024-01-26 16:21:25,005] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=12.10\n",
      "[2024-01-26 16:21:25,928] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=13.02\n",
      "[2024-01-26 16:21:26,848] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=13.93\n",
      "[2024-01-26 16:21:26,848] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=13.93\n",
      "[2024-01-26 16:21:27,812] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=14.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304087.814445.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304088.1269372.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304088.547157.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304088.9334762.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304089.1976051.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304089.4847412.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304089.754913.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304090.067044.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:21:30,907] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=4/8\n",
      "[2024-01-26 16:21:30,908] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:21:30,941] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3171\n",
      "[2024-01-26 16:21:30,954] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3655\n",
      "[2024-01-26 16:21:30,955] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3098\n",
      "[2024-01-26 16:21:30,962] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3662\n",
      "[2024-01-26 16:21:30,969] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3098\n",
      "[2024-01-26 16:21:30,970] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3400\n",
      "[2024-01-26 16:21:30,971] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3704\n",
      "[2024-01-26 16:21:30,974] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304090.476434.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:21:36,775] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=33, latency=5.82\n",
      "[2024-01-26 16:21:39,029] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=8.05\n",
      "[2024-01-26 16:21:40,917] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=9.93\n",
      "[2024-01-26 16:21:42,577] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=97, latency=11.61\n",
      "[2024-01-26 16:21:42,577] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=11.61\n",
      "[2024-01-26 16:21:43,482] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=12.51\n",
      "[2024-01-26 16:21:43,483] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=12.50\n",
      "[2024-01-26 16:21:44,341] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=13.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304104.343423.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304104.660204.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304105.006613.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304105.398032.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304105.825815.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304106.1951308.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304106.6021361.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304106.9713228.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:21:47,778] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=5/8\n",
      "[2024-01-26 16:21:47,778] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:21:47,795] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3030\n",
      "[2024-01-26 16:21:47,796] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3135\n",
      "[2024-01-26 16:21:47,798] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3132\n",
      "[2024-01-26 16:21:47,799] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3208\n",
      "[2024-01-26 16:21:47,806] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3848\n",
      "[2024-01-26 16:21:47,806] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3971\n",
      "[2024-01-26 16:21:47,808] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3475\n",
      "[2024-01-26 16:21:47,808] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304107.334589.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:21:53,008] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=5.19\n",
      "[2024-01-26 16:21:55,798] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=7, latency=7.98\n",
      "[2024-01-26 16:21:59,354] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=103, latency=11.55\n",
      "[2024-01-26 16:21:59,354] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=11.54\n",
      "[2024-01-26 16:21:59,354] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=11.54\n",
      "[2024-01-26 16:22:00,210] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=12.40\n",
      "[2024-01-26 16:22:01,100] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=13.28\n",
      "[2024-01-26 16:22:02,014] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=14.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304122.018197.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304122.3532531.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304122.669801.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304122.982709.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304123.2705631.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304123.6131022.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304124.1826482.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304124.598933.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:22:05,446] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=6/8\n",
      "[2024-01-26 16:22:05,446] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:22:05,475] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3581\n",
      "[2024-01-26 16:22:05,480] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3302\n",
      "[2024-01-26 16:22:05,481] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3614\n",
      "[2024-01-26 16:22:05,485] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3537\n",
      "[2024-01-26 16:22:05,487] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3476\n",
      "[2024-01-26 16:22:05,488] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3235\n",
      "[2024-01-26 16:22:05,488] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3165\n",
      "[2024-01-26 16:22:05,494] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304125.030932.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:22:08,489] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=3.00\n",
      "[2024-01-26 16:22:10,078] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=4.58\n",
      "[2024-01-26 16:22:10,126] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=4.63\n",
      "[2024-01-26 16:22:12,019] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=6.52\n",
      "[2024-01-26 16:22:12,117] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=9, latency=6.63\n",
      "[2024-01-26 16:22:12,117] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=6.62\n",
      "[2024-01-26 16:22:12,984] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=10, latency=7.49\n",
      "[2024-01-26 16:22:17,272] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=11.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304137.274979.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304137.61024.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304138.014307.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304138.383897.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304138.727608.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304139.148272.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304139.56345.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304139.9635332.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:22:20,762] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=7/8\n",
      "[2024-01-26 16:22:20,763] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:22:20,801] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3882\n",
      "[2024-01-26 16:22:20,801] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3330\n",
      "[2024-01-26 16:22:20,802] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3567\n",
      "[2024-01-26 16:22:20,804] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3616\n",
      "[2024-01-26 16:22:20,809] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3497\n",
      "[2024-01-26 16:22:20,810] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3003\n",
      "[2024-01-26 16:22:20,812] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3566\n",
      "[2024-01-26 16:22:20,813] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304140.3560882.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:22:23,659] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=8, latency=2.84\n",
      "[2024-01-26 16:22:25,566] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=6, latency=4.75\n",
      "[2024-01-26 16:22:32,543] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=11.74\n",
      "[2024-01-26 16:22:32,544] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=11.74\n",
      "[2024-01-26 16:22:32,929] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=12.11\n",
      "[2024-01-26 16:22:33,787] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=93, latency=12.98\n",
      "[2024-01-26 16:22:33,787] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=12.97\n",
      "[2024-01-26 16:22:34,640] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=102, latency=13.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304154.641855.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304154.94292.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304155.240814.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304155.57776.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304155.93897.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304156.37209.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304156.81478.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304157.228746.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:22:37,983] p91528 {3160928730.py:26} INFO - e_idx=1/1, chunk_index=8/8\n",
      "[2024-01-26 16:22:37,984] p91528 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 16:22:38,031] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:22:38,032] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:22:38,035] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:22:38,037] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:22:38,038] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:22:38,049] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:22:38,050] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n",
      "[2024-01-26 16:22:38,050] p91528 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=3271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304157.585929.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:22:41,831] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=3.79\n",
      "[2024-01-26 16:22:41,835] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=3.78\n",
      "[2024-01-26 16:22:41,835] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=3.78\n",
      "[2024-01-26 16:22:41,836] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=3.79\n",
      "[2024-01-26 16:22:45,541] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=7.50\n",
      "[2024-01-26 16:22:45,541] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=7.48\n",
      "[2024-01-26 16:22:45,541] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=7.48\n",
      "[2024-01-26 16:22:45,542] p91528 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=7.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304165.5509472.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304165.829972.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304166.205354.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304166.630198.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304166.8688798.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304167.1291578.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304167.3998551.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304167.722857.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:22:48,384] p91528 {3160928730.py:59} INFO - experiment=1/1, name=mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0, done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706304168.106317.json\n"
     ]
    }
   ],
   "source": [
    "# for each experiment\n",
    "#   - for each endpoint and concurrency in an experiment\n",
    "\n",
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "_ = list(map(clear_dir, [METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]))\n",
    "\n",
    "num_experiments: int = len(config['experiments'])\n",
    "for e_idx, experiment in enumerate(config['experiments']):\n",
    "    e_idx += 1  # Increment experiment index\n",
    "    # Call do_experiment function to create the predictor object\n",
    " \n",
    "    predictor = create_predictor_for_experiment(experiment, config, endpoint_info_list)\n",
    "    if predictor is None:\n",
    "        logger.error(f\"predictor could not be created for experiment={experiment}, moving to next...\")\n",
    "        continue\n",
    "\n",
    "    # Process combinations of concurrency levels and payload files\n",
    "    combination_data = create_combinations(experiment)\n",
    "\n",
    "    for concurrency, payload_file, split_payload in combination_data:\n",
    "        for chunk_index, chunk in enumerate(split_payload):\n",
    "            logger.info(f\"e_idx={e_idx}/{num_experiments}, chunk_index={chunk_index+1}/{len(split_payload)}\")\n",
    "\n",
    "            # Process each chunk and calculate metrics\n",
    "            responses, metrics = await run_inferences(predictor, chunk, experiment, concurrency, payload_file)\n",
    "            if metrics:\n",
    "                # Convert metrics to JSON string\n",
    "                metrics_json = json.dumps(metrics, indent=2)\n",
    "                # Define S3 file path\n",
    "                metrics_file_name = f\"{time.time()}.json\"\n",
    "                metrics_s3_path = os.path.join(METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "                # Write to S3\n",
    "                write_to_s3(metrics_json, BUCKET_NAME, \"\", METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "\n",
    "            if responses:\n",
    "                for r in responses:\n",
    "                    # Convert response to JSON string\n",
    "                    response_json = json.dumps(r, indent=2)\n",
    "                    # Define S3 file path\n",
    "                    response_file_name = f\"{time.time()}.json\"\n",
    "                    response_s3_path = os.path.join(METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "                    # Write to S3\n",
    "                    write_to_s3(response_json, BUCKET_NAME, \"\", METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "\n",
    "    logger.info(f\"experiment={e_idx}/{num_experiments}, name={experiment['name']}, done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list files in S3 bucket with a specific prefix\n",
    "def list_s3_files(bucket, prefix, suffix='.json'):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    return [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith(suffix)]\n",
    "\n",
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(BUCKET_NAME, METRICS_PER_INFERENCE_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = []\n",
    "for file_key in s3_files:\n",
    "    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=file_key)\n",
    "    file_content = response['Body'].read().decode('utf-8')\n",
    "    json_obj = json.loads(file_content)\n",
    "    json_list.append(json_obj)\n",
    "\n",
    "# Create DataFrame\n",
    "df_responses = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all responses\")\n",
    "df_responses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>3.724005</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>16</td>\n",
       "      <td>0.807353</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>1339</td>\n",
       "      <td>\\n\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n``...</td>\n",
       "      <td>1339</td>\n",
       "      <td>69</td>\n",
       "      <td>4.098635</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>1932</td>\n",
       "      <td>Altu elikbilek is younger.\\n\\nQuestion: Who...</td>\n",
       "      <td>1932</td>\n",
       "      <td>98</td>\n",
       "      <td>4.446627</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>1154</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nGepi\\nThe Gepi is a rig...</td>\n",
       "      <td>1154</td>\n",
       "      <td>101</td>\n",
       "      <td>3.987915</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3271</td>\n",
       "      <td>\\nThe Owl and the Pussy-Cat\\n```</td>\n",
       "      <td>3271</td>\n",
       "      <td>16</td>\n",
       "      <td>3.789928</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3271</td>\n",
       "      <td>\\nThe Owl and the Pussy-Cat\\n```</td>\n",
       "      <td>3271</td>\n",
       "      <td>16</td>\n",
       "      <td>3.777702</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3271</td>\n",
       "      <td>\\nThe Owl and the Pussy-Cat\\n```</td>\n",
       "      <td>3271</td>\n",
       "      <td>16</td>\n",
       "      <td>7.483910</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3271</td>\n",
       "      <td>\\nThe Owl and the Pussy-Cat\\n```</td>\n",
       "      <td>3271</td>\n",
       "      <td>16</td>\n",
       "      <td>7.477405</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3271</td>\n",
       "      <td>\\nThe Owl and the Pussy-Cat\\n```</td>\n",
       "      <td>3271</td>\n",
       "      <td>16</td>\n",
       "      <td>7.480637</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        endpoint_name  \\\n",
       "0    lmistral7b-g5-2xlarge-1706302351   \n",
       "1    lmistral7b-g5-2xlarge-1706302351   \n",
       "2    lmistral7b-g5-2xlarge-1706302351   \n",
       "3    lmistral7b-g5-2xlarge-1706302351   \n",
       "4    lmistral7b-g5-2xlarge-1706302351   \n",
       "..                                ...   \n",
       "581  lmistral7b-g5-2xlarge-1706302351   \n",
       "582  lmistral7b-g5-2xlarge-1706302351   \n",
       "583  lmistral7b-g5-2xlarge-1706302351   \n",
       "584  lmistral7b-g5-2xlarge-1706302351   \n",
       "585  lmistral7b-g5-2xlarge-1706302351   \n",
       "\n",
       "                                                prompt  do_sample  \\\n",
       "0    <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True   \n",
       "1    <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True   \n",
       "2    <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True   \n",
       "3    <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True   \n",
       "4    <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True   \n",
       "..                                                 ...        ...   \n",
       "581  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True   \n",
       "582  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True   \n",
       "583  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True   \n",
       "584  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True   \n",
       "585  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True   \n",
       "\n",
       "     temperature  top_p  top_k  max_new_tokens  truncate  \\\n",
       "0            0.1   0.92    120             100       304   \n",
       "1            0.1   0.92    120             100       980   \n",
       "2            0.1   0.92    120             100      1339   \n",
       "3            0.1   0.92    120             100      1932   \n",
       "4            0.1   0.92    120             100      1154   \n",
       "..           ...    ...    ...             ...       ...   \n",
       "581          0.1   0.92    120             100      3271   \n",
       "582          0.1   0.92    120             100      3271   \n",
       "583          0.1   0.92    120             100      3271   \n",
       "584          0.1   0.92    120             100      3271   \n",
       "585          0.1   0.92    120             100      3271   \n",
       "\n",
       "                                            completion  prompt_tokens  \\\n",
       "0    \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "1     Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "2    \\n\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n``...           1339   \n",
       "3     Altu elikbilek is younger.\\n\\nQuestion: Who...           1932   \n",
       "4    \\n\\n```\\nPassage 1:\\nGepi\\nThe Gepi is a rig...           1154   \n",
       "..                                                 ...            ...   \n",
       "581                   \\nThe Owl and the Pussy-Cat\\n```           3271   \n",
       "582                   \\nThe Owl and the Pussy-Cat\\n```           3271   \n",
       "583                   \\nThe Owl and the Pussy-Cat\\n```           3271   \n",
       "584                   \\nThe Owl and the Pussy-Cat\\n```           3271   \n",
       "585                   \\nThe Owl and the Pussy-Cat\\n```           3271   \n",
       "\n",
       "     completion_tokens   latency  \\\n",
       "0                  101  3.724005   \n",
       "1                   16  0.807353   \n",
       "2                   69  4.098635   \n",
       "3                   98  4.446627   \n",
       "4                  101  3.987915   \n",
       "..                 ...       ...   \n",
       "581                 16  3.789928   \n",
       "582                 16  3.777702   \n",
       "583                 16  7.483910   \n",
       "584                 16  7.477405   \n",
       "585                 16  7.480637   \n",
       "\n",
       "                                       experiment_name  concurrency  \n",
       "0    mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "1    mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "2    mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "3    mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "4    mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "..                                                 ...          ...  \n",
       "581  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "582  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "583  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "584  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "585  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "\n",
       "[586 rows x 14 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses[df_responses.endpoint_name.str.contains(\"g5\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:23:30,136] p91528 {2358223361.py:3} INFO - files recieved from s3 for per inference request --> {'ResponseMetadata': {'RequestId': 'TMTKW1ZPX4ZFP6Q3', 'HostId': 'i8XGZTcCLKh0xLuXYsbg5e8Mg+ZIQGR2RQwKRjQaz6dPXzE9g+Ww2dZ/FIOn1tbIxLNugNh/LoA=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'i8XGZTcCLKh0xLuXYsbg5e8Mg+ZIQGR2RQwKRjQaz6dPXzE9g+Ww2dZ/FIOn1tbIxLNugNh/LoA=', 'x-amz-request-id': 'TMTKW1ZPX4ZFP6Q3', 'date': 'Fri, 26 Jan 2024 21:23:31 GMT', 'x-amz-bucket-region': 'us-east-1', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'IsTruncated': False, 'Contents': [{'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302638.428124.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 19, tzinfo=tzutc()), 'ETag': '\"187cc797cd1226fde80bdb8886784a33\"', 'Size': 557, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302639.755789.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 20, tzinfo=tzutc()), 'ETag': '\"9a25791f16e04224cb0ab683575ddcf1\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302644.377101.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 25, tzinfo=tzutc()), 'ETag': '\"353ea76b6d76694af1bdc8eb4d4bc069\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302649.3651452.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 30, tzinfo=tzutc()), 'ETag': '\"bd85ec00e76e5b94dd02214bcbd2ffe5\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302653.956031.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 35, tzinfo=tzutc()), 'ETag': '\"4e1020d0e7c4d0b7d8cf67c818ed04a4\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302656.447826.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 37, tzinfo=tzutc()), 'ETag': '\"0481b3dbc9e57b825ac247cb475e8571\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302661.084521.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 42, tzinfo=tzutc()), 'ETag': '\"7f5641a0567cddd8d41f73e9ced7b0e0\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302662.977522.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 44, tzinfo=tzutc()), 'ETag': '\"4ac6c708bfe911e646fc999fcc552d20\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302664.5037491.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 45, tzinfo=tzutc()), 'ETag': '\"c930322eb33f9729abd7c80259714436\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302669.368408.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 50, tzinfo=tzutc()), 'ETag': '\"3ced15d9448e797c03b6f324b56c116d\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302674.233114.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 57, 55, tzinfo=tzutc()), 'ETag': '\"e44e70c88e52660e0511a55344f2df4b\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302679.0160031.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, tzinfo=tzutc()), 'ETag': '\"261d60f745c9aa98b7abfc1785a1cbac\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302683.7958899.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 4, tzinfo=tzutc()), 'ETag': '\"0892c3fbdd394e8d640176f8462903d5\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302688.393214.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 9, tzinfo=tzutc()), 'ETag': '\"e27532b1ccce1053e4ac54276ac04dd4\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302693.261578.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 14, tzinfo=tzutc()), 'ETag': '\"c73b0db5e0af509cc75e4f58a0b3acb9\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302698.2177129.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 19, tzinfo=tzutc()), 'ETag': '\"da0331c1d80848a5f935cda55ef48e7d\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302699.6476128.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 20, tzinfo=tzutc()), 'ETag': '\"2327cb198bd519de7ac13611403822bd\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302705.10882.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 26, tzinfo=tzutc()), 'ETag': '\"8f2d9fc1d4039bd6a65bb0b1cf663707\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302710.7214148.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 31, tzinfo=tzutc()), 'ETag': '\"bf8e88f2be739fbefb0206b88b2db7dd\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302716.057808.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 37, tzinfo=tzutc()), 'ETag': '\"e7d9e65a2793378614b577cf888a2071\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302721.637975.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 42, tzinfo=tzutc()), 'ETag': '\"f20a9e9e0618cc5ac4c95645b23d3b57\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302726.939769.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 48, tzinfo=tzutc()), 'ETag': '\"77d5e64f6344949d0f25d7ab54706845\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302732.050942.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 53, tzinfo=tzutc()), 'ETag': '\"271f3bec3ee1857c5d0d2108fbcc7ca9\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302737.601426.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 58, 58, tzinfo=tzutc()), 'ETag': '\"225150b1626df8a6eb688f5d43dae662\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302740.261344.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 1, tzinfo=tzutc()), 'ETag': '\"f57db76538b7e76cbe755a6267c4af4a\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302745.6398249.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 6, tzinfo=tzutc()), 'ETag': '\"b6aa7e2032b5a29505a064d1bdf1889d\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302750.954525.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 12, tzinfo=tzutc()), 'ETag': '\"dfdd11bcabba6b2b096fe9487ba760dd\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302753.496573.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 14, tzinfo=tzutc()), 'ETag': '\"993956f9e3606a1198706701d637422f\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302758.953084.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 20, tzinfo=tzutc()), 'ETag': '\"93a13d9688212ed8389ab519efb1bb96\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302761.553962.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 22, tzinfo=tzutc()), 'ETag': '\"eea23b7066ffbfda6900fd10c16ee34e\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302765.8902988.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 27, tzinfo=tzutc()), 'ETag': '\"feff3b04752424c5c215a4d5ba31ebef\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302771.170997.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 32, tzinfo=tzutc()), 'ETag': '\"79003eb4cb62f2503a6b1e5098f46c61\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302776.45154.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 37, tzinfo=tzutc()), 'ETag': '\"bb2ba6a6e092677fc5a2dab3c2eed9d7\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302778.027807.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 39, tzinfo=tzutc()), 'ETag': '\"a8ea231871850d170334b2899f11b958\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302783.3367648.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 44, tzinfo=tzutc()), 'ETag': '\"25af5f36c30831e021acd8efa11cd951\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302784.875665.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 46, tzinfo=tzutc()), 'ETag': '\"8f45cfeaba9930903e6810427f32e1d1\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302790.187519.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 51, tzinfo=tzutc()), 'ETag': '\"c889d9f0358825ca77513288e9d7626f\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302795.686893.json', 'LastModified': datetime.datetime(2024, 1, 26, 20, 59, 56, tzinfo=tzutc()), 'ETag': '\"59ebbcb67693fc105f0fa22d4a9d6db9\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302801.269886.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 2, tzinfo=tzutc()), 'ETag': '\"cf7cc602a349b6e6e9a335b19eec22c0\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302806.979458.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 8, tzinfo=tzutc()), 'ETag': '\"a8c3d56fc5b16c60b0cd1102afefcbf1\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302812.42817.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 13, tzinfo=tzutc()), 'ETag': '\"2abf32ed94b9c5b16c4e040c37776e56\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302817.603986.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 18, tzinfo=tzutc()), 'ETag': '\"f7c005f94b31e821d200b44a368299e3\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302822.8130429.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 23, tzinfo=tzutc()), 'ETag': '\"94f3b4355d3caaac2e613ec4a7635b1f\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302828.28596.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 29, tzinfo=tzutc()), 'ETag': '\"56951a71ca8054b2de813293d5f2eaaa\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302833.8323882.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 35, tzinfo=tzutc()), 'ETag': '\"6e32f5aeb9bbc9469572d8a1351c1271\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302839.285572.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 40, tzinfo=tzutc()), 'ETag': '\"094d083e51973447ff375e528b577e35\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302840.671992.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 41, tzinfo=tzutc()), 'ETag': '\"03324037334abaf5606fae0c34785da4\"', 'Size': 561, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302846.13871.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 47, tzinfo=tzutc()), 'ETag': '\"69effce1e03bc6a9a9c8b24f26c1839a\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302851.3617349.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 52, tzinfo=tzutc()), 'ETag': '\"c5c60c6ef2b11f9d521714abaf19a265\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302857.082331.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 0, 58, tzinfo=tzutc()), 'ETag': '\"df35ced9108bb1a1bc6869e3590d28c9\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302863.325108.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 4, tzinfo=tzutc()), 'ETag': '\"d3e59f747cc9ceace8c90398df27b71d\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302865.7161531.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 6, tzinfo=tzutc()), 'ETag': '\"6a41980284b4a98b45f83e6f884e2351\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302872.007861.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 13, tzinfo=tzutc()), 'ETag': '\"2fa335c8dd9f068ba7e14997707c8bdd\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302878.090278.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 19, tzinfo=tzutc()), 'ETag': '\"6e9b276ba19a6518deae5a4a3c39b000\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302880.720545.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 21, tzinfo=tzutc()), 'ETag': '\"b739265a218ada6845894849dab5eb35\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302886.792972.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 27, tzinfo=tzutc()), 'ETag': '\"c4f4790e5a7bc9e8bf07f9c7434cd504\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302889.9480839.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 31, tzinfo=tzutc()), 'ETag': '\"a9c51453bd1c847a85c907264e6855ad\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302892.087488.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 33, tzinfo=tzutc()), 'ETag': '\"f31b4baaac7b3c89cf72a1d1ddc39268\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302894.8419058.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 36, tzinfo=tzutc()), 'ETag': '\"98505295355d9ff390095bdfbc2a4b9d\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302900.866613.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 42, tzinfo=tzutc()), 'ETag': '\"04d452f77c93f5378eae08c04b27bdf7\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302905.774872.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 46, tzinfo=tzutc()), 'ETag': '\"0a24ddd4a4e03de65f4713443960c6af\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302911.809841.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 53, tzinfo=tzutc()), 'ETag': '\"8e7e7d0cd7908ee116b672556c69cbd2\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302917.879206.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 1, 59, tzinfo=tzutc()), 'ETag': '\"2b1b2ec30a871890aa097253d9af414a\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302923.6494348.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 2, 4, tzinfo=tzutc()), 'ETag': '\"9e04ccd53177c8f812a764c2bff0a556\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302929.418217.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 2, 10, tzinfo=tzutc()), 'ETag': '\"4b455dbdd76f3b61f46baa5f4ed4de1b\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302935.535815.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 2, 16, tzinfo=tzutc()), 'ETag': '\"b45473869cc1bf4b5991f302a568432d\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302937.654336.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 2, 18, tzinfo=tzutc()), 'ETag': '\"6bccc9d3b5cfaaa8e4dc94dffecd8c0c\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302943.491353.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 2, 24, tzinfo=tzutc()), 'ETag': '\"10b8a9d855d490f70b62d80b1af027e6\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302949.879245.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 2, 31, tzinfo=tzutc()), 'ETag': '\"4472144b94e5ade56e07fb7160cc75a9\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302954.857244.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 2, 36, tzinfo=tzutc()), 'ETag': '\"f0d86902e13a156c41ce0776ca957605\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302960.99323.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 2, 42, tzinfo=tzutc()), 'ETag': '\"35c53780f3ab9e6c0240150360ef101f\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302966.9620428.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 2, 48, tzinfo=tzutc()), 'ETag': '\"668c82e9602090c3305a816654d80db0\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302973.3802822.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 2, 54, tzinfo=tzutc()), 'ETag': '\"95370b3ee7a3d98119f4214e2a521af1\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302979.213707.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, tzinfo=tzutc()), 'ETag': '\"f2df455c824ad45a7e8bd45e1aa30008\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302982.020986.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 3, tzinfo=tzutc()), 'ETag': '\"d99423b44cb535c4c6f739795373d02c\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302988.152478.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 9, tzinfo=tzutc()), 'ETag': '\"b7939ee9418d653516abb5398f1a2906\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706302993.966861.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 15, tzinfo=tzutc()), 'ETag': '\"f49effa98f9e75ff1ca3a1cc91613614\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303000.0674088.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 21, tzinfo=tzutc()), 'ETag': '\"2b6cf2d48859232019ac5ded98bc0921\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303005.9448478.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 27, tzinfo=tzutc()), 'ETag': '\"613524147c71d84ce1aad799a7d17ea4\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303011.643404.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 32, tzinfo=tzutc()), 'ETag': '\"84073f6e82ae019cae58f3524e5d1099\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303013.638363.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 34, tzinfo=tzutc()), 'ETag': '\"8e367bda96664c86c0517f4bcaad6c8a\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303015.5758002.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 36, tzinfo=tzutc()), 'ETag': '\"d6518da8fc280a8404e489b8fb98dde3\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303017.3468428.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 38, tzinfo=tzutc()), 'ETag': '\"b5fa2bad5a82c26f7868aae4223c0473\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303019.151497.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 40, tzinfo=tzutc()), 'ETag': '\"e15facd9d5bfcdc933df8b58d218641b\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303024.790997.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 45, tzinfo=tzutc()), 'ETag': '\"3f69ce635c68f660915b3f11866d5687\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303026.802355.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 48, tzinfo=tzutc()), 'ETag': '\"2b8250c25dc5652870e4a30d73642a14\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303032.670969.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 53, tzinfo=tzutc()), 'ETag': '\"47e1879d4969a3b7ec20e03ae43b33df\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303038.4884539.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 3, 59, tzinfo=tzutc()), 'ETag': '\"72fceb5eda45aeab64bcdc52c0c6a3ae\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303044.734884.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 5, tzinfo=tzutc()), 'ETag': '\"55c3faa2cfab1b66b3a0dadf2ecb4022\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303046.614752.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 7, tzinfo=tzutc()), 'ETag': '\"390935c6e2dc4a673d77423dcf0a3ceb\"', 'Size': 561, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303052.406757.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 13, tzinfo=tzutc()), 'ETag': '\"fa2771493b0eae13b620cdd1d767f0ab\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303054.32445.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 15, tzinfo=tzutc()), 'ETag': '\"a29149cb9020b0e8b176d50e64b31659\"', 'Size': 560, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303056.341589.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 17, tzinfo=tzutc()), 'ETag': '\"6da7e34006be8b37aed041dc63005544\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303058.2296479.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 19, tzinfo=tzutc()), 'ETag': '\"0d18d3ab559391cc10dabcce0c68c8d4\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303060.156672.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 21, tzinfo=tzutc()), 'ETag': '\"6aba29ae6dfa18c9fe7efd515c829502\"', 'Size': 561, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303062.114924.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 23, tzinfo=tzutc()), 'ETag': '\"6c80fb6c97e2cfb571ac434c9efa64ee\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303063.919775.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 25, tzinfo=tzutc()), 'ETag': '\"fbdfce004209f49a84927cd5d471a0cf\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303070.24422.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 31, tzinfo=tzutc()), 'ETag': '\"674b8a4acba8f3121bd2e9c836b137a4\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303072.40541.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 33, tzinfo=tzutc()), 'ETag': '\"74e7a7c85b2290dd375905d3d5643e97\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303078.461049.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 39, tzinfo=tzutc()), 'ETag': '\"fd45753a58314ad95b945523d3086d9b\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303084.398207.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 45, tzinfo=tzutc()), 'ETag': '\"56c877105a15f5cdbace7a1c6fbc8b02\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303090.315256.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 51, tzinfo=tzutc()), 'ETag': '\"34497ff32ab951a225ac347caecbbecf\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303092.192445.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 53, tzinfo=tzutc()), 'ETag': '\"af87f85895bc16075f9847f95671ad05\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303097.941131.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 4, 59, tzinfo=tzutc()), 'ETag': '\"445ac53d7cc257c6d17b0a81d059fbf3\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303100.3634088.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 1, tzinfo=tzutc()), 'ETag': '\"0926318a8b4825a2d7171ead3b8d6a41\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303102.434187.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 3, tzinfo=tzutc()), 'ETag': '\"6ec8d7fd1f24117cc716eb3af9589223\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303107.024126.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 8, tzinfo=tzutc()), 'ETag': '\"c130376a6454aea7dd6ca4ac77189f3f\"', 'Size': 559, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303111.985153.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 13, tzinfo=tzutc()), 'ETag': '\"c3c6267fe04175593bab4e7842db4974\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303117.74057.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 18, tzinfo=tzutc()), 'ETag': '\"547007442aee8428ea3a30aa23b18444\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303123.1443.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 24, tzinfo=tzutc()), 'ETag': '\"1bf31f7af3a5bb9fb7a2680e80229f43\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303128.780271.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 29, tzinfo=tzutc()), 'ETag': '\"c90b79ff22f1bd866cedab8094a80625\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303134.251116.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 35, tzinfo=tzutc()), 'ETag': '\"a422e4854e4c77743bec822663d89ba7\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303140.108872.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 41, tzinfo=tzutc()), 'ETag': '\"f7809f2ee241290e2bb0d413209f5016\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303145.811285.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 47, tzinfo=tzutc()), 'ETag': '\"2d8c8698914fd26ae7e0d72b05f65642\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303151.6295629.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 52, tzinfo=tzutc()), 'ETag': '\"fd897fb56d84ff21e4a52b8f5aec1879\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303154.078698.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 5, 55, tzinfo=tzutc()), 'ETag': '\"d5344d9635bf7d69cafd7b7fac9978b8\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303160.9035761.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 6, 2, tzinfo=tzutc()), 'ETag': '\"a495811f83ae2252d1afc053f33b680c\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303167.330417.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 6, 8, tzinfo=tzutc()), 'ETag': '\"0f984242293a9ddd779dc8bb315a838e\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303173.884174.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 6, 15, tzinfo=tzutc()), 'ETag': '\"c2186c18f9b07bd01971e243d1e9b28c\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303180.278629.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 6, 21, tzinfo=tzutc()), 'ETag': '\"012329588f482bd47ddbe8f11d946bd0\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303187.0318332.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 6, 28, tzinfo=tzutc()), 'ETag': '\"ae910d8a642228764d75489686f868a7\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303193.574018.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 6, 34, tzinfo=tzutc()), 'ETag': '\"abb170eb36afeb841cfe973cf7114a7a\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303197.3127449.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 6, 38, tzinfo=tzutc()), 'ETag': '\"3554bb125ea0e5d06fcaed3f2f397807\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303203.6214368.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 6, 44, tzinfo=tzutc()), 'ETag': '\"86c57e1eab71c668a4bd55f0d4c2d2f2\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303210.024794.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 6, 51, tzinfo=tzutc()), 'ETag': '\"f2c8da47c007b94bda49d09be7b40013\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303216.142463.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 6, 57, tzinfo=tzutc()), 'ETag': '\"639d1cc007e5cdd1c14916850f9d94a3\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303218.943943.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 7, tzinfo=tzutc()), 'ETag': '\"f3d9399f9e8867a86132365707f500de\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303225.630322.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 7, 6, tzinfo=tzutc()), 'ETag': '\"a24a5a463f4301840d2a7d96b8158400\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303231.6989639.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 7, 12, tzinfo=tzutc()), 'ETag': '\"d92956085aea26e975f6d8b33bd9f13b\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303238.490898.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 7, 19, tzinfo=tzutc()), 'ETag': '\"81ea9c81520d7991b81c4f157bbe3d47\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303244.747381.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 7, 25, tzinfo=tzutc()), 'ETag': '\"f9e6381966dcef61413a514f47cce819\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303251.2987661.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 7, 32, tzinfo=tzutc()), 'ETag': '\"f8982e435803e5d1b4f2c6014540ca66\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303258.8782258.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 7, 40, tzinfo=tzutc()), 'ETag': '\"940c03fc275ddb6c4c577317622a6397\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303266.3509068.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 7, 47, tzinfo=tzutc()), 'ETag': '\"3e0e5a9de28bfe47d3523f99f53f0612\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303273.744912.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 7, 54, tzinfo=tzutc()), 'ETag': '\"e65f1f5a1254cad2a98fcaeef1fa73f1\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303281.317508.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 8, 2, tzinfo=tzutc()), 'ETag': '\"86ddad147c0270b13fc8a3be9f8a399c\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303285.786874.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 8, 6, tzinfo=tzutc()), 'ETag': '\"32e6d370e0d2b4bb2a0301cb11c0c2a2\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303293.1449332.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 8, 14, tzinfo=tzutc()), 'ETag': '\"aab69492765e02d9e06d2b318277a9ba\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303300.281173.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 8, 21, tzinfo=tzutc()), 'ETag': '\"1f937c5ed255e33e7c53165e80b3f899\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303307.218474.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 8, 28, tzinfo=tzutc()), 'ETag': '\"ef5e7d70c51fdf93761d4145327bfe99\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303314.654725.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 8, 35, tzinfo=tzutc()), 'ETag': '\"77bd4e2115bd8e268f956929d9159611\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303322.336.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 8, 43, tzinfo=tzutc()), 'ETag': '\"88d5ff04e5bcdfe2287f04d9363b8d92\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303329.8067589.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 8, 50, tzinfo=tzutc()), 'ETag': '\"e1765f42c754ec598fa954b4b2909761\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303337.60329.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 8, 58, tzinfo=tzutc()), 'ETag': '\"220ebba3cf8c92dc884f95c4789f3843\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303344.949783.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 9, 6, tzinfo=tzutc()), 'ETag': '\"4fc1dcf5d6070af6acca28fd7aff6fdd\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303352.370964.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 9, 13, tzinfo=tzutc()), 'ETag': '\"7f5c4a88240984adadd6d1641c6377ad\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303359.9260921.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 9, 21, tzinfo=tzutc()), 'ETag': '\"019c15b9dd89aa6fd2c15bec06903263\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303366.979311.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 9, 28, tzinfo=tzutc()), 'ETag': '\"75355e5f1009cc55c5b8312e7ddc85d7\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303374.074846.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 9, 35, tzinfo=tzutc()), 'ETag': '\"4b656f94ec4114b6ba6a0ddf796ca090\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303380.919981.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 9, 42, tzinfo=tzutc()), 'ETag': '\"2e24756aa31c594b76155a910147915e\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303388.109548.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 9, 49, tzinfo=tzutc()), 'ETag': '\"2c8d87c74a01083c41c732b6e37afde0\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303395.788579.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 9, 56, tzinfo=tzutc()), 'ETag': '\"ee6606b31625937b872ee6994a66486a\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303403.0609431.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 4, tzinfo=tzutc()), 'ETag': '\"f97e7b07b39e17ac426fd9d4238b2f38\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303406.424046.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 7, tzinfo=tzutc()), 'ETag': '\"b4a1d5e0eb017a21b5cba6bff997271c\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303409.561954.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 10, tzinfo=tzutc()), 'ETag': '\"6acb1e0defc3e6f0b9f1fdbf7bb89aea\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303416.679639.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 17, tzinfo=tzutc()), 'ETag': '\"1e094cbcd9d260543d5b4c0c05f6e107\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303419.981676.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 21, tzinfo=tzutc()), 'ETag': '\"ddbc811ab0a6fa96395b6456eede5c83\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303427.643556.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 28, tzinfo=tzutc()), 'ETag': '\"434b2d682bfc3ed8bbd0c5924785241c\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303435.2045991.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 36, tzinfo=tzutc()), 'ETag': '\"3e750b1e3a5050139dccfe087fdd7de8\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303442.392054.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 43, tzinfo=tzutc()), 'ETag': '\"245623ddafbace87b73239fd8a0f0329\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303445.759991.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 46, tzinfo=tzutc()), 'ETag': '\"694d44e515b4caa3f98fb3b2b4d71283\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303450.8534122.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 52, tzinfo=tzutc()), 'ETag': '\"2adda4692e523d333e56b506698e45ea\"', 'Size': 561, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303456.921258.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 10, 58, tzinfo=tzutc()), 'ETag': '\"64e1eb386d5086de2ea862418c505560\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303463.983859.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 11, 5, tzinfo=tzutc()), 'ETag': '\"c3c817589e645ab66af2ef49b63d4f1b\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303470.8298628.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 11, 12, tzinfo=tzutc()), 'ETag': '\"5d21415cdd1647074e416db01ffdc1a9\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303478.065763.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 11, 19, tzinfo=tzutc()), 'ETag': '\"cbc54cdec3a68c74b65650a132c2158c\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303485.490607.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 11, 26, tzinfo=tzutc()), 'ETag': '\"765de2643d848ee316d6c038e3b71db2\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303494.130046.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 11, 35, tzinfo=tzutc()), 'ETag': '\"3c98fb5f869ce1780a6b47fa9b7cdf53\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303502.655783.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 11, 43, tzinfo=tzutc()), 'ETag': '\"93c5579d5b37f363b4e40940a9852384\"', 'Size': 567, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303511.1902988.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 11, 52, tzinfo=tzutc()), 'ETag': '\"787fb84267d528af17a0d40992a30bcc\"', 'Size': 567, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303519.549258.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 12, tzinfo=tzutc()), 'ETag': '\"1e7abf4dee8d1a128d9a4bc16824df15\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303527.916165.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 12, 9, tzinfo=tzutc()), 'ETag': '\"0b88fbb02d209c9f06b8a33eda4f0a3e\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303536.6737752.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 12, 17, tzinfo=tzutc()), 'ETag': '\"1161d4103327901fe089dbdce3f5477e\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303545.2259278.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 12, 26, tzinfo=tzutc()), 'ETag': '\"71aa4b7b21593fcff0f79d3dd546f16b\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303553.718876.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 12, 34, tzinfo=tzutc()), 'ETag': '\"d00009cdc85edec061a535921e94f24f\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303563.954589.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 12, 45, tzinfo=tzutc()), 'ETag': '\"555e2854a6155cbde22d72e18269e289\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303574.5782368.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 12, 55, tzinfo=tzutc()), 'ETag': '\"ab766d8b7be44695a251e884cf771564\"', 'Size': 567, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303584.4236412.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 13, 5, tzinfo=tzutc()), 'ETag': '\"8708447606142e2854910287eba952c1\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303594.2605572.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 13, 15, tzinfo=tzutc()), 'ETag': '\"ed15fd0633c095bb8789170ebfb1296c\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303605.3805099.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 13, 26, tzinfo=tzutc()), 'ETag': '\"b5c4f732d32e1aeffc22409e14827f79\"', 'Size': 567, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303616.343383.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 13, 37, tzinfo=tzutc()), 'ETag': '\"f4249c7bb75f554c4da5389132776823\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303627.0807948.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 13, 48, tzinfo=tzutc()), 'ETag': '\"feb1d7fc6cfe3b5e8e015ab0892b60d4\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303637.118855.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 13, 58, tzinfo=tzutc()), 'ETag': '\"09781e9f56d1f75fb51e0874f23599bd\"', 'Size': 567, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303646.863864.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 14, 8, tzinfo=tzutc()), 'ETag': '\"04d096b86db7483795eed781caba13c6\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303657.628682.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 14, 18, tzinfo=tzutc()), 'ETag': '\"0565e2a203b1717023fe0a47e06f1e8c\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303667.6471.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 14, 28, tzinfo=tzutc()), 'ETag': '\"2d708598519d87976d7509de8ae7cfb1\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303677.2866619.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 14, 38, tzinfo=tzutc()), 'ETag': '\"77bf2bc474685210c7e331414778fa41\"', 'Size': 567, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303687.483849.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 14, 48, tzinfo=tzutc()), 'ETag': '\"96f850fdbcd103a19e58ca303e6d7b90\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303697.898054.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 14, 59, tzinfo=tzutc()), 'ETag': '\"8ef7185abb9e2b5fe368c1881a310965\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303704.0964859.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 15, 5, tzinfo=tzutc()), 'ETag': '\"f958df2c313c0e1d02574b85385fd8e1\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303710.686573.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 15, 11, tzinfo=tzutc()), 'ETag': '\"eb0f557df8780c9ee76c804612b3d681\"', 'Size': 560, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303717.924364.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 15, 19, tzinfo=tzutc()), 'ETag': '\"0e9d96b871bde96f2a981e401f92cc17\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303726.5668328.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 15, 27, tzinfo=tzutc()), 'ETag': '\"d47e16c681dc1e349625b7f300e04959\"', 'Size': 590, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303734.830336.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 15, 36, tzinfo=tzutc()), 'ETag': '\"cc07c6900a80ebfabf3575d88312765f\"', 'Size': 578, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303744.750816.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 15, 45, tzinfo=tzutc()), 'ETag': '\"8aeb22c365b4e839cfb4fcf23901f56a\"', 'Size': 591, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303756.1512058.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 15, 57, tzinfo=tzutc()), 'ETag': '\"2951db3242e84a3f777f74438c9ce235\"', 'Size': 578, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303767.40333.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 16, 8, tzinfo=tzutc()), 'ETag': '\"5d09297a86ff5825dd2a087977b64708\"', 'Size': 577, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303778.5508249.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 16, 19, tzinfo=tzutc()), 'ETag': '\"bff2fdd10632e5cf78df627a0ade3037\"', 'Size': 578, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303789.260301.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 16, 30, tzinfo=tzutc()), 'ETag': '\"4fcf3fb1ce1980c0d2ca16f0abd03e0e\"', 'Size': 577, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303800.622525.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 16, 41, tzinfo=tzutc()), 'ETag': '\"00b1eb3209e3b34f20c2c4a8c93139d4\"', 'Size': 578, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303812.411702.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 16, 53, tzinfo=tzutc()), 'ETag': '\"70b758ad798a249f82d4f3f82f121cd0\"', 'Size': 566, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303825.700682.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 17, 6, tzinfo=tzutc()), 'ETag': '\"2c94a3e3ceb5dbd1c523588c54a07ba5\"', 'Size': 590, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303839.2734752.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 17, 20, tzinfo=tzutc()), 'ETag': '\"5ab8715d7bb55e2cef8070b59041d050\"', 'Size': 591, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303853.8923352.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 17, 35, tzinfo=tzutc()), 'ETag': '\"28c53e91c1564c4cf658f957198e0574\"', 'Size': 579, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303868.293865.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 17, 49, tzinfo=tzutc()), 'ETag': '\"a567a1e11c21000cfda3e95cc62ed50a\"', 'Size': 578, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303882.439811.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 18, 3, tzinfo=tzutc()), 'ETag': '\"d948a1ecd5012150f2e7cd14ce37e52a\"', 'Size': 592, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303895.6825442.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 18, 16, tzinfo=tzutc()), 'ETag': '\"29ceff2c62c6577d2c1b4c4b4fc92fb5\"', 'Size': 578, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303908.9060009.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 18, 30, tzinfo=tzutc()), 'ETag': '\"e0da98c0dce8504f6d7337fbbfd22005\"', 'Size': 591, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303917.016334.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 18, 38, tzinfo=tzutc()), 'ETag': '\"edd4a8300ae31dafe4308d10805a96c2\"', 'Size': 588, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303928.59888.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 18, 49, tzinfo=tzutc()), 'ETag': '\"bdffc6db3079c67334c6112cb78360e3\"', 'Size': 591, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303941.635093.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 19, 2, tzinfo=tzutc()), 'ETag': '\"d6b179884b2d418f0fc61b5db641f6b3\"', 'Size': 591, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303949.112731.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 19, 10, tzinfo=tzutc()), 'ETag': '\"5775898f1ed79cf8d6f2f40e32393de2\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303957.3150778.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 19, 18, tzinfo=tzutc()), 'ETag': '\"adba8db1ec6ecc63f316ef21afdc4566\"', 'Size': 564, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303966.9512131.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 19, 28, tzinfo=tzutc()), 'ETag': '\"3bc652acd0f2c3486ed3eded193d2d0b\"', 'Size': 569, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303977.898761.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 19, 39, tzinfo=tzutc()), 'ETag': '\"9385d2d021ae4b6fd73532be1c7a3f4a\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706303991.916387.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 19, 53, tzinfo=tzutc()), 'ETag': '\"8fb3af0e56a120939ac7766e8d5a75e4\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304006.3011599.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 20, 7, tzinfo=tzutc()), 'ETag': '\"060edd9bc276dbdd12dada9bb52ec5e2\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304020.2355819.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 20, 21, tzinfo=tzutc()), 'ETag': '\"499fc3a7f3150756bcd1352a884f82ac\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304034.260759.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 20, 35, tzinfo=tzutc()), 'ETag': '\"8a21d025bfd10d622e2d8e2f52d5c8cc\"', 'Size': 569, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304051.539283.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 20, 52, tzinfo=tzutc()), 'ETag': '\"dd2b8556d9d6947d6e920b066a4e125e\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304069.370226.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 21, 10, tzinfo=tzutc()), 'ETag': '\"5aed29e6d11f46487447637a842671b5\"', 'Size': 571, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304087.814445.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 21, 29, tzinfo=tzutc()), 'ETag': '\"441bbed40778551d617effdc4c1b1b2c\"', 'Size': 570, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304104.343423.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 21, 45, tzinfo=tzutc()), 'ETag': '\"278fa1c0cea588346e279ee5398da036\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304122.018197.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 22, 3, tzinfo=tzutc()), 'ETag': '\"6cf230f034059c4908fbf3914483dd80\"', 'Size': 565, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304137.274979.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 22, 18, tzinfo=tzutc()), 'ETag': '\"d22373b54df0c0ebc84ac9895a4ac9aa\"', 'Size': 570, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304154.641855.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 22, 35, tzinfo=tzutc()), 'ETag': '\"7c1c0354bde362b47432962552f4c875\"', 'Size': 568, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706304165.5509472.json', 'LastModified': datetime.datetime(2024, 1, 26, 21, 22, 46, tzinfo=tzutc()), 'ETag': '\"d5ccf160167d6d0b442410d6418325f8\"', 'Size': 566, 'StorageClass': 'STANDARD'}], 'Name': 'fmbt', 'Prefix': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk', 'MaxKeys': 1000, 'EncodingType': 'url', 'KeyCount': 227}\n",
      "[2024-01-26 16:23:47,578] p91528 {2358223361.py:19} INFO - created dataframe of shape (227, 16) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>errors</th>\n",
       "      <th>successes</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>all_prompts_token_count</th>\n",
       "      <th>prompt_token_count_mean</th>\n",
       "      <th>prompt_token_throughput</th>\n",
       "      <th>all_completions_token_count</th>\n",
       "      <th>completion_token_count_mean</th>\n",
       "      <th>completion_token_throughput</th>\n",
       "      <th>transactions</th>\n",
       "      <th>transactions_per_second</th>\n",
       "      <th>transactions_per_minute</th>\n",
       "      <th>latency_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304</td>\n",
       "      <td>304.0</td>\n",
       "      <td>79.15</td>\n",
       "      <td>101</td>\n",
       "      <td>101.0</td>\n",
       "      <td>26.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>15</td>\n",
       "      <td>3.724005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_500-1000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>980</td>\n",
       "      <td>980.0</td>\n",
       "      <td>1199.37</td>\n",
       "      <td>16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1</td>\n",
       "      <td>1.22</td>\n",
       "      <td>73</td>\n",
       "      <td>0.807353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1339</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>326.07</td>\n",
       "      <td>69</td>\n",
       "      <td>69.0</td>\n",
       "      <td>16.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>14</td>\n",
       "      <td>4.098635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1932</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>433.06</td>\n",
       "      <td>98</td>\n",
       "      <td>98.0</td>\n",
       "      <td>21.97</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>13</td>\n",
       "      <td>4.446627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1154</td>\n",
       "      <td>1154.0</td>\n",
       "      <td>288.77</td>\n",
       "      <td>101</td>\n",
       "      <td>101.0</td>\n",
       "      <td>25.27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>15</td>\n",
       "      <td>3.987915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     experiment_name  concurrency  \\\n",
       "0  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1   \n",
       "1  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1   \n",
       "2  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1   \n",
       "3  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1   \n",
       "4  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1   \n",
       "\n",
       "                 payload_file errors  successes  error_rate  \\\n",
       "0      payload_en_1-500.jsonl     []          1         0.0   \n",
       "1   payload_en_500-1000.jsonl     []          1         0.0   \n",
       "2  payload_en_1000-2000.jsonl     []          1         0.0   \n",
       "3  payload_en_1000-2000.jsonl     []          1         0.0   \n",
       "4  payload_en_1000-2000.jsonl     []          1         0.0   \n",
       "\n",
       "   all_prompts_token_count  prompt_token_count_mean  prompt_token_throughput  \\\n",
       "0                      304                    304.0                    79.15   \n",
       "1                      980                    980.0                  1199.37   \n",
       "2                     1339                   1339.0                   326.07   \n",
       "3                     1932                   1932.0                   433.06   \n",
       "4                     1154                   1154.0                   288.77   \n",
       "\n",
       "   all_completions_token_count  completion_token_count_mean  \\\n",
       "0                          101                        101.0   \n",
       "1                           16                         16.0   \n",
       "2                           69                         69.0   \n",
       "3                           98                         98.0   \n",
       "4                          101                        101.0   \n",
       "\n",
       "   completion_token_throughput  transactions  transactions_per_second  \\\n",
       "0                        26.30             1                     0.26   \n",
       "1                        19.58             1                     1.22   \n",
       "2                        16.80             1                     0.24   \n",
       "3                        21.97             1                     0.22   \n",
       "4                        25.27             1                     0.25   \n",
       "\n",
       "   transactions_per_minute  latency_mean  \n",
       "0                       15      3.724005  \n",
       "1                       73      0.807353  \n",
       "2                       14      4.098635  \n",
       "3                       13      4.446627  \n",
       "4                       15      3.987915  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_s3_files(bucket, prefix, suffix='.json'):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    logger.info(f\"files recieved from s3 for per inference request --> {response}\")\n",
    "    return [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith(suffix)]\n",
    "\n",
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(BUCKET_NAME, METRICS_PER_CHUNK_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = []\n",
    "for file_key in s3_files:\n",
    "    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=file_key)\n",
    "    file_content = response['Body'].read().decode('utf-8')\n",
    "    json_obj = json.loads(file_content)\n",
    "    json_list.append(json_obj)\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_metrics.shape} from all responses\")\n",
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_config.PrimaryContainer.Environment.ENDPOINT_SERVER_TIMEOUT', 'model_config.PrimaryContainer.Environment.HF_MODEL_ID', 'model_config.PrimaryContainer.Environment.MAX_BATCH_PREFILL_TOKENS', 'model_config.PrimaryContainer.Environment.MAX_INPUT_LENGTH', 'model_config.PrimaryContainer.Environment.MAX_TOTAL_TOKENS', 'model_config.PrimaryContainer.Environment.MODEL_CACHE_ROOT', 'model_config.PrimaryContainer.Environment.SAGEMAKER_ENV', 'model_config.PrimaryContainer.Environment.SAGEMAKER_MODEL_SERVER_WORKERS', 'model_config.PrimaryContainer.Environment.SAGEMAKER_PROGRAM', 'model_config.PrimaryContainer.Environment.SM_NUM_GPUS']\n",
      "Columns in df_responses: Index(['endpoint_name', 'prompt', 'do_sample', 'temperature', 'top_p', 'top_k',\n",
      "       'max_new_tokens', 'truncate', 'completion', 'prompt_tokens',\n",
      "       'completion_tokens', 'latency', 'experiment_name', 'concurrency'],\n",
      "      dtype='object')\n",
      "Columns in df_endpoints: Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri', 'ENDPOINT_SERVER_TIMEOUT', 'HF_MODEL_ID',\n",
      "       'MAX_BATCH_PREFILL_TOKENS', 'MAX_INPUT_LENGTH', 'MAX_TOTAL_TOKENS',\n",
      "       'MODEL_CACHE_ROOT', 'SAGEMAKER_ENV', 'SAGEMAKER_MODEL_SERVER_WORKERS',\n",
      "       'SAGEMAKER_PROGRAM', 'SM_NUM_GPUS'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>ENDPOINT_SERVER_TIMEOUT</th>\n",
       "      <th>HF_MODEL_ID</th>\n",
       "      <th>MAX_BATCH_PREFILL_TOKENS</th>\n",
       "      <th>MAX_INPUT_LENGTH</th>\n",
       "      <th>MAX_TOTAL_TOKENS</th>\n",
       "      <th>MODEL_CACHE_ROOT</th>\n",
       "      <th>SAGEMAKER_ENV</th>\n",
       "      <th>SAGEMAKER_MODEL_SERVER_WORKERS</th>\n",
       "      <th>SAGEMAKER_PROGRAM</th>\n",
       "      <th>SM_NUM_GPUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>1339</td>\n",
       "      <td>\\n\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n``...</td>\n",
       "      <td>1339</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>1932</td>\n",
       "      <td>Altu elikbilek is younger.\\n\\nQuestion: Who...</td>\n",
       "      <td>1932</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>1154</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nGepi\\nThe Gepi is a rig...</td>\n",
       "      <td>1154</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706302351   \n",
       "1  lmistral7b-g5-2xlarge-1706302351   \n",
       "2  lmistral7b-g5-2xlarge-1706302351   \n",
       "3  lmistral7b-g5-2xlarge-1706302351   \n",
       "4  lmistral7b-g5-2xlarge-1706302351   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100       304   \n",
       "1   0.92    120             100       980   \n",
       "2   0.92    120             100      1339   \n",
       "3   0.92    120             100      1932   \n",
       "4   0.92    120             100      1154   \n",
       "\n",
       "                                          completion  prompt_tokens  ...  \\\n",
       "0  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304  ...   \n",
       "1   Both WAGS Atlanta and WAGS are reality televi...            980  ...   \n",
       "2  \\n\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n``...           1339  ...   \n",
       "3   Altu elikbilek is younger.\\n\\nQuestion: Who...           1932  ...   \n",
       "4  \\n\\n```\\nPassage 1:\\nGepi\\nThe Gepi is a rig...           1154  ...   \n",
       "\n",
       "   ENDPOINT_SERVER_TIMEOUT    HF_MODEL_ID MAX_BATCH_PREFILL_TOKENS  \\\n",
       "0                     3600  /opt/ml/model                     8191   \n",
       "1                     3600  /opt/ml/model                     8191   \n",
       "2                     3600  /opt/ml/model                     8191   \n",
       "3                     3600  /opt/ml/model                     8191   \n",
       "4                     3600  /opt/ml/model                     8191   \n",
       "\n",
       "   MAX_INPUT_LENGTH MAX_TOTAL_TOKENS MODEL_CACHE_ROOT SAGEMAKER_ENV  \\\n",
       "0              8191             8192    /opt/ml/model             1   \n",
       "1              8191             8192    /opt/ml/model             1   \n",
       "2              8191             8192    /opt/ml/model             1   \n",
       "3              8191             8192    /opt/ml/model             1   \n",
       "4              8191             8192    /opt/ml/model             1   \n",
       "\n",
       "  SAGEMAKER_MODEL_SERVER_WORKERS SAGEMAKER_PROGRAM SM_NUM_GPUS  \n",
       "0                              1      inference.py           1  \n",
       "1                              1      inference.py           1  \n",
       "2                              1      inference.py           1  \n",
       "3                              1      inference.py           1  \n",
       "4                              1      inference.py           1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_endpoints = pd.json_normalize(endpoint_info_list)\n",
    "df_endpoints['instance_type'] = df_endpoints['endpoint_config.ProductionVariants'].map(lambda x: x[0]['InstanceType'])\n",
    "df_endpoints\n",
    "cols_for_env = [c for c in df_endpoints.columns if 'Environment' in c]\n",
    "print(cols_for_env)\n",
    "cols_of_interest = ['experiment_name', \n",
    "                    'instance_type',\n",
    "                    'endpoint.EndpointName',\n",
    "                    'model_config.ModelName',\n",
    "                    'model_config.PrimaryContainer.Image',   \n",
    "                    'model_config.PrimaryContainer.ModelDataSource.S3DataSource.S3Uri']\n",
    "cols_of_interest.extend(cols_for_env)\n",
    "\n",
    "df_endpoints = df_endpoints[cols_of_interest]\n",
    "df_endpoints = df_endpoints[cols_of_interest]\n",
    "cols_of_interest_renamed = [c.split('.')[-1] for c in cols_of_interest]\n",
    "df_endpoints.columns = cols_of_interest_renamed\n",
    "\n",
    "# Check if 'experiment_name' column exists in both DataFrames\n",
    "print(\"Columns in df_responses:\", df_responses.columns)\n",
    "print(\"Columns in df_endpoints:\", df_endpoints.columns)\n",
    "\n",
    "# Merge operation\n",
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "\n",
    "# Inspect the result\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>ENDPOINT_SERVER_TIMEOUT</th>\n",
       "      <th>HF_MODEL_ID</th>\n",
       "      <th>MAX_BATCH_PREFILL_TOKENS</th>\n",
       "      <th>MAX_INPUT_LENGTH</th>\n",
       "      <th>MAX_TOTAL_TOKENS</th>\n",
       "      <th>MODEL_CACHE_ROOT</th>\n",
       "      <th>SAGEMAKER_ENV</th>\n",
       "      <th>SAGEMAKER_MODEL_SERVER_WORKERS</th>\n",
       "      <th>SAGEMAKER_PROGRAM</th>\n",
       "      <th>SM_NUM_GPUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>1339</td>\n",
       "      <td>\\n\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n``...</td>\n",
       "      <td>1339</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>1932</td>\n",
       "      <td>Altu elikbilek is younger.\\n\\nQuestion: Who...</td>\n",
       "      <td>1932</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>1154</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nGepi\\nThe Gepi is a rig...</td>\n",
       "      <td>1154</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706302351   \n",
       "1  lmistral7b-g5-2xlarge-1706302351   \n",
       "2  lmistral7b-g5-2xlarge-1706302351   \n",
       "3  lmistral7b-g5-2xlarge-1706302351   \n",
       "4  lmistral7b-g5-2xlarge-1706302351   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100       304   \n",
       "1   0.92    120             100       980   \n",
       "2   0.92    120             100      1339   \n",
       "3   0.92    120             100      1932   \n",
       "4   0.92    120             100      1154   \n",
       "\n",
       "                                          completion  prompt_tokens  ...  \\\n",
       "0  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304  ...   \n",
       "1   Both WAGS Atlanta and WAGS are reality televi...            980  ...   \n",
       "2  \\n\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n``...           1339  ...   \n",
       "3   Altu elikbilek is younger.\\n\\nQuestion: Who...           1932  ...   \n",
       "4  \\n\\n```\\nPassage 1:\\nGepi\\nThe Gepi is a rig...           1154  ...   \n",
       "\n",
       "   ENDPOINT_SERVER_TIMEOUT    HF_MODEL_ID MAX_BATCH_PREFILL_TOKENS  \\\n",
       "0                     3600  /opt/ml/model                     8191   \n",
       "1                     3600  /opt/ml/model                     8191   \n",
       "2                     3600  /opt/ml/model                     8191   \n",
       "3                     3600  /opt/ml/model                     8191   \n",
       "4                     3600  /opt/ml/model                     8191   \n",
       "\n",
       "   MAX_INPUT_LENGTH MAX_TOTAL_TOKENS MODEL_CACHE_ROOT SAGEMAKER_ENV  \\\n",
       "0              8191             8192    /opt/ml/model             1   \n",
       "1              8191             8192    /opt/ml/model             1   \n",
       "2              8191             8192    /opt/ml/model             1   \n",
       "3              8191             8192    /opt/ml/model             1   \n",
       "4              8191             8192    /opt/ml/model             1   \n",
       "\n",
       "  SAGEMAKER_MODEL_SERVER_WORKERS SAGEMAKER_PROGRAM SM_NUM_GPUS  \n",
       "0                              1      inference.py           1  \n",
       "1                              1      inference.py           1  \n",
       "2                              1      inference.py           1  \n",
       "3                              1      inference.py           1  \n",
       "4                              1      inference.py           1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:23:47,842] p91528 {2449880785.py:7} INFO - results s3 path for per inference csv --> data/metrics/mistral-7b-tgi-g5-v1/per_inference_request_results.csv\n",
      "[2024-01-26 16:23:51,486] p91528 {2449880785.py:9} INFO - saved results dataframe of shape=(586, 29) in s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference_request_results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference_request_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert df_results to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_results.to_csv(csv_buffer, index=False)\n",
    "csv_data_results = csv_buffer.getvalue()\n",
    "results_file_name = config['results']['per_inference_request_file'].format(datetime=date_time)\n",
    "results_s3_path = os.path.join(METRICS_DIR, results_file_name)\n",
    "logger.info(f\"results s3 path for per inference csv --> {results_s3_path}\")\n",
    "write_to_s3(csv_data_results, BUCKET_NAME, \"\", METRICS_DIR, results_file_name)\n",
    "logger.info(f\"saved results dataframe of shape={df_results.shape} in s3://{BUCKET_NAME}/{results_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 16:23:51,528] p91528 {3262244809.py:10} INFO - results s3 path for metrics csv --> data/metrics/mistral-7b-tgi-g5-v1/all_metrics.csv\n",
      "[2024-01-26 16:23:52,233] p91528 {3262244809.py:12} INFO - saved metrics results dataframe of shape=(227, 31) in s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/all_metrics.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "df_metrics = pd.merge(left=df_metrics, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "df_metrics.head()\n",
    "\n",
    "# Convert df_metrics to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_metrics.to_csv(csv_buffer, index=False)\n",
    "csv_data_metrics = csv_buffer.getvalue()\n",
    "metrics_file_name = config['results']['all_metrics_file'].format(datetime=date_time)\n",
    "metrics_s3_path = os.path.join(METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"results s3 path for metrics csv --> {metrics_s3_path}\")\n",
    "write_to_s3(csv_data_metrics, BUCKET_NAME, \"\", METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in s3://{BUCKET_NAME}/{metrics_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
