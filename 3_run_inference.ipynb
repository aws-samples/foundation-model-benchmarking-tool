{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on all deployed endpoints: Various combinations of payloads, concurrency levels, model configurations\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of our solution design includes running inferences on all deployed model endpoints (with different configurations, concurrency levels and payload sizes). This notebook runs inferences in a manner that is calls endpoints concurrently and asychronously to generate responses and record metrics. Here are some of the key components:\n",
    "\n",
    "- **Accessing the deployed endpoints**, creating a predictor object for these endpoints to call them during inference time.\n",
    "\n",
    "- **Functions to define metrics**: This notebook sets stage for metrics to be recorded during the time of invocation of all these models for benchmarking purposes.\n",
    "\n",
    "- **Running Actual Inferences**: Once the metrics are defined, we set a blocker function that is responsible for creating inference on a single payload called get_inference. We then run a series of asynchronous functions that can be viewed in the code (link above), to create asychronous inferefences on the deployed models. The way we send requests are by creating combinations: this means creating combinations of payloads of different sizes that can be viewed in the config.yml file, with different concurrency levels (in this case we first go through all patches of payloads with a concurrency level of 1, then 2, and then 4). You can set this to your desired value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## auto reload all of the changes made in the config/globals.py file \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import itertools\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from globals import *\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker.predictor import Predictor\n",
    "from utils import load_config, count_tokens\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from typing import Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36myaml\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36menum\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Enum\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILEPATH_FILE: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig_filepath.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILE: \u001b[36mstr\u001b[39;49;00m = Path(CONFIG_FILEPATH_FILE).read_text()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCONFIG_FILE=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mCONFIG_FILE\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(CONFIG_FILE, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file:\u001b[37m\u001b[39;49;00m\n",
      "    config = yaml.safe_load(file)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "DATA_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PROMPTS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mprompts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_INFERENCE_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_inference\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_CHUNK_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_chunk\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "MODELS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "DATASET_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "SCRIPTS_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mscripts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "DIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, DATASET_DIR, METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]\u001b[37m\u001b[39;49;00m\n",
      "TOKENIZER_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33mllama2_tokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "_ = \u001b[36mlist\u001b[39;49;00m(\u001b[36mmap\u001b[39;49;00m(\u001b[34mlambda\u001b[39;49;00m x: os.makedirs(x, exist_ok=\u001b[34mTrue\u001b[39;49;00m), DIR_LIST))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_LIST_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(MODELS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mendpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "REQUEST_PAYLOAD_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(PROMPTS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mpayload.jsonl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTRUNCATE_POLICY\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m, Enum):\u001b[37m\u001b[39;49;00m\n",
      "    AT_PROMPT_TOKEN_LENGTH = \u001b[33m'\u001b[39;49;00m\u001b[33mat-prompt-token-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# misc. metrics related\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PLACE_HOLDER: \u001b[36mint\u001b[39;49;00m = -\u001b[34m1705338041\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# metric filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "COUNTS_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mexperiment_counts.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_DESC_MD_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_W_PRICING_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_w_pricing.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "INSTANCE_PRICING_PER_HOUR_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33minstance_pricing_per_hour.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_w_scores.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option_each_instance_type.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "BUSINESS_SUMMARY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mbusiness_summary.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# plot filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mError rates for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mTokens vs latency for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mtokens_vs_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconcurrency_vs_inference_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mConcurrency Vs latency for different instance type for selected dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "LATENCY_BUDGET: \u001b[36mint\u001b[39;49;00m = \u001b[34m20\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "OVERALL_RESULTS_MD: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m# Results for performance benchmarking\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m**Last modified (UTC): \u001b[39;49;00m\u001b[33m{dttm}\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Summary\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m{business_summary}\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Per instance results\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33mThe following table provides the best combinations for running inference for different sizes prompts on different instance types.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|Dataset   | Instance type   | Recommendation   |\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|---|---|---|\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Dataset=`{dataset}`, instance_type=`{instance_type}`\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULT_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThe best option for staying within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset is a `concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m`. A concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m achieves an `average latency of \u001b[39;49;00m\u001b[33m{latency_mean}\u001b[39;49;00m\u001b[33m seconds`, for an `average prompt size of \u001b[39;49;00m\u001b[33m{prompt_size}\u001b[39;49;00m\u001b[33m tokens` and `completion size of \u001b[39;49;00m\u001b[33m{completion_size}\u001b[39;49;00m\u001b[33m tokens` with `\u001b[39;49;00m\u001b[33m{tpm}\u001b[39;49;00m\u001b[33m transactions/minute`.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_ROW: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33m|`\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m`|`\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m`|\u001b[39;49;00m\u001b[33m{desc}\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_FAILURE_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThis experiment did not find any combination of concurrency level and other configuration settings that could provide a response within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Config.yml file that contains information that is used across this benchmarking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 15:28:07,370] p1345 {635462509.py:2} INFO - {\n",
      "  \"general\": {\n",
      "    \"name\": \"llama2-70b-g5-p4d-trt-v1\",\n",
      "    \"model_name\": \"Llama2-70b\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"template_file\": \"prompt_template.txt\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\"\n",
      "  },\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1,\n",
      "      \"max_length_in_tokens\": 500,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 500,\n",
      "      \"max_length_in_tokens\": 1000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1000,\n",
      "      \"max_length_in_tokens\": 2000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 2000,\n",
      "      \"max_length_in_tokens\": 3000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 3000,\n",
      "      \"max_length_in_tokens\": 4000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 305,\n",
      "      \"max_length_in_tokens\": 3997,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    }\n",
      "  ],\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_2000-3000\",\n",
      "    \"weights\": {\n",
      "      \"price_per_tx_wt\": 0.65,\n",
      "      \"latenct_wt\": 0.35\n",
      "    }\n",
      "  },\n",
      "  \"pricing\": {\n",
      "    \"ml.g5.12xlarge\": 7.09,\n",
      "    \"ml.g5.24xlarge\": 10.18,\n",
      "    \"ml.g5.48xlarge\": 20.36,\n",
      "    \"ml.inf2.24xlarge\": 7.79,\n",
      "    \"ml.inf2.48xlarge\": 15.58,\n",
      "    \"ml.p4d.24xlarge\": 37.688\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.1,\n",
      "    \"top_p\": 0.92,\n",
      "    \"top_k\": 120,\n",
      "    \"max_new_tokens\": 100,\n",
      "    \"truncate\": \"at-prompt-token-length\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"meta-textgeneration-llama-2-70b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"llama2-70b\",\n",
      "      \"ep_name\": \"llama-2-70b-g5-48xlarge\",\n",
      "      \"instance_type\": \"ml.g5.48xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\",\n",
      "        \"payload_en_1000-2000.jsonl\",\n",
      "        \"payload_en_2000-3000.jsonl\",\n",
      "        \"payload_en_3000-4000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4,\n",
      "        6,\n",
      "        8\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"4095\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"4096\",\n",
      "        \"SM_NUM_GPUS\": \"8\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122\",\n",
      "      \"model_id\": \"meta-llama/Llama-2-70b-hf\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"llama2-70bdjl\",\n",
      "      \"bucket_name\": \"sagemaker-us-east-1-015469603702\",\n",
      "      \"key_name\": \"hf-large-model-djl/code_llama2\",\n",
      "      \"s3_path\": \"s3://sagemaker-us-east-1-015469603702/llama-2-70B-fp16/lmi/\",\n",
      "      \"ep_name\": \"llama-2-70b-chat-p4d-24xlarge\",\n",
      "      \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-tensorrtllm0.7.1-cu122\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"p4d_djl.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\",\n",
      "        \"payload_en_1000-2000.jsonl\",\n",
      "        \"payload_en_2000-3000.jsonl\",\n",
      "        \"payload_en_3000-4000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4,\n",
      "        6,\n",
      "        8\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"remove_truncate\": true,\n",
      "      \"env\": {\n",
      "        \"MODEL_LOADING_TIMEOUT\": \"3600\",\n",
      "        \"NUMBER_OF_GPU\": \"8\",\n",
      "        \"INSTANCE_COUNT\": \"1\",\n",
      "        \"HEALTH_CHECK_TIMOUT\": \"300\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"results\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the deployed model endpoints from the endpoints.json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 15:29:02,539] p1345 {3756670252.py:3} INFO - found information for 2 endpoints\n",
      "[2024-01-23 15:29:02,540] p1345 {3756670252.py:4} INFO - [\n",
      "  {\n",
      "    \"experiment_name\": \"llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "    \"endpoint\": {\n",
      "      \"EndpointName\": \"llama-2-70b-g5-48xlarge-1706022365\",\n",
      "      \"EndpointArn\": \"arn:aws:sagemaker:us-east-1:015469603702:endpoint/llama-2-70b-g5-48xlarge-1706022365\",\n",
      "      \"EndpointConfigName\": \"llama-2-70b-g5-48xlarge-1706022365\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"AllTraffic\",\n",
      "          \"DeployedImages\": [\n",
      "            {\n",
      "              \"SpecifiedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "              \"ResolvedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb\",\n",
      "              \"ResolutionTime\": \"2024-01-23 15:06:09.107000+00:00\"\n",
      "            }\n",
      "          ],\n",
      "          \"CurrentWeight\": 1.0,\n",
      "          \"DesiredWeight\": 1.0,\n",
      "          \"CurrentInstanceCount\": 1,\n",
      "          \"DesiredInstanceCount\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"EndpointStatus\": \"InService\",\n",
      "      \"CreationTime\": \"2024-01-23 15:06:07.676000+00:00\",\n",
      "      \"LastModifiedTime\": \"2024-01-23 15:12:13.384000+00:00\",\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"be2a1042-8adf-4e97-84de-e27b68e91b08\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"be2a1042-8adf-4e97-84de-e27b68e91b08\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"815\",\n",
      "          \"date\": \"Tue, 23 Jan 2024 15:19:09 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"endpoint_config\": {\n",
      "      \"EndpointConfigName\": \"llama-2-70b-g5-48xlarge-1706022365\",\n",
      "      \"EndpointConfigArn\": \"arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/llama-2-70b-g5-48xlarge-1706022365\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"AllTraffic\",\n",
      "          \"ModelName\": \"meta-textgeneration-llama-2-70b-2024-01-23-15-06-05-808\",\n",
      "          \"InitialInstanceCount\": 1,\n",
      "          \"InstanceType\": \"ml.g5.48xlarge\",\n",
      "          \"InitialVariantWeight\": 1.0,\n",
      "          \"ModelDataDownloadTimeoutInSeconds\": 1200,\n",
      "          \"ContainerStartupHealthCheckTimeoutInSeconds\": 1200\n",
      "        }\n",
      "      ],\n",
      "      \"CreationTime\": \"2024-01-23 15:06:07.072000+00:00\",\n",
      "      \"EnableNetworkIsolation\": false,\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"37c6d74f-7391-4150-912a-6755500ff809\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"37c6d74f-7391-4150-912a-6755500ff809\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"534\",\n",
      "          \"date\": \"Tue, 23 Jan 2024 15:19:09 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_config\": {\n",
      "      \"ModelName\": \"meta-textgeneration-llama-2-70b-2024-01-23-15-06-05-808\",\n",
      "      \"PrimaryContainer\": {\n",
      "        \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "        \"Mode\": \"SingleModel\",\n",
      "        \"ModelDataSource\": {\n",
      "          \"S3DataSource\": {\n",
      "            \"S3Uri\": \"s3://jumpstart-private-cache-prod-us-east-1/meta-textgeneration/meta-textgeneration-llama-2-70b/artifacts/inference-prepack/v1.0.0/\",\n",
      "            \"S3DataType\": \"S3Prefix\",\n",
      "            \"CompressionType\": \"None\",\n",
      "            \"ModelAccessConfig\": {\n",
      "              \"AcceptEula\": true\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"Environment\": {\n",
      "          \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "          \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "          \"MAX_INPUT_LENGTH\": \"4095\",\n",
      "          \"MAX_TOTAL_TOKENS\": \"4096\",\n",
      "          \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "          \"SAGEMAKER_ENV\": \"1\",\n",
      "          \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "          \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "          \"SM_NUM_GPUS\": \"8\"\n",
      "        }\n",
      "      },\n",
      "      \"ExecutionRoleArn\": \"arn:aws:iam::015469603702:role/service-role/AmazonSageMaker-ExecutionRole-20231116T132325\",\n",
      "      \"CreationTime\": \"2024-01-23 15:06:06.462000+00:00\",\n",
      "      \"ModelArn\": \"arn:aws:sagemaker:us-east-1:015469603702:model/meta-textgeneration-llama-2-70b-2024-01-23-15-06-05-808\",\n",
      "      \"EnableNetworkIsolation\": true,\n",
      "      \"DeploymentRecommendation\": {\n",
      "        \"RecommendationStatus\": \"COMPLETED\",\n",
      "        \"RealTimeInferenceRecommendations\": []\n",
      "      },\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"5bf5a819-98b1-4979-bb35-f60ccfba4dc1\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"5bf5a819-98b1-4979-bb35-f60ccfba4dc1\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"1179\",\n",
      "          \"date\": \"Tue, 23 Jan 2024 15:19:09 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"experiment_name\": \"llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122\",\n",
      "    \"endpoint\": {\n",
      "      \"EndpointName\": \"llama2-70bdjl-2024-01-23-15-06-05-901-endpoint\",\n",
      "      \"EndpointArn\": \"arn:aws:sagemaker:us-east-1:015469603702:endpoint/llama2-70bdjl-2024-01-23-15-06-05-901-endpoint\",\n",
      "      \"EndpointConfigName\": \"llama2-70bdjl-2024-01-23-15-06-05-901-config\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"variant1\",\n",
      "          \"DeployedImages\": [\n",
      "            {\n",
      "              \"SpecifiedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-tensorrtllm0.7.1-cu122\",\n",
      "              \"ResolvedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference@sha256:b38eb2b4ff0b49f65cefb2507c34b2669db86c5e427615c61136642710eda8ad\",\n",
      "              \"ResolutionTime\": \"2024-01-23 15:06:08.412000+00:00\"\n",
      "            }\n",
      "          ],\n",
      "          \"CurrentWeight\": 1.0,\n",
      "          \"DesiredWeight\": 1.0,\n",
      "          \"CurrentInstanceCount\": 1,\n",
      "          \"DesiredInstanceCount\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"EndpointStatus\": \"InService\",\n",
      "      \"CreationTime\": \"2024-01-23 15:06:07.539000+00:00\",\n",
      "      \"LastModifiedTime\": \"2024-01-23 15:18:25.913000+00:00\",\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"a32c5627-947f-444f-8b6e-d27c765df372\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"a32c5627-947f-444f-8b6e-d27c765df372\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"795\",\n",
      "          \"date\": \"Tue, 23 Jan 2024 15:19:09 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"endpoint_config\": {\n",
      "      \"EndpointConfigName\": \"llama2-70bdjl-2024-01-23-15-06-05-901-config\",\n",
      "      \"EndpointConfigArn\": \"arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/llama2-70bdjl-2024-01-23-15-06-05-901-config\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"variant1\",\n",
      "          \"ModelName\": \"llama2-70bdjl-2024-01-23-15-06-05-901\",\n",
      "          \"InitialInstanceCount\": 1,\n",
      "          \"InstanceType\": \"ml.p4d.24xlarge\",\n",
      "          \"InitialVariantWeight\": 1.0,\n",
      "          \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
      "          \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600\n",
      "        }\n",
      "      ],\n",
      "      \"CreationTime\": \"2024-01-23 15:06:07.092000+00:00\",\n",
      "      \"EnableNetworkIsolation\": false,\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"92d22d65-f409-4b91-94a7-dbca52c81210\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"92d22d65-f409-4b91-94a7-dbca52c81210\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"535\",\n",
      "          \"date\": \"Tue, 23 Jan 2024 15:19:10 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_config\": {\n",
      "      \"ModelName\": \"llama2-70bdjl-2024-01-23-15-06-05-901\",\n",
      "      \"PrimaryContainer\": {\n",
      "        \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-tensorrtllm0.7.1-cu122\",\n",
      "        \"Mode\": \"SingleModel\",\n",
      "        \"ModelDataUrl\": \"s3://sagemaker-us-east-1-015469603702/hf-large-model-djl/code_llama2/model.tar.gz\",\n",
      "        \"ModelDataSource\": {\n",
      "          \"S3DataSource\": {\n",
      "            \"S3Uri\": \"s3://sagemaker-us-east-1-015469603702/hf-large-model-djl/code_llama2/model.tar.gz\",\n",
      "            \"S3DataType\": \"S3Object\",\n",
      "            \"CompressionType\": \"Gzip\"\n",
      "          }\n",
      "        },\n",
      "        \"Environment\": {\n",
      "          \"HEALTH_CHECK_TIMOUT\": \"300\",\n",
      "          \"INSTANCE_COUNT\": \"1\",\n",
      "          \"MODEL_LOADING_TIMEOUT\": \"3600\",\n",
      "          \"NUMBER_OF_GPU\": \"8\"\n",
      "        }\n",
      "      },\n",
      "      \"ExecutionRoleArn\": \"arn:aws:iam::015469603702:role/service-role/AmazonSageMaker-ExecutionRole-20231116T132325\",\n",
      "      \"CreationTime\": \"2024-01-23 15:06:06.588000+00:00\",\n",
      "      \"ModelArn\": \"arn:aws:sagemaker:us-east-1:015469603702:model/llama2-70bdjl-2024-01-23-15-06-05-901\",\n",
      "      \"EnableNetworkIsolation\": false,\n",
      "      \"DeploymentRecommendation\": {\n",
      "        \"RecommendationStatus\": \"COMPLETED\",\n",
      "        \"RealTimeInferenceRecommendations\": [\n",
      "          {\n",
      "            \"RecommendationId\": \"llama2-70bdjl-2024-01-23-15-06-05-901/4NoLLdLK\",\n",
      "            \"InstanceType\": \"ml.c6i.2xlarge\",\n",
      "            \"Environment\": {}\n",
      "          },\n",
      "          {\n",
      "            \"RecommendationId\": \"llama2-70bdjl-2024-01-23-15-06-05-901/kVeqzUlL\",\n",
      "            \"InstanceType\": \"ml.c6i.large\",\n",
      "            \"Environment\": {}\n",
      "          },\n",
      "          {\n",
      "            \"RecommendationId\": \"llama2-70bdjl-2024-01-23-15-06-05-901/m0G7yFTL\",\n",
      "            \"InstanceType\": \"ml.c5.large\",\n",
      "            \"Environment\": {}\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"f4a4f169-aa27-45a9-b5de-af97eb6fa27c\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"f4a4f169-aa27-45a9-b5de-af97eb6fa27c\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"1312\",\n",
      "          \"date\": \"Tue, 23 Jan 2024 15:19:10 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# read the list of deployed endpoints\n",
    "endpoint_info_list = json.loads(Path(ENDPOINT_LIST_FPATH).read_text())\n",
    "logger.info(f\"found information for {len(endpoint_info_list)} endpoints\")\n",
    "logger.info(json.dumps(endpoint_info_list, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 15:29:03,937] p1345 {1455142584.py:3} INFO - there are 2 deployed endpoint(s), endpoint_name_list->['llama-2-70b-g5-48xlarge-1706022365', 'llama2-70bdjl-2024-01-23-15-06-05-901-endpoint']\n"
     ]
    }
   ],
   "source": [
    "# List down the endpoint names that have been deployed\n",
    "endpoint_name_list = [e['endpoint']['EndpointName'] for e in endpoint_info_list]\n",
    "logger.info(f\"there are {len(endpoint_name_list)} deployed endpoint(s), endpoint_name_list->{endpoint_name_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating predictor objects from the deployed endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 15:29:04,534] p1345 {3970465137.py:15} INFO - [<sagemaker.base_predictor.Predictor object at 0x7ff6d94a7310>, <sagemaker.base_predictor.Predictor object at 0x7ff6ea50d090>]\n"
     ]
    }
   ],
   "source": [
    "# create predictor objects\n",
    "\n",
    "## create a sagemaker predictor for these endpoints\n",
    "def create_predictor(endpoint_name: str) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "    # Create a SageMaker Predictor object\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "        serializer=JSONSerializer()\n",
    "    )\n",
    "    return predictor\n",
    "\n",
    "## Display the list of predictor objects that have been deployed ready for inferencing from\n",
    "predictor_list: List = [create_predictor(ep) for ep in endpoint_name_list]\n",
    "logger.info(predictor_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions to define and calculate metrics during the time of invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_sum(l: List) -> Union[int, float]:\n",
    "    return sum(filter(None, l))\n",
    "\n",
    "def safe_div(n: Union[int, float], d: Union[int, float]) -> Optional[Union[int, float]]:\n",
    "    return n/d if d else None\n",
    "\n",
    "## Represents the function to calculate all of the metrics at the time of inference\n",
    "def calculate_metrics(responses, chunk, elapsed_async, experiment_name, concurrency, payload_file) -> Dict:\n",
    "    \n",
    "    ## calculate errors based on the completion status of the inference prompt\n",
    "    errors = [r for r in responses if r['completion'] is None]\n",
    "    \n",
    "    ## Calculate the difference as the successes \n",
    "    successes = len(chunk) - len(errors)\n",
    "    \n",
    "    ## Count all of the prompts token count during inference\n",
    "    all_prompts_token_count = safe_sum([r['prompt_tokens'] for r in responses])\n",
    "    prompt_token_throughput = round(all_prompts_token_count / elapsed_async, 2)\n",
    "    prompt_token_count_mean = safe_div(all_prompts_token_count, successes)\n",
    "    all_completions_token_count = safe_sum([r['completion_tokens'] for r in responses])\n",
    "    completion_token_throughput = round(all_completions_token_count / elapsed_async, 2)\n",
    "    completion_token_count_mean = safe_div(all_completions_token_count, successes)\n",
    "    transactions_per_second = round(successes / elapsed_async, 2)\n",
    "    transactions_per_minute = int(transactions_per_second * 60)\n",
    "    \n",
    "    ## calculate the latency mean utilizing the safe_sum function defined above\n",
    "    latency_mean = safe_div(safe_sum([r['latency'] for r in responses]), successes)\n",
    "    \n",
    "    ## Function returns all these values at the time of the invocations\n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'concurrency': concurrency,\n",
    "        'payload_file': payload_file,\n",
    "        'errors': errors,\n",
    "        'successes': successes,\n",
    "        'error_rate': len(errors)/len(chunk),\n",
    "        'all_prompts_token_count': all_prompts_token_count,\n",
    "        'prompt_token_count_mean': prompt_token_count_mean,\n",
    "        'prompt_token_throughput': prompt_token_throughput,\n",
    "        'all_completions_token_count': all_completions_token_count,\n",
    "        'completion_token_count_mean': completion_token_count_mean,\n",
    "        'completion_token_throughput': completion_token_throughput,\n",
    "        'transactions': len(chunk),\n",
    "        'transactions_per_second': transactions_per_second,\n",
    "        'transactions_per_minute': transactions_per_minute,\n",
    "        'latency_mean': latency_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a blocker function and a series of asynchronous concurrent model prompt invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_metrics(endpoint_name=None,\n",
    "                    prompt=None,\n",
    "                    inference_params=None,\n",
    "                    completion=None,\n",
    "                    prompt_tokens=None,\n",
    "                    completion_tokens=None,\n",
    "                    latency=None) -> Dict:\n",
    "    return dict(endpoint_name=endpoint_name,                \n",
    "                prompt=prompt,\n",
    "                **inference_params,\n",
    "                completion=completion,\n",
    "                prompt_tokens=prompt_tokens,\n",
    "                completion_tokens=completion_tokens,\n",
    "                latency=latency)\n",
    "\n",
    "def get_inference(predictor, payload) -> Dict:\n",
    "    \n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "    latency = 0\n",
    "\n",
    "    try:\n",
    "        prompt_tokens = count_tokens(payload['inputs'])\n",
    "        logger.info(f\"get_inference, endpoint={predictor.endpoint_name}, prompt_tokens={prompt_tokens}\")\n",
    "\n",
    "        # get inference\n",
    "        st = time.perf_counter()        \n",
    "        response = predictor.predict(payload)        \n",
    "        latency = time.perf_counter() - st\n",
    "\n",
    "        if isinstance(response, bytes):\n",
    "            response = response.decode('utf-8')\n",
    "        response_json = json.loads(response)\n",
    "        if isinstance(response_json, list):\n",
    "            response_json = response_json[0]\n",
    "\n",
    "        completion = response_json.get(\"generated_text\", \"\")\n",
    "        completion_tokens = count_tokens(completion)\n",
    "\n",
    "        # Set metrics and logging for both cases\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               completion,\n",
    "                               prompt_tokens,\n",
    "                               completion_tokens,\n",
    "                               latency)\n",
    "        # logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, response={json.dumps(response, indent=2)}, latency={latency:.2f}\")\n",
    "        logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, completion_tokens={completion_tokens}, latency={latency:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error occurred with {predictor.endpoint_name}, exception={str(e)}\")\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               None,\n",
    "                               prompt_tokens,\n",
    "                               None,\n",
    "                               None)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a series of asynchronous functions to invoke and run inferences concurrently and asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents a function to start invoking models in separate thread asynchronously for the blocker function\n",
    "async def async_get_inference(predictor, payload: Dict) -> Dict:\n",
    "    return await asyncio.to_thread(get_inference, predictor, payload)\n",
    "\n",
    "## Gathers all of the tasks and sets of the concurrent calling of the asychronous invocations\n",
    "async def async_get_all_inferences(predictor, payload_list: List) -> List:\n",
    "    return await asyncio.gather(*[async_get_inference(predictor, payload) for payload in payload_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This function runs the asychronous function series above together for different experiments and concurrency levels.\n",
    "async def run_inferences(predictor: sagemaker.base_predictor.Predictor, chunk: List, experiment: Dict, concurrency: int, payload_file: str) -> Tuple[List, Dict]:\n",
    "    logger.info(f\"Processing chunk with concurrency={concurrency}\")\n",
    "    s = time.perf_counter()\n",
    "    responses = await async_get_all_inferences(predictor, chunk)\n",
    "    elapsed_async = time.perf_counter() - s\n",
    "\n",
    "    # Add more metadata about this experiment\n",
    "    for r in responses:\n",
    "        r['experiment_name'] = experiment['name']\n",
    "        r['concurrency'] = concurrency\n",
    "\n",
    "    metrics = calculate_metrics(responses, chunk, elapsed_async, experiment['name'], concurrency, payload_file)\n",
    "    return responses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create the predictors from the experiment we are iterating over\n",
    "def create_predictor_for_experiment(experiment: str, config: Dict, endpoint_info_list: List) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "\n",
    "    ## Here, we set the index and then iterate through the experiments\n",
    "    e_idx = config['experiments'].index(experiment) + 1\n",
    "\n",
    "    ## Iterate through the endpoint information to fetch the endpoint name\n",
    "    ep_info = [e for e in endpoint_info_list if e['experiment_name'] == experiment['name']]\n",
    "    if not ep_info:\n",
    "        logger.error(f\"endpoint for experiment={experiment['name']} not found, skipping\")\n",
    "        return None\n",
    "    ep_name = ep_info[0]['endpoint']['EndpointName']\n",
    "    logger.info(f\"experiment={e_idx}, name={experiment['name']}, ep_name={ep_name}\")\n",
    "\n",
    "    # create a predictor from each endpoint in experiments\n",
    "    return create_predictor(ep_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here, we will process combinations of concurrency levels, the payload files and then loop through the \n",
    "## different combinations to make payloads splitted in terms of the concurrency metric and how we can run \n",
    "## it and make inference\n",
    "\n",
    "def create_payload_dict(jline: str, experiment: Dict) -> Dict:\n",
    "    payload: Dict = json.loads(jline)\n",
    "    if experiment.get('remove_truncate', False) is True:\n",
    "        if payload['parameters'].get('truncate'):\n",
    "            del payload['parameters']['truncate']\n",
    "    return payload\n",
    "    \n",
    "    \n",
    "def create_combinations(experiment: Dict) -> List[Tuple]:\n",
    "    combinations_data = []\n",
    "\n",
    "    # Repeat for each concurrency level\n",
    "    combinations = list(itertools.product(experiment['concurrency_levels'], experiment['payload_files']))\n",
    "    logger.info(f\"there are {len(combinations)} combinations of {combinations} to run\")\n",
    "\n",
    "    for concurrency, payload_file in combinations:\n",
    "        # Read the payload file\n",
    "        fpath = os.path.join(PROMPTS_DIR, payload_file)\n",
    "        payload_list = [create_payload_dict(jline, experiment) for jline in Path(fpath).read_text().splitlines()]\n",
    "        logger.info(f\"read {fpath}, contains {len(payload_list)} lines\")      \n",
    "\n",
    "        logger.info(f\"creating combinations for concurrency={concurrency}, payload_file={payload_file}, payload_list length={len(payload_list)}\")\n",
    "        \n",
    "        # check if we have enough element in the list to run at least the concurrency count numberof transactions..\n",
    "        # for example if there are only 2 prompts and we want to run say 6 in parallel then take the first element (prompt)\n",
    "        # and replicate it 6-2=4 times and add it to the original list\n",
    "        n = concurrency\n",
    "        \n",
    "        if len(payload_list) < n:\n",
    "            elements_to_add = n - len(payload_list)\n",
    "            element_to_replicate = payload_list[0]\n",
    "            # payload_list = payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "        # Split the original list into sublists which contain the number of requests we want to send concurrently        \n",
    "        payload_list_splitted = [payload_list[i * n:(i + 1) * n] for i in range((len(payload_list) + n - 1) // n )]  \n",
    "        \n",
    "        for p in payload_list_splitted:\n",
    "            if len(p) < n:\n",
    "                elements_to_add = n - len(p)\n",
    "                element_to_replicate = p[0]\n",
    "                # p = p.extend([element_to_replicate]*elements_to_add)\n",
    "                p.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "\n",
    "        # Only keep lists that have at least concurrency number of elements\n",
    "        len_before = len(payload_list_splitted)\n",
    "        payload_list_splitted = [p for p in payload_list_splitted if len(p) == concurrency]\n",
    "        logger.info(f\"after only retaining chunks of length {concurrency}, we have {len(payload_list_splitted)} chunks, previously we had {len_before} chunks\")\n",
    "        combinations_data.append((concurrency, payload_file, payload_list_splitted))\n",
    "    logger.info(f\"there are {len(combinations)} for {experiment}\")\n",
    "    return combinations_data\n",
    "\n",
    "# process_combinations(experiment, predictor, PROMPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 15:29:08,900] p1345 {663335596.py:13} INFO - experiment=1, name=llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0, ep_name=llama-2-70b-g5-48xlarge-1706022365\n",
      "[2024-01-23 15:29:08,924] p1345 {1352403322.py:18} INFO - there are 25 combinations of [(1, 'payload_en_1-500.jsonl'), (1, 'payload_en_500-1000.jsonl'), (1, 'payload_en_1000-2000.jsonl'), (1, 'payload_en_2000-3000.jsonl'), (1, 'payload_en_3000-4000.jsonl'), (2, 'payload_en_1-500.jsonl'), (2, 'payload_en_500-1000.jsonl'), (2, 'payload_en_1000-2000.jsonl'), (2, 'payload_en_2000-3000.jsonl'), (2, 'payload_en_3000-4000.jsonl'), (4, 'payload_en_1-500.jsonl'), (4, 'payload_en_500-1000.jsonl'), (4, 'payload_en_1000-2000.jsonl'), (4, 'payload_en_2000-3000.jsonl'), (4, 'payload_en_3000-4000.jsonl'), (6, 'payload_en_1-500.jsonl'), (6, 'payload_en_500-1000.jsonl'), (6, 'payload_en_1000-2000.jsonl'), (6, 'payload_en_2000-3000.jsonl'), (6, 'payload_en_3000-4000.jsonl'), (8, 'payload_en_1-500.jsonl'), (8, 'payload_en_500-1000.jsonl'), (8, 'payload_en_1000-2000.jsonl'), (8, 'payload_en_2000-3000.jsonl'), (8, 'payload_en_3000-4000.jsonl')] to run\n",
      "[2024-01-23 15:29:08,929] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 15:29:08,930] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 15:29:08,930] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 15:29:08,934] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 15:29:08,934] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 15:29:08,936] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 15:29:08,940] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 15:29:08,941] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 15:29:08,943] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 15 chunks, previously we had 15 chunks\n",
      "[2024-01-23 15:29:08,947] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 15:29:08,948] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 15:29:08,950] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 32 chunks, previously we had 32 chunks\n",
      "[2024-01-23 15:29:08,956] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 15:29:08,957] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 15:29:08,958] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 57 chunks, previously we had 57 chunks\n",
      "[2024-01-23 15:29:08,960] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 15:29:08,961] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 15:29:08,962] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 15:29:08,964] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 15:29:08,965] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 15:29:08,965] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 15:29:08,968] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 15:29:08,969] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 15:29:08,969] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-23 15:29:08,974] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 15:29:08,974] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 15:29:08,975] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 16 chunks, previously we had 16 chunks\n",
      "[2024-01-23 15:29:08,982] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 15:29:08,983] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 15:29:08,985] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 29 chunks, previously we had 29 chunks\n",
      "[2024-01-23 15:29:08,987] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 15:29:08,987] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 15:29:08,989] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 15:29:08,991] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 15:29:08,991] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 15:29:08,992] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 15:29:08,994] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 15:29:08,995] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 15:29:08,995] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 4 chunks, previously we had 4 chunks\n",
      "[2024-01-23 15:29:08,998] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 15:29:09,000] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 15:29:09,001] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-23 15:29:09,007] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 15:29:09,008] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 15:29:09,009] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 15 chunks, previously we had 15 chunks\n",
      "[2024-01-23 15:29:09,011] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 15:29:09,011] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 15:29:09,013] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 15:29:09,015] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 15:29:09,016] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 15:29:09,016] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 15:29:09,019] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 15:29:09,019] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 15:29:09,021] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 3 chunks, previously we had 3 chunks\n",
      "[2024-01-23 15:29:09,024] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 15:29:09,025] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 15:29:09,025] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 6 chunks, previously we had 6 chunks\n",
      "[2024-01-23 15:29:09,031] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 15:29:09,032] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 15:29:09,032] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 10 chunks, previously we had 10 chunks\n",
      "[2024-01-23 15:29:09,034] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 15:29:09,035] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 15:29:09,035] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 15:29:09,038] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 15:29:09,038] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 15:29:09,039] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 15:29:09,041] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 15:29:09,041] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 15:29:09,042] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 2 chunks, previously we had 2 chunks\n",
      "[2024-01-23 15:29:09,046] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 15:29:09,047] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 15:29:09,047] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 4 chunks, previously we had 4 chunks\n",
      "[2024-01-23 15:29:09,054] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 15:29:09,055] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 15:29:09,055] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-23 15:29:09,056] p1345 {1352403322.py:55} INFO - there are 25 for {'name': 'llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'meta-textgeneration-llama-2-70b', 'model_version': '*', 'model_name': 'llama2-70b', 'ep_name': 'llama-2-70b-g5-48xlarge', 'instance_type': 'ml.g5.48xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl', 'payload_en_500-1000.jsonl', 'payload_en_1000-2000.jsonl', 'payload_en_2000-3000.jsonl', 'payload_en_3000-4000.jsonl'], 'concurrency_levels': [1, 2, 4, 6, 8], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '4095', 'MAX_TOTAL_TOKENS': '4096', 'SM_NUM_GPUS': '8', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}\n",
      "[2024-01-23 15:29:09,057] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/1\n",
      "[2024-01-23 15:29:09,058] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:29:09,062] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 15:29:15,223] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=6.16\n",
      "[2024-01-23 15:29:15,242] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=1\n",
      "[2024-01-23 15:29:15,243] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/1\n",
      "[2024-01-23 15:29:15,243] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:29:15,248] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 15:29:23,673] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=8.42\n",
      "[2024-01-23 15:29:23,696] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=1\n",
      "[2024-01-23 15:29:23,696] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/15\n",
      "[2024-01-23 15:29:23,697] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:29:23,704] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1339\n",
      "[2024-01-23 15:29:34,246] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=10.54\n",
      "[2024-01-23 15:29:34,268] p1345 {3496713318.py:39} INFO - completed processing chunk 1/15 with concurrency=1\n",
      "[2024-01-23 15:29:34,269] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/15\n",
      "[2024-01-23 15:29:34,270] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:29:34,277] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1932\n",
      "[2024-01-23 15:29:46,270] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.99\n",
      "[2024-01-23 15:29:46,288] p1345 {3496713318.py:39} INFO - completed processing chunk 2/15 with concurrency=1\n",
      "[2024-01-23 15:29:46,288] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/15\n",
      "[2024-01-23 15:29:46,289] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:29:46,294] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1154\n",
      "[2024-01-23 15:29:55,725] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=9.43\n",
      "[2024-01-23 15:29:55,745] p1345 {3496713318.py:39} INFO - completed processing chunk 3/15 with concurrency=1\n",
      "[2024-01-23 15:29:55,745] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/15\n",
      "[2024-01-23 15:29:55,746] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:29:55,752] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1646\n",
      "[2024-01-23 15:30:07,081] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.33\n",
      "[2024-01-23 15:30:07,101] p1345 {3496713318.py:39} INFO - completed processing chunk 4/15 with concurrency=1\n",
      "[2024-01-23 15:30:07,102] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/15\n",
      "[2024-01-23 15:30:07,103] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:30:07,108] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1397\n",
      "[2024-01-23 15:30:17,429] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=10.32\n",
      "[2024-01-23 15:30:17,447] p1345 {3496713318.py:39} INFO - completed processing chunk 5/15 with concurrency=1\n",
      "[2024-01-23 15:30:17,448] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/15\n",
      "[2024-01-23 15:30:17,448] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:30:17,455] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1746\n",
      "[2024-01-23 15:30:29,421] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.96\n",
      "[2024-01-23 15:30:29,439] p1345 {3496713318.py:39} INFO - completed processing chunk 6/15 with concurrency=1\n",
      "[2024-01-23 15:30:29,439] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=7/15\n",
      "[2024-01-23 15:30:29,440] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:30:29,446] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1373\n",
      "[2024-01-23 15:30:39,806] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=10.36\n",
      "[2024-01-23 15:30:39,825] p1345 {3496713318.py:39} INFO - completed processing chunk 7/15 with concurrency=1\n",
      "[2024-01-23 15:30:39,826] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=8/15\n",
      "[2024-01-23 15:30:39,827] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:30:39,833] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1598\n",
      "[2024-01-23 15:30:50,823] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=10.99\n",
      "[2024-01-23 15:30:50,840] p1345 {3496713318.py:39} INFO - completed processing chunk 8/15 with concurrency=1\n",
      "[2024-01-23 15:30:50,841] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=9/15\n",
      "[2024-01-23 15:30:50,841] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:30:50,848] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1743\n",
      "[2024-01-23 15:31:03,438] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=12.59\n",
      "[2024-01-23 15:31:03,458] p1345 {3496713318.py:39} INFO - completed processing chunk 9/15 with concurrency=1\n",
      "[2024-01-23 15:31:03,458] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=10/15\n",
      "[2024-01-23 15:31:03,459] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:31:03,465] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1539\n",
      "[2024-01-23 15:31:14,171] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=10.70\n",
      "[2024-01-23 15:31:14,194] p1345 {3496713318.py:39} INFO - completed processing chunk 10/15 with concurrency=1\n",
      "[2024-01-23 15:31:14,195] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=11/15\n",
      "[2024-01-23 15:31:14,196] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:31:14,202] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1695\n",
      "[2024-01-23 15:31:25,708] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.50\n",
      "[2024-01-23 15:31:25,728] p1345 {3496713318.py:39} INFO - completed processing chunk 11/15 with concurrency=1\n",
      "[2024-01-23 15:31:25,728] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=12/15\n",
      "[2024-01-23 15:31:25,729] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:31:25,734] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1421\n",
      "[2024-01-23 15:31:36,534] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=10.80\n",
      "[2024-01-23 15:31:36,556] p1345 {3496713318.py:39} INFO - completed processing chunk 12/15 with concurrency=1\n",
      "[2024-01-23 15:31:36,556] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=13/15\n",
      "[2024-01-23 15:31:36,557] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:31:36,564] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1918\n",
      "[2024-01-23 15:31:42,617] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=6.05\n",
      "[2024-01-23 15:31:42,637] p1345 {3496713318.py:39} INFO - completed processing chunk 13/15 with concurrency=1\n",
      "[2024-01-23 15:31:42,638] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=14/15\n",
      "[2024-01-23 15:31:42,638] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:31:42,646] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1910\n",
      "[2024-01-23 15:31:48,386] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=5.74\n",
      "[2024-01-23 15:31:48,405] p1345 {3496713318.py:39} INFO - completed processing chunk 14/15 with concurrency=1\n",
      "[2024-01-23 15:31:48,406] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=15/15\n",
      "[2024-01-23 15:31:48,407] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:31:48,416] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1939\n",
      "[2024-01-23 15:32:01,271] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=12.85\n",
      "[2024-01-23 15:32:01,296] p1345 {3496713318.py:39} INFO - completed processing chunk 15/15 with concurrency=1\n",
      "[2024-01-23 15:32:01,297] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/32\n",
      "[2024-01-23 15:32:01,297] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:32:01,305] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2637\n",
      "[2024-01-23 15:32:16,168] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=14.86\n",
      "[2024-01-23 15:32:16,190] p1345 {3496713318.py:39} INFO - completed processing chunk 1/32 with concurrency=1\n",
      "[2024-01-23 15:32:16,190] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/32\n",
      "[2024-01-23 15:32:16,191] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:32:16,200] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3000\n",
      "[2024-01-23 15:32:32,583] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.38\n",
      "[2024-01-23 15:32:32,602] p1345 {3496713318.py:39} INFO - completed processing chunk 2/32 with concurrency=1\n",
      "[2024-01-23 15:32:32,602] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/32\n",
      "[2024-01-23 15:32:32,603] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:32:32,610] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2148\n",
      "[2024-01-23 15:32:39,296] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=8, latency=6.69\n",
      "[2024-01-23 15:32:39,316] p1345 {3496713318.py:39} INFO - completed processing chunk 3/32 with concurrency=1\n",
      "[2024-01-23 15:32:39,316] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/32\n",
      "[2024-01-23 15:32:39,317] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:32:39,325] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2715\n",
      "[2024-01-23 15:32:48,599] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=20, latency=9.27\n",
      "[2024-01-23 15:32:48,619] p1345 {3496713318.py:39} INFO - completed processing chunk 4/32 with concurrency=1\n",
      "[2024-01-23 15:32:48,619] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/32\n",
      "[2024-01-23 15:32:48,620] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:32:48,628] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2404\n",
      "[2024-01-23 15:33:02,428] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=13.80\n",
      "[2024-01-23 15:33:02,448] p1345 {3496713318.py:39} INFO - completed processing chunk 5/32 with concurrency=1\n",
      "[2024-01-23 15:33:02,449] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/32\n",
      "[2024-01-23 15:33:02,449] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:33:02,457] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2150\n",
      "[2024-01-23 15:33:15,658] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=13.20\n",
      "[2024-01-23 15:33:15,674] p1345 {3496713318.py:39} INFO - completed processing chunk 6/32 with concurrency=1\n",
      "[2024-01-23 15:33:15,675] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=7/32\n",
      "[2024-01-23 15:33:15,675] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:33:15,684] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2803\n",
      "[2024-01-23 15:33:24,469] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=11, latency=8.78\n",
      "[2024-01-23 15:33:24,489] p1345 {3496713318.py:39} INFO - completed processing chunk 7/32 with concurrency=1\n",
      "[2024-01-23 15:33:24,490] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=8/32\n",
      "[2024-01-23 15:33:24,490] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:33:24,498] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2369\n",
      "[2024-01-23 15:33:38,433] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=13.93\n",
      "[2024-01-23 15:33:38,453] p1345 {3496713318.py:39} INFO - completed processing chunk 8/32 with concurrency=1\n",
      "[2024-01-23 15:33:38,454] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=9/32\n",
      "[2024-01-23 15:33:38,454] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:33:38,463] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2675\n",
      "[2024-01-23 15:33:48,689] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=30, latency=10.23\n",
      "[2024-01-23 15:33:48,708] p1345 {3496713318.py:39} INFO - completed processing chunk 9/32 with concurrency=1\n",
      "[2024-01-23 15:33:48,709] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=10/32\n",
      "[2024-01-23 15:33:48,709] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:33:48,717] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2541\n",
      "[2024-01-23 15:34:03,038] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=14.32\n",
      "[2024-01-23 15:34:03,059] p1345 {3496713318.py:39} INFO - completed processing chunk 10/32 with concurrency=1\n",
      "[2024-01-23 15:34:03,060] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=11/32\n",
      "[2024-01-23 15:34:03,060] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:34:03,068] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2186\n",
      "[2024-01-23 15:34:16,149] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=13.08\n",
      "[2024-01-23 15:34:16,168] p1345 {3496713318.py:39} INFO - completed processing chunk 11/32 with concurrency=1\n",
      "[2024-01-23 15:34:16,169] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=12/32\n",
      "[2024-01-23 15:34:16,170] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:34:16,178] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2775\n",
      "[2024-01-23 15:34:31,360] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=15.18\n",
      "[2024-01-23 15:34:31,382] p1345 {3496713318.py:39} INFO - completed processing chunk 12/32 with concurrency=1\n",
      "[2024-01-23 15:34:31,383] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=13/32\n",
      "[2024-01-23 15:34:31,383] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:34:31,391] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2686\n",
      "[2024-01-23 15:34:46,158] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=14.77\n",
      "[2024-01-23 15:34:46,176] p1345 {3496713318.py:39} INFO - completed processing chunk 13/32 with concurrency=1\n",
      "[2024-01-23 15:34:46,176] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=14/32\n",
      "[2024-01-23 15:34:46,177] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:34:46,185] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2500\n",
      "[2024-01-23 15:34:53,824] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=7.64\n",
      "[2024-01-23 15:34:53,844] p1345 {3496713318.py:39} INFO - completed processing chunk 14/32 with concurrency=1\n",
      "[2024-01-23 15:34:53,845] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=15/32\n",
      "[2024-01-23 15:34:53,846] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:34:53,854] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2443\n",
      "[2024-01-23 15:35:07,728] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=13.87\n",
      "[2024-01-23 15:35:07,748] p1345 {3496713318.py:39} INFO - completed processing chunk 15/32 with concurrency=1\n",
      "[2024-01-23 15:35:07,749] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=16/32\n",
      "[2024-01-23 15:35:07,749] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:35:07,757] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2321\n",
      "[2024-01-23 15:35:21,164] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=13.41\n",
      "[2024-01-23 15:35:21,186] p1345 {3496713318.py:39} INFO - completed processing chunk 16/32 with concurrency=1\n",
      "[2024-01-23 15:35:21,186] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=17/32\n",
      "[2024-01-23 15:35:21,187] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:35:21,195] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2428\n",
      "[2024-01-23 15:35:28,734] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=11, latency=7.54\n",
      "[2024-01-23 15:35:28,753] p1345 {3496713318.py:39} INFO - completed processing chunk 17/32 with concurrency=1\n",
      "[2024-01-23 15:35:28,754] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=18/32\n",
      "[2024-01-23 15:35:28,755] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:35:28,763] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2458\n",
      "[2024-01-23 15:35:42,725] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=13.96\n",
      "[2024-01-23 15:35:42,745] p1345 {3496713318.py:39} INFO - completed processing chunk 18/32 with concurrency=1\n",
      "[2024-01-23 15:35:42,745] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=19/32\n",
      "[2024-01-23 15:35:42,746] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:35:42,753] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2101\n",
      "[2024-01-23 15:35:55,533] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=12.78\n",
      "[2024-01-23 15:35:55,552] p1345 {3496713318.py:39} INFO - completed processing chunk 19/32 with concurrency=1\n",
      "[2024-01-23 15:35:55,552] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=20/32\n",
      "[2024-01-23 15:35:55,553] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:35:55,561] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2440\n",
      "[2024-01-23 15:36:09,490] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=13.93\n",
      "[2024-01-23 15:36:09,515] p1345 {3496713318.py:39} INFO - completed processing chunk 20/32 with concurrency=1\n",
      "[2024-01-23 15:36:09,515] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=21/32\n",
      "[2024-01-23 15:36:09,516] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:36:09,524] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2711\n",
      "[2024-01-23 15:36:24,796] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=15.27\n",
      "[2024-01-23 15:36:24,817] p1345 {3496713318.py:39} INFO - completed processing chunk 21/32 with concurrency=1\n",
      "[2024-01-23 15:36:24,818] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=22/32\n",
      "[2024-01-23 15:36:24,819] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:36:24,828] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2771\n",
      "[2024-01-23 15:36:34,547] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=26, latency=9.72\n",
      "[2024-01-23 15:36:34,565] p1345 {3496713318.py:39} INFO - completed processing chunk 22/32 with concurrency=1\n",
      "[2024-01-23 15:36:34,566] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=23/32\n",
      "[2024-01-23 15:36:34,566] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:36:34,576] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2691\n",
      "[2024-01-23 15:36:49,536] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=14.96\n",
      "[2024-01-23 15:36:49,557] p1345 {3496713318.py:39} INFO - completed processing chunk 23/32 with concurrency=1\n",
      "[2024-01-23 15:36:49,557] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=24/32\n",
      "[2024-01-23 15:36:49,558] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:36:49,566] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2624\n",
      "[2024-01-23 15:36:57,601] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=8, latency=8.03\n",
      "[2024-01-23 15:36:57,620] p1345 {3496713318.py:39} INFO - completed processing chunk 24/32 with concurrency=1\n",
      "[2024-01-23 15:36:57,620] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=25/32\n",
      "[2024-01-23 15:36:57,621] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:36:57,629] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2062\n",
      "[2024-01-23 15:37:10,353] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=12.72\n",
      "[2024-01-23 15:37:10,372] p1345 {3496713318.py:39} INFO - completed processing chunk 25/32 with concurrency=1\n",
      "[2024-01-23 15:37:10,373] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=26/32\n",
      "[2024-01-23 15:37:10,373] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:37:10,382] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2213\n",
      "[2024-01-23 15:37:23,488] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=13.11\n",
      "[2024-01-23 15:37:23,508] p1345 {3496713318.py:39} INFO - completed processing chunk 26/32 with concurrency=1\n",
      "[2024-01-23 15:37:23,509] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=27/32\n",
      "[2024-01-23 15:37:23,509] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:37:23,518] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2608\n",
      "[2024-01-23 15:37:31,255] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=7.74\n",
      "[2024-01-23 15:37:31,274] p1345 {3496713318.py:39} INFO - completed processing chunk 27/32 with concurrency=1\n",
      "[2024-01-23 15:37:31,274] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=28/32\n",
      "[2024-01-23 15:37:31,275] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:37:31,284] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2770\n",
      "[2024-01-23 15:37:46,662] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=15.38\n",
      "[2024-01-23 15:37:46,680] p1345 {3496713318.py:39} INFO - completed processing chunk 28/32 with concurrency=1\n",
      "[2024-01-23 15:37:46,680] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=29/32\n",
      "[2024-01-23 15:37:46,681] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:37:46,690] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2564\n",
      "[2024-01-23 15:38:01,068] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=14.38\n",
      "[2024-01-23 15:38:01,088] p1345 {3496713318.py:39} INFO - completed processing chunk 29/32 with concurrency=1\n",
      "[2024-01-23 15:38:01,089] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=30/32\n",
      "[2024-01-23 15:38:01,089] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:38:01,097] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2333\n",
      "[2024-01-23 15:38:14,481] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=13.38\n",
      "[2024-01-23 15:38:14,501] p1345 {3496713318.py:39} INFO - completed processing chunk 30/32 with concurrency=1\n",
      "[2024-01-23 15:38:14,502] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=31/32\n",
      "[2024-01-23 15:38:14,503] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:38:14,511] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2613\n",
      "[2024-01-23 15:38:22,198] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=7.69\n",
      "[2024-01-23 15:38:22,220] p1345 {3496713318.py:39} INFO - completed processing chunk 31/32 with concurrency=1\n",
      "[2024-01-23 15:38:22,221] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=32/32\n",
      "[2024-01-23 15:38:22,221] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:38:22,229] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2358\n",
      "[2024-01-23 15:38:35,859] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=13.63\n",
      "[2024-01-23 15:38:35,878] p1345 {3496713318.py:39} INFO - completed processing chunk 32/32 with concurrency=1\n",
      "[2024-01-23 15:38:35,878] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/57\n",
      "[2024-01-23 15:38:35,879] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:38:35,888] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3000\n",
      "[2024-01-23 15:38:52,379] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.49\n",
      "[2024-01-23 15:38:52,398] p1345 {3496713318.py:39} INFO - completed processing chunk 1/57 with concurrency=1\n",
      "[2024-01-23 15:38:52,398] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/57\n",
      "[2024-01-23 15:38:52,399] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:38:52,414] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3896\n",
      "[2024-01-23 15:39:12,018] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=19.60\n",
      "[2024-01-23 15:39:12,041] p1345 {3496713318.py:39} INFO - completed processing chunk 2/57 with concurrency=1\n",
      "[2024-01-23 15:39:12,041] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/57\n",
      "[2024-01-23 15:39:12,042] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:39:12,058] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3789\n",
      "[2024-01-23 15:39:24,635] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=17, latency=12.58\n",
      "[2024-01-23 15:39:24,654] p1345 {3496713318.py:39} INFO - completed processing chunk 3/57 with concurrency=1\n",
      "[2024-01-23 15:39:24,655] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/57\n",
      "[2024-01-23 15:39:24,655] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:39:24,669] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3450\n",
      "[2024-01-23 15:39:42,513] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.84\n",
      "[2024-01-23 15:39:42,533] p1345 {3496713318.py:39} INFO - completed processing chunk 4/57 with concurrency=1\n",
      "[2024-01-23 15:39:42,534] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/57\n",
      "[2024-01-23 15:39:42,534] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:39:42,548] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3482\n",
      "[2024-01-23 15:40:00,578] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=18.03\n",
      "[2024-01-23 15:40:00,596] p1345 {3496713318.py:39} INFO - completed processing chunk 5/57 with concurrency=1\n",
      "[2024-01-23 15:40:00,597] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/57\n",
      "[2024-01-23 15:40:00,598] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:40:00,611] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3144\n",
      "[2024-01-23 15:40:17,504] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.89\n",
      "[2024-01-23 15:40:17,524] p1345 {3496713318.py:39} INFO - completed processing chunk 6/57 with concurrency=1\n",
      "[2024-01-23 15:40:17,524] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=7/57\n",
      "[2024-01-23 15:40:17,525] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:40:17,539] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3639\n",
      "[2024-01-23 15:40:36,238] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.70\n",
      "[2024-01-23 15:40:36,259] p1345 {3496713318.py:39} INFO - completed processing chunk 7/57 with concurrency=1\n",
      "[2024-01-23 15:40:36,259] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=8/57\n",
      "[2024-01-23 15:40:36,261] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:40:36,273] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3014\n",
      "[2024-01-23 15:40:52,605] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.33\n",
      "[2024-01-23 15:40:52,623] p1345 {3496713318.py:39} INFO - completed processing chunk 8/57 with concurrency=1\n",
      "[2024-01-23 15:40:52,623] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=9/57\n",
      "[2024-01-23 15:40:52,624] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:40:52,638] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3891\n",
      "[2024-01-23 15:41:12,135] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=19.50\n",
      "[2024-01-23 15:41:12,156] p1345 {3496713318.py:39} INFO - completed processing chunk 9/57 with concurrency=1\n",
      "[2024-01-23 15:41:12,157] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=10/57\n",
      "[2024-01-23 15:41:12,158] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:41:12,173] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3575\n",
      "[2024-01-23 15:41:30,373] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.20\n",
      "[2024-01-23 15:41:30,394] p1345 {3496713318.py:39} INFO - completed processing chunk 10/57 with concurrency=1\n",
      "[2024-01-23 15:41:30,394] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=11/57\n",
      "[2024-01-23 15:41:30,395] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:41:30,406] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3419\n",
      "[2024-01-23 15:41:47,957] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=17.55\n",
      "[2024-01-23 15:41:47,976] p1345 {3496713318.py:39} INFO - completed processing chunk 11/57 with concurrency=1\n",
      "[2024-01-23 15:41:47,977] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=12/57\n",
      "[2024-01-23 15:41:47,977] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:41:47,990] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3458\n",
      "[2024-01-23 15:42:06,155] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=18.16\n",
      "[2024-01-23 15:42:06,178] p1345 {3496713318.py:39} INFO - completed processing chunk 12/57 with concurrency=1\n",
      "[2024-01-23 15:42:06,178] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=13/57\n",
      "[2024-01-23 15:42:06,179] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:42:06,192] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3508\n",
      "[2024-01-23 15:42:24,123] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.93\n",
      "[2024-01-23 15:42:24,141] p1345 {3496713318.py:39} INFO - completed processing chunk 13/57 with concurrency=1\n",
      "[2024-01-23 15:42:24,142] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=14/57\n",
      "[2024-01-23 15:42:24,143] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:42:24,155] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3436\n",
      "[2024-01-23 15:42:41,937] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.78\n",
      "[2024-01-23 15:42:41,961] p1345 {3496713318.py:39} INFO - completed processing chunk 14/57 with concurrency=1\n",
      "[2024-01-23 15:42:41,961] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=15/57\n",
      "[2024-01-23 15:42:41,962] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:42:41,976] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3249\n",
      "[2024-01-23 15:42:56,786] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=72, latency=14.81\n",
      "[2024-01-23 15:42:56,806] p1345 {3496713318.py:39} INFO - completed processing chunk 15/57 with concurrency=1\n",
      "[2024-01-23 15:42:56,806] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=16/57\n",
      "[2024-01-23 15:42:56,807] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:42:56,819] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3087\n",
      "[2024-01-23 15:43:13,616] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.80\n",
      "[2024-01-23 15:43:13,635] p1345 {3496713318.py:39} INFO - completed processing chunk 16/57 with concurrency=1\n",
      "[2024-01-23 15:43:13,635] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=17/57\n",
      "[2024-01-23 15:43:13,636] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:43:13,651] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3680\n",
      "[2024-01-23 15:43:25,021] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=8, latency=11.37\n",
      "[2024-01-23 15:43:25,043] p1345 {3496713318.py:39} INFO - completed processing chunk 17/57 with concurrency=1\n",
      "[2024-01-23 15:43:25,043] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=18/57\n",
      "[2024-01-23 15:43:25,044] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:43:25,059] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3950\n",
      "[2024-01-23 15:43:44,681] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=19.62\n",
      "[2024-01-23 15:43:44,703] p1345 {3496713318.py:39} INFO - completed processing chunk 18/57 with concurrency=1\n",
      "[2024-01-23 15:43:44,703] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=19/57\n",
      "[2024-01-23 15:43:44,704] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:43:44,718] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3463\n",
      "[2024-01-23 15:44:02,999] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.28\n",
      "[2024-01-23 15:44:03,017] p1345 {3496713318.py:39} INFO - completed processing chunk 19/57 with concurrency=1\n",
      "[2024-01-23 15:44:03,017] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=20/57\n",
      "[2024-01-23 15:44:03,018] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:44:03,037] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3996\n",
      "[2024-01-23 15:44:22,990] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=19.95\n",
      "[2024-01-23 15:44:23,013] p1345 {3496713318.py:39} INFO - completed processing chunk 20/57 with concurrency=1\n",
      "[2024-01-23 15:44:23,013] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=21/57\n",
      "[2024-01-23 15:44:23,014] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:44:23,027] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3050\n",
      "[2024-01-23 15:44:39,457] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.43\n",
      "[2024-01-23 15:44:39,475] p1345 {3496713318.py:39} INFO - completed processing chunk 21/57 with concurrency=1\n",
      "[2024-01-23 15:44:39,475] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=22/57\n",
      "[2024-01-23 15:44:39,476] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:44:39,494] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3737\n",
      "[2024-01-23 15:44:58,355] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.86\n",
      "[2024-01-23 15:44:58,377] p1345 {3496713318.py:39} INFO - completed processing chunk 22/57 with concurrency=1\n",
      "[2024-01-23 15:44:58,377] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=23/57\n",
      "[2024-01-23 15:44:58,378] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:44:58,392] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3410\n",
      "[2024-01-23 15:45:16,170] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.78\n",
      "[2024-01-23 15:45:16,190] p1345 {3496713318.py:39} INFO - completed processing chunk 23/57 with concurrency=1\n",
      "[2024-01-23 15:45:16,191] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=24/57\n",
      "[2024-01-23 15:45:16,191] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:45:16,206] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3979\n",
      "[2024-01-23 15:45:36,728] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=20.52\n",
      "[2024-01-23 15:45:36,750] p1345 {3496713318.py:39} INFO - completed processing chunk 24/57 with concurrency=1\n",
      "[2024-01-23 15:45:36,751] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=25/57\n",
      "[2024-01-23 15:45:36,751] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:45:36,764] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3171\n",
      "[2024-01-23 15:45:46,648] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=9.88\n",
      "[2024-01-23 15:45:46,666] p1345 {3496713318.py:39} INFO - completed processing chunk 25/57 with concurrency=1\n",
      "[2024-01-23 15:45:46,666] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=26/57\n",
      "[2024-01-23 15:45:46,667] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:45:46,681] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3655\n",
      "[2024-01-23 15:46:05,647] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.97\n",
      "[2024-01-23 15:46:05,666] p1345 {3496713318.py:39} INFO - completed processing chunk 26/57 with concurrency=1\n",
      "[2024-01-23 15:46:05,667] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=27/57\n",
      "[2024-01-23 15:46:05,668] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:46:05,682] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3662\n",
      "[2024-01-23 15:46:24,016] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.33\n",
      "[2024-01-23 15:46:24,035] p1345 {3496713318.py:39} INFO - completed processing chunk 27/57 with concurrency=1\n",
      "[2024-01-23 15:46:24,035] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=28/57\n",
      "[2024-01-23 15:46:24,036] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:46:24,047] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3098\n",
      "[2024-01-23 15:46:40,502] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.45\n",
      "[2024-01-23 15:46:40,522] p1345 {3496713318.py:39} INFO - completed processing chunk 28/57 with concurrency=1\n",
      "[2024-01-23 15:46:40,523] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=29/57\n",
      "[2024-01-23 15:46:40,523] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:46:40,538] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3704\n",
      "[2024-01-23 15:46:59,556] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=19.02\n",
      "[2024-01-23 15:46:59,574] p1345 {3496713318.py:39} INFO - completed processing chunk 29/57 with concurrency=1\n",
      "[2024-01-23 15:46:59,575] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=30/57\n",
      "[2024-01-23 15:46:59,575] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:46:59,588] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3400\n",
      "[2024-01-23 15:47:17,473] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.88\n",
      "[2024-01-23 15:47:17,494] p1345 {3496713318.py:39} INFO - completed processing chunk 30/57 with concurrency=1\n",
      "[2024-01-23 15:47:17,494] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=31/57\n",
      "[2024-01-23 15:47:17,495] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:47:17,509] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3098\n",
      "[2024-01-23 15:47:27,046] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=9.54\n",
      "[2024-01-23 15:47:27,067] p1345 {3496713318.py:39} INFO - completed processing chunk 31/57 with concurrency=1\n",
      "[2024-01-23 15:47:27,068] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=32/57\n",
      "[2024-01-23 15:47:27,068] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:47:27,084] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3909\n",
      "[2024-01-23 15:47:46,670] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=19.58\n",
      "[2024-01-23 15:47:46,689] p1345 {3496713318.py:39} INFO - completed processing chunk 32/57 with concurrency=1\n",
      "[2024-01-23 15:47:46,690] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=33/57\n",
      "[2024-01-23 15:47:46,691] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:47:46,708] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3971\n",
      "[2024-01-23 15:48:07,136] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=20.43\n",
      "[2024-01-23 15:48:07,156] p1345 {3496713318.py:39} INFO - completed processing chunk 33/57 with concurrency=1\n",
      "[2024-01-23 15:48:07,156] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=34/57\n",
      "[2024-01-23 15:48:07,157] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:48:07,170] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3132\n",
      "[2024-01-23 15:48:24,159] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.99\n",
      "[2024-01-23 15:48:24,179] p1345 {3496713318.py:39} INFO - completed processing chunk 34/57 with concurrency=1\n",
      "[2024-01-23 15:48:24,180] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=35/57\n",
      "[2024-01-23 15:48:24,180] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:48:24,194] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3135\n",
      "[2024-01-23 15:48:41,263] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.07\n",
      "[2024-01-23 15:48:41,283] p1345 {3496713318.py:39} INFO - completed processing chunk 35/57 with concurrency=1\n",
      "[2024-01-23 15:48:41,284] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=36/57\n",
      "[2024-01-23 15:48:41,285] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:48:41,298] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3030\n",
      "[2024-01-23 15:48:50,529] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=8, latency=9.23\n",
      "[2024-01-23 15:48:50,549] p1345 {3496713318.py:39} INFO - completed processing chunk 36/57 with concurrency=1\n",
      "[2024-01-23 15:48:50,549] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=37/57\n",
      "[2024-01-23 15:48:50,550] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:48:50,566] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3848\n",
      "[2024-01-23 15:49:10,112] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=19.55\n",
      "[2024-01-23 15:49:10,140] p1345 {3496713318.py:39} INFO - completed processing chunk 37/57 with concurrency=1\n",
      "[2024-01-23 15:49:10,141] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=38/57\n",
      "[2024-01-23 15:49:10,142] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:49:10,155] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3208\n",
      "[2024-01-23 15:49:20,067] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=9.91\n",
      "[2024-01-23 15:49:20,085] p1345 {3496713318.py:39} INFO - completed processing chunk 38/57 with concurrency=1\n",
      "[2024-01-23 15:49:20,085] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=39/57\n",
      "[2024-01-23 15:49:20,086] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:49:20,100] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3475\n",
      "[2024-01-23 15:49:38,676] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.57\n",
      "[2024-01-23 15:49:38,695] p1345 {3496713318.py:39} INFO - completed processing chunk 39/57 with concurrency=1\n",
      "[2024-01-23 15:49:38,696] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=40/57\n",
      "[2024-01-23 15:49:38,696] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:49:38,711] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3681\n",
      "[2024-01-23 15:49:57,647] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.93\n",
      "[2024-01-23 15:49:57,669] p1345 {3496713318.py:39} INFO - completed processing chunk 40/57 with concurrency=1\n",
      "[2024-01-23 15:49:57,669] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=41/57\n",
      "[2024-01-23 15:49:57,670] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:49:57,685] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3581\n",
      "[2024-01-23 15:50:08,521] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=10.84\n",
      "[2024-01-23 15:50:08,539] p1345 {3496713318.py:39} INFO - completed processing chunk 41/57 with concurrency=1\n",
      "[2024-01-23 15:50:08,540] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=42/57\n",
      "[2024-01-23 15:50:08,540] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:50:08,554] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3302\n",
      "[2024-01-23 15:50:25,712] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.16\n",
      "[2024-01-23 15:50:25,731] p1345 {3496713318.py:39} INFO - completed processing chunk 42/57 with concurrency=1\n",
      "[2024-01-23 15:50:25,732] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=43/57\n",
      "[2024-01-23 15:50:25,733] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:50:25,747] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3614\n",
      "[2024-01-23 15:50:44,394] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.65\n",
      "[2024-01-23 15:50:44,414] p1345 {3496713318.py:39} INFO - completed processing chunk 43/57 with concurrency=1\n",
      "[2024-01-23 15:50:44,415] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=44/57\n",
      "[2024-01-23 15:50:44,415] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:50:44,430] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3537\n",
      "[2024-01-23 15:50:55,651] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=11.22\n",
      "[2024-01-23 15:50:55,670] p1345 {3496713318.py:39} INFO - completed processing chunk 44/57 with concurrency=1\n",
      "[2024-01-23 15:50:55,671] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=45/57\n",
      "[2024-01-23 15:50:55,671] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:50:55,686] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3165\n",
      "[2024-01-23 15:51:12,410] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.72\n",
      "[2024-01-23 15:51:12,431] p1345 {3496713318.py:39} INFO - completed processing chunk 45/57 with concurrency=1\n",
      "[2024-01-23 15:51:12,432] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=46/57\n",
      "[2024-01-23 15:51:12,432] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:51:12,446] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3476\n",
      "[2024-01-23 15:51:30,435] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.99\n",
      "[2024-01-23 15:51:30,453] p1345 {3496713318.py:39} INFO - completed processing chunk 46/57 with concurrency=1\n",
      "[2024-01-23 15:51:30,454] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=47/57\n",
      "[2024-01-23 15:51:30,455] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:51:30,468] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3235\n",
      "[2024-01-23 15:51:47,526] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.06\n",
      "[2024-01-23 15:51:47,547] p1345 {3496713318.py:39} INFO - completed processing chunk 47/57 with concurrency=1\n",
      "[2024-01-23 15:51:47,547] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=48/57\n",
      "[2024-01-23 15:51:47,548] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:51:47,562] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3363\n",
      "[2024-01-23 15:52:05,412] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.85\n",
      "[2024-01-23 15:52:05,431] p1345 {3496713318.py:39} INFO - completed processing chunk 48/57 with concurrency=1\n",
      "[2024-01-23 15:52:05,432] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=49/57\n",
      "[2024-01-23 15:52:05,432] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:52:05,448] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3882\n",
      "[2024-01-23 15:52:17,427] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=11.98\n",
      "[2024-01-23 15:52:17,447] p1345 {3496713318.py:39} INFO - completed processing chunk 49/57 with concurrency=1\n",
      "[2024-01-23 15:52:17,448] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=50/57\n",
      "[2024-01-23 15:52:17,449] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:52:17,465] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3616\n",
      "[2024-01-23 15:52:35,740] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.27\n",
      "[2024-01-23 15:52:35,766] p1345 {3496713318.py:39} INFO - completed processing chunk 50/57 with concurrency=1\n",
      "[2024-01-23 15:52:35,767] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=51/57\n",
      "[2024-01-23 15:52:35,767] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:52:35,783] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3567\n",
      "[2024-01-23 15:52:53,965] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.18\n",
      "[2024-01-23 15:52:53,991] p1345 {3496713318.py:39} INFO - completed processing chunk 51/57 with concurrency=1\n",
      "[2024-01-23 15:52:53,992] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=52/57\n",
      "[2024-01-23 15:52:53,992] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:52:54,006] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3330\n",
      "[2024-01-23 15:53:03,806] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=9.80\n",
      "[2024-01-23 15:53:03,827] p1345 {3496713318.py:39} INFO - completed processing chunk 52/57 with concurrency=1\n",
      "[2024-01-23 15:53:03,828] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=53/57\n",
      "[2024-01-23 15:53:03,829] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:53:03,845] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3566\n",
      "[2024-01-23 15:53:22,028] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.18\n",
      "[2024-01-23 15:53:22,046] p1345 {3496713318.py:39} INFO - completed processing chunk 53/57 with concurrency=1\n",
      "[2024-01-23 15:53:22,046] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=54/57\n",
      "[2024-01-23 15:53:22,047] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:53:22,061] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3497\n",
      "[2024-01-23 15:53:39,941] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.88\n",
      "[2024-01-23 15:53:39,962] p1345 {3496713318.py:39} INFO - completed processing chunk 54/57 with concurrency=1\n",
      "[2024-01-23 15:53:39,962] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=55/57\n",
      "[2024-01-23 15:53:39,963] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:53:39,974] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3003\n",
      "[2024-01-23 15:53:49,118] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=9.14\n",
      "[2024-01-23 15:53:49,136] p1345 {3496713318.py:39} INFO - completed processing chunk 55/57 with concurrency=1\n",
      "[2024-01-23 15:53:49,137] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=56/57\n",
      "[2024-01-23 15:53:49,138] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:53:49,152] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3783\n",
      "[2024-01-23 15:54:01,046] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=8, latency=11.89\n",
      "[2024-01-23 15:54:01,066] p1345 {3496713318.py:39} INFO - completed processing chunk 56/57 with concurrency=1\n",
      "[2024-01-23 15:54:01,067] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=57/57\n",
      "[2024-01-23 15:54:01,067] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 15:54:01,081] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 15:54:11,492] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=10, latency=10.41\n",
      "[2024-01-23 15:54:11,511] p1345 {3496713318.py:39} INFO - completed processing chunk 57/57 with concurrency=1\n",
      "[2024-01-23 15:54:11,512] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/1\n",
      "[2024-01-23 15:54:11,512] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:54:11,517] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 15:54:11,520] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 15:54:18,898] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=7.38\n",
      "[2024-01-23 15:54:18,898] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=7.38\n",
      "[2024-01-23 15:54:18,927] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=2\n",
      "[2024-01-23 15:54:18,927] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/1\n",
      "[2024-01-23 15:54:18,928] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:54:18,936] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 15:54:18,938] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 15:54:30,485] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.54\n",
      "[2024-01-23 15:54:30,486] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.55\n",
      "[2024-01-23 15:54:30,518] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=2\n",
      "[2024-01-23 15:54:30,519] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/8\n",
      "[2024-01-23 15:54:30,519] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:54:30,527] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1339\n",
      "[2024-01-23 15:54:30,530] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1932\n",
      "[2024-01-23 15:54:47,454] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.92\n",
      "[2024-01-23 15:54:47,454] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.93\n",
      "[2024-01-23 15:54:47,484] p1345 {3496713318.py:39} INFO - completed processing chunk 1/8 with concurrency=2\n",
      "[2024-01-23 15:54:47,484] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/8\n",
      "[2024-01-23 15:54:47,485] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:54:47,495] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1154\n",
      "[2024-01-23 15:54:47,498] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1646\n",
      "[2024-01-23 15:55:03,072] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=15.57\n",
      "[2024-01-23 15:55:03,072] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=15.58\n",
      "[2024-01-23 15:55:03,101] p1345 {3496713318.py:39} INFO - completed processing chunk 2/8 with concurrency=2\n",
      "[2024-01-23 15:55:03,102] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/8\n",
      "[2024-01-23 15:55:03,102] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:55:03,109] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1397\n",
      "[2024-01-23 15:55:03,113] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1746\n",
      "[2024-01-23 15:55:12,628] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=9.51\n",
      "[2024-01-23 15:55:18,604] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=15.49\n",
      "[2024-01-23 15:55:18,633] p1345 {3496713318.py:39} INFO - completed processing chunk 3/8 with concurrency=2\n",
      "[2024-01-23 15:55:18,634] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/8\n",
      "[2024-01-23 15:55:18,635] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:55:18,642] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1373\n",
      "[2024-01-23 15:55:18,647] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1598\n",
      "[2024-01-23 15:55:34,223] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=15.58\n",
      "[2024-01-23 15:55:34,223] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=15.57\n",
      "[2024-01-23 15:55:34,254] p1345 {3496713318.py:39} INFO - completed processing chunk 4/8 with concurrency=2\n",
      "[2024-01-23 15:55:34,254] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/8\n",
      "[2024-01-23 15:55:34,255] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:55:34,265] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1743\n",
      "[2024-01-23 15:55:34,265] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1539\n",
      "[2024-01-23 15:55:51,863] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.59\n",
      "[2024-01-23 15:55:51,863] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.60\n",
      "[2024-01-23 15:55:51,892] p1345 {3496713318.py:39} INFO - completed processing chunk 5/8 with concurrency=2\n",
      "[2024-01-23 15:55:51,893] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/8\n",
      "[2024-01-23 15:55:51,894] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:55:51,903] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1695\n",
      "[2024-01-23 15:55:51,903] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1421\n",
      "[2024-01-23 15:56:02,376] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=6, latency=10.47\n",
      "[2024-01-23 15:56:08,457] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=16.55\n",
      "[2024-01-23 15:56:08,489] p1345 {3496713318.py:39} INFO - completed processing chunk 6/8 with concurrency=2\n",
      "[2024-01-23 15:56:08,490] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=7/8\n",
      "[2024-01-23 15:56:08,490] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:56:08,498] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1918\n",
      "[2024-01-23 15:56:08,503] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1910\n",
      "[2024-01-23 15:56:19,737] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=11.24\n",
      "[2024-01-23 15:56:26,102] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=17.60\n",
      "[2024-01-23 15:56:26,131] p1345 {3496713318.py:39} INFO - completed processing chunk 7/8 with concurrency=2\n",
      "[2024-01-23 15:56:26,131] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=8/8\n",
      "[2024-01-23 15:56:26,132] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:56:26,143] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1939\n",
      "[2024-01-23 15:56:26,145] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1939\n",
      "[2024-01-23 15:56:45,609] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=19.46\n",
      "[2024-01-23 15:56:45,609] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=19.46\n",
      "[2024-01-23 15:56:45,636] p1345 {3496713318.py:39} INFO - completed processing chunk 8/8 with concurrency=2\n",
      "[2024-01-23 15:56:45,636] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/16\n",
      "[2024-01-23 15:56:45,637] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:56:45,650] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2637\n",
      "[2024-01-23 15:56:45,650] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3000\n",
      "[2024-01-23 15:57:05,036] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=36, latency=19.39\n",
      "[2024-01-23 15:57:09,932] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=24.28\n",
      "[2024-01-23 15:57:09,966] p1345 {3496713318.py:39} INFO - completed processing chunk 1/16 with concurrency=2\n",
      "[2024-01-23 15:57:09,966] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/16\n",
      "[2024-01-23 15:57:09,967] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:57:09,974] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2148\n",
      "[2024-01-23 15:57:09,981] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2715\n",
      "[2024-01-23 15:57:32,065] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=22.09\n",
      "[2024-01-23 15:57:32,068] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=22.08\n",
      "[2024-01-23 15:57:32,096] p1345 {3496713318.py:39} INFO - completed processing chunk 2/16 with concurrency=2\n",
      "[2024-01-23 15:57:32,097] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/16\n",
      "[2024-01-23 15:57:32,097] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:57:32,106] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2404\n",
      "[2024-01-23 15:57:32,110] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2150\n",
      "[2024-01-23 15:57:45,809] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=13.70\n",
      "[2024-01-23 15:57:45,811] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=13.70\n",
      "[2024-01-23 15:57:45,840] p1345 {3496713318.py:39} INFO - completed processing chunk 3/16 with concurrency=2\n",
      "[2024-01-23 15:57:45,840] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/16\n",
      "[2024-01-23 15:57:45,841] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:57:45,850] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2803\n",
      "[2024-01-23 15:57:45,854] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2369\n",
      "[2024-01-23 15:58:08,506] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=22.65\n",
      "[2024-01-23 15:58:08,508] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=22.65\n",
      "[2024-01-23 15:58:08,535] p1345 {3496713318.py:39} INFO - completed processing chunk 4/16 with concurrency=2\n",
      "[2024-01-23 15:58:08,536] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/16\n",
      "[2024-01-23 15:58:08,537] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:58:08,549] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2541\n",
      "[2024-01-23 15:58:08,549] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2675\n",
      "[2024-01-23 15:58:27,836] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=60, latency=19.28\n",
      "[2024-01-23 15:58:30,772] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=22.22\n",
      "[2024-01-23 15:58:30,802] p1345 {3496713318.py:39} INFO - completed processing chunk 5/16 with concurrency=2\n",
      "[2024-01-23 15:58:30,803] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/16\n",
      "[2024-01-23 15:58:30,803] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:58:30,813] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2186\n",
      "[2024-01-23 15:58:30,817] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2775\n",
      "[2024-01-23 15:58:45,383] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=14.56\n",
      "[2024-01-23 15:58:51,916] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=21.10\n",
      "[2024-01-23 15:58:51,946] p1345 {3496713318.py:39} INFO - completed processing chunk 6/16 with concurrency=2\n",
      "[2024-01-23 15:58:51,947] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=7/16\n",
      "[2024-01-23 15:58:51,948] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:58:51,957] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2500\n",
      "[2024-01-23 15:58:51,962] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2686\n",
      "[2024-01-23 15:59:09,522] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=34, latency=17.56\n",
      "[2024-01-23 15:59:14,292] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=22.33\n",
      "[2024-01-23 15:59:14,324] p1345 {3496713318.py:39} INFO - completed processing chunk 7/16 with concurrency=2\n",
      "[2024-01-23 15:59:14,325] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=8/16\n",
      "[2024-01-23 15:59:14,325] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:59:14,338] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2321\n",
      "[2024-01-23 15:59:14,338] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2443\n",
      "[2024-01-23 15:59:35,773] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=21.43\n",
      "[2024-01-23 15:59:35,774] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=21.43\n",
      "[2024-01-23 15:59:35,802] p1345 {3496713318.py:39} INFO - completed processing chunk 8/16 with concurrency=2\n",
      "[2024-01-23 15:59:35,803] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=9/16\n",
      "[2024-01-23 15:59:35,803] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:59:35,815] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2428\n",
      "[2024-01-23 15:59:35,818] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2458\n",
      "[2024-01-23 15:59:57,351] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=21.53\n",
      "[2024-01-23 15:59:57,352] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=21.54\n",
      "[2024-01-23 15:59:57,381] p1345 {3496713318.py:39} INFO - completed processing chunk 9/16 with concurrency=2\n",
      "[2024-01-23 15:59:57,382] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=10/16\n",
      "[2024-01-23 15:59:57,382] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 15:59:57,394] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2101\n",
      "[2024-01-23 15:59:57,398] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2440\n",
      "[2024-01-23 16:00:11,260] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=10, latency=13.86\n",
      "[2024-01-23 16:00:17,655] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=20.26\n",
      "[2024-01-23 16:00:17,686] p1345 {3496713318.py:39} INFO - completed processing chunk 10/16 with concurrency=2\n",
      "[2024-01-23 16:00:17,687] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=11/16\n",
      "[2024-01-23 16:00:17,687] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:00:17,701] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2771\n",
      "[2024-01-23 16:00:17,702] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2711\n",
      "[2024-01-23 16:00:41,712] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=24.01\n",
      "[2024-01-23 16:00:41,712] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=24.01\n",
      "[2024-01-23 16:00:41,740] p1345 {3496713318.py:39} INFO - completed processing chunk 11/16 with concurrency=2\n",
      "[2024-01-23 16:00:41,741] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=12/16\n",
      "[2024-01-23 16:00:41,742] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:00:41,753] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2624\n",
      "[2024-01-23 16:00:41,757] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2691\n",
      "[2024-01-23 16:01:04,895] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.13\n",
      "[2024-01-23 16:01:04,895] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.14\n",
      "[2024-01-23 16:01:04,929] p1345 {3496713318.py:39} INFO - completed processing chunk 12/16 with concurrency=2\n",
      "[2024-01-23 16:01:04,930] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=13/16\n",
      "[2024-01-23 16:01:04,931] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:01:04,941] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2062\n",
      "[2024-01-23 16:01:04,944] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2213\n",
      "[2024-01-23 16:01:17,864] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=12.92\n",
      "[2024-01-23 16:01:24,173] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=19.23\n",
      "[2024-01-23 16:01:24,200] p1345 {3496713318.py:39} INFO - completed processing chunk 13/16 with concurrency=2\n",
      "[2024-01-23 16:01:24,201] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=14/16\n",
      "[2024-01-23 16:01:24,201] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:01:24,211] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2608\n",
      "[2024-01-23 16:01:24,216] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2770\n",
      "[2024-01-23 16:01:47,428] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.21\n",
      "[2024-01-23 16:01:47,428] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.22\n",
      "[2024-01-23 16:01:47,457] p1345 {3496713318.py:39} INFO - completed processing chunk 14/16 with concurrency=2\n",
      "[2024-01-23 16:01:47,458] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=15/16\n",
      "[2024-01-23 16:01:47,458] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:01:47,469] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2333\n",
      "[2024-01-23 16:01:47,474] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2564\n",
      "[2024-01-23 16:02:02,097] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=14.62\n",
      "[2024-01-23 16:02:08,700] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=21.23\n",
      "[2024-01-23 16:02:08,729] p1345 {3496713318.py:39} INFO - completed processing chunk 15/16 with concurrency=2\n",
      "[2024-01-23 16:02:08,729] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=16/16\n",
      "[2024-01-23 16:02:08,730] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:02:08,742] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2613\n",
      "[2024-01-23 16:02:08,743] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2358\n",
      "[2024-01-23 16:02:30,794] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=22.05\n",
      "[2024-01-23 16:02:30,794] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=22.05\n",
      "[2024-01-23 16:02:30,821] p1345 {3496713318.py:39} INFO - completed processing chunk 16/16 with concurrency=2\n",
      "[2024-01-23 16:02:30,822] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/29\n",
      "[2024-01-23 16:02:30,822] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:02:30,834] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3000\n",
      "[2024-01-23 16:02:30,837] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3896\n",
      "[2024-01-23 16:02:52,371] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=15, latency=21.53\n",
      "[2024-01-23 16:02:58,828] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=27.99\n",
      "[2024-01-23 16:02:58,864] p1345 {3496713318.py:39} INFO - completed processing chunk 1/29 with concurrency=2\n",
      "[2024-01-23 16:02:58,864] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/29\n",
      "[2024-01-23 16:02:58,865] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:02:58,880] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3450\n",
      "[2024-01-23 16:02:58,886] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3789\n",
      "[2024-01-23 16:03:28,595] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.71\n",
      "[2024-01-23 16:03:28,597] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.72\n",
      "[2024-01-23 16:03:28,623] p1345 {3496713318.py:39} INFO - completed processing chunk 2/29 with concurrency=2\n",
      "[2024-01-23 16:03:28,624] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/29\n",
      "[2024-01-23 16:03:28,625] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:03:28,639] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3144\n",
      "[2024-01-23 16:03:28,642] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3482\n",
      "[2024-01-23 16:03:56,497] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=27.85\n",
      "[2024-01-23 16:03:56,498] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=27.86\n",
      "[2024-01-23 16:03:56,527] p1345 {3496713318.py:39} INFO - completed processing chunk 3/29 with concurrency=2\n",
      "[2024-01-23 16:03:56,528] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/29\n",
      "[2024-01-23 16:03:56,529] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:03:56,544] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3014\n",
      "[2024-01-23 16:03:56,546] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3639\n",
      "[2024-01-23 16:04:16,549] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=20.00\n",
      "[2024-01-23 16:04:17,304] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=17, latency=20.76\n",
      "[2024-01-23 16:04:17,333] p1345 {3496713318.py:39} INFO - completed processing chunk 4/29 with concurrency=2\n",
      "[2024-01-23 16:04:17,334] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/29\n",
      "[2024-01-23 16:04:17,335] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:04:17,350] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3891\n",
      "[2024-01-23 16:04:17,352] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3575\n",
      "[2024-01-23 16:04:47,444] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=30.09\n",
      "[2024-01-23 16:04:47,446] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=30.09\n",
      "[2024-01-23 16:04:47,474] p1345 {3496713318.py:39} INFO - completed processing chunk 5/29 with concurrency=2\n",
      "[2024-01-23 16:04:47,475] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/29\n",
      "[2024-01-23 16:04:47,476] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:04:47,491] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3419\n",
      "[2024-01-23 16:04:47,494] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3458\n",
      "[2024-01-23 16:05:09,534] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=30, latency=22.04\n",
      "[2024-01-23 16:05:15,126] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=27.63\n",
      "[2024-01-23 16:05:15,175] p1345 {3496713318.py:39} INFO - completed processing chunk 6/29 with concurrency=2\n",
      "[2024-01-23 16:05:15,176] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=7/29\n",
      "[2024-01-23 16:05:15,177] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:05:15,188] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3436\n",
      "[2024-01-23 16:05:15,193] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3508\n",
      "[2024-01-23 16:05:43,399] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.21\n",
      "[2024-01-23 16:05:43,400] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.21\n",
      "[2024-01-23 16:05:43,430] p1345 {3496713318.py:39} INFO - completed processing chunk 7/29 with concurrency=2\n",
      "[2024-01-23 16:05:43,430] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=8/29\n",
      "[2024-01-23 16:05:43,431] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:05:43,445] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3087\n",
      "[2024-01-23 16:05:43,447] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3249\n",
      "[2024-01-23 16:06:02,500] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=19.05\n",
      "[2024-01-23 16:06:02,883] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=10, latency=19.44\n",
      "[2024-01-23 16:06:02,914] p1345 {3496713318.py:39} INFO - completed processing chunk 8/29 with concurrency=2\n",
      "[2024-01-23 16:06:02,915] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=9/29\n",
      "[2024-01-23 16:06:02,915] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:06:02,932] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3950\n",
      "[2024-01-23 16:06:02,933] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3680\n",
      "[2024-01-23 16:06:34,298] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=31.36\n",
      "[2024-01-23 16:06:34,298] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=31.36\n",
      "[2024-01-23 16:06:34,331] p1345 {3496713318.py:39} INFO - completed processing chunk 9/29 with concurrency=2\n",
      "[2024-01-23 16:06:34,331] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=10/29\n",
      "[2024-01-23 16:06:34,332] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:06:34,347] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3463\n",
      "[2024-01-23 16:06:34,354] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3996\n",
      "[2024-01-23 16:07:05,101] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=30.75\n",
      "[2024-01-23 16:07:05,101] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=30.75\n",
      "[2024-01-23 16:07:05,135] p1345 {3496713318.py:39} INFO - completed processing chunk 10/29 with concurrency=2\n",
      "[2024-01-23 16:07:05,135] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=11/29\n",
      "[2024-01-23 16:07:05,136] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:07:05,150] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3050\n",
      "[2024-01-23 16:07:05,155] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3737\n",
      "[2024-01-23 16:07:33,407] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.26\n",
      "[2024-01-23 16:07:33,409] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.25\n",
      "[2024-01-23 16:07:33,435] p1345 {3496713318.py:39} INFO - completed processing chunk 11/29 with concurrency=2\n",
      "[2024-01-23 16:07:33,436] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=12/29\n",
      "[2024-01-23 16:07:33,437] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:07:33,450] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3410\n",
      "[2024-01-23 16:07:33,456] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3979\n",
      "[2024-01-23 16:07:56,321] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=11, latency=22.86\n",
      "[2024-01-23 16:08:03,374] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.92\n",
      "[2024-01-23 16:08:03,404] p1345 {3496713318.py:39} INFO - completed processing chunk 12/29 with concurrency=2\n",
      "[2024-01-23 16:08:03,405] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=13/29\n",
      "[2024-01-23 16:08:03,405] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:08:03,419] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3655\n",
      "[2024-01-23 16:08:03,423] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3171\n",
      "[2024-01-23 16:08:23,944] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=10, latency=20.52\n",
      "[2024-01-23 16:08:30,887] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=27.46\n",
      "[2024-01-23 16:08:30,917] p1345 {3496713318.py:39} INFO - completed processing chunk 13/29 with concurrency=2\n",
      "[2024-01-23 16:08:30,918] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=14/29\n",
      "[2024-01-23 16:08:30,919] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:08:30,933] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3098\n",
      "[2024-01-23 16:08:30,937] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3662\n",
      "[2024-01-23 16:08:59,150] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.21\n",
      "[2024-01-23 16:08:59,150] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.21\n",
      "[2024-01-23 16:08:59,181] p1345 {3496713318.py:39} INFO - completed processing chunk 14/29 with concurrency=2\n",
      "[2024-01-23 16:08:59,182] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=15/29\n",
      "[2024-01-23 16:08:59,182] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:08:59,198] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3400\n",
      "[2024-01-23 16:08:59,201] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3704\n",
      "[2024-01-23 16:09:28,151] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=28.95\n",
      "[2024-01-23 16:09:28,153] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=28.95\n",
      "[2024-01-23 16:09:28,183] p1345 {3496713318.py:39} INFO - completed processing chunk 15/29 with concurrency=2\n",
      "[2024-01-23 16:09:28,183] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=16/29\n",
      "[2024-01-23 16:09:28,184] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:09:28,202] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3098\n",
      "[2024-01-23 16:09:28,202] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3909\n",
      "[2024-01-23 16:09:48,474] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=20.27\n",
      "[2024-01-23 16:09:56,564] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.36\n",
      "[2024-01-23 16:09:56,595] p1345 {3496713318.py:39} INFO - completed processing chunk 16/29 with concurrency=2\n",
      "[2024-01-23 16:09:56,596] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=17/29\n",
      "[2024-01-23 16:09:56,596] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:09:56,613] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3132\n",
      "[2024-01-23 16:09:56,616] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3971\n",
      "[2024-01-23 16:10:17,234] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=20.62\n",
      "[2024-01-23 16:10:25,362] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.74\n",
      "[2024-01-23 16:10:25,389] p1345 {3496713318.py:39} INFO - completed processing chunk 17/29 with concurrency=2\n",
      "[2024-01-23 16:10:25,390] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=18/29\n",
      "[2024-01-23 16:10:25,391] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:10:25,406] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3135\n",
      "[2024-01-23 16:10:25,406] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3030\n",
      "[2024-01-23 16:10:51,340] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=25.93\n",
      "[2024-01-23 16:10:51,341] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=25.93\n",
      "[2024-01-23 16:10:51,371] p1345 {3496713318.py:39} INFO - completed processing chunk 18/29 with concurrency=2\n",
      "[2024-01-23 16:10:51,371] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=19/29\n",
      "[2024-01-23 16:10:51,372] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:10:51,388] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3208\n",
      "[2024-01-23 16:10:51,390] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3848\n",
      "[2024-01-23 16:11:21,059] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.67\n",
      "[2024-01-23 16:11:21,059] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=29.67\n",
      "[2024-01-23 16:11:21,093] p1345 {3496713318.py:39} INFO - completed processing chunk 19/29 with concurrency=2\n",
      "[2024-01-23 16:11:21,094] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=20/29\n",
      "[2024-01-23 16:11:21,094] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:11:21,107] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3475\n",
      "[2024-01-23 16:11:21,113] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3681\n",
      "[2024-01-23 16:11:42,773] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=21.67\n",
      "[2024-01-23 16:11:50,667] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.55\n",
      "[2024-01-23 16:11:50,695] p1345 {3496713318.py:39} INFO - completed processing chunk 20/29 with concurrency=2\n",
      "[2024-01-23 16:11:50,696] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=21/29\n",
      "[2024-01-23 16:11:50,696] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:11:50,712] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3581\n",
      "[2024-01-23 16:11:50,713] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3302\n",
      "[2024-01-23 16:12:19,134] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.42\n",
      "[2024-01-23 16:12:19,134] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.42\n",
      "[2024-01-23 16:12:19,166] p1345 {3496713318.py:39} INFO - completed processing chunk 21/29 with concurrency=2\n",
      "[2024-01-23 16:12:19,167] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=22/29\n",
      "[2024-01-23 16:12:19,167] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:12:19,185] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3614\n",
      "[2024-01-23 16:12:19,185] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3537\n",
      "[2024-01-23 16:12:48,614] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.43\n",
      "[2024-01-23 16:12:48,614] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.42\n",
      "[2024-01-23 16:12:48,646] p1345 {3496713318.py:39} INFO - completed processing chunk 22/29 with concurrency=2\n",
      "[2024-01-23 16:12:48,647] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=23/29\n",
      "[2024-01-23 16:12:48,648] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:12:48,663] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3165\n",
      "[2024-01-23 16:12:48,666] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3476\n",
      "[2024-01-23 16:13:08,265] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=19.60\n",
      "[2024-01-23 16:13:15,432] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=26.77\n",
      "[2024-01-23 16:13:15,465] p1345 {3496713318.py:39} INFO - completed processing chunk 23/29 with concurrency=2\n",
      "[2024-01-23 16:13:15,465] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=24/29\n",
      "[2024-01-23 16:13:15,466] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:13:15,481] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3235\n",
      "[2024-01-23 16:13:15,482] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3363\n",
      "[2024-01-23 16:13:42,684] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=27.20\n",
      "[2024-01-23 16:13:42,684] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=27.20\n",
      "[2024-01-23 16:13:42,713] p1345 {3496713318.py:39} INFO - completed processing chunk 24/29 with concurrency=2\n",
      "[2024-01-23 16:13:42,713] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=25/29\n",
      "[2024-01-23 16:13:42,714] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:13:42,730] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3882\n",
      "[2024-01-23 16:13:42,734] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3616\n",
      "[2024-01-23 16:14:05,054] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=22.32\n",
      "[2024-01-23 16:14:12,630] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.90\n",
      "[2024-01-23 16:14:12,662] p1345 {3496713318.py:39} INFO - completed processing chunk 25/29 with concurrency=2\n",
      "[2024-01-23 16:14:12,663] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=26/29\n",
      "[2024-01-23 16:14:12,663] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:14:12,676] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3330\n",
      "[2024-01-23 16:14:12,686] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3567\n",
      "[2024-01-23 16:14:33,206] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=12, latency=20.53\n",
      "[2024-01-23 16:14:40,300] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=27.61\n",
      "[2024-01-23 16:14:40,327] p1345 {3496713318.py:39} INFO - completed processing chunk 26/29 with concurrency=2\n",
      "[2024-01-23 16:14:40,327] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=27/29\n",
      "[2024-01-23 16:14:40,328] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:14:40,346] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3566\n",
      "[2024-01-23 16:14:40,346] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3497\n",
      "[2024-01-23 16:15:01,129] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=8, latency=20.78\n",
      "[2024-01-23 16:15:08,526] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.18\n",
      "[2024-01-23 16:15:08,557] p1345 {3496713318.py:39} INFO - completed processing chunk 27/29 with concurrency=2\n",
      "[2024-01-23 16:15:08,558] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=28/29\n",
      "[2024-01-23 16:15:08,559] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:15:08,573] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3003\n",
      "[2024-01-23 16:15:08,576] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3783\n",
      "[2024-01-23 16:15:37,122] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.55\n",
      "[2024-01-23 16:15:37,124] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=28.55\n",
      "[2024-01-23 16:15:37,153] p1345 {3496713318.py:39} INFO - completed processing chunk 28/29 with concurrency=2\n",
      "[2024-01-23 16:15:37,154] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=29/29\n",
      "[2024-01-23 16:15:37,155] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 16:15:37,171] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 16:15:37,171] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 16:15:57,208] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=20.03\n",
      "[2024-01-23 16:16:04,700] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=27.53\n",
      "[2024-01-23 16:16:04,730] p1345 {3496713318.py:39} INFO - completed processing chunk 29/29 with concurrency=2\n",
      "[2024-01-23 16:16:04,731] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/1\n",
      "[2024-01-23 16:16:04,732] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:16:04,742] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:16:04,742] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:16:04,742] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:16:04,743] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:16:14,170] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=9.42\n",
      "[2024-01-23 16:16:14,170] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=9.43\n",
      "[2024-01-23 16:16:14,171] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=9.42\n",
      "[2024-01-23 16:16:14,172] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=9.43\n",
      "[2024-01-23 16:16:14,223] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=4\n",
      "[2024-01-23 16:16:14,223] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/1\n",
      "[2024-01-23 16:16:14,224] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:16:14,235] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:16:14,235] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:16:14,241] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:16:14,241] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:16:32,379] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.14\n",
      "[2024-01-23 16:16:32,379] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.14\n",
      "[2024-01-23 16:16:32,380] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.13\n",
      "[2024-01-23 16:16:32,381] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.14\n",
      "[2024-01-23 16:16:32,426] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=4\n",
      "[2024-01-23 16:16:32,427] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/4\n",
      "[2024-01-23 16:16:32,428] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:16:32,435] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1339\n",
      "[2024-01-23 16:16:32,440] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1932\n",
      "[2024-01-23 16:16:32,442] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1154\n",
      "[2024-01-23 16:16:32,445] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1646\n",
      "[2024-01-23 16:16:57,332] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=24.89\n",
      "[2024-01-23 16:16:57,333] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=24.89\n",
      "[2024-01-23 16:16:57,333] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=24.88\n",
      "[2024-01-23 16:16:58,554] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=26.10\n",
      "[2024-01-23 16:16:58,599] p1345 {3496713318.py:39} INFO - completed processing chunk 1/4 with concurrency=4\n",
      "[2024-01-23 16:16:58,600] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/4\n",
      "[2024-01-23 16:16:58,601] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:16:58,613] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1746\n",
      "[2024-01-23 16:16:58,616] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1598\n",
      "[2024-01-23 16:16:58,617] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1373\n",
      "[2024-01-23 16:16:58,617] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1397\n",
      "[2024-01-23 16:17:23,550] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=24.93\n",
      "[2024-01-23 16:17:23,550] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=24.93\n",
      "[2024-01-23 16:17:23,551] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=24.93\n",
      "[2024-01-23 16:17:24,726] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=26.10\n",
      "[2024-01-23 16:17:24,771] p1345 {3496713318.py:39} INFO - completed processing chunk 2/4 with concurrency=4\n",
      "[2024-01-23 16:17:24,772] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/4\n",
      "[2024-01-23 16:17:24,773] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:17:24,781] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1539\n",
      "[2024-01-23 16:17:24,785] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1743\n",
      "[2024-01-23 16:17:24,790] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1695\n",
      "[2024-01-23 16:17:24,791] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1421\n",
      "[2024-01-23 16:17:46,105] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=12, latency=21.32\n",
      "[2024-01-23 16:17:51,072] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=26.27\n",
      "[2024-01-23 16:17:51,083] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=26.29\n",
      "[2024-01-23 16:17:51,084] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=26.29\n",
      "[2024-01-23 16:17:51,131] p1345 {3496713318.py:39} INFO - completed processing chunk 3/4 with concurrency=4\n",
      "[2024-01-23 16:17:51,132] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/4\n",
      "[2024-01-23 16:17:51,132] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:17:51,142] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1918\n",
      "[2024-01-23 16:17:51,146] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1939\n",
      "[2024-01-23 16:17:51,152] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1910\n",
      "[2024-01-23 16:17:51,152] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1918\n",
      "[2024-01-23 16:18:21,003] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.86\n",
      "[2024-01-23 16:18:21,003] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.85\n",
      "[2024-01-23 16:18:21,007] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=29.85\n",
      "[2024-01-23 16:18:22,258] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=31.10\n",
      "[2024-01-23 16:18:22,306] p1345 {3496713318.py:39} INFO - completed processing chunk 4/4 with concurrency=4\n",
      "[2024-01-23 16:18:22,307] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/8\n",
      "[2024-01-23 16:18:22,307] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:18:22,318] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2637\n",
      "[2024-01-23 16:18:22,325] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3000\n",
      "[2024-01-23 16:18:22,327] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2715\n",
      "[2024-01-23 16:18:22,331] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2148\n",
      "[2024-01-23 16:19:01,617] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=39.28\n",
      "[2024-01-23 16:19:01,617] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=39.29\n",
      "[2024-01-23 16:19:03,129] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=40.79\n",
      "[2024-01-23 16:19:04,565] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=42.23\n",
      "[2024-01-23 16:19:04,609] p1345 {3496713318.py:39} INFO - completed processing chunk 1/8 with concurrency=4\n",
      "[2024-01-23 16:19:04,610] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/8\n",
      "[2024-01-23 16:19:04,610] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:19:04,626] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2404\n",
      "[2024-01-23 16:19:04,628] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2369\n",
      "[2024-01-23 16:19:04,629] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2150\n",
      "[2024-01-23 16:19:04,631] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2803\n",
      "[2024-01-23 16:19:19,451] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=16, latency=14.82\n",
      "[2024-01-23 16:19:40,343] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=35.71\n",
      "[2024-01-23 16:19:41,442] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=36.80\n",
      "[2024-01-23 16:19:42,846] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=38.21\n",
      "[2024-01-23 16:19:42,895] p1345 {3496713318.py:39} INFO - completed processing chunk 2/8 with concurrency=4\n",
      "[2024-01-23 16:19:42,895] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/8\n",
      "[2024-01-23 16:19:42,896] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:19:42,912] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2186\n",
      "[2024-01-23 16:19:42,913] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2541\n",
      "[2024-01-23 16:19:42,917] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2775\n",
      "[2024-01-23 16:19:42,922] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2675\n",
      "[2024-01-23 16:20:20,736] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=37.82\n",
      "[2024-01-23 16:20:20,736] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=37.82\n",
      "[2024-01-23 16:20:22,213] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=39.29\n",
      "[2024-01-23 16:20:23,608] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=40.69\n",
      "[2024-01-23 16:20:23,657] p1345 {3496713318.py:39} INFO - completed processing chunk 3/8 with concurrency=4\n",
      "[2024-01-23 16:20:23,658] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/8\n",
      "[2024-01-23 16:20:23,658] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:20:23,671] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2686\n",
      "[2024-01-23 16:20:23,679] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2443\n",
      "[2024-01-23 16:20:23,684] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2500\n",
      "[2024-01-23 16:20:23,685] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2321\n",
      "[2024-01-23 16:20:46,540] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=6, latency=22.87\n",
      "[2024-01-23 16:20:59,571] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.88\n",
      "[2024-01-23 16:20:59,571] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.89\n",
      "[2024-01-23 16:21:02,266] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=38.58\n",
      "[2024-01-23 16:21:02,315] p1345 {3496713318.py:39} INFO - completed processing chunk 4/8 with concurrency=4\n",
      "[2024-01-23 16:21:02,315] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/8\n",
      "[2024-01-23 16:21:02,316] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:21:02,328] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2428\n",
      "[2024-01-23 16:21:02,337] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2101\n",
      "[2024-01-23 16:21:02,338] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2440\n",
      "[2024-01-23 16:21:02,340] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2458\n",
      "[2024-01-23 16:21:15,708] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=6, latency=13.37\n",
      "[2024-01-23 16:21:37,461] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.12\n",
      "[2024-01-23 16:21:37,775] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.43\n",
      "[2024-01-23 16:21:39,116] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=36.77\n",
      "[2024-01-23 16:21:39,160] p1345 {3496713318.py:39} INFO - completed processing chunk 5/8 with concurrency=4\n",
      "[2024-01-23 16:21:39,161] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/8\n",
      "[2024-01-23 16:21:39,161] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:21:39,173] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2711\n",
      "[2024-01-23 16:21:39,186] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2624\n",
      "[2024-01-23 16:21:39,190] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2771\n",
      "[2024-01-23 16:21:39,192] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2691\n",
      "[2024-01-23 16:21:55,419] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=16.23\n",
      "[2024-01-23 16:22:18,628] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=39.44\n",
      "[2024-01-23 16:22:18,874] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=39.68\n",
      "[2024-01-23 16:22:20,254] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=41.06\n",
      "[2024-01-23 16:22:20,317] p1345 {3496713318.py:39} INFO - completed processing chunk 6/8 with concurrency=4\n",
      "[2024-01-23 16:22:20,318] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=7/8\n",
      "[2024-01-23 16:22:20,319] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:22:20,328] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2062\n",
      "[2024-01-23 16:22:20,337] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2608\n",
      "[2024-01-23 16:22:20,343] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2770\n",
      "[2024-01-23 16:22:20,348] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2213\n",
      "[2024-01-23 16:22:34,062] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=13.72\n",
      "[2024-01-23 16:22:40,921] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=10, latency=20.59\n",
      "[2024-01-23 16:22:56,450] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=36.10\n",
      "[2024-01-23 16:22:56,976] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=36.63\n",
      "[2024-01-23 16:22:57,040] p1345 {3496713318.py:39} INFO - completed processing chunk 7/8 with concurrency=4\n",
      "[2024-01-23 16:22:57,041] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=8/8\n",
      "[2024-01-23 16:22:57,041] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:22:57,053] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2333\n",
      "[2024-01-23 16:22:57,064] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2358\n",
      "[2024-01-23 16:22:57,065] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2613\n",
      "[2024-01-23 16:22:57,065] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2564\n",
      "[2024-01-23 16:23:19,711] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=22.64\n",
      "[2024-01-23 16:23:28,640] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=31.57\n",
      "[2024-01-23 16:23:32,968] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.91\n",
      "[2024-01-23 16:23:32,969] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.90\n",
      "[2024-01-23 16:23:33,026] p1345 {3496713318.py:39} INFO - completed processing chunk 8/8 with concurrency=4\n",
      "[2024-01-23 16:23:33,027] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/15\n",
      "[2024-01-23 16:23:33,028] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:23:33,055] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3450\n",
      "[2024-01-23 16:23:33,060] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3896\n",
      "[2024-01-23 16:23:33,060] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3789\n",
      "[2024-01-23 16:23:33,061] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3000\n",
      "[2024-01-23 16:24:19,858] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=11, latency=46.79\n",
      "[2024-01-23 16:24:24,531] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=51.47\n",
      "[2024-01-23 16:24:24,531] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=51.47\n",
      "[2024-01-23 16:24:25,966] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=52.90\n",
      "[2024-01-23 16:24:26,018] p1345 {3496713318.py:39} INFO - completed processing chunk 1/15 with concurrency=4\n",
      "[2024-01-23 16:24:26,019] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/15\n",
      "[2024-01-23 16:24:26,020] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:24:26,035] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3482\n",
      "[2024-01-23 16:24:26,046] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3014\n",
      "[2024-01-23 16:24:26,047] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3144\n",
      "[2024-01-23 16:24:26,050] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3639\n",
      "[2024-01-23 16:25:13,495] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=47.44\n",
      "[2024-01-23 16:25:13,503] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=47.47\n",
      "[2024-01-23 16:25:15,101] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=49.05\n",
      "[2024-01-23 16:25:16,642] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=50.59\n",
      "[2024-01-23 16:25:16,701] p1345 {3496713318.py:39} INFO - completed processing chunk 2/15 with concurrency=4\n",
      "[2024-01-23 16:25:16,702] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/15\n",
      "[2024-01-23 16:25:16,703] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:25:16,726] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3575\n",
      "[2024-01-23 16:25:16,728] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3891\n",
      "[2024-01-23 16:25:16,733] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3458\n",
      "[2024-01-23 16:25:16,735] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3419\n",
      "[2024-01-23 16:25:38,882] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=10, latency=22.15\n",
      "[2024-01-23 16:26:06,683] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=49.95\n",
      "[2024-01-23 16:26:07,360] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=50.62\n",
      "[2024-01-23 16:26:08,869] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=52.13\n",
      "[2024-01-23 16:26:08,926] p1345 {3496713318.py:39} INFO - completed processing chunk 3/15 with concurrency=4\n",
      "[2024-01-23 16:26:08,927] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/15\n",
      "[2024-01-23 16:26:08,928] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:26:08,947] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3087\n",
      "[2024-01-23 16:26:08,948] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3436\n",
      "[2024-01-23 16:26:08,956] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3249\n",
      "[2024-01-23 16:26:08,958] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3508\n",
      "[2024-01-23 16:26:40,846] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=15, latency=31.89\n",
      "[2024-01-23 16:26:56,665] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=47.71\n",
      "[2024-01-23 16:26:56,667] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=47.71\n",
      "[2024-01-23 16:26:59,671] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=50.72\n",
      "[2024-01-23 16:26:59,734] p1345 {3496713318.py:39} INFO - completed processing chunk 4/15 with concurrency=4\n",
      "[2024-01-23 16:26:59,735] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/15\n",
      "[2024-01-23 16:26:59,735] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:26:59,749] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3950\n",
      "[2024-01-23 16:26:59,761] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3463\n",
      "[2024-01-23 16:26:59,769] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3680\n",
      "[2024-01-23 16:26:59,773] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3996\n",
      "[2024-01-23 16:27:22,134] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=22.38\n",
      "[2024-01-23 16:27:52,541] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=52.78\n",
      "[2024-01-23 16:27:53,164] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=53.39\n",
      "[2024-01-23 16:27:54,697] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=54.93\n",
      "[2024-01-23 16:27:54,760] p1345 {3496713318.py:39} INFO - completed processing chunk 5/15 with concurrency=4\n",
      "[2024-01-23 16:27:54,760] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/15\n",
      "[2024-01-23 16:27:54,761] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:27:54,780] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3050\n",
      "[2024-01-23 16:27:54,786] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3979\n",
      "[2024-01-23 16:27:54,794] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3737\n",
      "[2024-01-23 16:27:54,794] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3410\n",
      "[2024-01-23 16:28:45,274] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=50.49\n",
      "[2024-01-23 16:28:45,274] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=50.49\n",
      "[2024-01-23 16:28:46,905] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=52.11\n",
      "[2024-01-23 16:28:48,447] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=53.65\n",
      "[2024-01-23 16:28:48,510] p1345 {3496713318.py:39} INFO - completed processing chunk 6/15 with concurrency=4\n",
      "[2024-01-23 16:28:48,511] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=7/15\n",
      "[2024-01-23 16:28:48,512] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:28:48,533] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3098\n",
      "[2024-01-23 16:28:48,534] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3171\n",
      "[2024-01-23 16:28:48,540] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3655\n",
      "[2024-01-23 16:28:48,547] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3662\n",
      "[2024-01-23 16:29:37,044] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=48.51\n",
      "[2024-01-23 16:29:37,044] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=48.50\n",
      "[2024-01-23 16:29:38,685] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=50.14\n",
      "[2024-01-23 16:29:40,137] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=51.60\n",
      "[2024-01-23 16:29:40,197] p1345 {3496713318.py:39} INFO - completed processing chunk 7/15 with concurrency=4\n",
      "[2024-01-23 16:29:40,198] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=8/15\n",
      "[2024-01-23 16:29:40,199] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:29:40,220] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3098\n",
      "[2024-01-23 16:29:40,225] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3400\n",
      "[2024-01-23 16:29:40,230] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3909\n",
      "[2024-01-23 16:29:40,221] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3704\n",
      "[2024-01-23 16:30:30,204] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=49.98\n",
      "[2024-01-23 16:30:30,204] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=49.98\n",
      "[2024-01-23 16:30:31,885] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=51.65\n",
      "[2024-01-23 16:30:33,434] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=53.20\n",
      "[2024-01-23 16:30:33,484] p1345 {3496713318.py:39} INFO - completed processing chunk 8/15 with concurrency=4\n",
      "[2024-01-23 16:30:33,484] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=9/15\n",
      "[2024-01-23 16:30:33,485] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:30:33,506] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3132\n",
      "[2024-01-23 16:30:33,513] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3135\n",
      "[2024-01-23 16:30:33,518] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3030\n",
      "[2024-01-23 16:30:33,521] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3971\n",
      "[2024-01-23 16:31:18,840] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=38, latency=45.31\n",
      "[2024-01-23 16:31:20,970] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=47.46\n",
      "[2024-01-23 16:31:20,971] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=47.45\n",
      "[2024-01-23 16:31:22,418] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=48.90\n",
      "[2024-01-23 16:31:22,467] p1345 {3496713318.py:39} INFO - completed processing chunk 9/15 with concurrency=4\n",
      "[2024-01-23 16:31:22,467] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=10/15\n",
      "[2024-01-23 16:31:22,468] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:31:22,496] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3208\n",
      "[2024-01-23 16:31:22,499] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3475\n",
      "[2024-01-23 16:31:22,500] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3848\n",
      "[2024-01-23 16:31:22,500] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3681\n",
      "[2024-01-23 16:31:42,647] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=20.14\n",
      "[2024-01-23 16:32:06,643] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=44.14\n",
      "[2024-01-23 16:32:12,604] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=50.10\n",
      "[2024-01-23 16:32:12,850] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=50.34\n",
      "[2024-01-23 16:32:12,897] p1345 {3496713318.py:39} INFO - completed processing chunk 10/15 with concurrency=4\n",
      "[2024-01-23 16:32:12,898] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=11/15\n",
      "[2024-01-23 16:32:12,898] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:32:12,917] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3581\n",
      "[2024-01-23 16:32:12,926] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3302\n",
      "[2024-01-23 16:32:12,928] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3614\n",
      "[2024-01-23 16:32:12,933] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3537\n",
      "[2024-01-23 16:32:32,893] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=6, latency=19.97\n",
      "[2024-01-23 16:32:56,447] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=35, latency=43.52\n",
      "[2024-01-23 16:33:02,429] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=49.50\n",
      "[2024-01-23 16:33:03,955] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=51.02\n",
      "[2024-01-23 16:33:04,001] p1345 {3496713318.py:39} INFO - completed processing chunk 11/15 with concurrency=4\n",
      "[2024-01-23 16:33:04,002] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=12/15\n",
      "[2024-01-23 16:33:04,002] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:33:04,027] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3476\n",
      "[2024-01-23 16:33:04,029] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3235\n",
      "[2024-01-23 16:33:04,031] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3165\n",
      "[2024-01-23 16:33:04,032] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3363\n",
      "[2024-01-23 16:33:22,927] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=18.89\n",
      "[2024-01-23 16:33:33,070] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=29.03\n",
      "[2024-01-23 16:33:50,929] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=46.89\n",
      "[2024-01-23 16:33:51,501] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=47.47\n",
      "[2024-01-23 16:33:51,549] p1345 {3496713318.py:39} INFO - completed processing chunk 12/15 with concurrency=4\n",
      "[2024-01-23 16:33:51,550] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=13/15\n",
      "[2024-01-23 16:33:51,550] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:33:51,568] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3882\n",
      "[2024-01-23 16:33:51,577] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3567\n",
      "[2024-01-23 16:33:51,584] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3330\n",
      "[2024-01-23 16:33:51,585] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3616\n",
      "[2024-01-23 16:34:14,383] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=18, latency=22.81\n",
      "[2024-01-23 16:34:25,446] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=8, latency=33.86\n",
      "[2024-01-23 16:34:35,644] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=44.06\n",
      "[2024-01-23 16:34:41,243] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=49.66\n",
      "[2024-01-23 16:34:41,292] p1345 {3496713318.py:39} INFO - completed processing chunk 13/15 with concurrency=4\n",
      "[2024-01-23 16:34:41,293] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=14/15\n",
      "[2024-01-23 16:34:41,293] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:34:41,310] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3566\n",
      "[2024-01-23 16:34:41,317] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3783\n",
      "[2024-01-23 16:34:41,325] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3003\n",
      "[2024-01-23 16:34:41,333] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3497\n",
      "[2024-01-23 16:35:02,090] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=20.76\n",
      "[2024-01-23 16:35:13,304] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=15, latency=31.97\n",
      "[2024-01-23 16:35:23,429] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=42.11\n",
      "[2024-01-23 16:35:29,440] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=48.11\n",
      "[2024-01-23 16:35:29,487] p1345 {3496713318.py:39} INFO - completed processing chunk 14/15 with concurrency=4\n",
      "[2024-01-23 16:35:29,488] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=15/15\n",
      "[2024-01-23 16:35:29,489] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 16:35:29,507] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 16:35:29,517] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 16:35:29,520] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 16:35:29,520] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 16:35:49,944] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=10, latency=20.42\n",
      "[2024-01-23 16:36:00,804] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=15, latency=31.28\n",
      "[2024-01-23 16:36:16,953] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=47.44\n",
      "[2024-01-23 16:36:18,588] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=49.08\n",
      "[2024-01-23 16:36:18,636] p1345 {3496713318.py:39} INFO - completed processing chunk 15/15 with concurrency=4\n",
      "[2024-01-23 16:36:18,637] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/1\n",
      "[2024-01-23 16:36:18,638] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:36:18,645] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:36:18,649] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:36:18,649] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:36:18,653] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:36:18,654] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:36:18,656] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:36:25,576] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=28, latency=6.91\n",
      "[2024-01-23 16:36:25,644] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=29, latency=6.98\n",
      "[2024-01-23 16:36:30,143] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.49\n",
      "[2024-01-23 16:36:30,144] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.48\n",
      "[2024-01-23 16:36:30,144] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.48\n",
      "[2024-01-23 16:36:30,145] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.49\n",
      "[2024-01-23 16:36:30,213] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=6\n",
      "[2024-01-23 16:36:30,214] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/1\n",
      "[2024-01-23 16:36:30,214] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:36:30,219] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:36:30,225] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:36:30,229] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:36:30,230] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:36:30,234] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:36:30,235] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:36:44,761] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=14.52\n",
      "[2024-01-23 16:36:44,965] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=14.73\n",
      "[2024-01-23 16:36:53,913] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.68\n",
      "[2024-01-23 16:36:53,914] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.67\n",
      "[2024-01-23 16:36:53,914] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.68\n",
      "[2024-01-23 16:36:55,020] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=24.77\n",
      "[2024-01-23 16:36:55,085] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=6\n",
      "[2024-01-23 16:36:55,085] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/3\n",
      "[2024-01-23 16:36:55,086] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:36:55,101] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1646\n",
      "[2024-01-23 16:36:55,105] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1154\n",
      "[2024-01-23 16:36:55,105] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1932\n",
      "[2024-01-23 16:36:55,107] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1397\n",
      "[2024-01-23 16:36:55,108] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1746\n",
      "[2024-01-23 16:36:55,108] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1339\n",
      "[2024-01-23 16:37:25,578] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=30.47\n",
      "[2024-01-23 16:37:29,074] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=67, latency=33.96\n",
      "[2024-01-23 16:37:30,211] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.10\n",
      "[2024-01-23 16:37:30,211] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.09\n",
      "[2024-01-23 16:37:30,212] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.10\n",
      "[2024-01-23 16:37:31,382] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=36.26\n",
      "[2024-01-23 16:37:31,442] p1345 {3496713318.py:39} INFO - completed processing chunk 1/3 with concurrency=6\n",
      "[2024-01-23 16:37:31,443] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/3\n",
      "[2024-01-23 16:37:31,443] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:37:31,452] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1373\n",
      "[2024-01-23 16:37:31,464] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1539\n",
      "[2024-01-23 16:37:31,464] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1695\n",
      "[2024-01-23 16:37:31,467] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1421\n",
      "[2024-01-23 16:37:31,468] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1598\n",
      "[2024-01-23 16:37:31,468] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1743\n",
      "[2024-01-23 16:37:55,252] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=23.78\n",
      "[2024-01-23 16:37:55,400] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=6, latency=23.93\n",
      "[2024-01-23 16:38:06,544] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.07\n",
      "[2024-01-23 16:38:06,545] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.08\n",
      "[2024-01-23 16:38:06,546] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.08\n",
      "[2024-01-23 16:38:08,994] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=37.52\n",
      "[2024-01-23 16:38:09,054] p1345 {3496713318.py:39} INFO - completed processing chunk 2/3 with concurrency=6\n",
      "[2024-01-23 16:38:09,054] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/3\n",
      "[2024-01-23 16:38:09,055] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:38:09,074] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1918\n",
      "[2024-01-23 16:38:09,074] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1939\n",
      "[2024-01-23 16:38:09,075] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1910\n",
      "[2024-01-23 16:38:09,080] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1918\n",
      "[2024-01-23 16:38:09,081] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1918\n",
      "[2024-01-23 16:38:09,077] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1918\n",
      "[2024-01-23 16:38:50,248] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=41.16\n",
      "[2024-01-23 16:38:50,249] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=41.16\n",
      "[2024-01-23 16:38:50,250] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=41.16\n",
      "[2024-01-23 16:38:51,655] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=42.57\n",
      "[2024-01-23 16:38:51,656] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=42.56\n",
      "[2024-01-23 16:38:52,921] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=43.83\n",
      "[2024-01-23 16:38:52,993] p1345 {3496713318.py:39} INFO - completed processing chunk 3/3 with concurrency=6\n",
      "[2024-01-23 16:38:52,994] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/6\n",
      "[2024-01-23 16:38:52,994] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:38:53,015] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3000\n",
      "[2024-01-23 16:38:53,016] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2148\n",
      "[2024-01-23 16:38:53,020] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2150\n",
      "[2024-01-23 16:38:53,021] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2404\n",
      "[2024-01-23 16:38:53,024] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2715\n",
      "[2024-01-23 16:38:53,025] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2637\n",
      "[2024-01-23 16:39:26,042] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=15, latency=33.01\n",
      "[2024-01-23 16:39:44,663] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=51.64\n",
      "[2024-01-23 16:39:44,663] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=51.64\n",
      "[2024-01-23 16:39:46,177] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=53.15\n",
      "[2024-01-23 16:39:49,141] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=56.10\n",
      "[2024-01-23 16:39:50,479] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=57.45\n",
      "[2024-01-23 16:39:50,547] p1345 {3496713318.py:39} INFO - completed processing chunk 1/6 with concurrency=6\n",
      "[2024-01-23 16:39:50,547] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/6\n",
      "[2024-01-23 16:39:50,548] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:39:50,566] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2675\n",
      "[2024-01-23 16:39:50,571] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2803\n",
      "[2024-01-23 16:39:50,576] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2775\n",
      "[2024-01-23 16:39:50,578] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2186\n",
      "[2024-01-23 16:39:50,579] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2541\n",
      "[2024-01-23 16:39:50,583] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2369\n",
      "[2024-01-23 16:40:24,354] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=17, latency=33.76\n",
      "[2024-01-23 16:40:43,438] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=52.86\n",
      "[2024-01-23 16:40:43,440] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=52.86\n",
      "[2024-01-23 16:40:44,968] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=54.38\n",
      "[2024-01-23 16:40:47,941] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=57.34\n",
      "[2024-01-23 16:40:49,339] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=58.75\n",
      "[2024-01-23 16:40:49,403] p1345 {3496713318.py:39} INFO - completed processing chunk 2/6 with concurrency=6\n",
      "[2024-01-23 16:40:49,403] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/6\n",
      "[2024-01-23 16:40:49,404] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:40:49,426] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2321\n",
      "[2024-01-23 16:40:49,431] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2443\n",
      "[2024-01-23 16:40:49,437] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2686\n",
      "[2024-01-23 16:40:49,431] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2428\n",
      "[2024-01-23 16:40:49,442] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2458\n",
      "[2024-01-23 16:40:49,442] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2500\n",
      "[2024-01-23 16:41:04,745] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=6, latency=15.31\n",
      "[2024-01-23 16:41:21,333] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=35, latency=31.89\n",
      "[2024-01-23 16:41:40,670] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=51.22\n",
      "[2024-01-23 16:41:42,174] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=52.72\n",
      "[2024-01-23 16:41:43,618] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=54.18\n",
      "[2024-01-23 16:41:44,965] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=55.52\n",
      "[2024-01-23 16:41:45,030] p1345 {3496713318.py:39} INFO - completed processing chunk 3/6 with concurrency=6\n",
      "[2024-01-23 16:41:45,031] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/6\n",
      "[2024-01-23 16:41:45,031] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:41:45,045] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2101\n",
      "[2024-01-23 16:41:45,059] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2771\n",
      "[2024-01-23 16:41:45,059] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2711\n",
      "[2024-01-23 16:41:45,062] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2624\n",
      "[2024-01-23 16:41:45,066] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2691\n",
      "[2024-01-23 16:41:45,067] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2440\n",
      "[2024-01-23 16:42:08,950] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=23.88\n",
      "[2024-01-23 16:42:18,181] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=11, latency=33.10\n",
      "[2024-01-23 16:42:35,373] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=1, latency=50.29\n",
      "[2024-01-23 16:42:37,238] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=52.17\n",
      "[2024-01-23 16:42:37,238] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=52.17\n",
      "[2024-01-23 16:42:41,267] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=56.19\n",
      "[2024-01-23 16:42:41,339] p1345 {3496713318.py:39} INFO - completed processing chunk 4/6 with concurrency=6\n",
      "[2024-01-23 16:42:41,339] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/6\n",
      "[2024-01-23 16:42:41,340] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:42:41,355] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2608\n",
      "[2024-01-23 16:42:41,522] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2770\n",
      "[2024-01-23 16:42:41,522] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2062\n",
      "[2024-01-23 16:42:41,535] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2564\n",
      "[2024-01-23 16:42:41,590] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2333\n",
      "[2024-01-23 16:42:41,596] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2213\n",
      "[2024-01-23 16:43:13,870] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=12, latency=32.27\n",
      "[2024-01-23 16:43:21,235] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=39.63\n",
      "[2024-01-23 16:43:30,901] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=49.37\n",
      "[2024-01-23 16:43:30,901] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=49.31\n",
      "[2024-01-23 16:43:30,971] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=26, latency=49.37\n",
      "[2024-01-23 16:43:32,283] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=50.76\n",
      "[2024-01-23 16:43:32,349] p1345 {3496713318.py:39} INFO - completed processing chunk 5/6 with concurrency=6\n",
      "[2024-01-23 16:43:32,350] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/6\n",
      "[2024-01-23 16:43:32,350] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:43:32,363] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2358\n",
      "[2024-01-23 16:43:32,364] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2613\n",
      "[2024-01-23 16:43:32,385] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2613\n",
      "[2024-01-23 16:43:32,389] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2613\n",
      "[2024-01-23 16:43:32,390] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2613\n",
      "[2024-01-23 16:43:32,390] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2613\n",
      "[2024-01-23 16:43:47,229] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=8, latency=14.85\n",
      "[2024-01-23 16:43:55,259] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=10, latency=22.87\n",
      "[2024-01-23 16:44:12,281] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=12, latency=39.89\n",
      "[2024-01-23 16:44:22,599] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=81, latency=50.22\n",
      "[2024-01-23 16:44:25,240] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=52.85\n",
      "[2024-01-23 16:44:27,990] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=55.60\n",
      "[2024-01-23 16:44:28,071] p1345 {3496713318.py:39} INFO - completed processing chunk 6/6 with concurrency=6\n",
      "[2024-01-23 16:44:28,072] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/10\n",
      "[2024-01-23 16:44:28,072] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:44:28,103] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3789\n",
      "[2024-01-23 16:44:28,104] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3896\n",
      "[2024-01-23 16:44:28,110] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3450\n",
      "[2024-01-23 16:44:28,117] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3482\n",
      "[2024-01-23 16:44:28,113] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3144\n",
      "[2024-01-23 16:44:28,117] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3000\n",
      "[2024-01-23 16:44:51,363] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=23.26\n",
      "[2024-01-23 16:45:28,331] p1345 {3496713318.py:39} INFO - completed processing chunk 1/10 with concurrency=6\n",
      "[2024-01-23 16:45:28,332] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/10\n",
      "[2024-01-23 16:45:28,333] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:45:28,356] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3891\n",
      "[2024-01-23 16:45:28,358] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3014\n",
      "[2024-01-23 16:45:28,370] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3639\n",
      "[2024-01-23 16:45:28,374] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3419\n",
      "[2024-01-23 16:45:28,376] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3458\n",
      "[2024-01-23 16:45:28,384] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:45:56,498] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=28.14\n",
      "[2024-01-23 16:46:28,567] p1345 {3496713318.py:39} INFO - completed processing chunk 2/10 with concurrency=6\n",
      "[2024-01-23 16:46:28,567] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/10\n",
      "[2024-01-23 16:46:28,568] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:46:28,583] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3508\n",
      "[2024-01-23 16:46:28,605] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3680\n",
      "[2024-01-23 16:46:28,603] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3087\n",
      "[2024-01-23 16:46:28,607] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3249\n",
      "[2024-01-23 16:46:28,584] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3436\n",
      "[2024-01-23 16:46:28,612] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:47:01,544] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=27, latency=32.94\n",
      "[2024-01-23 16:47:28,837] p1345 {3496713318.py:39} INFO - completed processing chunk 3/10 with concurrency=6\n",
      "[2024-01-23 16:47:28,837] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/10\n",
      "[2024-01-23 16:47:28,838] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:47:28,906] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3463\n",
      "[2024-01-23 16:47:28,907] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3050\n",
      "[2024-01-23 16:47:28,911] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3410\n",
      "[2024-01-23 16:47:28,912] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3979\n",
      "[2024-01-23 16:47:28,928] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3737\n",
      "[2024-01-23 16:47:28,935] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3996\n",
      "[2024-01-23 16:48:29,160] p1345 {3496713318.py:39} INFO - completed processing chunk 4/10 with concurrency=6\n",
      "[2024-01-23 16:48:29,160] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/10\n",
      "[2024-01-23 16:48:29,161] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:48:29,186] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3662\n",
      "[2024-01-23 16:48:29,187] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3655\n",
      "[2024-01-23 16:48:29,200] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3171\n",
      "[2024-01-23 16:48:29,201] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3098\n",
      "[2024-01-23 16:48:29,202] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3400\n",
      "[2024-01-23 16:48:29,203] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3704\n",
      "[2024-01-23 16:49:29,366] p1345 {3496713318.py:39} INFO - completed processing chunk 5/10 with concurrency=6\n",
      "[2024-01-23 16:49:29,367] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/10\n",
      "[2024-01-23 16:49:29,367] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:49:29,393] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3098\n",
      "[2024-01-23 16:49:29,406] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3132\n",
      "[2024-01-23 16:49:29,408] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3030\n",
      "[2024-01-23 16:49:29,416] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3971\n",
      "[2024-01-23 16:49:29,416] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3135\n",
      "[2024-01-23 16:49:29,416] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:50:01,602] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=11, latency=32.18\n",
      "[2024-01-23 16:50:29,647] p1345 {3496713318.py:39} INFO - completed processing chunk 6/10 with concurrency=6\n",
      "[2024-01-23 16:50:29,647] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=7/10\n",
      "[2024-01-23 16:50:29,648] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:50:29,664] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:50:29,674] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3208\n",
      "[2024-01-23 16:50:29,678] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3475\n",
      "[2024-01-23 16:50:29,694] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3681\n",
      "[2024-01-23 16:50:29,698] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3302\n",
      "[2024-01-23 16:50:29,702] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3581\n",
      "[2024-01-23 16:51:29,871] p1345 {3496713318.py:39} INFO - completed processing chunk 7/10 with concurrency=6\n",
      "[2024-01-23 16:51:29,872] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=8/10\n",
      "[2024-01-23 16:51:29,873] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:51:29,908] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3614\n",
      "[2024-01-23 16:51:29,909] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3165\n",
      "[2024-01-23 16:51:29,912] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3235\n",
      "[2024-01-23 16:51:29,914] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3537\n",
      "[2024-01-23 16:51:29,914] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3476\n",
      "[2024-01-23 16:51:29,914] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:51:56,137] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=26.21\n",
      "[2024-01-23 16:52:17,311] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=47.39\n",
      "[2024-01-23 16:52:28,502] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=8, latency=58.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:52:30,546] p1345 {3496713318.py:39} INFO - completed processing chunk 8/10 with concurrency=6\n",
      "[2024-01-23 16:52:30,547] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=9/10\n",
      "[2024-01-23 16:52:30,547] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:52:30,582] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3882\n",
      "[2024-01-23 16:52:30,587] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3567\n",
      "[2024-01-23 16:52:30,592] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3566\n",
      "[2024-01-23 16:52:30,592] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3330\n",
      "[2024-01-23 16:52:30,594] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3497\n",
      "[2024-01-23 16:52:30,595] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:53:00,865] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=30.28\n",
      "[2024-01-23 16:53:22,376] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=51.78\n",
      "[2024-01-23 16:53:30,751] p1345 {3496713318.py:39} INFO - completed processing chunk 9/10 with concurrency=6\n",
      "[2024-01-23 16:53:30,752] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=10/10\n",
      "[2024-01-23 16:53:30,752] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 16:53:30,776] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3003\n",
      "[2024-01-23 16:53:30,780] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3003\n",
      "[2024-01-23 16:53:30,793] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3003\n",
      "[2024-01-23 16:53:30,804] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3783\n",
      "[2024-01-23 16:53:30,800] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3003\n",
      "[2024-01-23 16:53:30,807] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:54:06,141] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=35.33\n",
      "[2024-01-23 16:54:16,456] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=45.68\n",
      "[2024-01-23 16:54:31,032] p1345 {3496713318.py:39} INFO - completed processing chunk 10/10 with concurrency=6\n",
      "[2024-01-23 16:54:31,032] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/1\n",
      "[2024-01-23 16:54:31,033] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 16:54:31,038] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:54:31,047] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:54:31,050] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:54:31,051] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:54:31,051] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:54:31,052] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:54:42,808] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=11.77\n",
      "[2024-01-23 16:54:42,812] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:54:49,880] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.82\n",
      "[2024-01-23 16:54:49,882] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.82\n",
      "[2024-01-23 16:54:49,882] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.83\n",
      "[2024-01-23 16:54:49,882] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.83\n",
      "[2024-01-23 16:54:49,883] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=18.83\n",
      "[2024-01-23 16:54:49,887] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=304\n",
      "[2024-01-23 16:54:51,806] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=8.99\n",
      "[2024-01-23 16:54:56,096] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=6.20\n",
      "[2024-01-23 16:54:56,172] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=8\n",
      "[2024-01-23 16:54:56,173] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/1\n",
      "[2024-01-23 16:54:56,173] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 16:54:56,181] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:54:56,189] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:54:56,192] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:54:56,193] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:54:56,193] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:54:56,190] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:55:20,112] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.91\n",
      "[2024-01-23 16:55:20,112] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.91\n",
      "[2024-01-23 16:55:20,118] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.92\n",
      "[2024-01-23 16:55:20,124] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:55:20,119] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.92\n",
      "[2024-01-23 16:55:20,119] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=23.92\n",
      "[2024-01-23 16:55:20,119] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=980\n",
      "[2024-01-23 16:55:23,905] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=27.70\n",
      "[2024-01-23 16:55:26,957] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=27, latency=6.83\n",
      "[2024-01-23 16:55:32,312] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=12.18\n",
      "[2024-01-23 16:55:32,392] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=8\n",
      "[2024-01-23 16:55:32,393] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/2\n",
      "[2024-01-23 16:55:32,393] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 16:55:32,404] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1339\n",
      "[2024-01-23 16:55:32,404] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1154\n",
      "[2024-01-23 16:55:32,413] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1646\n",
      "[2024-01-23 16:55:32,413] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1397\n",
      "[2024-01-23 16:55:32,414] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1746\n",
      "[2024-01-23 16:55:32,414] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1932\n",
      "[2024-01-23 16:55:57,361] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=24.94\n",
      "[2024-01-23 16:55:57,366] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1373\n",
      "[2024-01-23 16:56:11,494] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=39.08\n",
      "[2024-01-23 16:56:11,495] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=39.07\n",
      "[2024-01-23 16:56:11,496] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=39.08\n",
      "[2024-01-23 16:56:11,502] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1598\n",
      "[2024-01-23 16:56:17,429] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=45.00\n",
      "[2024-01-23 16:56:18,761] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=46.34\n",
      "[2024-01-23 16:56:18,763] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=21.40\n",
      "[2024-01-23 16:56:22,699] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=11.20\n",
      "[2024-01-23 16:56:22,782] p1345 {3496713318.py:39} INFO - completed processing chunk 1/2 with concurrency=8\n",
      "[2024-01-23 16:56:22,782] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/2\n",
      "[2024-01-23 16:56:22,783] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 16:56:22,794] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1539\n",
      "[2024-01-23 16:56:22,801] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1695\n",
      "[2024-01-23 16:56:22,801] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1743\n",
      "[2024-01-23 16:56:22,804] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1421\n",
      "[2024-01-23 16:56:22,807] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1918\n",
      "[2024-01-23 16:56:22,809] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1910\n",
      "[2024-01-23 16:56:37,383] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=15, latency=14.57\n",
      "[2024-01-23 16:56:37,390] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1939\n",
      "[2024-01-23 16:56:48,766] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=10, latency=25.95\n",
      "[2024-01-23 16:56:48,772] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=1743\n",
      "[2024-01-23 16:57:01,067] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=23.68\n",
      "[2024-01-23 16:57:11,738] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=48.93\n",
      "[2024-01-23 16:57:11,738] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=48.92\n",
      "[2024-01-23 16:57:12,693] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=49.88\n",
      "[2024-01-23 16:57:14,062] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=51.24\n",
      "[2024-01-23 16:57:15,298] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=26.53\n",
      "[2024-01-23 16:57:15,378] p1345 {3496713318.py:39} INFO - completed processing chunk 2/2 with concurrency=8\n",
      "[2024-01-23 16:57:15,378] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/4\n",
      "[2024-01-23 16:57:15,379] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 16:57:15,400] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2637\n",
      "[2024-01-23 16:57:15,406] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2150\n",
      "[2024-01-23 16:57:15,406] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2148\n",
      "[2024-01-23 16:57:15,406] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3000\n",
      "[2024-01-23 16:57:15,402] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2404\n",
      "[2024-01-23 16:57:15,408] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2715\n",
      "[2024-01-23 16:57:46,167] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=30.75\n",
      "[2024-01-23 16:57:46,175] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2803\n",
      "[2024-01-23 16:57:57,579] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=48, latency=42.16\n",
      "[2024-01-23 16:57:57,586] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:58:23,071] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=10, latency=25.48\n",
      "[2024-01-23 16:58:29,689] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=43.51\n",
      "[2024-01-23 16:58:29,778] p1345 {3496713318.py:39} INFO - completed processing chunk 1/4 with concurrency=8\n",
      "[2024-01-23 16:58:29,779] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/4\n",
      "[2024-01-23 16:58:29,780] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 16:58:29,808] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2541\n",
      "[2024-01-23 16:58:29,812] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2186\n",
      "[2024-01-23 16:58:29,812] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2775\n",
      "[2024-01-23 16:58:29,812] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2686\n",
      "[2024-01-23 16:58:29,812] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2675\n",
      "[2024-01-23 16:58:29,813] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2500\n",
      "[2024-01-23 16:59:02,141] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=11, latency=32.31\n",
      "[2024-01-23 16:59:02,149] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2443\n",
      "[2024-01-23 16:59:29,092] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=59.27\n",
      "[2024-01-23 16:59:29,092] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=59.28\n",
      "[2024-01-23 16:59:29,100] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 16:59:43,114] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=40.96\n",
      "[2024-01-23 16:59:44,162] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=15.06\n",
      "[2024-01-23 16:59:44,251] p1345 {3496713318.py:39} INFO - completed processing chunk 2/4 with concurrency=8\n",
      "[2024-01-23 16:59:44,251] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/4\n",
      "[2024-01-23 16:59:44,252] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 16:59:44,269] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2458\n",
      "[2024-01-23 16:59:44,278] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2428\n",
      "[2024-01-23 16:59:44,284] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2440\n",
      "[2024-01-23 16:59:44,288] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2101\n",
      "[2024-01-23 16:59:44,288] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2711\n",
      "[2024-01-23 16:59:44,288] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2771\n",
      "[2024-01-23 16:59:58,193] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=13.91\n",
      "[2024-01-23 16:59:58,201] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2691\n",
      "[2024-01-23 17:00:05,667] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=21.39\n",
      "[2024-01-23 17:00:05,675] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2624\n",
      "[2024-01-23 17:00:39,761] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=41.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:00:49,790] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=19, latency=44.11\n",
      "[2024-01-23 17:00:49,884] p1345 {3496713318.py:39} INFO - completed processing chunk 3/4 with concurrency=8\n",
      "[2024-01-23 17:00:49,885] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/4\n",
      "[2024-01-23 17:00:49,886] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:00:49,902] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2213\n",
      "[2024-01-23 17:00:49,912] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2770\n",
      "[2024-01-23 17:00:49,913] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2062\n",
      "[2024-01-23 17:00:49,914] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2564\n",
      "[2024-01-23 17:00:49,914] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2608\n",
      "[2024-01-23 17:00:49,921] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2333\n",
      "[2024-01-23 17:01:04,581] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=14.66\n",
      "[2024-01-23 17:01:04,589] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2613\n",
      "[2024-01-23 17:01:21,026] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=16, latency=31.10\n",
      "[2024-01-23 17:01:21,033] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=2358\n",
      "[2024-01-23 17:01:28,938] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=39.01\n",
      "[2024-01-23 17:01:46,482] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=4, latency=41.89\n",
      "[2024-01-23 17:01:47,780] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=57.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:02:01,381] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=40.35\n",
      "[2024-01-23 17:02:01,478] p1345 {3496713318.py:39} INFO - completed processing chunk 4/4 with concurrency=8\n",
      "[2024-01-23 17:02:01,479] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=1/8\n",
      "[2024-01-23 17:02:01,479] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:02:01,498] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3000\n",
      "[2024-01-23 17:02:01,506] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3789\n",
      "[2024-01-23 17:02:01,525] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3144\n",
      "[2024-01-23 17:02:01,521] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3450\n",
      "[2024-01-23 17:02:01,527] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3896\n",
      "[2024-01-23 17:02:01,536] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3482\n",
      "[2024-01-23 17:02:57,662] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=6, latency=56.12\n",
      "[2024-01-23 17:02:57,672] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3639\n",
      "[2024-01-23 17:03:01,582] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:03:38,759] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=37.18\n",
      "[2024-01-23 17:03:38,760] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=41.09\n",
      "[2024-01-23 17:03:38,848] p1345 {3496713318.py:39} INFO - completed processing chunk 1/8 with concurrency=8\n",
      "[2024-01-23 17:03:38,849] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=2/8\n",
      "[2024-01-23 17:03:38,850] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:03:38,882] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3458\n",
      "[2024-01-23 17:03:38,884] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3891\n",
      "[2024-01-23 17:03:38,888] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3508\n",
      "[2024-01-23 17:03:38,889] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3436\n",
      "[2024-01-23 17:03:38,889] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3419\n",
      "[2024-01-23 17:03:38,892] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3575\n",
      "[2024-01-23 17:04:38,982] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3087\n",
      "[2024-01-23 17:04:38,985] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:05:14,615] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.63\n",
      "[2024-01-23 17:05:14,617] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=101, latency=35.63\n",
      "[2024-01-23 17:05:14,723] p1345 {3496713318.py:39} INFO - completed processing chunk 2/8 with concurrency=8\n",
      "[2024-01-23 17:05:14,724] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=3/8\n",
      "[2024-01-23 17:05:14,724] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:05:14,759] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3680\n",
      "[2024-01-23 17:05:14,760] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3463\n",
      "[2024-01-23 17:05:14,771] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3950\n",
      "[2024-01-23 17:05:14,773] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3737\n",
      "[2024-01-23 17:05:14,774] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3050\n",
      "[2024-01-23 17:05:14,781] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3996\n",
      "[2024-01-23 17:06:14,881] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3410\n",
      "[2024-01-23 17:06:14,888] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:06:57,034] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=99, latency=42.14\n",
      "[2024-01-23 17:06:57,034] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=42.15\n",
      "[2024-01-23 17:06:57,144] p1345 {3496713318.py:39} INFO - completed processing chunk 3/8 with concurrency=8\n",
      "[2024-01-23 17:06:57,145] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=4/8\n",
      "[2024-01-23 17:06:57,145] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:06:57,177] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3171\n",
      "[2024-01-23 17:06:57,181] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3655\n",
      "[2024-01-23 17:06:57,185] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3098\n",
      "[2024-01-23 17:06:57,186] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3662\n",
      "[2024-01-23 17:06:57,192] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3400\n",
      "[2024-01-23 17:06:57,191] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3704\n",
      "[2024-01-23 17:07:43,329] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=9, latency=46.13\n",
      "[2024-01-23 17:07:43,338] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3098\n",
      "[2024-01-23 17:07:57,286] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:08:25,761] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=11, latency=42.42\n",
      "[2024-01-23 17:08:33,206] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=35.92\n",
      "[2024-01-23 17:08:33,301] p1345 {3496713318.py:39} INFO - completed processing chunk 4/8 with concurrency=8\n",
      "[2024-01-23 17:08:33,302] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=5/8\n",
      "[2024-01-23 17:08:33,302] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:08:33,326] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3132\n",
      "[2024-01-23 17:08:33,332] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3135\n",
      "[2024-01-23 17:08:33,343] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3030\n",
      "[2024-01-23 17:08:33,349] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3848\n",
      "[2024-01-23 17:08:33,358] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3971\n",
      "[2024-01-23 17:08:33,365] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3208\n",
      "[2024-01-23 17:09:33,374] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3475\n",
      "[2024-01-23 17:09:33,485] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:10:09,600] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=36.11\n",
      "[2024-01-23 17:10:09,600] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=36.22\n",
      "[2024-01-23 17:10:09,712] p1345 {3496713318.py:39} INFO - completed processing chunk 5/8 with concurrency=8\n",
      "[2024-01-23 17:10:09,713] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=6/8\n",
      "[2024-01-23 17:10:09,714] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:10:09,736] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3581\n",
      "[2024-01-23 17:10:09,737] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3302\n",
      "[2024-01-23 17:10:09,750] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3614\n",
      "[2024-01-23 17:10:09,758] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3537\n",
      "[2024-01-23 17:10:09,759] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3476\n",
      "[2024-01-23 17:10:09,759] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3165\n",
      "[2024-01-23 17:10:29,988] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=8, latency=20.25\n",
      "[2024-01-23 17:10:29,998] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3235\n",
      "[2024-01-23 17:10:41,070] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=15, latency=31.31\n",
      "[2024-01-23 17:10:41,080] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3363\n",
      "[2024-01-23 17:11:01,822] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=3, latency=52.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:11:40,833] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=59.75\n",
      "[2024-01-23 17:11:40,921] p1345 {3496713318.py:39} INFO - completed processing chunk 6/8 with concurrency=8\n",
      "[2024-01-23 17:11:40,922] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=7/8\n",
      "[2024-01-23 17:11:40,923] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:11:40,952] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3330\n",
      "[2024-01-23 17:11:40,962] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3616\n",
      "[2024-01-23 17:11:40,966] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3882\n",
      "[2024-01-23 17:11:40,976] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3566\n",
      "[2024-01-23 17:11:40,984] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3567\n",
      "[2024-01-23 17:11:40,984] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3497\n",
      "[2024-01-23 17:12:15,104] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=21, latency=34.11\n",
      "[2024-01-23 17:12:15,113] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3003\n",
      "[2024-01-23 17:12:38,698] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=15, latency=57.73\n",
      "[2024-01-23 17:12:38,709] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:13:09,844] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=5, latency=54.73\n",
      "[2024-01-23 17:13:10,010] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=7, latency=31.30\n",
      "[2024-01-23 17:13:10,100] p1345 {3496713318.py:39} INFO - completed processing chunk 7/8 with concurrency=8\n",
      "[2024-01-23 17:13:10,101] p1345 {3496713318.py:26} INFO - e_idx=1/2, chunk_index=8/8\n",
      "[2024-01-23 17:13:10,101] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:13:10,132] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 17:13:10,140] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 17:13:10,134] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 17:13:10,141] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 17:13:10,149] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 17:13:10,150] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 17:14:05,365] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=16, latency=55.22\n",
      "[2024-01-23 17:14:05,374] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n",
      "[2024-01-23 17:14:10,177] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama-2-70b-g5-48xlarge-1706022365, prompt_tokens=3271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n",
      "error occurred with llama-2-70b-g5-48xlarge-1706022365, exception=An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/llama-2-70b-g5-48xlarge-1706022365 in account 015469603702 for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:14:36,831] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=18, latency=31.46\n",
      "[2024-01-23 17:14:43,284] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama-2-70b-g5-48xlarge-1706022365, completion_tokens=102, latency=33.11\n",
      "[2024-01-23 17:14:43,369] p1345 {3496713318.py:39} INFO - completed processing chunk 8/8 with concurrency=8\n",
      "[2024-01-23 17:14:43,370] p1345 {3496713318.py:41} INFO - experiment=1/2, name=llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0, done\n",
      "[2024-01-23 17:14:43,371] p1345 {663335596.py:13} INFO - experiment=2, name=llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122, ep_name=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint\n",
      "[2024-01-23 17:14:43,524] p1345 {1352403322.py:18} INFO - there are 25 combinations of [(1, 'payload_en_1-500.jsonl'), (1, 'payload_en_500-1000.jsonl'), (1, 'payload_en_1000-2000.jsonl'), (1, 'payload_en_2000-3000.jsonl'), (1, 'payload_en_3000-4000.jsonl'), (2, 'payload_en_1-500.jsonl'), (2, 'payload_en_500-1000.jsonl'), (2, 'payload_en_1000-2000.jsonl'), (2, 'payload_en_2000-3000.jsonl'), (2, 'payload_en_3000-4000.jsonl'), (4, 'payload_en_1-500.jsonl'), (4, 'payload_en_500-1000.jsonl'), (4, 'payload_en_1000-2000.jsonl'), (4, 'payload_en_2000-3000.jsonl'), (4, 'payload_en_3000-4000.jsonl'), (6, 'payload_en_1-500.jsonl'), (6, 'payload_en_500-1000.jsonl'), (6, 'payload_en_1000-2000.jsonl'), (6, 'payload_en_2000-3000.jsonl'), (6, 'payload_en_3000-4000.jsonl'), (8, 'payload_en_1-500.jsonl'), (8, 'payload_en_500-1000.jsonl'), (8, 'payload_en_1000-2000.jsonl'), (8, 'payload_en_2000-3000.jsonl'), (8, 'payload_en_3000-4000.jsonl')] to run\n",
      "[2024-01-23 17:14:43,528] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 17:14:43,528] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 17:14:43,529] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 17:14:43,531] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 17:14:43,531] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 17:14:43,532] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 17:14:43,535] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 17:14:43,535] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 17:14:43,536] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 15 chunks, previously we had 15 chunks\n",
      "[2024-01-23 17:14:43,540] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 17:14:43,541] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 17:14:43,541] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 32 chunks, previously we had 32 chunks\n",
      "[2024-01-23 17:14:43,548] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 17:14:43,548] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 17:14:43,548] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 57 chunks, previously we had 57 chunks\n",
      "[2024-01-23 17:14:43,550] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 17:14:43,551] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 17:14:43,551] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 17:14:43,553] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 17:14:43,553] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 17:14:43,554] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 17:14:43,556] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 17:14:43,557] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 17:14:43,557] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-23 17:14:43,561] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 17:14:43,562] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 17:14:43,562] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 16 chunks, previously we had 16 chunks\n",
      "[2024-01-23 17:14:43,569] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 17:14:43,570] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 17:14:43,570] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 29 chunks, previously we had 29 chunks\n",
      "[2024-01-23 17:14:43,572] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 17:14:43,572] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 17:14:43,573] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 17:14:43,575] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 17:14:43,575] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 17:14:43,575] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 17:14:43,578] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 17:14:43,578] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 17:14:43,579] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 4 chunks, previously we had 4 chunks\n",
      "[2024-01-23 17:14:43,583] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 17:14:43,584] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 17:14:43,584] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-23 17:14:43,591] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 17:14:43,591] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 17:14:43,592] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 15 chunks, previously we had 15 chunks\n",
      "[2024-01-23 17:14:43,594] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 17:14:43,594] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 17:14:43,595] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 17:14:43,596] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 17:14:43,597] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 17:14:43,597] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 17:14:43,600] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 17:14:43,600] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 17:14:43,601] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 3 chunks, previously we had 3 chunks\n",
      "[2024-01-23 17:14:43,605] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 17:14:43,605] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 17:14:43,606] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 6 chunks, previously we had 6 chunks\n",
      "[2024-01-23 17:14:43,613] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 17:14:43,614] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 17:14:43,614] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 10 chunks, previously we had 10 chunks\n",
      "[2024-01-23 17:14:43,616] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 17:14:43,616] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 17:14:43,617] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 17:14:43,618] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 17:14:43,619] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 17:14:43,619] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 17:14:43,622] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 17:14:43,625] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 17:14:43,626] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 2 chunks, previously we had 2 chunks\n",
      "[2024-01-23 17:14:43,629] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 17:14:43,629] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 17:14:43,630] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 4 chunks, previously we had 4 chunks\n",
      "[2024-01-23 17:14:43,635] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 17:14:43,638] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 17:14:43,638] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-23 17:14:43,640] p1345 {1352403322.py:55} INFO - there are 25 for {'name': 'llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122', 'model_id': 'meta-llama/Llama-2-70b-hf', 'model_version': '*', 'model_name': 'llama2-70bdjl', 'bucket_name': 'sagemaker-us-east-1-015469603702', 'key_name': 'hf-large-model-djl/code_llama2', 's3_path': 's3://sagemaker-us-east-1-015469603702/llama-2-70B-fp16/lmi/', 'ep_name': 'llama-2-70b-chat-p4d-24xlarge', 'instance_type': 'ml.p4d.24xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-tensorrtllm0.7.1-cu122', 'deploy': True, 'instance_count': 1, 'deployment_script': 'p4d_djl.py', 'payload_files': ['payload_en_1-500.jsonl', 'payload_en_500-1000.jsonl', 'payload_en_1000-2000.jsonl', 'payload_en_2000-3000.jsonl', 'payload_en_3000-4000.jsonl'], 'concurrency_levels': [1, 2, 4, 6, 8], 'accept_eula': True, 'remove_truncate': True, 'env': {'MODEL_LOADING_TIMEOUT': '3600', 'NUMBER_OF_GPU': '8', 'INSTANCE_COUNT': '1', 'HEALTH_CHECK_TIMOUT': '300'}}\n",
      "[2024-01-23 17:14:43,641] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/1\n",
      "[2024-01-23 17:14:43,641] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:14:43,645] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:14:44,172] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=0.53\n",
      "[2024-01-23 17:14:44,191] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=1\n",
      "[2024-01-23 17:14:44,192] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/1\n",
      "[2024-01-23 17:14:44,192] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:14:44,198] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:14:46,847] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.65\n",
      "[2024-01-23 17:14:46,863] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=1\n",
      "[2024-01-23 17:14:46,864] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/15\n",
      "[2024-01-23 17:14:46,864] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:14:46,870] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1339\n",
      "[2024-01-23 17:14:49,596] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.73\n",
      "[2024-01-23 17:14:49,618] p1345 {3496713318.py:39} INFO - completed processing chunk 1/15 with concurrency=1\n",
      "[2024-01-23 17:14:49,619] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/15\n",
      "[2024-01-23 17:14:49,620] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:14:49,627] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1932\n",
      "[2024-01-23 17:14:52,508] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.88\n",
      "[2024-01-23 17:14:52,528] p1345 {3496713318.py:39} INFO - completed processing chunk 2/15 with concurrency=1\n",
      "[2024-01-23 17:14:52,529] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/15\n",
      "[2024-01-23 17:14:52,529] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:14:52,534] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1154\n",
      "[2024-01-23 17:14:55,216] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.68\n",
      "[2024-01-23 17:14:55,236] p1345 {3496713318.py:39} INFO - completed processing chunk 3/15 with concurrency=1\n",
      "[2024-01-23 17:14:55,237] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/15\n",
      "[2024-01-23 17:14:55,237] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:14:55,244] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1646\n",
      "[2024-01-23 17:14:58,047] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.80\n",
      "[2024-01-23 17:14:58,065] p1345 {3496713318.py:39} INFO - completed processing chunk 4/15 with concurrency=1\n",
      "[2024-01-23 17:14:58,066] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/15\n",
      "[2024-01-23 17:14:58,067] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:14:58,072] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1397\n",
      "[2024-01-23 17:15:00,813] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.74\n",
      "[2024-01-23 17:15:00,831] p1345 {3496713318.py:39} INFO - completed processing chunk 5/15 with concurrency=1\n",
      "[2024-01-23 17:15:00,832] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/15\n",
      "[2024-01-23 17:15:00,832] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:00,839] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1746\n",
      "[2024-01-23 17:15:01,307] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=11, latency=0.47\n",
      "[2024-01-23 17:15:01,328] p1345 {3496713318.py:39} INFO - completed processing chunk 6/15 with concurrency=1\n",
      "[2024-01-23 17:15:01,328] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=7/15\n",
      "[2024-01-23 17:15:01,329] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:01,335] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1373\n",
      "[2024-01-23 17:15:01,577] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.24\n",
      "[2024-01-23 17:15:01,597] p1345 {3496713318.py:39} INFO - completed processing chunk 7/15 with concurrency=1\n",
      "[2024-01-23 17:15:01,598] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=8/15\n",
      "[2024-01-23 17:15:01,598] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:01,604] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1598\n",
      "[2024-01-23 17:15:04,400] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.79\n",
      "[2024-01-23 17:15:04,419] p1345 {3496713318.py:39} INFO - completed processing chunk 8/15 with concurrency=1\n",
      "[2024-01-23 17:15:04,420] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=9/15\n",
      "[2024-01-23 17:15:04,421] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:04,428] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1743\n",
      "[2024-01-23 17:15:07,252] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.82\n",
      "[2024-01-23 17:15:07,271] p1345 {3496713318.py:39} INFO - completed processing chunk 9/15 with concurrency=1\n",
      "[2024-01-23 17:15:07,272] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=10/15\n",
      "[2024-01-23 17:15:07,272] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:07,278] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1539\n",
      "[2024-01-23 17:15:10,059] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.78\n",
      "[2024-01-23 17:15:10,077] p1345 {3496713318.py:39} INFO - completed processing chunk 10/15 with concurrency=1\n",
      "[2024-01-23 17:15:10,077] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=11/15\n",
      "[2024-01-23 17:15:10,079] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:10,084] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1695\n",
      "[2024-01-23 17:15:12,900] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.81\n",
      "[2024-01-23 17:15:12,919] p1345 {3496713318.py:39} INFO - completed processing chunk 11/15 with concurrency=1\n",
      "[2024-01-23 17:15:12,920] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=12/15\n",
      "[2024-01-23 17:15:12,920] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:12,926] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1421\n",
      "[2024-01-23 17:15:15,684] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.76\n",
      "[2024-01-23 17:15:15,703] p1345 {3496713318.py:39} INFO - completed processing chunk 12/15 with concurrency=1\n",
      "[2024-01-23 17:15:15,704] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=13/15\n",
      "[2024-01-23 17:15:15,704] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:15,711] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1918\n",
      "[2024-01-23 17:15:18,568] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.86\n",
      "[2024-01-23 17:15:18,588] p1345 {3496713318.py:39} INFO - completed processing chunk 13/15 with concurrency=1\n",
      "[2024-01-23 17:15:18,588] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=14/15\n",
      "[2024-01-23 17:15:18,589] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:18,596] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1910\n",
      "[2024-01-23 17:15:19,260] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=19, latency=0.66\n",
      "[2024-01-23 17:15:19,279] p1345 {3496713318.py:39} INFO - completed processing chunk 14/15 with concurrency=1\n",
      "[2024-01-23 17:15:19,280] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=15/15\n",
      "[2024-01-23 17:15:19,280] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:19,288] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1939\n",
      "[2024-01-23 17:15:22,172] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.88\n",
      "[2024-01-23 17:15:22,195] p1345 {3496713318.py:39} INFO - completed processing chunk 15/15 with concurrency=1\n",
      "[2024-01-23 17:15:22,195] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/32\n",
      "[2024-01-23 17:15:22,196] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:22,202] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2637\n",
      "[2024-01-23 17:15:25,236] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.03\n",
      "[2024-01-23 17:15:25,255] p1345 {3496713318.py:39} INFO - completed processing chunk 1/32 with concurrency=1\n",
      "[2024-01-23 17:15:25,255] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/32\n",
      "[2024-01-23 17:15:25,256] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:25,264] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3000\n",
      "[2024-01-23 17:15:28,426] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.16\n",
      "[2024-01-23 17:15:28,447] p1345 {3496713318.py:39} INFO - completed processing chunk 2/32 with concurrency=1\n",
      "[2024-01-23 17:15:28,448] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/32\n",
      "[2024-01-23 17:15:28,448] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:28,455] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2148\n",
      "[2024-01-23 17:15:31,385] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.93\n",
      "[2024-01-23 17:15:31,407] p1345 {3496713318.py:39} INFO - completed processing chunk 3/32 with concurrency=1\n",
      "[2024-01-23 17:15:31,408] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/32\n",
      "[2024-01-23 17:15:31,408] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:31,416] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2715\n",
      "[2024-01-23 17:15:31,776] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.36\n",
      "[2024-01-23 17:15:31,799] p1345 {3496713318.py:39} INFO - completed processing chunk 4/32 with concurrency=1\n",
      "[2024-01-23 17:15:31,799] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/32\n",
      "[2024-01-23 17:15:31,800] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:31,807] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2404\n",
      "[2024-01-23 17:15:34,787] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.98\n",
      "[2024-01-23 17:15:34,806] p1345 {3496713318.py:39} INFO - completed processing chunk 5/32 with concurrency=1\n",
      "[2024-01-23 17:15:34,806] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/32\n",
      "[2024-01-23 17:15:34,807] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:34,814] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2150\n",
      "[2024-01-23 17:15:37,744] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.93\n",
      "[2024-01-23 17:15:37,762] p1345 {3496713318.py:39} INFO - completed processing chunk 6/32 with concurrency=1\n",
      "[2024-01-23 17:15:37,763] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=7/32\n",
      "[2024-01-23 17:15:37,763] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:37,772] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2803\n",
      "[2024-01-23 17:15:40,847] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=98, latency=3.07\n",
      "[2024-01-23 17:15:40,866] p1345 {3496713318.py:39} INFO - completed processing chunk 7/32 with concurrency=1\n",
      "[2024-01-23 17:15:40,867] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=8/32\n",
      "[2024-01-23 17:15:40,867] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:40,874] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2369\n",
      "[2024-01-23 17:15:43,848] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.97\n",
      "[2024-01-23 17:15:43,881] p1345 {3496713318.py:39} INFO - completed processing chunk 8/32 with concurrency=1\n",
      "[2024-01-23 17:15:43,882] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=9/32\n",
      "[2024-01-23 17:15:43,882] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:43,891] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2675\n",
      "[2024-01-23 17:15:46,931] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.04\n",
      "[2024-01-23 17:15:46,950] p1345 {3496713318.py:39} INFO - completed processing chunk 9/32 with concurrency=1\n",
      "[2024-01-23 17:15:46,951] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=10/32\n",
      "[2024-01-23 17:15:46,951] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:46,958] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2541\n",
      "[2024-01-23 17:15:49,995] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.04\n",
      "[2024-01-23 17:15:50,017] p1345 {3496713318.py:39} INFO - completed processing chunk 10/32 with concurrency=1\n",
      "[2024-01-23 17:15:50,018] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=11/32\n",
      "[2024-01-23 17:15:50,018] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:50,026] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2186\n",
      "[2024-01-23 17:15:52,960] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.93\n",
      "[2024-01-23 17:15:52,979] p1345 {3496713318.py:39} INFO - completed processing chunk 11/32 with concurrency=1\n",
      "[2024-01-23 17:15:52,979] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=12/32\n",
      "[2024-01-23 17:15:52,980] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:52,989] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2775\n",
      "[2024-01-23 17:15:56,054] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.07\n",
      "[2024-01-23 17:15:56,073] p1345 {3496713318.py:39} INFO - completed processing chunk 12/32 with concurrency=1\n",
      "[2024-01-23 17:15:56,074] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=13/32\n",
      "[2024-01-23 17:15:56,074] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:56,082] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2686\n",
      "[2024-01-23 17:15:59,123] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.04\n",
      "[2024-01-23 17:15:59,141] p1345 {3496713318.py:39} INFO - completed processing chunk 13/32 with concurrency=1\n",
      "[2024-01-23 17:15:59,142] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=14/32\n",
      "[2024-01-23 17:15:59,143] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:15:59,150] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2500\n",
      "[2024-01-23 17:16:02,153] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.00\n",
      "[2024-01-23 17:16:02,172] p1345 {3496713318.py:39} INFO - completed processing chunk 14/32 with concurrency=1\n",
      "[2024-01-23 17:16:02,173] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=15/32\n",
      "[2024-01-23 17:16:02,173] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:02,181] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2443\n",
      "[2024-01-23 17:16:05,175] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.99\n",
      "[2024-01-23 17:16:05,194] p1345 {3496713318.py:39} INFO - completed processing chunk 15/32 with concurrency=1\n",
      "[2024-01-23 17:16:05,195] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=16/32\n",
      "[2024-01-23 17:16:05,195] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:05,202] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2321\n",
      "[2024-01-23 17:16:08,170] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.97\n",
      "[2024-01-23 17:16:08,189] p1345 {3496713318.py:39} INFO - completed processing chunk 16/32 with concurrency=1\n",
      "[2024-01-23 17:16:08,190] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=17/32\n",
      "[2024-01-23 17:16:08,190] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:08,199] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2428\n",
      "[2024-01-23 17:16:11,185] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.99\n",
      "[2024-01-23 17:16:11,203] p1345 {3496713318.py:39} INFO - completed processing chunk 17/32 with concurrency=1\n",
      "[2024-01-23 17:16:11,204] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=18/32\n",
      "[2024-01-23 17:16:11,204] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:11,212] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2458\n",
      "[2024-01-23 17:16:14,207] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.99\n",
      "[2024-01-23 17:16:14,225] p1345 {3496713318.py:39} INFO - completed processing chunk 18/32 with concurrency=1\n",
      "[2024-01-23 17:16:14,225] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=19/32\n",
      "[2024-01-23 17:16:14,226] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:14,233] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2101\n",
      "[2024-01-23 17:16:17,156] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.92\n",
      "[2024-01-23 17:16:17,177] p1345 {3496713318.py:39} INFO - completed processing chunk 19/32 with concurrency=1\n",
      "[2024-01-23 17:16:17,178] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=20/32\n",
      "[2024-01-23 17:16:17,178] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:17,186] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2440\n",
      "[2024-01-23 17:16:20,196] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.01\n",
      "[2024-01-23 17:16:20,215] p1345 {3496713318.py:39} INFO - completed processing chunk 20/32 with concurrency=1\n",
      "[2024-01-23 17:16:20,215] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=21/32\n",
      "[2024-01-23 17:16:20,216] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:20,224] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2711\n",
      "[2024-01-23 17:16:23,277] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.05\n",
      "[2024-01-23 17:16:23,297] p1345 {3496713318.py:39} INFO - completed processing chunk 21/32 with concurrency=1\n",
      "[2024-01-23 17:16:23,298] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=22/32\n",
      "[2024-01-23 17:16:23,298] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:23,307] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2771\n",
      "[2024-01-23 17:16:26,372] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.06\n",
      "[2024-01-23 17:16:26,391] p1345 {3496713318.py:39} INFO - completed processing chunk 22/32 with concurrency=1\n",
      "[2024-01-23 17:16:26,391] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=23/32\n",
      "[2024-01-23 17:16:26,392] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:26,401] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2691\n",
      "[2024-01-23 17:16:29,453] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.05\n",
      "[2024-01-23 17:16:29,471] p1345 {3496713318.py:39} INFO - completed processing chunk 23/32 with concurrency=1\n",
      "[2024-01-23 17:16:29,472] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=24/32\n",
      "[2024-01-23 17:16:29,473] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:29,481] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2624\n",
      "[2024-01-23 17:16:32,513] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.03\n",
      "[2024-01-23 17:16:32,533] p1345 {3496713318.py:39} INFO - completed processing chunk 24/32 with concurrency=1\n",
      "[2024-01-23 17:16:32,534] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=25/32\n",
      "[2024-01-23 17:16:32,534] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:32,542] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2062\n",
      "[2024-01-23 17:16:35,459] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.92\n",
      "[2024-01-23 17:16:35,478] p1345 {3496713318.py:39} INFO - completed processing chunk 25/32 with concurrency=1\n",
      "[2024-01-23 17:16:35,479] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=26/32\n",
      "[2024-01-23 17:16:35,479] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:35,487] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2213\n",
      "[2024-01-23 17:16:38,429] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.94\n",
      "[2024-01-23 17:16:38,447] p1345 {3496713318.py:39} INFO - completed processing chunk 26/32 with concurrency=1\n",
      "[2024-01-23 17:16:38,448] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=27/32\n",
      "[2024-01-23 17:16:38,448] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:38,456] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2608\n",
      "[2024-01-23 17:16:38,805] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.35\n",
      "[2024-01-23 17:16:38,823] p1345 {3496713318.py:39} INFO - completed processing chunk 27/32 with concurrency=1\n",
      "[2024-01-23 17:16:38,824] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=28/32\n",
      "[2024-01-23 17:16:38,824] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:38,833] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2770\n",
      "[2024-01-23 17:16:41,898] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.06\n",
      "[2024-01-23 17:16:41,916] p1345 {3496713318.py:39} INFO - completed processing chunk 28/32 with concurrency=1\n",
      "[2024-01-23 17:16:41,917] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=29/32\n",
      "[2024-01-23 17:16:41,917] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:41,926] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2564\n",
      "[2024-01-23 17:16:44,949] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.02\n",
      "[2024-01-23 17:16:44,970] p1345 {3496713318.py:39} INFO - completed processing chunk 29/32 with concurrency=1\n",
      "[2024-01-23 17:16:44,970] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=30/32\n",
      "[2024-01-23 17:16:44,971] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:44,979] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2333\n",
      "[2024-01-23 17:16:45,358] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=0.38\n",
      "[2024-01-23 17:16:45,376] p1345 {3496713318.py:39} INFO - completed processing chunk 30/32 with concurrency=1\n",
      "[2024-01-23 17:16:45,377] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=31/32\n",
      "[2024-01-23 17:16:45,378] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:45,386] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2613\n",
      "[2024-01-23 17:16:45,734] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.35\n",
      "[2024-01-23 17:16:45,755] p1345 {3496713318.py:39} INFO - completed processing chunk 31/32 with concurrency=1\n",
      "[2024-01-23 17:16:45,755] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=32/32\n",
      "[2024-01-23 17:16:45,756] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:45,765] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2358\n",
      "[2024-01-23 17:16:48,739] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.97\n",
      "[2024-01-23 17:16:48,759] p1345 {3496713318.py:39} INFO - completed processing chunk 32/32 with concurrency=1\n",
      "[2024-01-23 17:16:48,759] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/57\n",
      "[2024-01-23 17:16:48,760] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:48,769] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3000\n",
      "[2024-01-23 17:16:51,931] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.16\n",
      "[2024-01-23 17:16:51,949] p1345 {3496713318.py:39} INFO - completed processing chunk 1/57 with concurrency=1\n",
      "[2024-01-23 17:16:51,950] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/57\n",
      "[2024-01-23 17:16:51,950] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:51,961] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3896\n",
      "[2024-01-23 17:16:55,321] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.36\n",
      "[2024-01-23 17:16:55,346] p1345 {3496713318.py:39} INFO - completed processing chunk 2/57 with concurrency=1\n",
      "[2024-01-23 17:16:55,346] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/57\n",
      "[2024-01-23 17:16:55,347] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:55,358] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3789\n",
      "[2024-01-23 17:16:58,664] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.31\n",
      "[2024-01-23 17:16:58,686] p1345 {3496713318.py:39} INFO - completed processing chunk 3/57 with concurrency=1\n",
      "[2024-01-23 17:16:58,687] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/57\n",
      "[2024-01-23 17:16:58,687] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:16:58,697] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3450\n",
      "[2024-01-23 17:17:01,931] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.23\n",
      "[2024-01-23 17:17:01,951] p1345 {3496713318.py:39} INFO - completed processing chunk 4/57 with concurrency=1\n",
      "[2024-01-23 17:17:01,952] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/57\n",
      "[2024-01-23 17:17:01,952] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:01,962] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3482\n",
      "[2024-01-23 17:17:05,217] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.25\n",
      "[2024-01-23 17:17:05,235] p1345 {3496713318.py:39} INFO - completed processing chunk 5/57 with concurrency=1\n",
      "[2024-01-23 17:17:05,236] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/57\n",
      "[2024-01-23 17:17:05,237] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:05,245] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3144\n",
      "[2024-01-23 17:17:08,417] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.17\n",
      "[2024-01-23 17:17:08,436] p1345 {3496713318.py:39} INFO - completed processing chunk 6/57 with concurrency=1\n",
      "[2024-01-23 17:17:08,436] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=7/57\n",
      "[2024-01-23 17:17:08,437] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:08,447] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3639\n",
      "[2024-01-23 17:17:11,726] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.28\n",
      "[2024-01-23 17:17:11,744] p1345 {3496713318.py:39} INFO - completed processing chunk 7/57 with concurrency=1\n",
      "[2024-01-23 17:17:11,745] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=8/57\n",
      "[2024-01-23 17:17:11,745] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:11,754] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3014\n",
      "[2024-01-23 17:17:14,895] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.14\n",
      "[2024-01-23 17:17:14,914] p1345 {3496713318.py:39} INFO - completed processing chunk 8/57 with concurrency=1\n",
      "[2024-01-23 17:17:14,914] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=9/57\n",
      "[2024-01-23 17:17:14,915] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:14,924] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3891\n",
      "[2024-01-23 17:17:15,579] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=0.65\n",
      "[2024-01-23 17:17:15,602] p1345 {3496713318.py:39} INFO - completed processing chunk 9/57 with concurrency=1\n",
      "[2024-01-23 17:17:15,602] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=10/57\n",
      "[2024-01-23 17:17:15,603] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:15,614] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3575\n",
      "[2024-01-23 17:17:18,891] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.28\n",
      "[2024-01-23 17:17:18,911] p1345 {3496713318.py:39} INFO - completed processing chunk 10/57 with concurrency=1\n",
      "[2024-01-23 17:17:18,912] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=11/57\n",
      "[2024-01-23 17:17:18,912] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:18,921] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3419\n",
      "[2024-01-23 17:17:22,147] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.23\n",
      "[2024-01-23 17:17:22,167] p1345 {3496713318.py:39} INFO - completed processing chunk 11/57 with concurrency=1\n",
      "[2024-01-23 17:17:22,168] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=12/57\n",
      "[2024-01-23 17:17:22,168] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:22,178] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3458\n",
      "[2024-01-23 17:17:22,912] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=13, latency=0.73\n",
      "[2024-01-23 17:17:22,930] p1345 {3496713318.py:39} INFO - completed processing chunk 12/57 with concurrency=1\n",
      "[2024-01-23 17:17:22,930] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=13/57\n",
      "[2024-01-23 17:17:22,931] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:22,940] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3508\n",
      "[2024-01-23 17:17:26,198] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.26\n",
      "[2024-01-23 17:17:26,217] p1345 {3496713318.py:39} INFO - completed processing chunk 13/57 with concurrency=1\n",
      "[2024-01-23 17:17:26,217] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=14/57\n",
      "[2024-01-23 17:17:26,218] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:26,226] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3436\n",
      "[2024-01-23 17:17:29,455] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.23\n",
      "[2024-01-23 17:17:29,475] p1345 {3496713318.py:39} INFO - completed processing chunk 14/57 with concurrency=1\n",
      "[2024-01-23 17:17:29,475] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=15/57\n",
      "[2024-01-23 17:17:29,476] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:29,486] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3249\n",
      "[2024-01-23 17:17:32,683] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.20\n",
      "[2024-01-23 17:17:32,702] p1345 {3496713318.py:39} INFO - completed processing chunk 15/57 with concurrency=1\n",
      "[2024-01-23 17:17:32,703] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=16/57\n",
      "[2024-01-23 17:17:32,704] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:32,712] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3087\n",
      "[2024-01-23 17:17:35,873] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.16\n",
      "[2024-01-23 17:17:35,892] p1345 {3496713318.py:39} INFO - completed processing chunk 16/57 with concurrency=1\n",
      "[2024-01-23 17:17:35,893] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=17/57\n",
      "[2024-01-23 17:17:35,893] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:35,904] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3680\n",
      "[2024-01-23 17:17:39,191] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.29\n",
      "[2024-01-23 17:17:39,211] p1345 {3496713318.py:39} INFO - completed processing chunk 17/57 with concurrency=1\n",
      "[2024-01-23 17:17:39,212] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=18/57\n",
      "[2024-01-23 17:17:39,212] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:39,222] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3950\n",
      "[2024-01-23 17:17:42,585] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.36\n",
      "[2024-01-23 17:17:42,604] p1345 {3496713318.py:39} INFO - completed processing chunk 18/57 with concurrency=1\n",
      "[2024-01-23 17:17:42,605] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=19/57\n",
      "[2024-01-23 17:17:42,605] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:42,615] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3463\n",
      "[2024-01-23 17:17:45,855] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.24\n",
      "[2024-01-23 17:17:45,874] p1345 {3496713318.py:39} INFO - completed processing chunk 19/57 with concurrency=1\n",
      "[2024-01-23 17:17:45,874] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=20/57\n",
      "[2024-01-23 17:17:45,875] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:45,888] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3996\n",
      "[2024-01-23 17:17:46,492] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=0.60\n",
      "[2024-01-23 17:17:46,512] p1345 {3496713318.py:39} INFO - completed processing chunk 20/57 with concurrency=1\n",
      "[2024-01-23 17:17:46,513] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=21/57\n",
      "[2024-01-23 17:17:46,514] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:46,523] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3050\n",
      "[2024-01-23 17:17:49,670] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.15\n",
      "[2024-01-23 17:17:49,691] p1345 {3496713318.py:39} INFO - completed processing chunk 21/57 with concurrency=1\n",
      "[2024-01-23 17:17:49,692] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=22/57\n",
      "[2024-01-23 17:17:49,692] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:49,705] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3737\n",
      "[2024-01-23 17:17:53,001] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.29\n",
      "[2024-01-23 17:17:53,019] p1345 {3496713318.py:39} INFO - completed processing chunk 22/57 with concurrency=1\n",
      "[2024-01-23 17:17:53,019] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=23/57\n",
      "[2024-01-23 17:17:53,020] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:53,030] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3410\n",
      "[2024-01-23 17:17:54,324] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=34, latency=1.29\n",
      "[2024-01-23 17:17:54,342] p1345 {3496713318.py:39} INFO - completed processing chunk 23/57 with concurrency=1\n",
      "[2024-01-23 17:17:54,343] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=24/57\n",
      "[2024-01-23 17:17:54,343] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:54,354] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3979\n",
      "[2024-01-23 17:17:57,723] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.37\n",
      "[2024-01-23 17:17:57,746] p1345 {3496713318.py:39} INFO - completed processing chunk 24/57 with concurrency=1\n",
      "[2024-01-23 17:17:57,746] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=25/57\n",
      "[2024-01-23 17:17:57,747] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:17:57,756] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3171\n",
      "[2024-01-23 17:18:00,932] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.18\n",
      "[2024-01-23 17:18:00,951] p1345 {3496713318.py:39} INFO - completed processing chunk 25/57 with concurrency=1\n",
      "[2024-01-23 17:18:00,951] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=26/57\n",
      "[2024-01-23 17:18:00,952] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:00,961] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3655\n",
      "[2024-01-23 17:18:04,242] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.28\n",
      "[2024-01-23 17:18:04,262] p1345 {3496713318.py:39} INFO - completed processing chunk 26/57 with concurrency=1\n",
      "[2024-01-23 17:18:04,263] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=27/57\n",
      "[2024-01-23 17:18:04,264] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:04,276] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3662\n",
      "[2024-01-23 17:18:07,562] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.29\n",
      "[2024-01-23 17:18:07,580] p1345 {3496713318.py:39} INFO - completed processing chunk 27/57 with concurrency=1\n",
      "[2024-01-23 17:18:07,580] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=28/57\n",
      "[2024-01-23 17:18:07,581] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:07,589] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3098\n",
      "[2024-01-23 17:18:10,749] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.16\n",
      "[2024-01-23 17:18:10,769] p1345 {3496713318.py:39} INFO - completed processing chunk 28/57 with concurrency=1\n",
      "[2024-01-23 17:18:10,770] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=29/57\n",
      "[2024-01-23 17:18:10,771] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:10,781] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3704\n",
      "[2024-01-23 17:18:11,423] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=0.64\n",
      "[2024-01-23 17:18:11,444] p1345 {3496713318.py:39} INFO - completed processing chunk 29/57 with concurrency=1\n",
      "[2024-01-23 17:18:11,445] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=30/57\n",
      "[2024-01-23 17:18:11,445] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:11,454] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3400\n",
      "[2024-01-23 17:18:14,680] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.22\n",
      "[2024-01-23 17:18:14,703] p1345 {3496713318.py:39} INFO - completed processing chunk 30/57 with concurrency=1\n",
      "[2024-01-23 17:18:14,703] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=31/57\n",
      "[2024-01-23 17:18:14,704] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:14,715] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3098\n",
      "[2024-01-23 17:18:15,245] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=0.53\n",
      "[2024-01-23 17:18:15,267] p1345 {3496713318.py:39} INFO - completed processing chunk 31/57 with concurrency=1\n",
      "[2024-01-23 17:18:15,268] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=32/57\n",
      "[2024-01-23 17:18:15,268] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:15,279] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3909\n",
      "[2024-01-23 17:18:15,850] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=0.57\n",
      "[2024-01-23 17:18:15,871] p1345 {3496713318.py:39} INFO - completed processing chunk 32/57 with concurrency=1\n",
      "[2024-01-23 17:18:15,872] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=33/57\n",
      "[2024-01-23 17:18:15,872] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:15,885] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3971\n",
      "[2024-01-23 17:18:19,283] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.40\n",
      "[2024-01-23 17:18:19,304] p1345 {3496713318.py:39} INFO - completed processing chunk 33/57 with concurrency=1\n",
      "[2024-01-23 17:18:19,305] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=34/57\n",
      "[2024-01-23 17:18:19,305] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:19,314] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3132\n",
      "[2024-01-23 17:18:22,485] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.17\n",
      "[2024-01-23 17:18:22,509] p1345 {3496713318.py:39} INFO - completed processing chunk 34/57 with concurrency=1\n",
      "[2024-01-23 17:18:22,509] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=35/57\n",
      "[2024-01-23 17:18:22,510] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:22,520] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3135\n",
      "[2024-01-23 17:18:22,941] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.42\n",
      "[2024-01-23 17:18:22,960] p1345 {3496713318.py:39} INFO - completed processing chunk 35/57 with concurrency=1\n",
      "[2024-01-23 17:18:22,961] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=36/57\n",
      "[2024-01-23 17:18:22,961] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:22,971] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3030\n",
      "[2024-01-23 17:18:26,114] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.14\n",
      "[2024-01-23 17:18:26,132] p1345 {3496713318.py:39} INFO - completed processing chunk 36/57 with concurrency=1\n",
      "[2024-01-23 17:18:26,133] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=37/57\n",
      "[2024-01-23 17:18:26,134] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:26,145] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3848\n",
      "[2024-01-23 17:18:29,494] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.35\n",
      "[2024-01-23 17:18:29,512] p1345 {3496713318.py:39} INFO - completed processing chunk 37/57 with concurrency=1\n",
      "[2024-01-23 17:18:29,513] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=38/57\n",
      "[2024-01-23 17:18:29,513] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:29,523] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3208\n",
      "[2024-01-23 17:18:32,710] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.19\n",
      "[2024-01-23 17:18:32,732] p1345 {3496713318.py:39} INFO - completed processing chunk 38/57 with concurrency=1\n",
      "[2024-01-23 17:18:32,733] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=39/57\n",
      "[2024-01-23 17:18:32,734] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:32,744] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3475\n",
      "[2024-01-23 17:18:35,996] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.25\n",
      "[2024-01-23 17:18:36,016] p1345 {3496713318.py:39} INFO - completed processing chunk 39/57 with concurrency=1\n",
      "[2024-01-23 17:18:36,017] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=40/57\n",
      "[2024-01-23 17:18:36,017] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:36,031] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3681\n",
      "[2024-01-23 17:18:39,367] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.33\n",
      "[2024-01-23 17:18:39,386] p1345 {3496713318.py:39} INFO - completed processing chunk 40/57 with concurrency=1\n",
      "[2024-01-23 17:18:39,387] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=41/57\n",
      "[2024-01-23 17:18:39,388] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:39,402] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3581\n",
      "[2024-01-23 17:18:40,044] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=0.64\n",
      "[2024-01-23 17:18:40,065] p1345 {3496713318.py:39} INFO - completed processing chunk 41/57 with concurrency=1\n",
      "[2024-01-23 17:18:40,066] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=42/57\n",
      "[2024-01-23 17:18:40,067] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:40,078] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3302\n",
      "[2024-01-23 17:18:43,285] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.21\n",
      "[2024-01-23 17:18:43,306] p1345 {3496713318.py:39} INFO - completed processing chunk 42/57 with concurrency=1\n",
      "[2024-01-23 17:18:43,306] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=43/57\n",
      "[2024-01-23 17:18:43,307] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:43,318] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3614\n",
      "[2024-01-23 17:18:43,812] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=5, latency=0.49\n",
      "[2024-01-23 17:18:43,831] p1345 {3496713318.py:39} INFO - completed processing chunk 43/57 with concurrency=1\n",
      "[2024-01-23 17:18:43,831] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=44/57\n",
      "[2024-01-23 17:18:43,832] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:43,842] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3537\n",
      "[2024-01-23 17:18:47,108] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.27\n",
      "[2024-01-23 17:18:47,129] p1345 {3496713318.py:39} INFO - completed processing chunk 44/57 with concurrency=1\n",
      "[2024-01-23 17:18:47,129] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=45/57\n",
      "[2024-01-23 17:18:47,130] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:47,139] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3165\n",
      "[2024-01-23 17:18:47,562] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.42\n",
      "[2024-01-23 17:18:47,580] p1345 {3496713318.py:39} INFO - completed processing chunk 45/57 with concurrency=1\n",
      "[2024-01-23 17:18:47,581] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=46/57\n",
      "[2024-01-23 17:18:47,581] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:47,592] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3476\n",
      "[2024-01-23 17:18:48,454] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=18, latency=0.86\n",
      "[2024-01-23 17:18:48,472] p1345 {3496713318.py:39} INFO - completed processing chunk 46/57 with concurrency=1\n",
      "[2024-01-23 17:18:48,473] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=47/57\n",
      "[2024-01-23 17:18:48,473] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:48,482] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3235\n",
      "[2024-01-23 17:18:51,673] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.19\n",
      "[2024-01-23 17:18:51,695] p1345 {3496713318.py:39} INFO - completed processing chunk 47/57 with concurrency=1\n",
      "[2024-01-23 17:18:51,695] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=48/57\n",
      "[2024-01-23 17:18:51,696] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:51,705] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3363\n",
      "[2024-01-23 17:18:54,925] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.22\n",
      "[2024-01-23 17:18:54,945] p1345 {3496713318.py:39} INFO - completed processing chunk 48/57 with concurrency=1\n",
      "[2024-01-23 17:18:54,946] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=49/57\n",
      "[2024-01-23 17:18:54,946] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:54,958] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3882\n",
      "[2024-01-23 17:18:55,468] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.51\n",
      "[2024-01-23 17:18:55,488] p1345 {3496713318.py:39} INFO - completed processing chunk 49/57 with concurrency=1\n",
      "[2024-01-23 17:18:55,489] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=50/57\n",
      "[2024-01-23 17:18:55,490] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:55,501] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3616\n",
      "[2024-01-23 17:18:58,779] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.28\n",
      "[2024-01-23 17:18:58,800] p1345 {3496713318.py:39} INFO - completed processing chunk 50/57 with concurrency=1\n",
      "[2024-01-23 17:18:58,800] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=51/57\n",
      "[2024-01-23 17:18:58,801] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:18:58,811] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3567\n",
      "[2024-01-23 17:19:02,087] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.27\n",
      "[2024-01-23 17:19:02,106] p1345 {3496713318.py:39} INFO - completed processing chunk 51/57 with concurrency=1\n",
      "[2024-01-23 17:19:02,107] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=52/57\n",
      "[2024-01-23 17:19:02,107] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:19:02,117] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3330\n",
      "[2024-01-23 17:19:05,332] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.21\n",
      "[2024-01-23 17:19:05,353] p1345 {3496713318.py:39} INFO - completed processing chunk 52/57 with concurrency=1\n",
      "[2024-01-23 17:19:05,354] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=53/57\n",
      "[2024-01-23 17:19:05,354] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:19:05,365] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3566\n",
      "[2024-01-23 17:19:08,641] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.28\n",
      "[2024-01-23 17:19:08,668] p1345 {3496713318.py:39} INFO - completed processing chunk 53/57 with concurrency=1\n",
      "[2024-01-23 17:19:08,669] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=54/57\n",
      "[2024-01-23 17:19:08,670] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:19:08,679] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3497\n",
      "[2024-01-23 17:19:11,935] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.26\n",
      "[2024-01-23 17:19:11,954] p1345 {3496713318.py:39} INFO - completed processing chunk 54/57 with concurrency=1\n",
      "[2024-01-23 17:19:11,954] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=55/57\n",
      "[2024-01-23 17:19:11,955] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:19:11,964] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3003\n",
      "[2024-01-23 17:19:15,103] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.14\n",
      "[2024-01-23 17:19:15,120] p1345 {3496713318.py:39} INFO - completed processing chunk 55/57 with concurrency=1\n",
      "[2024-01-23 17:19:15,121] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=56/57\n",
      "[2024-01-23 17:19:15,121] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:19:15,131] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3783\n",
      "[2024-01-23 17:19:18,434] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.30\n",
      "[2024-01-23 17:19:18,453] p1345 {3496713318.py:39} INFO - completed processing chunk 56/57 with concurrency=1\n",
      "[2024-01-23 17:19:18,454] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=57/57\n",
      "[2024-01-23 17:19:18,454] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 17:19:18,465] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:19:21,665] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.20\n",
      "[2024-01-23 17:19:21,685] p1345 {3496713318.py:39} INFO - completed processing chunk 57/57 with concurrency=1\n",
      "[2024-01-23 17:19:21,685] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/1\n",
      "[2024-01-23 17:19:21,686] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:21,691] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:19:21,692] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:19:24,303] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.61\n",
      "[2024-01-23 17:19:24,319] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.62\n",
      "[2024-01-23 17:19:24,345] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=2\n",
      "[2024-01-23 17:19:24,345] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/1\n",
      "[2024-01-23 17:19:24,346] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:24,353] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:19:24,354] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:19:27,222] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.87\n",
      "[2024-01-23 17:19:27,238] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.88\n",
      "[2024-01-23 17:19:27,263] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=2\n",
      "[2024-01-23 17:19:27,264] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/8\n",
      "[2024-01-23 17:19:27,264] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:27,272] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1339\n",
      "[2024-01-23 17:19:27,274] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1932\n",
      "[2024-01-23 17:19:30,406] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.13\n",
      "[2024-01-23 17:19:30,423] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.15\n",
      "[2024-01-23 17:19:30,449] p1345 {3496713318.py:39} INFO - completed processing chunk 1/8 with concurrency=2\n",
      "[2024-01-23 17:19:30,450] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/8\n",
      "[2024-01-23 17:19:30,450] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:30,457] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1154\n",
      "[2024-01-23 17:19:30,459] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1646\n",
      "[2024-01-23 17:19:31,559] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=26, latency=1.10\n",
      "[2024-01-23 17:19:33,382] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.92\n",
      "[2024-01-23 17:19:33,409] p1345 {3496713318.py:39} INFO - completed processing chunk 2/8 with concurrency=2\n",
      "[2024-01-23 17:19:33,410] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/8\n",
      "[2024-01-23 17:19:33,410] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:33,417] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1397\n",
      "[2024-01-23 17:19:33,422] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1746\n",
      "[2024-01-23 17:19:34,233] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=16, latency=0.81\n",
      "[2024-01-23 17:19:36,364] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.94\n",
      "[2024-01-23 17:19:36,391] p1345 {3496713318.py:39} INFO - completed processing chunk 3/8 with concurrency=2\n",
      "[2024-01-23 17:19:36,392] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/8\n",
      "[2024-01-23 17:19:36,392] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:36,399] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1373\n",
      "[2024-01-23 17:19:36,401] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1598\n",
      "[2024-01-23 17:19:36,906] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.50\n",
      "[2024-01-23 17:19:39,315] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.91\n",
      "[2024-01-23 17:19:39,346] p1345 {3496713318.py:39} INFO - completed processing chunk 4/8 with concurrency=2\n",
      "[2024-01-23 17:19:39,346] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/8\n",
      "[2024-01-23 17:19:39,347] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:39,357] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1743\n",
      "[2024-01-23 17:19:39,357] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1539\n",
      "[2024-01-23 17:19:42,463] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.10\n",
      "[2024-01-23 17:19:42,480] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.12\n",
      "[2024-01-23 17:19:42,507] p1345 {3496713318.py:39} INFO - completed processing chunk 5/8 with concurrency=2\n",
      "[2024-01-23 17:19:42,508] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/8\n",
      "[2024-01-23 17:19:42,508] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:42,516] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1695\n",
      "[2024-01-23 17:19:42,518] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1421\n",
      "[2024-01-23 17:19:45,607] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.09\n",
      "[2024-01-23 17:19:45,623] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.10\n",
      "[2024-01-23 17:19:45,652] p1345 {3496713318.py:39} INFO - completed processing chunk 6/8 with concurrency=2\n",
      "[2024-01-23 17:19:45,653] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=7/8\n",
      "[2024-01-23 17:19:45,653] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:45,664] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1910\n",
      "[2024-01-23 17:19:45,664] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1918\n",
      "[2024-01-23 17:19:48,830] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.16\n",
      "[2024-01-23 17:19:48,848] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.18\n",
      "[2024-01-23 17:19:48,875] p1345 {3496713318.py:39} INFO - completed processing chunk 7/8 with concurrency=2\n",
      "[2024-01-23 17:19:48,875] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=8/8\n",
      "[2024-01-23 17:19:48,876] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:48,884] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1939\n",
      "[2024-01-23 17:19:48,889] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1939\n",
      "[2024-01-23 17:19:49,543] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=0.65\n",
      "[2024-01-23 17:19:51,997] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.11\n",
      "[2024-01-23 17:19:52,026] p1345 {3496713318.py:39} INFO - completed processing chunk 8/8 with concurrency=2\n",
      "[2024-01-23 17:19:52,026] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/16\n",
      "[2024-01-23 17:19:52,027] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:52,038] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2637\n",
      "[2024-01-23 17:19:52,042] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3000\n",
      "[2024-01-23 17:19:52,868] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=0.83\n",
      "[2024-01-23 17:19:55,408] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.37\n",
      "[2024-01-23 17:19:55,434] p1345 {3496713318.py:39} INFO - completed processing chunk 1/16 with concurrency=2\n",
      "[2024-01-23 17:19:55,434] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/16\n",
      "[2024-01-23 17:19:55,435] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:55,445] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2148\n",
      "[2024-01-23 17:19:55,448] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2715\n",
      "[2024-01-23 17:19:56,243] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=0.80\n",
      "[2024-01-23 17:19:58,819] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.37\n",
      "[2024-01-23 17:19:58,847] p1345 {3496713318.py:39} INFO - completed processing chunk 2/16 with concurrency=2\n",
      "[2024-01-23 17:19:58,848] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/16\n",
      "[2024-01-23 17:19:58,849] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:19:58,857] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2404\n",
      "[2024-01-23 17:19:58,861] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2150\n",
      "[2024-01-23 17:20:02,188] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.33\n",
      "[2024-01-23 17:20:02,206] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.34\n",
      "[2024-01-23 17:20:02,237] p1345 {3496713318.py:39} INFO - completed processing chunk 3/16 with concurrency=2\n",
      "[2024-01-23 17:20:02,237] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/16\n",
      "[2024-01-23 17:20:02,238] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:02,250] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2369\n",
      "[2024-01-23 17:20:02,250] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2803\n",
      "[2024-01-23 17:20:03,184] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=11, latency=0.93\n",
      "[2024-01-23 17:20:05,529] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.28\n",
      "[2024-01-23 17:20:05,558] p1345 {3496713318.py:39} INFO - completed processing chunk 4/16 with concurrency=2\n",
      "[2024-01-23 17:20:05,559] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/16\n",
      "[2024-01-23 17:20:05,559] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:05,569] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2541\n",
      "[2024-01-23 17:20:05,573] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2675\n",
      "[2024-01-23 17:20:07,253] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=40, latency=1.68\n",
      "[2024-01-23 17:20:08,977] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.40\n",
      "[2024-01-23 17:20:09,005] p1345 {3496713318.py:39} INFO - completed processing chunk 5/16 with concurrency=2\n",
      "[2024-01-23 17:20:09,005] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/16\n",
      "[2024-01-23 17:20:09,006] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:09,018] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2186\n",
      "[2024-01-23 17:20:09,021] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2775\n",
      "[2024-01-23 17:20:09,790] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=7, latency=0.77\n",
      "[2024-01-23 17:20:12,405] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.38\n",
      "[2024-01-23 17:20:12,432] p1345 {3496713318.py:39} INFO - completed processing chunk 6/16 with concurrency=2\n",
      "[2024-01-23 17:20:12,433] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=7/16\n",
      "[2024-01-23 17:20:12,433] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:12,444] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2686\n",
      "[2024-01-23 17:20:12,447] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2500\n",
      "[2024-01-23 17:20:15,863] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.42\n",
      "[2024-01-23 17:20:15,881] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.43\n",
      "[2024-01-23 17:20:15,910] p1345 {3496713318.py:39} INFO - completed processing chunk 7/16 with concurrency=2\n",
      "[2024-01-23 17:20:15,911] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=8/16\n",
      "[2024-01-23 17:20:15,911] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:15,922] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2443\n",
      "[2024-01-23 17:20:15,924] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2321\n",
      "[2024-01-23 17:20:16,627] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=0.70\n",
      "[2024-01-23 17:20:19,240] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.31\n",
      "[2024-01-23 17:20:19,270] p1345 {3496713318.py:39} INFO - completed processing chunk 8/16 with concurrency=2\n",
      "[2024-01-23 17:20:19,270] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=9/16\n",
      "[2024-01-23 17:20:19,271] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:19,283] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2428\n",
      "[2024-01-23 17:20:19,284] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2458\n",
      "[2024-01-23 17:20:22,650] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.37\n",
      "[2024-01-23 17:20:22,668] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.38\n",
      "[2024-01-23 17:20:22,698] p1345 {3496713318.py:39} INFO - completed processing chunk 9/16 with concurrency=2\n",
      "[2024-01-23 17:20:22,699] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=10/16\n",
      "[2024-01-23 17:20:22,699] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:22,706] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2101\n",
      "[2024-01-23 17:20:22,714] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2440\n",
      "[2024-01-23 17:20:23,421] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=7, latency=0.71\n",
      "[2024-01-23 17:20:26,020] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.31\n",
      "[2024-01-23 17:20:26,057] p1345 {3496713318.py:39} INFO - completed processing chunk 10/16 with concurrency=2\n",
      "[2024-01-23 17:20:26,058] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=11/16\n",
      "[2024-01-23 17:20:26,058] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:26,070] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2711\n",
      "[2024-01-23 17:20:26,073] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2771\n",
      "[2024-01-23 17:20:29,538] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.47\n",
      "[2024-01-23 17:20:29,556] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.48\n",
      "[2024-01-23 17:20:29,583] p1345 {3496713318.py:39} INFO - completed processing chunk 11/16 with concurrency=2\n",
      "[2024-01-23 17:20:29,584] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=12/16\n",
      "[2024-01-23 17:20:29,584] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:29,599] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2691\n",
      "[2024-01-23 17:20:29,599] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2624\n",
      "[2024-01-23 17:20:30,325] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.72\n",
      "[2024-01-23 17:20:32,927] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.33\n",
      "[2024-01-23 17:20:32,956] p1345 {3496713318.py:39} INFO - completed processing chunk 12/16 with concurrency=2\n",
      "[2024-01-23 17:20:32,957] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=13/16\n",
      "[2024-01-23 17:20:32,957] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:32,967] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2062\n",
      "[2024-01-23 17:20:32,970] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2213\n",
      "[2024-01-23 17:20:36,254] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.29\n",
      "[2024-01-23 17:20:36,273] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.30\n",
      "[2024-01-23 17:20:36,300] p1345 {3496713318.py:39} INFO - completed processing chunk 13/16 with concurrency=2\n",
      "[2024-01-23 17:20:36,301] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=14/16\n",
      "[2024-01-23 17:20:36,301] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:36,314] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2770\n",
      "[2024-01-23 17:20:36,314] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2608\n",
      "[2024-01-23 17:20:37,041] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.72\n",
      "[2024-01-23 17:20:39,652] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.34\n",
      "[2024-01-23 17:20:39,683] p1345 {3496713318.py:39} INFO - completed processing chunk 14/16 with concurrency=2\n",
      "[2024-01-23 17:20:39,684] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=15/16\n",
      "[2024-01-23 17:20:39,685] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:39,696] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2333\n",
      "[2024-01-23 17:20:39,700] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2564\n",
      "[2024-01-23 17:20:40,632] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=13, latency=0.93\n",
      "[2024-01-23 17:20:42,946] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.25\n",
      "[2024-01-23 17:20:42,973] p1345 {3496713318.py:39} INFO - completed processing chunk 15/16 with concurrency=2\n",
      "[2024-01-23 17:20:42,974] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=16/16\n",
      "[2024-01-23 17:20:42,974] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:42,984] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2358\n",
      "[2024-01-23 17:20:42,986] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2613\n",
      "[2024-01-23 17:20:43,732] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=0.74\n",
      "[2024-01-23 17:20:46,235] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.25\n",
      "[2024-01-23 17:20:46,261] p1345 {3496713318.py:39} INFO - completed processing chunk 16/16 with concurrency=2\n",
      "[2024-01-23 17:20:46,261] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/29\n",
      "[2024-01-23 17:20:46,262] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:46,275] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3000\n",
      "[2024-01-23 17:20:46,281] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3896\n",
      "[2024-01-23 17:20:48,215] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=38, latency=1.93\n",
      "[2024-01-23 17:20:49,909] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.63\n",
      "[2024-01-23 17:20:49,938] p1345 {3496713318.py:39} INFO - completed processing chunk 1/29 with concurrency=2\n",
      "[2024-01-23 17:20:49,938] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/29\n",
      "[2024-01-23 17:20:49,939] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:49,955] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3789\n",
      "[2024-01-23 17:20:49,958] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3450\n",
      "[2024-01-23 17:20:51,155] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=14, latency=1.20\n",
      "[2024-01-23 17:20:53,682] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.72\n",
      "[2024-01-23 17:20:53,711] p1345 {3496713318.py:39} INFO - completed processing chunk 2/29 with concurrency=2\n",
      "[2024-01-23 17:20:53,711] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/29\n",
      "[2024-01-23 17:20:53,712] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:53,726] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3482\n",
      "[2024-01-23 17:20:53,729] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3144\n",
      "[2024-01-23 17:20:57,439] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.71\n",
      "[2024-01-23 17:20:57,458] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.73\n",
      "[2024-01-23 17:20:57,484] p1345 {3496713318.py:39} INFO - completed processing chunk 3/29 with concurrency=2\n",
      "[2024-01-23 17:20:57,485] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/29\n",
      "[2024-01-23 17:20:57,485] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:20:57,499] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3014\n",
      "[2024-01-23 17:20:57,505] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3639\n",
      "[2024-01-23 17:20:58,474] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=7, latency=0.97\n",
      "[2024-01-23 17:21:01,198] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.69\n",
      "[2024-01-23 17:21:01,229] p1345 {3496713318.py:39} INFO - completed processing chunk 4/29 with concurrency=2\n",
      "[2024-01-23 17:21:01,229] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/29\n",
      "[2024-01-23 17:21:01,230] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:01,244] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3891\n",
      "[2024-01-23 17:21:01,246] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3575\n",
      "[2024-01-23 17:21:05,111] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.86\n",
      "[2024-01-23 17:21:05,131] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.88\n",
      "[2024-01-23 17:21:05,158] p1345 {3496713318.py:39} INFO - completed processing chunk 5/29 with concurrency=2\n",
      "[2024-01-23 17:21:05,159] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/29\n",
      "[2024-01-23 17:21:05,159] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:05,173] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3419\n",
      "[2024-01-23 17:21:05,176] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3458\n",
      "[2024-01-23 17:21:08,899] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.72\n",
      "[2024-01-23 17:21:08,918] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.74\n",
      "[2024-01-23 17:21:08,945] p1345 {3496713318.py:39} INFO - completed processing chunk 6/29 with concurrency=2\n",
      "[2024-01-23 17:21:08,945] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=7/29\n",
      "[2024-01-23 17:21:08,946] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:08,959] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3508\n",
      "[2024-01-23 17:21:08,963] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3436\n",
      "[2024-01-23 17:21:12,702] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.74\n",
      "[2024-01-23 17:21:12,720] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.76\n",
      "[2024-01-23 17:21:12,746] p1345 {3496713318.py:39} INFO - completed processing chunk 7/29 with concurrency=2\n",
      "[2024-01-23 17:21:12,746] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=8/29\n",
      "[2024-01-23 17:21:12,747] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:12,761] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3087\n",
      "[2024-01-23 17:21:12,762] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3249\n",
      "[2024-01-23 17:21:16,406] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.64\n",
      "[2024-01-23 17:21:16,425] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.66\n",
      "[2024-01-23 17:21:16,453] p1345 {3496713318.py:39} INFO - completed processing chunk 8/29 with concurrency=2\n",
      "[2024-01-23 17:21:16,454] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=9/29\n",
      "[2024-01-23 17:21:16,455] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:16,471] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3680\n",
      "[2024-01-23 17:21:16,474] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3950\n",
      "[2024-01-23 17:21:20,341] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.87\n",
      "[2024-01-23 17:21:20,360] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=100, latency=3.88\n",
      "[2024-01-23 17:21:20,388] p1345 {3496713318.py:39} INFO - completed processing chunk 9/29 with concurrency=2\n",
      "[2024-01-23 17:21:20,389] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=10/29\n",
      "[2024-01-23 17:21:20,390] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:20,403] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3463\n",
      "[2024-01-23 17:21:20,410] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3996\n",
      "[2024-01-23 17:21:24,282] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.88\n",
      "[2024-01-23 17:21:24,298] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.89\n",
      "[2024-01-23 17:21:24,326] p1345 {3496713318.py:39} INFO - completed processing chunk 10/29 with concurrency=2\n",
      "[2024-01-23 17:21:24,327] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=11/29\n",
      "[2024-01-23 17:21:24,327] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:24,342] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3050\n",
      "[2024-01-23 17:21:24,346] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3737\n",
      "[2024-01-23 17:21:28,080] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.74\n",
      "[2024-01-23 17:21:28,099] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.75\n",
      "[2024-01-23 17:21:28,126] p1345 {3496713318.py:39} INFO - completed processing chunk 11/29 with concurrency=2\n",
      "[2024-01-23 17:21:28,127] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=12/29\n",
      "[2024-01-23 17:21:28,127] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:28,140] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3410\n",
      "[2024-01-23 17:21:28,145] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3979\n",
      "[2024-01-23 17:21:31,992] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.85\n",
      "[2024-01-23 17:21:32,015] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.87\n",
      "[2024-01-23 17:21:32,047] p1345 {3496713318.py:39} INFO - completed processing chunk 12/29 with concurrency=2\n",
      "[2024-01-23 17:21:32,048] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=13/29\n",
      "[2024-01-23 17:21:32,048] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:32,063] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3171\n",
      "[2024-01-23 17:21:32,065] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3655\n",
      "[2024-01-23 17:21:35,803] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.74\n",
      "[2024-01-23 17:21:35,821] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.75\n",
      "[2024-01-23 17:21:35,848] p1345 {3496713318.py:39} INFO - completed processing chunk 13/29 with concurrency=2\n",
      "[2024-01-23 17:21:35,848] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=14/29\n",
      "[2024-01-23 17:21:35,849] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:35,860] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3098\n",
      "[2024-01-23 17:21:35,865] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3662\n",
      "[2024-01-23 17:21:39,596] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.73\n",
      "[2024-01-23 17:21:39,615] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.75\n",
      "[2024-01-23 17:21:39,646] p1345 {3496713318.py:39} INFO - completed processing chunk 14/29 with concurrency=2\n",
      "[2024-01-23 17:21:39,647] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=15/29\n",
      "[2024-01-23 17:21:39,647] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:39,658] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3400\n",
      "[2024-01-23 17:21:39,666] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3704\n",
      "[2024-01-23 17:21:43,433] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.77\n",
      "[2024-01-23 17:21:43,452] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.78\n",
      "[2024-01-23 17:21:43,480] p1345 {3496713318.py:39} INFO - completed processing chunk 15/29 with concurrency=2\n",
      "[2024-01-23 17:21:43,480] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=16/29\n",
      "[2024-01-23 17:21:43,481] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:43,498] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3098\n",
      "[2024-01-23 17:21:43,502] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3909\n",
      "[2024-01-23 17:21:44,527] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=1.03\n",
      "[2024-01-23 17:21:47,282] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.78\n",
      "[2024-01-23 17:21:47,308] p1345 {3496713318.py:39} INFO - completed processing chunk 16/29 with concurrency=2\n",
      "[2024-01-23 17:21:47,309] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=17/29\n",
      "[2024-01-23 17:21:47,309] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:47,326] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3132\n",
      "[2024-01-23 17:21:47,329] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3971\n",
      "[2024-01-23 17:21:48,838] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=24, latency=1.51\n",
      "[2024-01-23 17:21:51,134] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.80\n",
      "[2024-01-23 17:21:51,170] p1345 {3496713318.py:39} INFO - completed processing chunk 17/29 with concurrency=2\n",
      "[2024-01-23 17:21:51,171] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=18/29\n",
      "[2024-01-23 17:21:51,171] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:51,186] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3135\n",
      "[2024-01-23 17:21:51,187] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3030\n",
      "[2024-01-23 17:21:52,031] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.84\n",
      "[2024-01-23 17:21:54,690] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.50\n",
      "[2024-01-23 17:21:54,723] p1345 {3496713318.py:39} INFO - completed processing chunk 18/29 with concurrency=2\n",
      "[2024-01-23 17:21:54,724] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=19/29\n",
      "[2024-01-23 17:21:54,725] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:54,738] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3848\n",
      "[2024-01-23 17:21:54,743] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3208\n",
      "[2024-01-23 17:21:55,985] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=15, latency=1.25\n",
      "[2024-01-23 17:21:58,456] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.71\n",
      "[2024-01-23 17:21:58,482] p1345 {3496713318.py:39} INFO - completed processing chunk 19/29 with concurrency=2\n",
      "[2024-01-23 17:21:58,482] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=20/29\n",
      "[2024-01-23 17:21:58,483] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:21:58,500] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3681\n",
      "[2024-01-23 17:21:58,502] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3475\n",
      "[2024-01-23 17:21:59,625] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=1.12\n",
      "[2024-01-23 17:22:02,152] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.65\n",
      "[2024-01-23 17:22:02,182] p1345 {3496713318.py:39} INFO - completed processing chunk 20/29 with concurrency=2\n",
      "[2024-01-23 17:22:02,183] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=21/29\n",
      "[2024-01-23 17:22:02,183] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:22:02,197] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3581\n",
      "[2024-01-23 17:22:02,200] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3302\n",
      "[2024-01-23 17:22:05,955] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.76\n",
      "[2024-01-23 17:22:05,975] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.77\n",
      "[2024-01-23 17:22:06,002] p1345 {3496713318.py:39} INFO - completed processing chunk 21/29 with concurrency=2\n",
      "[2024-01-23 17:22:06,002] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=22/29\n",
      "[2024-01-23 17:22:06,003] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:22:06,017] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3537\n",
      "[2024-01-23 17:22:06,022] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3614\n",
      "[2024-01-23 17:22:07,108] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=1.09\n",
      "[2024-01-23 17:22:09,770] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.75\n",
      "[2024-01-23 17:22:09,801] p1345 {3496713318.py:39} INFO - completed processing chunk 22/29 with concurrency=2\n",
      "[2024-01-23 17:22:09,801] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=23/29\n",
      "[2024-01-23 17:22:09,802] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:22:09,816] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3165\n",
      "[2024-01-23 17:22:09,819] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3476\n",
      "[2024-01-23 17:22:10,862] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=1.05\n",
      "[2024-01-23 17:22:13,503] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.68\n",
      "[2024-01-23 17:22:13,531] p1345 {3496713318.py:39} INFO - completed processing chunk 23/29 with concurrency=2\n",
      "[2024-01-23 17:22:13,532] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=24/29\n",
      "[2024-01-23 17:22:13,532] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:22:13,547] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3235\n",
      "[2024-01-23 17:22:13,549] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3363\n",
      "[2024-01-23 17:22:14,580] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=1.03\n",
      "[2024-01-23 17:22:14,684] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=13, latency=1.13\n",
      "[2024-01-23 17:22:14,710] p1345 {3496713318.py:39} INFO - completed processing chunk 24/29 with concurrency=2\n",
      "[2024-01-23 17:22:14,711] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=25/29\n",
      "[2024-01-23 17:22:14,712] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:22:14,730] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3882\n",
      "[2024-01-23 17:22:14,734] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3616\n",
      "[2024-01-23 17:22:16,314] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=24, latency=1.58\n",
      "[2024-01-23 17:22:18,484] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.75\n",
      "[2024-01-23 17:22:18,513] p1345 {3496713318.py:39} INFO - completed processing chunk 25/29 with concurrency=2\n",
      "[2024-01-23 17:22:18,514] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=26/29\n",
      "[2024-01-23 17:22:18,514] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:22:18,530] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3567\n",
      "[2024-01-23 17:22:18,533] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3330\n",
      "[2024-01-23 17:22:19,480] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=0.95\n",
      "[2024-01-23 17:22:22,223] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.69\n",
      "[2024-01-23 17:22:22,250] p1345 {3496713318.py:39} INFO - completed processing chunk 26/29 with concurrency=2\n",
      "[2024-01-23 17:22:22,251] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=27/29\n",
      "[2024-01-23 17:22:22,251] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:22:22,268] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3566\n",
      "[2024-01-23 17:22:22,276] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3497\n",
      "[2024-01-23 17:22:23,221] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.94\n",
      "[2024-01-23 17:22:25,932] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.66\n",
      "[2024-01-23 17:22:25,986] p1345 {3496713318.py:39} INFO - completed processing chunk 27/29 with concurrency=2\n",
      "[2024-01-23 17:22:25,986] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=28/29\n",
      "[2024-01-23 17:22:25,987] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:22:26,002] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3783\n",
      "[2024-01-23 17:22:26,003] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3003\n",
      "[2024-01-23 17:22:26,990] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=0.99\n",
      "[2024-01-23 17:22:29,631] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.62\n",
      "[2024-01-23 17:22:29,659] p1345 {3496713318.py:39} INFO - completed processing chunk 28/29 with concurrency=2\n",
      "[2024-01-23 17:22:29,659] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=29/29\n",
      "[2024-01-23 17:22:29,660] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 17:22:29,676] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:22:29,678] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:22:33,354] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.67\n",
      "[2024-01-23 17:22:33,372] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.70\n",
      "[2024-01-23 17:22:33,398] p1345 {3496713318.py:39} INFO - completed processing chunk 29/29 with concurrency=2\n",
      "[2024-01-23 17:22:33,398] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/1\n",
      "[2024-01-23 17:22:33,399] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:22:33,410] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:22:33,410] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:22:33,410] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:22:33,410] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:22:34,426] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=29, latency=1.01\n",
      "[2024-01-23 17:22:36,385] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.97\n",
      "[2024-01-23 17:22:36,404] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.99\n",
      "[2024-01-23 17:22:36,404] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.99\n",
      "[2024-01-23 17:22:36,450] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=4\n",
      "[2024-01-23 17:22:36,451] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/1\n",
      "[2024-01-23 17:22:36,451] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:22:36,458] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:22:36,464] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:22:36,465] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:22:36,466] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:22:39,806] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.34\n",
      "[2024-01-23 17:22:39,826] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.35\n",
      "[2024-01-23 17:22:39,826] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.36\n",
      "[2024-01-23 17:22:39,839] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.37\n",
      "[2024-01-23 17:22:39,882] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=4\n",
      "[2024-01-23 17:22:39,882] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/4\n",
      "[2024-01-23 17:22:39,883] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:22:39,890] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1339\n",
      "[2024-01-23 17:22:39,892] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1932\n",
      "[2024-01-23 17:22:39,897] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1154\n",
      "[2024-01-23 17:22:39,898] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1646\n",
      "[2024-01-23 17:22:40,717] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.81\n",
      "[2024-01-23 17:22:43,567] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.67\n",
      "[2024-01-23 17:22:43,592] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.69\n",
      "[2024-01-23 17:22:43,592] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.69\n",
      "[2024-01-23 17:22:43,637] p1345 {3496713318.py:39} INFO - completed processing chunk 1/4 with concurrency=4\n",
      "[2024-01-23 17:22:43,638] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/4\n",
      "[2024-01-23 17:22:43,638] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:22:43,647] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1397\n",
      "[2024-01-23 17:22:43,653] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1373\n",
      "[2024-01-23 17:22:43,654] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1746\n",
      "[2024-01-23 17:22:43,654] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1598\n",
      "[2024-01-23 17:22:47,332] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.68\n",
      "[2024-01-23 17:22:47,356] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.70\n",
      "[2024-01-23 17:22:47,356] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.70\n",
      "[2024-01-23 17:22:47,358] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.70\n",
      "[2024-01-23 17:22:47,402] p1345 {3496713318.py:39} INFO - completed processing chunk 2/4 with concurrency=4\n",
      "[2024-01-23 17:22:47,403] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/4\n",
      "[2024-01-23 17:22:47,403] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:22:47,410] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1743\n",
      "[2024-01-23 17:22:47,421] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1539\n",
      "[2024-01-23 17:22:47,423] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1695\n",
      "[2024-01-23 17:22:47,424] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1421\n",
      "[2024-01-23 17:22:51,125] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.71\n",
      "[2024-01-23 17:22:51,147] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.72\n",
      "[2024-01-23 17:22:51,147] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.72\n",
      "[2024-01-23 17:22:51,147] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.73\n",
      "[2024-01-23 17:22:51,199] p1345 {3496713318.py:39} INFO - completed processing chunk 3/4 with concurrency=4\n",
      "[2024-01-23 17:22:51,199] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/4\n",
      "[2024-01-23 17:22:51,200] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:22:51,212] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1910\n",
      "[2024-01-23 17:22:51,217] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1939\n",
      "[2024-01-23 17:22:51,217] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1918\n",
      "[2024-01-23 17:22:51,217] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1918\n",
      "[2024-01-23 17:22:52,581] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=19, latency=1.36\n",
      "[2024-01-23 17:22:55,075] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.85\n",
      "[2024-01-23 17:22:55,077] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.85\n",
      "[2024-01-23 17:22:55,081] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.86\n",
      "[2024-01-23 17:22:55,123] p1345 {3496713318.py:39} INFO - completed processing chunk 4/4 with concurrency=4\n",
      "[2024-01-23 17:22:55,123] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/8\n",
      "[2024-01-23 17:22:55,124] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:22:55,140] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2148\n",
      "[2024-01-23 17:22:55,141] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2637\n",
      "[2024-01-23 17:22:55,146] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3000\n",
      "[2024-01-23 17:22:55,147] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2715\n",
      "[2024-01-23 17:22:56,753] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=14, latency=1.61\n",
      "[2024-01-23 17:22:58,308] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=63, latency=3.16\n",
      "[2024-01-23 17:22:59,476] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.32\n",
      "[2024-01-23 17:22:59,476] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.32\n",
      "[2024-01-23 17:22:59,523] p1345 {3496713318.py:39} INFO - completed processing chunk 1/8 with concurrency=4\n",
      "[2024-01-23 17:22:59,523] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/8\n",
      "[2024-01-23 17:22:59,524] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:22:59,530] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2404\n",
      "[2024-01-23 17:22:59,538] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2150\n",
      "[2024-01-23 17:22:59,544] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2369\n",
      "[2024-01-23 17:22:59,548] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2803\n",
      "[2024-01-23 17:23:01,026] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=12, latency=1.48\n",
      "[2024-01-23 17:23:03,739] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.19\n",
      "[2024-01-23 17:23:03,758] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.20\n",
      "[2024-01-23 17:23:03,758] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.21\n",
      "[2024-01-23 17:23:03,806] p1345 {3496713318.py:39} INFO - completed processing chunk 2/8 with concurrency=4\n",
      "[2024-01-23 17:23:03,807] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/8\n",
      "[2024-01-23 17:23:03,808] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:03,818] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2675\n",
      "[2024-01-23 17:23:03,824] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2541\n",
      "[2024-01-23 17:23:03,827] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2186\n",
      "[2024-01-23 17:23:03,828] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2775\n",
      "[2024-01-23 17:23:05,037] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.21\n",
      "[2024-01-23 17:23:08,069] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.24\n",
      "[2024-01-23 17:23:08,070] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.24\n",
      "[2024-01-23 17:23:08,071] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.24\n",
      "[2024-01-23 17:23:08,123] p1345 {3496713318.py:39} INFO - completed processing chunk 3/8 with concurrency=4\n",
      "[2024-01-23 17:23:08,123] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/8\n",
      "[2024-01-23 17:23:08,124] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:08,133] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2500\n",
      "[2024-01-23 17:23:08,144] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2443\n",
      "[2024-01-23 17:23:08,147] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2686\n",
      "[2024-01-23 17:23:08,148] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2321\n",
      "[2024-01-23 17:23:09,827] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=18, latency=1.67\n",
      "[2024-01-23 17:23:12,316] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.17\n",
      "[2024-01-23 17:23:12,338] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.19\n",
      "[2024-01-23 17:23:12,339] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.18\n",
      "[2024-01-23 17:23:12,384] p1345 {3496713318.py:39} INFO - completed processing chunk 4/8 with concurrency=4\n",
      "[2024-01-23 17:23:12,385] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/8\n",
      "[2024-01-23 17:23:12,385] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:12,403] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2101\n",
      "[2024-01-23 17:23:12,405] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2458\n",
      "[2024-01-23 17:23:12,406] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2428\n",
      "[2024-01-23 17:23:12,407] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2440\n",
      "[2024-01-23 17:23:16,567] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.16\n",
      "[2024-01-23 17:23:16,590] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.17\n",
      "[2024-01-23 17:23:16,590] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.18\n",
      "[2024-01-23 17:23:16,590] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.18\n",
      "[2024-01-23 17:23:16,636] p1345 {3496713318.py:39} INFO - completed processing chunk 5/8 with concurrency=4\n",
      "[2024-01-23 17:23:16,637] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/8\n",
      "[2024-01-23 17:23:16,637] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:16,657] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2691\n",
      "[2024-01-23 17:23:16,659] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2711\n",
      "[2024-01-23 17:23:16,665] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2771\n",
      "[2024-01-23 17:23:16,666] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2624\n",
      "[2024-01-23 17:23:17,989] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.32\n",
      "[2024-01-23 17:23:18,154] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=1.49\n",
      "[2024-01-23 17:23:20,708] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.05\n",
      "[2024-01-23 17:23:20,726] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.07\n",
      "[2024-01-23 17:23:20,769] p1345 {3496713318.py:39} INFO - completed processing chunk 6/8 with concurrency=4\n",
      "[2024-01-23 17:23:20,770] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=7/8\n",
      "[2024-01-23 17:23:20,771] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:20,785] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2213\n",
      "[2024-01-23 17:23:20,787] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2062\n",
      "[2024-01-23 17:23:20,789] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2770\n",
      "[2024-01-23 17:23:20,791] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2608\n",
      "[2024-01-23 17:23:22,040] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=1.24\n",
      "[2024-01-23 17:23:24,707] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.92\n",
      "[2024-01-23 17:23:24,728] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.93\n",
      "[2024-01-23 17:23:24,728] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.93\n",
      "[2024-01-23 17:23:24,775] p1345 {3496713318.py:39} INFO - completed processing chunk 7/8 with concurrency=4\n",
      "[2024-01-23 17:23:24,775] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=8/8\n",
      "[2024-01-23 17:23:24,776] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:24,793] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2613\n",
      "[2024-01-23 17:23:24,798] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2564\n",
      "[2024-01-23 17:23:24,798] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2358\n",
      "[2024-01-23 17:23:24,803] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2333\n",
      "[2024-01-23 17:23:26,060] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=1.25\n",
      "[2024-01-23 17:23:28,980] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.18\n",
      "[2024-01-23 17:23:29,002] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.20\n",
      "[2024-01-23 17:23:29,002] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.20\n",
      "[2024-01-23 17:23:29,049] p1345 {3496713318.py:39} INFO - completed processing chunk 8/8 with concurrency=4\n",
      "[2024-01-23 17:23:29,049] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/15\n",
      "[2024-01-23 17:23:29,049] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:29,066] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3000\n",
      "[2024-01-23 17:23:29,075] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3896\n",
      "[2024-01-23 17:23:29,080] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3450\n",
      "[2024-01-23 17:23:29,083] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3789\n",
      "[2024-01-23 17:23:31,368] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=24, latency=2.30\n",
      "[2024-01-23 17:23:33,918] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.83\n",
      "[2024-01-23 17:23:33,918] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.84\n",
      "[2024-01-23 17:23:33,919] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.84\n",
      "[2024-01-23 17:23:33,965] p1345 {3496713318.py:39} INFO - completed processing chunk 1/15 with concurrency=4\n",
      "[2024-01-23 17:23:33,966] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/15\n",
      "[2024-01-23 17:23:33,967] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:33,982] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3482\n",
      "[2024-01-23 17:23:33,994] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3014\n",
      "[2024-01-23 17:23:33,986] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3144\n",
      "[2024-01-23 17:23:34,000] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3639\n",
      "[2024-01-23 17:23:35,665] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=7, latency=1.67\n",
      "[2024-01-23 17:23:35,848] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=12, latency=1.85\n",
      "[2024-01-23 17:23:38,647] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.65\n",
      "[2024-01-23 17:23:38,647] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.65\n",
      "[2024-01-23 17:23:38,696] p1345 {3496713318.py:39} INFO - completed processing chunk 2/15 with concurrency=4\n",
      "[2024-01-23 17:23:38,696] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/15\n",
      "[2024-01-23 17:23:38,697] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:38,721] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3419\n",
      "[2024-01-23 17:23:38,724] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3891\n",
      "[2024-01-23 17:23:38,727] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3458\n",
      "[2024-01-23 17:23:38,728] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3575\n",
      "[2024-01-23 17:23:40,960] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=20, latency=2.23\n",
      "[2024-01-23 17:23:41,230] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=29, latency=2.50\n",
      "[2024-01-23 17:23:43,363] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.64\n",
      "[2024-01-23 17:23:43,382] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.65\n",
      "[2024-01-23 17:23:43,429] p1345 {3496713318.py:39} INFO - completed processing chunk 3/15 with concurrency=4\n",
      "[2024-01-23 17:23:43,430] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/15\n",
      "[2024-01-23 17:23:43,431] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:43,443] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3508\n",
      "[2024-01-23 17:23:43,451] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3436\n",
      "[2024-01-23 17:23:43,455] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3087\n",
      "[2024-01-23 17:23:43,461] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3249\n",
      "[2024-01-23 17:23:45,413] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=15, latency=1.95\n",
      "[2024-01-23 17:23:45,925] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=32, latency=2.46\n",
      "[2024-01-23 17:23:48,085] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.63\n",
      "[2024-01-23 17:23:48,107] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.65\n",
      "[2024-01-23 17:23:48,151] p1345 {3496713318.py:39} INFO - completed processing chunk 4/15 with concurrency=4\n",
      "[2024-01-23 17:23:48,152] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/15\n",
      "[2024-01-23 17:23:48,153] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:48,167] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3680\n",
      "[2024-01-23 17:23:48,184] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3950\n",
      "[2024-01-23 17:23:48,190] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3996\n",
      "[2024-01-23 17:23:48,190] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3463\n",
      "[2024-01-23 17:23:50,142] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=1.95\n",
      "[2024-01-23 17:23:52,873] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.70\n",
      "[2024-01-23 17:23:52,895] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.71\n",
      "[2024-01-23 17:23:52,896] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.70\n",
      "[2024-01-23 17:23:52,943] p1345 {3496713318.py:39} INFO - completed processing chunk 5/15 with concurrency=4\n",
      "[2024-01-23 17:23:52,943] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/15\n",
      "[2024-01-23 17:23:52,944] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:52,954] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3050\n",
      "[2024-01-23 17:23:52,973] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3979\n",
      "[2024-01-23 17:23:52,977] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3410\n",
      "[2024-01-23 17:23:52,981] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3737\n",
      "[2024-01-23 17:23:57,805] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.84\n",
      "[2024-01-23 17:23:57,830] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.85\n",
      "[2024-01-23 17:23:57,830] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.85\n",
      "[2024-01-23 17:23:57,833] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.86\n",
      "[2024-01-23 17:23:57,884] p1345 {3496713318.py:39} INFO - completed processing chunk 6/15 with concurrency=4\n",
      "[2024-01-23 17:23:57,885] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=7/15\n",
      "[2024-01-23 17:23:57,886] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:23:57,898] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3171\n",
      "[2024-01-23 17:23:57,911] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3098\n",
      "[2024-01-23 17:23:57,913] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3655\n",
      "[2024-01-23 17:23:57,916] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3662\n",
      "[2024-01-23 17:24:00,215] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=27, latency=2.31\n",
      "[2024-01-23 17:24:02,653] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.73\n",
      "[2024-01-23 17:24:02,653] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.73\n",
      "[2024-01-23 17:24:02,655] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.74\n",
      "[2024-01-23 17:24:02,701] p1345 {3496713318.py:39} INFO - completed processing chunk 7/15 with concurrency=4\n",
      "[2024-01-23 17:24:02,701] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=8/15\n",
      "[2024-01-23 17:24:02,702] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:24:02,727] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3400\n",
      "[2024-01-23 17:24:02,732] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3098\n",
      "[2024-01-23 17:24:02,735] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3909\n",
      "[2024-01-23 17:24:02,739] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3704\n",
      "[2024-01-23 17:24:04,395] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=1.67\n",
      "[2024-01-23 17:24:04,547] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=9, latency=1.81\n",
      "[2024-01-23 17:24:07,449] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.71\n",
      "[2024-01-23 17:24:07,449] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.71\n",
      "[2024-01-23 17:24:07,496] p1345 {3496713318.py:39} INFO - completed processing chunk 8/15 with concurrency=4\n",
      "[2024-01-23 17:24:07,496] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=9/15\n",
      "[2024-01-23 17:24:07,497] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:24:07,521] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3030\n",
      "[2024-01-23 17:24:07,524] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3971\n",
      "[2024-01-23 17:24:07,530] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3135\n",
      "[2024-01-23 17:24:07,531] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3132\n",
      "[2024-01-23 17:24:09,125] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=1.60\n",
      "[2024-01-23 17:24:09,308] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=1.78\n",
      "[2024-01-23 17:24:09,508] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=17, latency=1.98\n",
      "[2024-01-23 17:24:11,869] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.33\n",
      "[2024-01-23 17:24:11,915] p1345 {3496713318.py:39} INFO - completed processing chunk 9/15 with concurrency=4\n",
      "[2024-01-23 17:24:11,916] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=10/15\n",
      "[2024-01-23 17:24:11,916] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:24:11,929] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3848\n",
      "[2024-01-23 17:24:11,940] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3208\n",
      "[2024-01-23 17:24:11,949] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3681\n",
      "[2024-01-23 17:24:11,950] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3475\n",
      "[2024-01-23 17:24:13,609] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.67\n",
      "[2024-01-23 17:24:16,776] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.83\n",
      "[2024-01-23 17:24:16,777] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.82\n",
      "[2024-01-23 17:24:16,778] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.82\n",
      "[2024-01-23 17:24:16,828] p1345 {3496713318.py:39} INFO - completed processing chunk 10/15 with concurrency=4\n",
      "[2024-01-23 17:24:16,829] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=11/15\n",
      "[2024-01-23 17:24:16,829] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:24:16,848] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3581\n",
      "[2024-01-23 17:24:16,856] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3537\n",
      "[2024-01-23 17:24:16,865] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3614\n",
      "[2024-01-23 17:24:16,858] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3302\n",
      "[2024-01-23 17:24:21,631] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.77\n",
      "[2024-01-23 17:24:21,656] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.79\n",
      "[2024-01-23 17:24:21,656] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.79\n",
      "[2024-01-23 17:24:21,656] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.79\n",
      "[2024-01-23 17:24:21,707] p1345 {3496713318.py:39} INFO - completed processing chunk 11/15 with concurrency=4\n",
      "[2024-01-23 17:24:21,708] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=12/15\n",
      "[2024-01-23 17:24:21,709] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:24:21,722] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3476\n",
      "[2024-01-23 17:24:21,727] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3165\n",
      "[2024-01-23 17:24:21,742] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3235\n",
      "[2024-01-23 17:24:21,748] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3363\n",
      "[2024-01-23 17:24:23,334] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.59\n",
      "[2024-01-23 17:24:23,597] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=14, latency=1.87\n",
      "[2024-01-23 17:24:26,364] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.62\n",
      "[2024-01-23 17:24:26,364] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.62\n",
      "[2024-01-23 17:24:26,418] p1345 {3496713318.py:39} INFO - completed processing chunk 12/15 with concurrency=4\n",
      "[2024-01-23 17:24:26,419] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=13/15\n",
      "[2024-01-23 17:24:26,419] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:24:26,442] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3616\n",
      "[2024-01-23 17:24:26,449] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3882\n",
      "[2024-01-23 17:24:26,453] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3330\n",
      "[2024-01-23 17:24:26,455] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3567\n",
      "[2024-01-23 17:24:28,132] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.69\n",
      "[2024-01-23 17:24:31,320] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.86\n",
      "[2024-01-23 17:24:31,320] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.87\n",
      "[2024-01-23 17:24:31,320] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.85\n",
      "[2024-01-23 17:24:31,373] p1345 {3496713318.py:39} INFO - completed processing chunk 13/15 with concurrency=4\n",
      "[2024-01-23 17:24:31,374] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=14/15\n",
      "[2024-01-23 17:24:31,375] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:24:31,392] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3566\n",
      "[2024-01-23 17:24:31,400] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3497\n",
      "[2024-01-23 17:24:31,408] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3003\n",
      "[2024-01-23 17:24:31,409] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3783\n",
      "[2024-01-23 17:24:33,403] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=15, latency=1.99\n",
      "[2024-01-23 17:24:35,948] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.55\n",
      "[2024-01-23 17:24:35,969] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.56\n",
      "[2024-01-23 17:24:35,969] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.56\n",
      "[2024-01-23 17:24:36,023] p1345 {3496713318.py:39} INFO - completed processing chunk 14/15 with concurrency=4\n",
      "[2024-01-23 17:24:36,023] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=15/15\n",
      "[2024-01-23 17:24:36,024] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 17:24:36,035] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:24:36,050] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:24:36,057] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:24:36,060] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:24:37,659] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=1.60\n",
      "[2024-01-23 17:24:37,717] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=1.66\n",
      "[2024-01-23 17:24:40,397] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.35\n",
      "[2024-01-23 17:24:40,416] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.37\n",
      "[2024-01-23 17:24:40,460] p1345 {3496713318.py:39} INFO - completed processing chunk 15/15 with concurrency=4\n",
      "[2024-01-23 17:24:40,460] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/1\n",
      "[2024-01-23 17:24:40,461] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:24:40,466] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:24:40,471] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:24:40,474] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:24:40,475] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:24:40,477] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:24:40,477] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:24:43,670] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.19\n",
      "[2024-01-23 17:24:43,695] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.21\n",
      "[2024-01-23 17:24:43,697] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.22\n",
      "[2024-01-23 17:24:43,697] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.21\n",
      "[2024-01-23 17:24:43,697] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.21\n",
      "[2024-01-23 17:24:43,710] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.22\n",
      "[2024-01-23 17:24:43,768] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=6\n",
      "[2024-01-23 17:24:43,768] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/1\n",
      "[2024-01-23 17:24:43,769] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:24:43,778] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:24:43,782] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:24:43,785] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:24:43,785] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:24:43,788] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:24:43,789] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:24:44,527] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.74\n",
      "[2024-01-23 17:24:44,779] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=11, latency=0.98\n",
      "[2024-01-23 17:24:47,368] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.58\n",
      "[2024-01-23 17:24:47,369] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.57\n",
      "[2024-01-23 17:24:47,370] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.58\n",
      "[2024-01-23 17:24:47,370] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.57\n",
      "[2024-01-23 17:24:47,433] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=6\n",
      "[2024-01-23 17:24:47,433] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/3\n",
      "[2024-01-23 17:24:47,434] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:24:47,442] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1932\n",
      "[2024-01-23 17:24:47,450] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1339\n",
      "[2024-01-23 17:24:47,448] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1154\n",
      "[2024-01-23 17:24:47,461] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1746\n",
      "[2024-01-23 17:24:47,461] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1646\n",
      "[2024-01-23 17:24:47,456] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1397\n",
      "[2024-01-23 17:24:51,628] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.18\n",
      "[2024-01-23 17:24:51,651] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.19\n",
      "[2024-01-23 17:24:51,651] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.18\n",
      "[2024-01-23 17:24:51,652] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.19\n",
      "[2024-01-23 17:24:51,653] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.18\n",
      "[2024-01-23 17:24:51,653] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.19\n",
      "[2024-01-23 17:24:51,716] p1345 {3496713318.py:39} INFO - completed processing chunk 1/3 with concurrency=6\n",
      "[2024-01-23 17:24:51,716] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/3\n",
      "[2024-01-23 17:24:51,717] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:24:51,734] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1743\n",
      "[2024-01-23 17:24:51,734] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1373\n",
      "[2024-01-23 17:24:51,734] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1598\n",
      "[2024-01-23 17:24:51,736] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1539\n",
      "[2024-01-23 17:24:51,737] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1695\n",
      "[2024-01-23 17:24:51,742] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1421\n",
      "[2024-01-23 17:24:52,936] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.18\n",
      "[2024-01-23 17:24:52,936] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.19\n",
      "[2024-01-23 17:24:55,832] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.09\n",
      "[2024-01-23 17:24:55,854] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.11\n",
      "[2024-01-23 17:24:55,854] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.10\n",
      "[2024-01-23 17:24:55,856] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.10\n",
      "[2024-01-23 17:24:55,917] p1345 {3496713318.py:39} INFO - completed processing chunk 2/3 with concurrency=6\n",
      "[2024-01-23 17:24:55,918] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/3\n",
      "[2024-01-23 17:24:55,919] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:24:55,937] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1918\n",
      "[2024-01-23 17:24:55,941] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1918\n",
      "[2024-01-23 17:24:55,942] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1918\n",
      "[2024-01-23 17:24:55,943] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1910\n",
      "[2024-01-23 17:24:55,946] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1939\n",
      "[2024-01-23 17:24:55,946] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1918\n",
      "[2024-01-23 17:25:00,357] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.41\n",
      "[2024-01-23 17:25:00,381] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.42\n",
      "[2024-01-23 17:25:00,383] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.43\n",
      "[2024-01-23 17:25:00,383] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.43\n",
      "[2024-01-23 17:25:00,383] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.43\n",
      "[2024-01-23 17:25:00,388] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.43\n",
      "[2024-01-23 17:25:00,451] p1345 {3496713318.py:39} INFO - completed processing chunk 3/3 with concurrency=6\n",
      "[2024-01-23 17:25:00,451] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/6\n",
      "[2024-01-23 17:25:00,452] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:00,468] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3000\n",
      "[2024-01-23 17:25:00,485] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2637\n",
      "[2024-01-23 17:25:00,486] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2148\n",
      "[2024-01-23 17:25:00,486] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2715\n",
      "[2024-01-23 17:25:00,486] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2404\n",
      "[2024-01-23 17:25:00,486] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2150\n",
      "[2024-01-23 17:25:02,477] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=1.98\n",
      "[2024-01-23 17:25:03,161] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=34, latency=2.68\n",
      "[2024-01-23 17:25:05,342] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.85\n",
      "[2024-01-23 17:25:05,342] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.85\n",
      "[2024-01-23 17:25:05,342] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.84\n",
      "[2024-01-23 17:25:05,344] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.84\n",
      "[2024-01-23 17:25:05,413] p1345 {3496713318.py:39} INFO - completed processing chunk 1/6 with concurrency=6\n",
      "[2024-01-23 17:25:05,414] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/6\n",
      "[2024-01-23 17:25:05,414] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:05,428] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2803\n",
      "[2024-01-23 17:25:05,434] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2369\n",
      "[2024-01-23 17:25:05,444] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2186\n",
      "[2024-01-23 17:25:05,451] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2541\n",
      "[2024-01-23 17:25:05,452] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2675\n",
      "[2024-01-23 17:25:05,454] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2775\n",
      "[2024-01-23 17:25:07,498] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=12, latency=2.05\n",
      "[2024-01-23 17:25:08,012] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=28, latency=2.57\n",
      "[2024-01-23 17:25:10,286] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.85\n",
      "[2024-01-23 17:25:10,310] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.87\n",
      "[2024-01-23 17:25:10,311] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.85\n",
      "[2024-01-23 17:25:10,311] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.85\n",
      "[2024-01-23 17:25:10,376] p1345 {3496713318.py:39} INFO - completed processing chunk 2/6 with concurrency=6\n",
      "[2024-01-23 17:25:10,377] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/6\n",
      "[2024-01-23 17:25:10,378] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:10,536] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2686\n",
      "[2024-01-23 17:25:10,536] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2443\n",
      "[2024-01-23 17:25:10,545] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2500\n",
      "[2024-01-23 17:25:10,551] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2321\n",
      "[2024-01-23 17:25:10,556] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2428\n",
      "[2024-01-23 17:25:10,557] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2458\n",
      "[2024-01-23 17:25:12,223] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=5, latency=1.69\n",
      "[2024-01-23 17:25:12,402] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=9, latency=1.86\n",
      "[2024-01-23 17:25:12,495] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=12, latency=1.94\n",
      "[2024-01-23 17:25:12,555] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=14, latency=2.00\n",
      "[2024-01-23 17:25:14,964] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.41\n",
      "[2024-01-23 17:25:14,965] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.41\n",
      "[2024-01-23 17:25:15,034] p1345 {3496713318.py:39} INFO - completed processing chunk 3/6 with concurrency=6\n",
      "[2024-01-23 17:25:15,034] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/6\n",
      "[2024-01-23 17:25:15,035] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:15,062] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2771\n",
      "[2024-01-23 17:25:15,062] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2440\n",
      "[2024-01-23 17:25:15,062] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2101\n",
      "[2024-01-23 17:25:15,071] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2711\n",
      "[2024-01-23 17:25:15,071] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2691\n",
      "[2024-01-23 17:25:15,071] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2624\n",
      "[2024-01-23 17:25:19,999] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.93\n",
      "[2024-01-23 17:25:20,022] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.95\n",
      "[2024-01-23 17:25:20,023] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.94\n",
      "[2024-01-23 17:25:20,024] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.94\n",
      "[2024-01-23 17:25:20,024] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.94\n",
      "[2024-01-23 17:25:20,025] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.95\n",
      "[2024-01-23 17:25:20,092] p1345 {3496713318.py:39} INFO - completed processing chunk 4/6 with concurrency=6\n",
      "[2024-01-23 17:25:20,093] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/6\n",
      "[2024-01-23 17:25:20,093] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:20,102] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2062\n",
      "[2024-01-23 17:25:20,113] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2608\n",
      "[2024-01-23 17:25:20,119] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2213\n",
      "[2024-01-23 17:25:20,132] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2770\n",
      "[2024-01-23 17:25:20,137] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2564\n",
      "[2024-01-23 17:25:20,136] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2333\n",
      "[2024-01-23 17:25:21,884] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=1.75\n",
      "[2024-01-23 17:25:22,227] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=17, latency=2.11\n",
      "[2024-01-23 17:25:24,812] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.69\n",
      "[2024-01-23 17:25:24,836] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.69\n",
      "[2024-01-23 17:25:24,836] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.70\n",
      "[2024-01-23 17:25:24,836] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.71\n",
      "[2024-01-23 17:25:24,901] p1345 {3496713318.py:39} INFO - completed processing chunk 5/6 with concurrency=6\n",
      "[2024-01-23 17:25:24,901] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/6\n",
      "[2024-01-23 17:25:24,902] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:24,925] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2613\n",
      "[2024-01-23 17:25:24,928] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2613\n",
      "[2024-01-23 17:25:24,931] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2358\n",
      "[2024-01-23 17:25:24,935] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2613\n",
      "[2024-01-23 17:25:24,938] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2613\n",
      "[2024-01-23 17:25:24,938] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2613\n",
      "[2024-01-23 17:25:26,726] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.78\n",
      "[2024-01-23 17:25:29,755] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.82\n",
      "[2024-01-23 17:25:29,777] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.83\n",
      "[2024-01-23 17:25:29,778] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.84\n",
      "[2024-01-23 17:25:29,779] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.84\n",
      "[2024-01-23 17:25:29,779] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.83\n",
      "[2024-01-23 17:25:29,851] p1345 {3496713318.py:39} INFO - completed processing chunk 6/6 with concurrency=6\n",
      "[2024-01-23 17:25:29,852] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/10\n",
      "[2024-01-23 17:25:29,852] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:29,883] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3000\n",
      "[2024-01-23 17:25:29,891] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3896\n",
      "[2024-01-23 17:25:29,893] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3450\n",
      "[2024-01-23 17:25:29,896] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3482\n",
      "[2024-01-23 17:25:29,897] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3144\n",
      "[2024-01-23 17:25:29,906] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3789\n",
      "[2024-01-23 17:25:32,555] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=11, latency=2.65\n",
      "[2024-01-23 17:25:35,523] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.62\n",
      "[2024-01-23 17:25:35,523] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.63\n",
      "[2024-01-23 17:25:35,523] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.62\n",
      "[2024-01-23 17:25:35,524] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.63\n",
      "[2024-01-23 17:25:35,524] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.62\n",
      "[2024-01-23 17:25:35,596] p1345 {3496713318.py:39} INFO - completed processing chunk 1/10 with concurrency=6\n",
      "[2024-01-23 17:25:35,596] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/10\n",
      "[2024-01-23 17:25:35,597] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:35,613] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3639\n",
      "[2024-01-23 17:25:35,622] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3891\n",
      "[2024-01-23 17:25:35,634] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3014\n",
      "[2024-01-23 17:25:35,638] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3458\n",
      "[2024-01-23 17:25:35,640] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3419\n",
      "[2024-01-23 17:25:35,646] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3575\n",
      "[2024-01-23 17:25:38,114] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=7, latency=2.49\n",
      "[2024-01-23 17:25:41,264] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.63\n",
      "[2024-01-23 17:25:41,265] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.63\n",
      "[2024-01-23 17:25:41,266] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.62\n",
      "[2024-01-23 17:25:41,266] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.62\n",
      "[2024-01-23 17:25:41,267] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.62\n",
      "[2024-01-23 17:25:41,338] p1345 {3496713318.py:39} INFO - completed processing chunk 2/10 with concurrency=6\n",
      "[2024-01-23 17:25:41,338] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/10\n",
      "[2024-01-23 17:25:41,339] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:41,363] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3508\n",
      "[2024-01-23 17:25:41,364] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3249\n",
      "[2024-01-23 17:25:41,378] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3436\n",
      "[2024-01-23 17:25:41,383] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3087\n",
      "[2024-01-23 17:25:41,385] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3680\n",
      "[2024-01-23 17:25:41,388] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3950\n",
      "[2024-01-23 17:25:44,039] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=12, latency=2.65\n",
      "[2024-01-23 17:25:44,166] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=16, latency=2.80\n",
      "[2024-01-23 17:25:44,197] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=18, latency=2.83\n",
      "[2024-01-23 17:25:46,750] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.36\n",
      "[2024-01-23 17:25:46,750] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.36\n",
      "[2024-01-23 17:25:46,750] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.36\n",
      "[2024-01-23 17:25:46,823] p1345 {3496713318.py:39} INFO - completed processing chunk 3/10 with concurrency=6\n",
      "[2024-01-23 17:25:46,823] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/10\n",
      "[2024-01-23 17:25:46,824] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:46,845] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3050\n",
      "[2024-01-23 17:25:46,863] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3996\n",
      "[2024-01-23 17:25:46,871] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3979\n",
      "[2024-01-23 17:25:46,872] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3463\n",
      "[2024-01-23 17:25:46,871] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3410\n",
      "[2024-01-23 17:25:46,875] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3737\n",
      "[2024-01-23 17:25:49,529] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=2.65\n",
      "[2024-01-23 17:25:52,625] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.75\n",
      "[2024-01-23 17:25:52,626] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.75\n",
      "[2024-01-23 17:25:52,626] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.77\n",
      "[2024-01-23 17:25:52,626] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.74\n",
      "[2024-01-23 17:25:52,627] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.75\n",
      "[2024-01-23 17:25:52,696] p1345 {3496713318.py:39} INFO - completed processing chunk 4/10 with concurrency=6\n",
      "[2024-01-23 17:25:52,697] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/10\n",
      "[2024-01-23 17:25:52,697] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:52,728] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3098\n",
      "[2024-01-23 17:25:52,733] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3662\n",
      "[2024-01-23 17:25:52,738] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3400\n",
      "[2024-01-23 17:25:52,734] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3171\n",
      "[2024-01-23 17:25:52,741] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3655\n",
      "[2024-01-23 17:25:52,743] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3704\n",
      "[2024-01-23 17:25:55,219] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=2.47\n",
      "[2024-01-23 17:25:58,344] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.61\n",
      "[2024-01-23 17:25:58,347] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.61\n",
      "[2024-01-23 17:25:58,348] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.60\n",
      "[2024-01-23 17:25:58,349] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.60\n",
      "[2024-01-23 17:25:58,381] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.63\n",
      "[2024-01-23 17:25:58,451] p1345 {3496713318.py:39} INFO - completed processing chunk 5/10 with concurrency=6\n",
      "[2024-01-23 17:25:58,452] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/10\n",
      "[2024-01-23 17:25:58,453] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:25:58,476] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3909\n",
      "[2024-01-23 17:25:58,481] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3098\n",
      "[2024-01-23 17:25:58,495] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3132\n",
      "[2024-01-23 17:25:58,500] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3030\n",
      "[2024-01-23 17:25:58,502] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3971\n",
      "[2024-01-23 17:25:58,505] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3135\n",
      "[2024-01-23 17:26:01,042] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=2.53\n",
      "[2024-01-23 17:26:03,372] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=81, latency=4.89\n",
      "[2024-01-23 17:26:04,068] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.58\n",
      "[2024-01-23 17:26:04,068] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.56\n",
      "[2024-01-23 17:26:04,068] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.56\n",
      "[2024-01-23 17:26:04,069] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.56\n",
      "[2024-01-23 17:26:04,134] p1345 {3496713318.py:39} INFO - completed processing chunk 6/10 with concurrency=6\n",
      "[2024-01-23 17:26:04,134] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=7/10\n",
      "[2024-01-23 17:26:04,135] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:26:04,153] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3208\n",
      "[2024-01-23 17:26:04,174] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3302\n",
      "[2024-01-23 17:26:04,179] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3681\n",
      "[2024-01-23 17:26:04,179] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3848\n",
      "[2024-01-23 17:26:04,180] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3581\n",
      "[2024-01-23 17:26:04,180] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3475\n",
      "[2024-01-23 17:26:06,626] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=2.44\n",
      "[2024-01-23 17:26:06,692] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=2.51\n",
      "[2024-01-23 17:26:06,692] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=2.50\n",
      "[2024-01-23 17:26:09,740] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.55\n",
      "[2024-01-23 17:26:09,741] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.58\n",
      "[2024-01-23 17:26:09,742] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.56\n",
      "[2024-01-23 17:26:09,809] p1345 {3496713318.py:39} INFO - completed processing chunk 7/10 with concurrency=6\n",
      "[2024-01-23 17:26:09,810] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=8/10\n",
      "[2024-01-23 17:26:09,810] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:26:09,825] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3614\n",
      "[2024-01-23 17:26:09,853] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3537\n",
      "[2024-01-23 17:26:09,858] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3476\n",
      "[2024-01-23 17:26:09,858] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3235\n",
      "[2024-01-23 17:26:09,859] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3363\n",
      "[2024-01-23 17:26:09,859] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3165\n",
      "[2024-01-23 17:26:12,164] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=2.32\n",
      "[2024-01-23 17:26:12,197] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=2.34\n",
      "[2024-01-23 17:26:12,197] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=2.33\n",
      "[2024-01-23 17:26:12,228] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=5, latency=2.36\n",
      "[2024-01-23 17:26:12,239] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=2.37\n",
      "[2024-01-23 17:26:14,950] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.10\n",
      "[2024-01-23 17:26:15,019] p1345 {3496713318.py:39} INFO - completed processing chunk 8/10 with concurrency=6\n",
      "[2024-01-23 17:26:15,019] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=9/10\n",
      "[2024-01-23 17:26:15,020] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:26:15,054] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3616\n",
      "[2024-01-23 17:26:15,056] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3567\n",
      "[2024-01-23 17:26:15,061] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3882\n",
      "[2024-01-23 17:26:15,068] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3497\n",
      "[2024-01-23 17:26:15,069] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3330\n",
      "[2024-01-23 17:26:15,069] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3566\n",
      "[2024-01-23 17:26:17,608] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=2.53\n",
      "[2024-01-23 17:26:17,851] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=15, latency=2.79\n",
      "[2024-01-23 17:26:17,851] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=14, latency=2.78\n",
      "[2024-01-23 17:26:17,947] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=17, latency=2.88\n",
      "[2024-01-23 17:26:20,640] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.56\n",
      "[2024-01-23 17:26:20,641] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.57\n",
      "[2024-01-23 17:26:20,708] p1345 {3496713318.py:39} INFO - completed processing chunk 9/10 with concurrency=6\n",
      "[2024-01-23 17:26:20,708] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=10/10\n",
      "[2024-01-23 17:26:20,709] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 17:26:20,735] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3783\n",
      "[2024-01-23 17:26:20,740] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3003\n",
      "[2024-01-23 17:26:20,746] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3003\n",
      "[2024-01-23 17:26:20,746] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3003\n",
      "[2024-01-23 17:26:20,747] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:26:20,751] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3003\n",
      "[2024-01-23 17:26:22,908] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=2.15\n",
      "[2024-01-23 17:26:22,972] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=2.22\n",
      "[2024-01-23 17:26:22,984] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=7, latency=2.24\n",
      "[2024-01-23 17:26:22,984] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=2.25\n",
      "[2024-01-23 17:26:25,923] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.16\n",
      "[2024-01-23 17:26:25,924] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.16\n",
      "[2024-01-23 17:26:25,995] p1345 {3496713318.py:39} INFO - completed processing chunk 10/10 with concurrency=6\n",
      "[2024-01-23 17:26:25,996] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/1\n",
      "[2024-01-23 17:26:25,996] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:26:26,007] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:26:26,010] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:26:26,011] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:26:26,013] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:26:26,014] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:26:26,014] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:26:29,155] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.14\n",
      "[2024-01-23 17:26:29,160] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:26:29,176] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.15\n",
      "[2024-01-23 17:26:29,182] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=304\n",
      "[2024-01-23 17:26:29,178] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.15\n",
      "[2024-01-23 17:26:29,179] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.16\n",
      "[2024-01-23 17:26:29,178] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.16\n",
      "[2024-01-23 17:26:29,183] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.16\n",
      "[2024-01-23 17:26:31,778] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.62\n",
      "[2024-01-23 17:26:31,794] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.61\n",
      "[2024-01-23 17:26:31,881] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=8\n",
      "[2024-01-23 17:26:31,881] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/1\n",
      "[2024-01-23 17:26:31,882] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:26:31,894] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:26:31,896] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:26:31,897] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:26:31,897] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:26:31,901] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:26:31,903] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:26:33,005] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=14, latency=1.09\n",
      "[2024-01-23 17:26:33,010] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:26:35,752] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.85\n",
      "[2024-01-23 17:26:35,756] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=980\n",
      "[2024-01-23 17:26:35,777] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.87\n",
      "[2024-01-23 17:26:35,778] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.87\n",
      "[2024-01-23 17:26:35,779] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.87\n",
      "[2024-01-23 17:26:35,779] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.87\n",
      "[2024-01-23 17:26:36,299] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.29\n",
      "[2024-01-23 17:26:38,465] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.71\n",
      "[2024-01-23 17:26:38,549] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=8\n",
      "[2024-01-23 17:26:38,550] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/2\n",
      "[2024-01-23 17:26:38,550] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:26:38,559] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1339\n",
      "[2024-01-23 17:26:38,566] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1932\n",
      "[2024-01-23 17:26:38,568] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1397\n",
      "[2024-01-23 17:26:38,570] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1646\n",
      "[2024-01-23 17:26:38,571] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1154\n",
      "[2024-01-23 17:26:38,586] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1746\n",
      "[2024-01-23 17:26:39,744] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.16\n",
      "[2024-01-23 17:26:39,750] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1373\n",
      "[2024-01-23 17:26:42,945] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.37\n",
      "[2024-01-23 17:26:42,951] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1598\n",
      "[2024-01-23 17:26:42,973] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.40\n",
      "[2024-01-23 17:26:42,973] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.39\n",
      "[2024-01-23 17:26:42,973] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.39\n",
      "[2024-01-23 17:26:42,973] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.40\n",
      "[2024-01-23 17:26:43,303] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.55\n",
      "[2024-01-23 17:26:45,798] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=2.85\n",
      "[2024-01-23 17:26:45,886] p1345 {3496713318.py:39} INFO - completed processing chunk 1/2 with concurrency=8\n",
      "[2024-01-23 17:26:45,887] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/2\n",
      "[2024-01-23 17:26:45,888] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:26:45,906] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1539\n",
      "[2024-01-23 17:26:45,907] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1421\n",
      "[2024-01-23 17:26:45,912] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1743\n",
      "[2024-01-23 17:26:45,914] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1910\n",
      "[2024-01-23 17:26:45,911] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1695\n",
      "[2024-01-23 17:26:45,915] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1918\n",
      "[2024-01-23 17:26:47,232] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=1.30\n",
      "[2024-01-23 17:26:47,239] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1939\n",
      "[2024-01-23 17:26:47,262] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=7, latency=1.34\n",
      "[2024-01-23 17:26:47,269] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=1743\n",
      "[2024-01-23 17:26:47,895] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=12, latency=1.98\n",
      "[2024-01-23 17:26:50,639] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.71\n",
      "[2024-01-23 17:26:50,641] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.72\n",
      "[2024-01-23 17:26:50,641] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.72\n",
      "[2024-01-23 17:26:50,774] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.53\n",
      "[2024-01-23 17:26:50,791] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.52\n",
      "[2024-01-23 17:26:50,870] p1345 {3496713318.py:39} INFO - completed processing chunk 2/2 with concurrency=8\n",
      "[2024-01-23 17:26:50,871] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/4\n",
      "[2024-01-23 17:26:50,871] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:26:50,893] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2148\n",
      "[2024-01-23 17:26:50,894] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2150\n",
      "[2024-01-23 17:26:50,900] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3000\n",
      "[2024-01-23 17:26:50,900] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2404\n",
      "[2024-01-23 17:26:50,901] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2637\n",
      "[2024-01-23 17:26:50,901] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2715\n",
      "[2024-01-23 17:26:52,865] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=9, latency=1.96\n",
      "[2024-01-23 17:26:52,873] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2803\n",
      "[2024-01-23 17:26:53,849] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=30, latency=2.94\n",
      "[2024-01-23 17:26:53,857] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2369\n",
      "[2024-01-23 17:26:56,458] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.55\n",
      "[2024-01-23 17:26:56,490] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.58\n",
      "[2024-01-23 17:26:56,490] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.58\n",
      "[2024-01-23 17:26:56,490] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.58\n",
      "[2024-01-23 17:26:56,768] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.89\n",
      "[2024-01-23 17:26:57,276] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=3.42\n",
      "[2024-01-23 17:26:57,360] p1345 {3496713318.py:39} INFO - completed processing chunk 1/4 with concurrency=8\n",
      "[2024-01-23 17:26:57,360] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/4\n",
      "[2024-01-23 17:26:57,361] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:26:57,374] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2675\n",
      "[2024-01-23 17:26:57,386] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2541\n",
      "[2024-01-23 17:26:57,389] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2186\n",
      "[2024-01-23 17:26:57,390] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2775\n",
      "[2024-01-23 17:26:57,399] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2686\n",
      "[2024-01-23 17:26:57,401] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2500\n",
      "[2024-01-23 17:26:59,130] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.74\n",
      "[2024-01-23 17:26:59,138] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2443\n",
      "[2024-01-23 17:27:02,643] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.24\n",
      "[2024-01-23 17:27:02,647] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.25\n",
      "[2024-01-23 17:27:02,647] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.25\n",
      "[2024-01-23 17:27:02,647] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.24\n",
      "[2024-01-23 17:27:02,649] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.24\n",
      "[2024-01-23 17:27:02,658] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2321\n",
      "[2024-01-23 17:27:02,973] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.83\n",
      "[2024-01-23 17:27:05,639] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=2.98\n",
      "[2024-01-23 17:27:05,719] p1345 {3496713318.py:39} INFO - completed processing chunk 2/4 with concurrency=8\n",
      "[2024-01-23 17:27:05,720] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/4\n",
      "[2024-01-23 17:27:05,721] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:27:05,735] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2428\n",
      "[2024-01-23 17:27:05,746] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2101\n",
      "[2024-01-23 17:27:05,754] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2711\n",
      "[2024-01-23 17:27:05,755] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2458\n",
      "[2024-01-23 17:27:05,755] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2440\n",
      "[2024-01-23 17:27:05,765] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2771\n",
      "[2024-01-23 17:27:07,698] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=11, latency=1.93\n",
      "[2024-01-23 17:27:07,707] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2691\n",
      "[2024-01-23 17:27:10,925] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.19\n",
      "[2024-01-23 17:27:10,933] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2624\n",
      "[2024-01-23 17:27:10,954] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.19\n",
      "[2024-01-23 17:27:10,954] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.20\n",
      "[2024-01-23 17:27:10,955] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.20\n",
      "[2024-01-23 17:27:10,955] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.20\n",
      "[2024-01-23 17:27:11,508] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=0.57\n",
      "[2024-01-23 17:27:11,525] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.82\n",
      "[2024-01-23 17:27:11,605] p1345 {3496713318.py:39} INFO - completed processing chunk 3/4 with concurrency=8\n",
      "[2024-01-23 17:27:11,606] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/4\n",
      "[2024-01-23 17:27:11,606] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:27:11,615] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2062\n",
      "[2024-01-23 17:27:11,624] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2213\n",
      "[2024-01-23 17:27:11,636] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2608\n",
      "[2024-01-23 17:27:11,644] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2333\n",
      "[2024-01-23 17:27:11,635] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2770\n",
      "[2024-01-23 17:27:11,648] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2564\n",
      "[2024-01-23 17:27:13,334] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=1.68\n",
      "[2024-01-23 17:27:13,342] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2613\n",
      "[2024-01-23 17:27:13,397] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=1.76\n",
      "[2024-01-23 17:27:13,404] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=2358\n",
      "[2024-01-23 17:27:13,747] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=7, latency=2.10\n",
      "[2024-01-23 17:27:14,155] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=2.51\n",
      "[2024-01-23 17:27:14,310] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=0.90\n",
      "[2024-01-23 17:27:16,899] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.26\n",
      "[2024-01-23 17:27:16,927] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.29\n",
      "[2024-01-23 17:27:17,029] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.69\n",
      "[2024-01-23 17:27:17,114] p1345 {3496713318.py:39} INFO - completed processing chunk 4/4 with concurrency=8\n",
      "[2024-01-23 17:27:17,115] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=1/8\n",
      "[2024-01-23 17:27:17,115] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:27:17,139] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3896\n",
      "[2024-01-23 17:27:17,151] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3000\n",
      "[2024-01-23 17:27:17,142] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3789\n",
      "[2024-01-23 17:27:17,161] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3450\n",
      "[2024-01-23 17:27:17,164] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3144\n",
      "[2024-01-23 17:27:17,165] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3482\n",
      "[2024-01-23 17:27:19,616] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=5, latency=2.45\n",
      "[2024-01-23 17:27:19,626] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3639\n",
      "[2024-01-23 17:27:22,717] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=85, latency=5.56\n",
      "[2024-01-23 17:27:22,726] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3014\n",
      "[2024-01-23 17:27:23,252] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.53\n",
      "[2024-01-23 17:27:23,548] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.40\n",
      "[2024-01-23 17:27:23,577] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.43\n",
      "[2024-01-23 17:27:23,577] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=6.41\n",
      "[2024-01-23 17:27:23,578] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.41\n",
      "[2024-01-23 17:27:23,741] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.11\n",
      "[2024-01-23 17:27:23,832] p1345 {3496713318.py:39} INFO - completed processing chunk 1/8 with concurrency=8\n",
      "[2024-01-23 17:27:23,832] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=2/8\n",
      "[2024-01-23 17:27:23,833] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:27:23,859] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3891\n",
      "[2024-01-23 17:27:23,870] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3419\n",
      "[2024-01-23 17:27:23,880] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3458\n",
      "[2024-01-23 17:27:23,886] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3575\n",
      "[2024-01-23 17:27:23,886] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3508\n",
      "[2024-01-23 17:27:23,886] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3436\n",
      "[2024-01-23 17:27:26,264] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=2.40\n",
      "[2024-01-23 17:27:26,274] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3249\n",
      "[2024-01-23 17:27:26,983] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=9, latency=0.71\n",
      "[2024-01-23 17:27:26,992] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3087\n",
      "[2024-01-23 17:27:30,294] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.40\n",
      "[2024-01-23 17:27:30,295] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.41\n",
      "[2024-01-23 17:27:30,295] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.40\n",
      "[2024-01-23 17:27:30,296] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.43\n",
      "[2024-01-23 17:27:30,296] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.40\n",
      "[2024-01-23 17:27:30,650] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.66\n",
      "[2024-01-23 17:27:30,738] p1345 {3496713318.py:39} INFO - completed processing chunk 2/8 with concurrency=8\n",
      "[2024-01-23 17:27:30,738] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=3/8\n",
      "[2024-01-23 17:27:30,739] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:27:30,771] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3050\n",
      "[2024-01-23 17:27:30,777] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3463\n",
      "[2024-01-23 17:27:30,781] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3950\n",
      "[2024-01-23 17:27:30,789] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3680\n",
      "[2024-01-23 17:27:30,794] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3737\n",
      "[2024-01-23 17:27:30,797] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3996\n",
      "[2024-01-23 17:27:33,577] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=12, latency=2.80\n",
      "[2024-01-23 17:27:33,586] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3410\n",
      "[2024-01-23 17:27:35,184] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=47, latency=4.39\n",
      "[2024-01-23 17:27:35,195] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3979\n",
      "[2024-01-23 17:27:37,515] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=6.73\n",
      "[2024-01-23 17:27:37,515] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=6.72\n",
      "[2024-01-23 17:27:37,516] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.73\n",
      "[2024-01-23 17:27:37,517] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.73\n",
      "[2024-01-23 17:27:37,842] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=4.25\n",
      "[2024-01-23 17:27:38,855] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=100, latency=3.66\n",
      "[2024-01-23 17:27:38,936] p1345 {3496713318.py:39} INFO - completed processing chunk 3/8 with concurrency=8\n",
      "[2024-01-23 17:27:38,937] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=4/8\n",
      "[2024-01-23 17:27:38,937] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:27:38,954] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3655\n",
      "[2024-01-23 17:27:38,963] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3098\n",
      "[2024-01-23 17:27:38,969] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3704\n",
      "[2024-01-23 17:27:38,980] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3171\n",
      "[2024-01-23 17:27:38,986] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3662\n",
      "[2024-01-23 17:27:38,987] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3400\n",
      "[2024-01-23 17:27:44,604] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.62\n",
      "[2024-01-23 17:27:44,604] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.62\n",
      "[2024-01-23 17:27:44,604] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.63\n",
      "[2024-01-23 17:27:44,604] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.63\n",
      "[2024-01-23 17:27:44,605] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=5.63\n",
      "[2024-01-23 17:27:44,606] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=5.62\n",
      "[2024-01-23 17:27:44,625] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3098\n",
      "[2024-01-23 17:27:44,634] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3909\n",
      "[2024-01-23 17:27:45,617] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=0.99\n",
      "[2024-01-23 17:27:45,617] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=6, latency=0.98\n",
      "[2024-01-23 17:27:45,703] p1345 {3496713318.py:39} INFO - completed processing chunk 4/8 with concurrency=8\n",
      "[2024-01-23 17:27:45,703] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=5/8\n",
      "[2024-01-23 17:27:45,704] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:27:45,728] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3135\n",
      "[2024-01-23 17:27:45,740] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3030\n",
      "[2024-01-23 17:27:45,742] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3132\n",
      "[2024-01-23 17:27:45,744] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3208\n",
      "[2024-01-23 17:27:45,744] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3971\n",
      "[2024-01-23 17:27:45,756] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3848\n",
      "[2024-01-23 17:27:49,145] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=35, latency=3.40\n",
      "[2024-01-23 17:27:49,155] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3475\n",
      "[2024-01-23 17:27:49,697] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=0.54\n",
      "[2024-01-23 17:27:49,708] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3681\n",
      "[2024-01-23 17:27:50,357] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=7, latency=0.65\n",
      "[2024-01-23 17:27:52,148] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.41\n",
      "[2024-01-23 17:27:52,149] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.39\n",
      "[2024-01-23 17:27:52,150] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.40\n",
      "[2024-01-23 17:27:52,151] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=101, latency=6.40\n",
      "[2024-01-23 17:27:52,151] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.41\n",
      "[2024-01-23 17:27:52,234] p1345 {3496713318.py:39} INFO - completed processing chunk 5/8 with concurrency=8\n",
      "[2024-01-23 17:27:52,235] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=6/8\n",
      "[2024-01-23 17:27:52,236] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:27:52,250] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3581\n",
      "[2024-01-23 17:27:52,264] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3614\n",
      "[2024-01-23 17:27:52,266] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3302\n",
      "[2024-01-23 17:27:52,276] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3537\n",
      "[2024-01-23 17:27:52,280] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3165\n",
      "[2024-01-23 17:27:52,288] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3476\n",
      "[2024-01-23 17:27:54,656] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=2.37\n",
      "[2024-01-23 17:27:54,666] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3235\n",
      "[2024-01-23 17:27:55,242] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=9, latency=2.96\n",
      "[2024-01-23 17:27:55,252] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3363\n",
      "[2024-01-23 17:27:55,812] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=10, latency=1.15\n",
      "[2024-01-23 17:27:58,691] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.41\n",
      "[2024-01-23 17:27:58,720] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.43\n",
      "[2024-01-23 17:27:58,721] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.43\n",
      "[2024-01-23 17:27:58,721] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.43\n",
      "[2024-01-23 17:27:58,940] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.69\n",
      "[2024-01-23 17:27:59,022] p1345 {3496713318.py:39} INFO - completed processing chunk 6/8 with concurrency=8\n",
      "[2024-01-23 17:27:59,023] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=7/8\n",
      "[2024-01-23 17:27:59,024] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:27:59,047] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3616\n",
      "[2024-01-23 17:27:59,056] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3567\n",
      "[2024-01-23 17:27:59,060] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3330\n",
      "[2024-01-23 17:27:59,063] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3882\n",
      "[2024-01-23 17:27:59,067] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3497\n",
      "[2024-01-23 17:27:59,072] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3566\n",
      "[2024-01-23 17:28:01,561] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=2.48\n",
      "[2024-01-23 17:28:01,570] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3003\n",
      "[2024-01-23 17:28:02,130] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=9, latency=3.05\n",
      "[2024-01-23 17:28:02,140] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3783\n",
      "[2024-01-23 17:28:02,859] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=9, latency=0.72\n",
      "[2024-01-23 17:28:05,585] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.52\n",
      "[2024-01-23 17:28:05,614] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.54\n",
      "[2024-01-23 17:28:05,614] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.53\n",
      "[2024-01-23 17:28:05,617] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.54\n",
      "[2024-01-23 17:28:05,688] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=4.12\n",
      "[2024-01-23 17:28:05,773] p1345 {3496713318.py:39} INFO - completed processing chunk 7/8 with concurrency=8\n",
      "[2024-01-23 17:28:05,774] p1345 {3496713318.py:26} INFO - e_idx=2/2, chunk_index=8/8\n",
      "[2024-01-23 17:28:05,774] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 17:28:05,804] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:28:05,808] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:28:05,810] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:28:05,825] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:28:05,825] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:28:05,827] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:28:08,010] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=2.20\n",
      "[2024-01-23 17:28:08,021] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:28:08,064] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=4, latency=2.23\n",
      "[2024-01-23 17:28:08,073] p1345 {701838357.py:23} INFO - get_inference, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, prompt_tokens=3271\n",
      "[2024-01-23 17:28:09,118] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=8, latency=1.04\n",
      "[2024-01-23 17:28:09,508] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=25, latency=3.68\n",
      "[2024-01-23 17:28:10,427] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=54, latency=4.61\n",
      "[2024-01-23 17:28:11,945] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.11\n",
      "[2024-01-23 17:28:11,945] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=6.13\n",
      "[2024-01-23 17:28:11,993] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=llama2-70bdjl-2024-01-23-15-06-05-901-endpoint, completion_tokens=102, latency=3.97\n",
      "[2024-01-23 17:28:12,079] p1345 {3496713318.py:39} INFO - completed processing chunk 8/8 with concurrency=8\n",
      "[2024-01-23 17:28:12,080] p1345 {3496713318.py:41} INFO - experiment=2/2, name=llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122, done\n"
     ]
    }
   ],
   "source": [
    "# for each experiment\n",
    "#   - for each endpoint and concurrency in an experiment\n",
    "\n",
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "_ = list(map(clear_dir, [METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]))\n",
    "\n",
    "num_experiments: int = len(config['experiments'])\n",
    "for e_idx, experiment in enumerate(config['experiments']):\n",
    "    e_idx += 1  # Increment experiment index\n",
    "    # Call do_experiment function to create the predictor object\n",
    " \n",
    "    predictor = create_predictor_for_experiment(experiment, config, endpoint_info_list)\n",
    "    if predictor is None:\n",
    "        logger.error(f\"predictor could not be created for experiment={experiment}, moving to next...\")\n",
    "        continue\n",
    "\n",
    "    # Process combinations of concurrency levels and payload files\n",
    "    combination_data = create_combinations(experiment)\n",
    "\n",
    "    for concurrency, payload_file, split_payload in combination_data:\n",
    "        for chunk_index, chunk in enumerate(split_payload):\n",
    "            logger.info(f\"e_idx={e_idx}/{num_experiments}, chunk_index={chunk_index+1}/{len(split_payload)}\")\n",
    "\n",
    "            # Process each chunk and calculate metrics\n",
    "            responses, metrics = await run_inferences(predictor, chunk, experiment, concurrency, payload_file)\n",
    "            if metrics:\n",
    "                #per_concurrency_level_response_metrics.append(metrics)\n",
    "                fpath: str = os.path.join(METRICS_PER_CHUNK_DIR, f\"{time.time()}.json\")\n",
    "                Path(fpath).write_text(json.dumps(metrics, indent=2))\n",
    "            if responses:\n",
    "                for r in responses:\n",
    "                    fpath: str = os.path.join(METRICS_PER_INFERENCE_DIR, f\"{time.time()}.json\")\n",
    "                    Path(fpath).write_text(json.dumps(r, indent=2))\n",
    "            \n",
    "            logger.info(f\"completed processing chunk {chunk_index+1}/{len(split_payload)} with concurrency={concurrency}\")\n",
    "\n",
    "    logger.info(f\"experiment={e_idx}/{num_experiments}, name={experiment['name']}, done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:28:14,262] p1345 {3838589563.py:4} INFO - created dataframe of shape (1172, 14) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>truncate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n&lt;/s&gt;</td>\n",
       "      <td>2675</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.737736</td>\n",
       "      <td>llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n\\n[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for...</td>\n",
       "      <td>3132</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.169297</td>\n",
       "      <td>llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-2-70b-g5-48xlarge-1706022365</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n```\\n&lt;answer&gt;\\nPassage 1:\\n\\n```\\n\\n```\\nP...</td>\n",
       "      <td>3436</td>\n",
       "      <td>102.0</td>\n",
       "      <td>17.781332</td>\n",
       "      <td>llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...</td>\n",
       "      <td>1</td>\n",
       "      <td>3436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>3003</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.138371</td>\n",
       "      <td>llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-2-70b-g5-48xlarge-1706022365</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n```\\n\\n[/INST]\\nAnswer:\\n```\\n\\n[INST] &lt;&lt;SYS...</td>\n",
       "      <td>3614</td>\n",
       "      <td>102.0</td>\n",
       "      <td>18.645712</td>\n",
       "      <td>llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...</td>\n",
       "      <td>1</td>\n",
       "      <td>3614.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    endpoint_name  \\\n",
       "0  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "1  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "2              llama-2-70b-g5-48xlarge-1706022365   \n",
       "3  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "4              llama-2-70b-g5-48xlarge-1706022365   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  \\\n",
       "0   0.92    120             100   \n",
       "1   0.92    120             100   \n",
       "2   0.92    120             100   \n",
       "3   0.92    120             100   \n",
       "4   0.92    120             100   \n",
       "\n",
       "                                          completion  prompt_tokens  \\\n",
       "0                                             \\n</s>           2675   \n",
       "1  \\n\\n\\n[INST] <<SYS>>\\nYou are an assistant for...           3132   \n",
       "2  \\n\\n```\\n<answer>\\nPassage 1:\\n\\n```\\n\\n```\\nP...           3436   \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...           3003   \n",
       "4  \\n```\\n\\n[/INST]\\nAnswer:\\n```\\n\\n[INST] <<SYS...           3614   \n",
       "\n",
       "   completion_tokens    latency  \\\n",
       "0                4.0   1.737736   \n",
       "1              102.0   3.169297   \n",
       "2              102.0  17.781332   \n",
       "3              102.0   3.138371   \n",
       "4              102.0  18.645712   \n",
       "\n",
       "                                     experiment_name  concurrency  truncate  \n",
       "0  llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...            8       NaN  \n",
       "1  llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...            1       NaN  \n",
       "2  llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...            1    3436.0  \n",
       "3  llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...            1       NaN  \n",
       "4  llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...            1    3614.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all per chunk files\n",
    "json_list: List[Dict] = [json.loads(Path(f_name).read_text()) for f_name in glob.glob(os.path.join(METRICS_PER_INFERENCE_DIR, \"*.json\"))]\n",
    "df_responses = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all responses\")\n",
    "df_responses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:28:15,578] p1345 {1971204328.py:4} INFO - created dataframe of shape (1172, 14) from all inference responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>truncate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n&lt;/s&gt;</td>\n",
       "      <td>2675</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.737736</td>\n",
       "      <td>llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n\\n[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for...</td>\n",
       "      <td>3132</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.169297</td>\n",
       "      <td>llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-2-70b-g5-48xlarge-1706022365</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n```\\n&lt;answer&gt;\\nPassage 1:\\n\\n```\\n\\n```\\nP...</td>\n",
       "      <td>3436</td>\n",
       "      <td>102.0</td>\n",
       "      <td>17.781332</td>\n",
       "      <td>llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...</td>\n",
       "      <td>1</td>\n",
       "      <td>3436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>3003</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.138371</td>\n",
       "      <td>llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-2-70b-g5-48xlarge-1706022365</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n```\\n\\n[/INST]\\nAnswer:\\n```\\n\\n[INST] &lt;&lt;SYS...</td>\n",
       "      <td>3614</td>\n",
       "      <td>102.0</td>\n",
       "      <td>18.645712</td>\n",
       "      <td>llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...</td>\n",
       "      <td>1</td>\n",
       "      <td>3614.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    endpoint_name  \\\n",
       "0  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "1  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "2              llama-2-70b-g5-48xlarge-1706022365   \n",
       "3  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "4              llama-2-70b-g5-48xlarge-1706022365   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  \\\n",
       "0   0.92    120             100   \n",
       "1   0.92    120             100   \n",
       "2   0.92    120             100   \n",
       "3   0.92    120             100   \n",
       "4   0.92    120             100   \n",
       "\n",
       "                                          completion  prompt_tokens  \\\n",
       "0                                             \\n</s>           2675   \n",
       "1  \\n\\n\\n[INST] <<SYS>>\\nYou are an assistant for...           3132   \n",
       "2  \\n\\n```\\n<answer>\\nPassage 1:\\n\\n```\\n\\n```\\nP...           3436   \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...           3003   \n",
       "4  \\n```\\n\\n[/INST]\\nAnswer:\\n```\\n\\n[INST] <<SYS...           3614   \n",
       "\n",
       "   completion_tokens    latency  \\\n",
       "0                4.0   1.737736   \n",
       "1              102.0   3.169297   \n",
       "2              102.0  17.781332   \n",
       "3              102.0   3.138371   \n",
       "4              102.0  18.645712   \n",
       "\n",
       "                                     experiment_name  concurrency  truncate  \n",
       "0  llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...            8       NaN  \n",
       "1  llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...            1       NaN  \n",
       "2  llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...            1    3436.0  \n",
       "3  llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...            1       NaN  \n",
       "4  llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...            1    3614.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all per inference files\n",
    "json_list: List[Dict] = [json.loads(Path(f_name).read_text()) for f_name in glob.glob(os.path.join(METRICS_PER_INFERENCE_DIR, \"*.json\"))]\n",
    "df_responses = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all inference responses\")\n",
    "df_responses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>truncate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [endpoint_name, prompt, do_sample, temperature, top_p, top_k, max_new_tokens, completion, prompt_tokens, completion_tokens, latency, experiment_name, concurrency, truncate]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses[df_responses.endpoint_name.str.contains(\"inf2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:28:16,393] p1345 {976317414.py:4} INFO - created dataframe of shape (454, 16) from all chunk responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>errors</th>\n",
       "      <th>successes</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>all_prompts_token_count</th>\n",
       "      <th>prompt_token_count_mean</th>\n",
       "      <th>prompt_token_throughput</th>\n",
       "      <th>all_completions_token_count</th>\n",
       "      <th>completion_token_count_mean</th>\n",
       "      <th>completion_token_throughput</th>\n",
       "      <th>transactions</th>\n",
       "      <th>transactions_per_second</th>\n",
       "      <th>transactions_per_minute</th>\n",
       "      <th>latency_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3662</td>\n",
       "      <td>3662.0</td>\n",
       "      <td>1110.16</td>\n",
       "      <td>102</td>\n",
       "      <td>102.0</td>\n",
       "      <td>30.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>18</td>\n",
       "      <td>3.285360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3475</td>\n",
       "      <td>3475.0</td>\n",
       "      <td>1064.98</td>\n",
       "      <td>102</td>\n",
       "      <td>102.0</td>\n",
       "      <td>31.26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.31</td>\n",
       "      <td>18</td>\n",
       "      <td>3.251735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1598</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>145.32</td>\n",
       "      <td>102</td>\n",
       "      <td>102.0</td>\n",
       "      <td>9.28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5</td>\n",
       "      <td>10.988445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...</td>\n",
       "      <td>2</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7389</td>\n",
       "      <td>3694.5</td>\n",
       "      <td>246.81</td>\n",
       "      <td>113</td>\n",
       "      <td>56.5</td>\n",
       "      <td>3.77</td>\n",
       "      <td>2</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4</td>\n",
       "      <td>26.394050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...</td>\n",
       "      <td>2</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6877</td>\n",
       "      <td>3438.5</td>\n",
       "      <td>1829.69</td>\n",
       "      <td>203</td>\n",
       "      <td>101.5</td>\n",
       "      <td>54.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.53</td>\n",
       "      <td>31</td>\n",
       "      <td>3.731961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     experiment_name  concurrency  \\\n",
       "0  llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...            1   \n",
       "1  llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...            1   \n",
       "2  llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...            1   \n",
       "3  llama2-70b-g5.48xlarge-huggingface-pytorch-tgi...            2   \n",
       "4  llama2-70b-chat-p4d.24xlarge-djl-inference-0.2...            2   \n",
       "\n",
       "                 payload_file errors  successes  error_rate  \\\n",
       "0  payload_en_3000-4000.jsonl     []          1         0.0   \n",
       "1  payload_en_3000-4000.jsonl     []          1         0.0   \n",
       "2  payload_en_1000-2000.jsonl     []          1         0.0   \n",
       "3  payload_en_3000-4000.jsonl     []          2         0.0   \n",
       "4  payload_en_3000-4000.jsonl     []          2         0.0   \n",
       "\n",
       "   all_prompts_token_count  prompt_token_count_mean  prompt_token_throughput  \\\n",
       "0                     3662                   3662.0                  1110.16   \n",
       "1                     3475                   3475.0                  1064.98   \n",
       "2                     1598                   1598.0                   145.32   \n",
       "3                     7389                   3694.5                   246.81   \n",
       "4                     6877                   3438.5                  1829.69   \n",
       "\n",
       "   all_completions_token_count  completion_token_count_mean  \\\n",
       "0                          102                        102.0   \n",
       "1                          102                        102.0   \n",
       "2                          102                        102.0   \n",
       "3                          113                         56.5   \n",
       "4                          203                        101.5   \n",
       "\n",
       "   completion_token_throughput  transactions  transactions_per_second  \\\n",
       "0                        30.92             1                     0.30   \n",
       "1                        31.26             1                     0.31   \n",
       "2                         9.28             1                     0.09   \n",
       "3                         3.77             2                     0.07   \n",
       "4                        54.01             2                     0.53   \n",
       "\n",
       "   transactions_per_minute  latency_mean  \n",
       "0                       18      3.285360  \n",
       "1                       18      3.251735  \n",
       "2                        5     10.988445  \n",
       "3                        4     26.394050  \n",
       "4                       31      3.731961  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all per inference files\n",
    "json_list: List[Dict] = [json.loads(Path(f_name).read_text()) for f_name in glob.glob(os.path.join(METRICS_PER_CHUNK_DIR, \"*.json\"))]\n",
    "df_metrics = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_metrics.shape} from all chunk responses\")\n",
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_config.PrimaryContainer.Environment.ENDPOINT_SERVER_TIMEOUT', 'model_config.PrimaryContainer.Environment.HF_MODEL_ID', 'model_config.PrimaryContainer.Environment.MAX_INPUT_LENGTH', 'model_config.PrimaryContainer.Environment.MAX_TOTAL_TOKENS', 'model_config.PrimaryContainer.Environment.MODEL_CACHE_ROOT', 'model_config.PrimaryContainer.Environment.SAGEMAKER_ENV', 'model_config.PrimaryContainer.Environment.SAGEMAKER_MODEL_SERVER_WORKERS', 'model_config.PrimaryContainer.Environment.SAGEMAKER_PROGRAM', 'model_config.PrimaryContainer.Environment.SM_NUM_GPUS', 'model_config.PrimaryContainer.Environment.HEALTH_CHECK_TIMOUT', 'model_config.PrimaryContainer.Environment.INSTANCE_COUNT', 'model_config.PrimaryContainer.Environment.MODEL_LOADING_TIMEOUT', 'model_config.PrimaryContainer.Environment.NUMBER_OF_GPU']\n",
      "Columns in df_responses: Index(['endpoint_name', 'prompt', 'do_sample', 'temperature', 'top_p', 'top_k',\n",
      "       'max_new_tokens', 'completion', 'prompt_tokens', 'completion_tokens',\n",
      "       'latency', 'experiment_name', 'concurrency', 'truncate'],\n",
      "      dtype='object')\n",
      "Columns in df_endpoints: Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri', 'ENDPOINT_SERVER_TIMEOUT', 'HF_MODEL_ID',\n",
      "       'MAX_INPUT_LENGTH', 'MAX_TOTAL_TOKENS', 'MODEL_CACHE_ROOT',\n",
      "       'SAGEMAKER_ENV', 'SAGEMAKER_MODEL_SERVER_WORKERS', 'SAGEMAKER_PROGRAM',\n",
      "       'SM_NUM_GPUS', 'HEALTH_CHECK_TIMOUT', 'INSTANCE_COUNT',\n",
      "       'MODEL_LOADING_TIMEOUT', 'NUMBER_OF_GPU'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>MAX_TOTAL_TOKENS</th>\n",
       "      <th>MODEL_CACHE_ROOT</th>\n",
       "      <th>SAGEMAKER_ENV</th>\n",
       "      <th>SAGEMAKER_MODEL_SERVER_WORKERS</th>\n",
       "      <th>SAGEMAKER_PROGRAM</th>\n",
       "      <th>SM_NUM_GPUS</th>\n",
       "      <th>HEALTH_CHECK_TIMOUT</th>\n",
       "      <th>INSTANCE_COUNT</th>\n",
       "      <th>MODEL_LOADING_TIMEOUT</th>\n",
       "      <th>NUMBER_OF_GPU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n&lt;/s&gt;</td>\n",
       "      <td>2675</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>3600</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n\\n[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for...</td>\n",
       "      <td>3132</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>3600</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-2-70b-g5-48xlarge-1706022365</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n```\\n&lt;answer&gt;\\nPassage 1:\\n\\n```\\n\\n```\\nP...</td>\n",
       "      <td>3436</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4096</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>3003</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>3600</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-2-70b-g5-48xlarge-1706022365</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n```\\n\\n[/INST]\\nAnswer:\\n```\\n\\n[INST] &lt;&lt;SYS...</td>\n",
       "      <td>3614</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4096</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    endpoint_name  \\\n",
       "0  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "1  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "2              llama-2-70b-g5-48xlarge-1706022365   \n",
       "3  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "4              llama-2-70b-g5-48xlarge-1706022365   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  \\\n",
       "0   0.92    120             100   \n",
       "1   0.92    120             100   \n",
       "2   0.92    120             100   \n",
       "3   0.92    120             100   \n",
       "4   0.92    120             100   \n",
       "\n",
       "                                          completion  prompt_tokens  \\\n",
       "0                                             \\n</s>           2675   \n",
       "1  \\n\\n\\n[INST] <<SYS>>\\nYou are an assistant for...           3132   \n",
       "2  \\n\\n```\\n<answer>\\nPassage 1:\\n\\n```\\n\\n```\\nP...           3436   \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...           3003   \n",
       "4  \\n```\\n\\n[/INST]\\nAnswer:\\n```\\n\\n[INST] <<SYS...           3614   \n",
       "\n",
       "   completion_tokens  ...  MAX_TOTAL_TOKENS MODEL_CACHE_ROOT  SAGEMAKER_ENV  \\\n",
       "0                4.0  ...               NaN              NaN            NaN   \n",
       "1              102.0  ...               NaN              NaN            NaN   \n",
       "2              102.0  ...              4096    /opt/ml/model              1   \n",
       "3              102.0  ...               NaN              NaN            NaN   \n",
       "4              102.0  ...              4096    /opt/ml/model              1   \n",
       "\n",
       "   SAGEMAKER_MODEL_SERVER_WORKERS SAGEMAKER_PROGRAM SM_NUM_GPUS  \\\n",
       "0                             NaN               NaN         NaN   \n",
       "1                             NaN               NaN         NaN   \n",
       "2                               1      inference.py           8   \n",
       "3                             NaN               NaN         NaN   \n",
       "4                               1      inference.py           8   \n",
       "\n",
       "  HEALTH_CHECK_TIMOUT INSTANCE_COUNT MODEL_LOADING_TIMEOUT NUMBER_OF_GPU  \n",
       "0                 300              1                  3600             8  \n",
       "1                 300              1                  3600             8  \n",
       "2                 NaN            NaN                   NaN           NaN  \n",
       "3                 300              1                  3600             8  \n",
       "4                 NaN            NaN                   NaN           NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_endpoints = pd.json_normalize(endpoint_info_list)\n",
    "df_endpoints['instance_type'] = df_endpoints['endpoint_config.ProductionVariants'].map(lambda x: x[0]['InstanceType'])\n",
    "df_endpoints\n",
    "cols_for_env = [c for c in df_endpoints.columns if 'Environment' in c]\n",
    "print(cols_for_env)\n",
    "cols_of_interest = ['experiment_name', \n",
    "                    'instance_type',\n",
    "                    'endpoint.EndpointName',\n",
    "                    'model_config.ModelName',\n",
    "                    'model_config.PrimaryContainer.Image',   \n",
    "                    'model_config.PrimaryContainer.ModelDataSource.S3DataSource.S3Uri']\n",
    "cols_of_interest.extend(cols_for_env)\n",
    "\n",
    "df_endpoints = df_endpoints[cols_of_interest]\n",
    "df_endpoints = df_endpoints[cols_of_interest]\n",
    "cols_of_interest_renamed = [c.split('.')[-1] for c in cols_of_interest]\n",
    "df_endpoints.columns = cols_of_interest_renamed\n",
    "\n",
    "# Check if 'experiment_name' column exists in both DataFrames\n",
    "print(\"Columns in df_responses:\", df_responses.columns)\n",
    "print(\"Columns in df_endpoints:\", df_endpoints.columns)\n",
    "\n",
    "# Merge operation\n",
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "\n",
    "# Inspect the result\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>MAX_TOTAL_TOKENS</th>\n",
       "      <th>MODEL_CACHE_ROOT</th>\n",
       "      <th>SAGEMAKER_ENV</th>\n",
       "      <th>SAGEMAKER_MODEL_SERVER_WORKERS</th>\n",
       "      <th>SAGEMAKER_PROGRAM</th>\n",
       "      <th>SM_NUM_GPUS</th>\n",
       "      <th>HEALTH_CHECK_TIMOUT</th>\n",
       "      <th>INSTANCE_COUNT</th>\n",
       "      <th>MODEL_LOADING_TIMEOUT</th>\n",
       "      <th>NUMBER_OF_GPU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n&lt;/s&gt;</td>\n",
       "      <td>2675</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>3600</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n\\n[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for...</td>\n",
       "      <td>3132</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>3600</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-2-70b-g5-48xlarge-1706022365</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n```\\n&lt;answer&gt;\\nPassage 1:\\n\\n```\\n\\n```\\nP...</td>\n",
       "      <td>3436</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4096</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama2-70bdjl-2024-01-23-15-06-05-901-endpoint</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>3003</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>3600</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-2-70b-g5-48xlarge-1706022365</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>\\n```\\n\\n[/INST]\\nAnswer:\\n```\\n\\n[INST] &lt;&lt;SYS...</td>\n",
       "      <td>3614</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4096</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    endpoint_name  \\\n",
       "0  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "1  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "2              llama-2-70b-g5-48xlarge-1706022365   \n",
       "3  llama2-70bdjl-2024-01-23-15-06-05-901-endpoint   \n",
       "4              llama-2-70b-g5-48xlarge-1706022365   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  \\\n",
       "0   0.92    120             100   \n",
       "1   0.92    120             100   \n",
       "2   0.92    120             100   \n",
       "3   0.92    120             100   \n",
       "4   0.92    120             100   \n",
       "\n",
       "                                          completion  prompt_tokens  \\\n",
       "0                                             \\n</s>           2675   \n",
       "1  \\n\\n\\n[INST] <<SYS>>\\nYou are an assistant for...           3132   \n",
       "2  \\n\\n```\\n<answer>\\nPassage 1:\\n\\n```\\n\\n```\\nP...           3436   \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...           3003   \n",
       "4  \\n```\\n\\n[/INST]\\nAnswer:\\n```\\n\\n[INST] <<SYS...           3614   \n",
       "\n",
       "   completion_tokens  ...  MAX_TOTAL_TOKENS MODEL_CACHE_ROOT  SAGEMAKER_ENV  \\\n",
       "0                4.0  ...               NaN              NaN            NaN   \n",
       "1              102.0  ...               NaN              NaN            NaN   \n",
       "2              102.0  ...              4096    /opt/ml/model              1   \n",
       "3              102.0  ...               NaN              NaN            NaN   \n",
       "4              102.0  ...              4096    /opt/ml/model              1   \n",
       "\n",
       "   SAGEMAKER_MODEL_SERVER_WORKERS SAGEMAKER_PROGRAM SM_NUM_GPUS  \\\n",
       "0                             NaN               NaN         NaN   \n",
       "1                             NaN               NaN         NaN   \n",
       "2                               1      inference.py           8   \n",
       "3                             NaN               NaN         NaN   \n",
       "4                               1      inference.py           8   \n",
       "\n",
       "  HEALTH_CHECK_TIMOUT INSTANCE_COUNT MODEL_LOADING_TIMEOUT NUMBER_OF_GPU  \n",
       "0                 300              1                  3600             8  \n",
       "1                 300              1                  3600             8  \n",
       "2                 NaN            NaN                   NaN           NaN  \n",
       "3                 300              1                  3600             8  \n",
       "4                 NaN            NaN                   NaN           NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:28:17,042] p1345 {3435721265.py:3} INFO - saved results dataframe of shape=(1172, 32) in data/metrics/llama2-70b-g5-p4d-trt-v1/per_inference_request_results.csv\n"
     ]
    }
   ],
   "source": [
    "fpath: str = os.path.join(METRICS_DIR, config['results']['per_inference_request_file']).format(datetime=date_time)\n",
    "df_results.to_csv(fpath, index=False)\n",
    "logger.info(f\"saved results dataframe of shape={df_results.shape} in {fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 17:28:17,189] p1345 {3880536130.py:5} INFO - saved metrics results dataframe of shape=(454, 34) in data/metrics/llama2-70b-g5-p4d-trt-v1/all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "df_metrics = pd.merge(left=df_metrics, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "df_metrics.head()\n",
    "fpath: str = os.path.join(METRICS_DIR, config['results']['all_metrics_file']).format(datetime=date_time)\n",
    "df_metrics.to_csv(fpath, index=False)\n",
    "logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in {fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
