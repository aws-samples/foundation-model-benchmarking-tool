{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on all deployed endpoints: Various combinations of payloads, concurrency levels, model configurations\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of our solution design includes running inferences on all deployed model endpoints (with different configurations, concurrency levels and payload sizes). This notebook runs inferences in a manner that is calls endpoints concurrently and asychronously to generate responses and record metrics. Here are some of the key components:\n",
    "\n",
    "- **Accessing the deployed endpoints**, creating a predictor object for these endpoints to call them during inference time.\n",
    "\n",
    "- **Functions to define metrics**: This notebook sets stage for metrics to be recorded during the time of invocation of all these models for benchmarking purposes.\n",
    "\n",
    "- **Running Actual Inferences**: Once the metrics are defined, we set a blocker function that is responsible for creating inference on a single payload called get_inference. We then run a series of asynchronous functions that can be viewed in the code (link above), to create asychronous inferefences on the deployed models. The way we send requests are by creating combinations: this means creating combinations of payloads of different sizes that can be viewed in the config.yml file, with different concurrency levels (in this case we first go through all patches of payloads with a concurrency level of 1, then 2, and then 4). You can set this to your desired value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## auto reload all of the changes made in the config/globals.py file \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /opt/homebrew/share/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/madhurpt/Library/Application Support/sagemaker/config.yaml\n",
      "CONFIG_FILE=configs/config-mistral-7b-tgi-g5.yml\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import copy\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import itertools\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from globals import *\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker.predictor import Predictor\n",
    "from utils import load_config, count_tokens, write_to_s3, read_from_s3\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from typing import Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36myaml\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36menum\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Enum\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILEPATH_FILE: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig_filepath.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# S3 client initialization\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "s3_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILE: \u001b[36mstr\u001b[39;49;00m = Path(CONFIG_FILEPATH_FILE).read_text()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCONFIG_FILE=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mCONFIG_FILE\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(CONFIG_FILE, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file:\u001b[37m\u001b[39;49;00m\n",
      "    config = yaml.safe_load(file)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "DATA_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PROMPTS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mprompts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_INFERENCE_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_inference\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_CHUNK_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_chunk\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# MODELS_DIR = os.path.join(DATA_DIR, \"models\", config['general']['name'])\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "MODELS_DIR = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] + \u001b[33m\"\u001b[39;49;00m\u001b[33m/models\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## DEFINE THE S3 PATH FOR ENDPOINTS TO READ FROM DURING RUN INFERENCE\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_S3_PATH = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mMODELS_DIR\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m/endpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Use this to upload to the s3 bucket (extracted from the config file)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "BUCKET_NAME = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mbucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## S3 prefix\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PREFIX_NAME = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## SOURCE data is where your actual data resides in s3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SOURCE_DATA = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33msource_data_bucket_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Read the prompt template that the user uploads\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PROMPT_TEMPLATE_S3_PREFIX = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprompt_template\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "DATASET_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "SCRIPTS_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mscripts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "DIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, DATASET_DIR, METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# TOKENIZER_DIR = 'llama2_tokenizer'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## this is for custom tokenizers\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENIZER_DIR_S3 = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mcustom_tokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "LOCAL_CUSTOM_TOKENIZER = \u001b[33m'\u001b[39;49;00m\u001b[33mcustom_tokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "_ = \u001b[36mlist\u001b[39;49;00m(\u001b[36mmap\u001b[39;49;00m(\u001b[34mlambda\u001b[39;49;00m x: os.makedirs(x, exist_ok=\u001b[34mTrue\u001b[39;49;00m), DIR_LIST))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_LIST_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(MODELS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mendpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "REQUEST_PAYLOAD_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(PROMPTS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mpayload.jsonl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTRUNCATE_POLICY\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m, Enum):\u001b[37m\u001b[39;49;00m\n",
      "    AT_PROMPT_TOKEN_LENGTH = \u001b[33m'\u001b[39;49;00m\u001b[33mat-prompt-token-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# misc. metrics related\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PLACE_HOLDER: \u001b[36mint\u001b[39;49;00m = -\u001b[34m1705338041\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# metric filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "COUNTS_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mexperiment_counts.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_DESC_MD_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_W_PRICING_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_w_pricing.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "INSTANCE_PRICING_PER_HOUR_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33minstance_pricing_per_hour.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_w_scores.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option_each_instance_type.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "BUSINESS_SUMMARY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mbusiness_summary.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# plot filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mError rates for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mTokens vs latency for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mtokens_vs_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconcurrency_vs_inference_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mConcurrency Vs latency for different instance type for selected dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "LATENCY_BUDGET: \u001b[36mint\u001b[39;49;00m = \u001b[34m20\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "OVERALL_RESULTS_MD: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m# Results for performance benchmarking\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m**Last modified (UTC): \u001b[39;49;00m\u001b[33m{dttm}\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Summary\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m{business_summary}\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Per instance results\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33mThe following table provides the best combinations for running inference for different sizes prompts on different instance types.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|Dataset   | Instance type   | Recommendation   |\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|---|---|---|\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Dataset=`{dataset}`, instance_type=`{instance_type}`\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULT_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThe best option for staying within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset is a `concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m`. A concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m achieves an `average latency of \u001b[39;49;00m\u001b[33m{latency_mean}\u001b[39;49;00m\u001b[33m seconds`, for an `average prompt size of \u001b[39;49;00m\u001b[33m{prompt_size}\u001b[39;49;00m\u001b[33m tokens` and `completion size of \u001b[39;49;00m\u001b[33m{completion_size}\u001b[39;49;00m\u001b[33m tokens` with `\u001b[39;49;00m\u001b[33m{tpm}\u001b[39;49;00m\u001b[33m transactions/minute`.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_ROW: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33m|`\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m`|`\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m`|\u001b[39;49;00m\u001b[33m{desc}\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_FAILURE_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThis experiment did not find any combination of concurrency level and other configuration settings that could provide a response within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Config.yml file that contains information that is used across this benchmarking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:33,953] p14684 {635462509.py:2} INFO - {\n",
      "  \"general\": {\n",
      "    \"name\": \"mistral-7b-tgi-g5-v1\",\n",
      "    \"model_name\": \"mistral7b\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::218208277580:role/service-role/AmazonSageMaker-ExecutionRole-20230807T175994\",\n",
      "    \"bucket\": \"fmbt\",\n",
      "    \"prefix\": \"data\",\n",
      "    \"source_data_bucket_prefix\": \"source_data\",\n",
      "    \"prompt_template\": \"prompt_template/prompt_template.txt\",\n",
      "    \"custom_tokenizer\": \"tokenizer\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"template_file\": \"prompt_template.txt\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\"\n",
      "  },\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1,\n",
      "      \"max_length_in_tokens\": 500,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 500,\n",
      "      \"max_length_in_tokens\": 1000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1000,\n",
      "      \"max_length_in_tokens\": 2000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 2000,\n",
      "      \"max_length_in_tokens\": 3000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 3000,\n",
      "      \"max_length_in_tokens\": 4000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 305,\n",
      "      \"max_length_in_tokens\": 3997,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    }\n",
      "  ],\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_2000-3000\",\n",
      "    \"weights\": {\n",
      "      \"price_per_tx_wt\": 0.65,\n",
      "      \"latenct_wt\": 0.35\n",
      "    }\n",
      "  },\n",
      "  \"pricing\": {\n",
      "    \"ml.g5.2xlarge\": 1.515,\n",
      "    \"ml.g5.12xlarge\": 7.09,\n",
      "    \"ml.g5.24xlarge\": 10.18,\n",
      "    \"ml.g5.48xlarge\": 20.36,\n",
      "    \"ml.inf2.24xlarge\": 7.79,\n",
      "    \"ml.inf2.48xlarge\": 15.58,\n",
      "    \"ml.p4d.24xlarge\": 37.688\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.1,\n",
      "    \"top_p\": 0.92,\n",
      "    \"top_k\": 120,\n",
      "    \"max_new_tokens\": 100,\n",
      "    \"truncate\": \"at-prompt-token-length\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"huggingface-llm-mistral-7b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"mistral7b\",\n",
      "      \"ep_name\": \"lmistral7b-g5-2xlarge\",\n",
      "      \"instance_type\": \"ml.g5.2xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4,\n",
      "        6,\n",
      "        8\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"8191\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"8192\",\n",
      "        \"MAX_BATCH_PREFILL_TOKENS\": \"8191\",\n",
      "        \"SM_NUM_GPUS\": \"1\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"results\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting access to the s3 bucket where endpoints.json for different models resides\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the deployed model endpoints from the endpoints.json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:34,273] p14684 {1472297992.py:2} INFO - endpoint from --> s3://fmbt/data/models/mistral-7b-tgi-g5-v1/endpoints.json\n",
      "[2024-01-26 19:27:34,275] p14684 {1472297992.py:8} INFO - Found information for 1 endpoints in the respective S3 path\n",
      "[2024-01-26 19:27:34,277] p14684 {1472297992.py:9} INFO - [\n",
      "  {\n",
      "    \"experiment_name\": \"mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "    \"endpoint\": {\n",
      "      \"EndpointName\": \"lmistral7b-g5-2xlarge-1706302351\",\n",
      "      \"EndpointArn\": \"arn:aws:sagemaker:us-east-1:218208277580:endpoint/lmistral7b-g5-2xlarge-1706302351\",\n",
      "      \"EndpointConfigName\": \"lmistral7b-g5-2xlarge-1706302351\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"AllTraffic\",\n",
      "          \"DeployedImages\": [\n",
      "            {\n",
      "              \"SpecifiedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "              \"ResolvedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb\",\n",
      "              \"ResolutionTime\": \"2024-01-26 15:52:34.019000-05:00\"\n",
      "            }\n",
      "          ],\n",
      "          \"CurrentWeight\": 1.0,\n",
      "          \"DesiredWeight\": 1.0,\n",
      "          \"CurrentInstanceCount\": 1,\n",
      "          \"DesiredInstanceCount\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"EndpointStatus\": \"InService\",\n",
      "      \"CreationTime\": \"2024-01-26 15:52:33.043000-05:00\",\n",
      "      \"LastModifiedTime\": \"2024-01-26 15:56:33.206000-05:00\",\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"7a48742f-bc99-4eed-9b1f-6664b8abd29b\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"7a48742f-bc99-4eed-9b1f-6664b8abd29b\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"809\",\n",
      "          \"date\": \"Fri, 26 Jan 2024 20:56:33 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"endpoint_config\": {\n",
      "      \"EndpointConfigName\": \"lmistral7b-g5-2xlarge-1706302351\",\n",
      "      \"EndpointConfigArn\": \"arn:aws:sagemaker:us-east-1:218208277580:endpoint-config/lmistral7b-g5-2xlarge-1706302351\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"AllTraffic\",\n",
      "          \"ModelName\": \"hf-llm-mistral-7b-2024-01-26-20-52-31-041\",\n",
      "          \"InitialInstanceCount\": 1,\n",
      "          \"InstanceType\": \"ml.g5.2xlarge\",\n",
      "          \"InitialVariantWeight\": 1.0,\n",
      "          \"ModelDataDownloadTimeoutInSeconds\": 1200,\n",
      "          \"ContainerStartupHealthCheckTimeoutInSeconds\": 1200\n",
      "        }\n",
      "      ],\n",
      "      \"CreationTime\": \"2024-01-26 15:52:32.570000-05:00\",\n",
      "      \"EnableNetworkIsolation\": false,\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"cc479efc-0434-4dba-b824-d1d64e253535\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"cc479efc-0434-4dba-b824-d1d64e253535\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"514\",\n",
      "          \"date\": \"Fri, 26 Jan 2024 20:56:33 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_config\": {\n",
      "      \"ModelName\": \"hf-llm-mistral-7b-2024-01-26-20-52-31-041\",\n",
      "      \"PrimaryContainer\": {\n",
      "        \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "        \"Mode\": \"SingleModel\",\n",
      "        \"ModelDataSource\": {\n",
      "          \"S3DataSource\": {\n",
      "            \"S3Uri\": \"s3://jumpstart-cache-prod-us-east-1/huggingface-llm/huggingface-llm-mistral-7b/artifacts/inference-prepack/v1.0.0/\",\n",
      "            \"S3DataType\": \"S3Prefix\",\n",
      "            \"CompressionType\": \"None\",\n",
      "            \"ModelAccessConfig\": {\n",
      "              \"AcceptEula\": true\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"Environment\": {\n",
      "          \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "          \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "          \"MAX_BATCH_PREFILL_TOKENS\": \"8191\",\n",
      "          \"MAX_INPUT_LENGTH\": \"8191\",\n",
      "          \"MAX_TOTAL_TOKENS\": \"8192\",\n",
      "          \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "          \"SAGEMAKER_ENV\": \"1\",\n",
      "          \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "          \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "          \"SM_NUM_GPUS\": \"1\"\n",
      "        }\n",
      "      },\n",
      "      \"ExecutionRoleArn\": \"arn:aws:iam::218208277580:role/service-role/AmazonSageMaker-ExecutionRole-20230807T175994\",\n",
      "      \"CreationTime\": \"2024-01-26 15:52:31.867000-05:00\",\n",
      "      \"ModelArn\": \"arn:aws:sagemaker:us-east-1:218208277580:model/hf-llm-mistral-7b-2024-01-26-20-52-31-041\",\n",
      "      \"EnableNetworkIsolation\": true,\n",
      "      \"DeploymentRecommendation\": {\n",
      "        \"RecommendationStatus\": \"COMPLETED\",\n",
      "        \"RealTimeInferenceRecommendations\": []\n",
      "      },\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"1c2fee7e-d0f1-4a63-9d36-c05a5283237a\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"1c2fee7e-d0f1-4a63-9d36-c05a5283237a\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"1168\",\n",
      "          \"date\": \"Fri, 26 Jan 2024 20:56:33 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "endpoint_info_str = read_from_s3(BUCKET_NAME, ENDPOINT_S3_PATH)\n",
    "logger.info(f\"endpoint from --> s3://{BUCKET_NAME}/{ENDPOINT_S3_PATH}\")\n",
    "\n",
    "# Process the retrieved content\n",
    "if endpoint_info_str:\n",
    "    try:\n",
    "        endpoint_info_list = json.loads(endpoint_info_str)\n",
    "        logger.info(f\"Found information for {len(endpoint_info_list)} endpoints in the respective S3 path\")\n",
    "        logger.info(json.dumps(endpoint_info_list, indent=2))\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error parsing JSON from S3: {e}\")\n",
    "else:\n",
    "    logger.error(\"Error reading from S3 or no data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:34,324] p14684 {1455142584.py:3} INFO - there are 1 deployed endpoint(s), endpoint_name_list->['lmistral7b-g5-2xlarge-1706302351']\n"
     ]
    }
   ],
   "source": [
    "# List down the endpoint names that have been deployed\n",
    "endpoint_name_list = [e['endpoint']['EndpointName'] for e in endpoint_info_list]\n",
    "logger.info(f\"there are {len(endpoint_name_list)} deployed endpoint(s), endpoint_name_list->{endpoint_name_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating predictor objects from the deployed endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:34,389] p14684 {3970465137.py:15} INFO - [<sagemaker.base_predictor.Predictor object at 0x163d66cd0>]\n"
     ]
    }
   ],
   "source": [
    "# create predictor objects\n",
    "\n",
    "## create a sagemaker predictor for these endpoints\n",
    "def create_predictor(endpoint_name: str) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "    # Create a SageMaker Predictor object\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "        serializer=JSONSerializer()\n",
    "    )\n",
    "    return predictor\n",
    "\n",
    "## Display the list of predictor objects that have been deployed ready for inferencing from\n",
    "predictor_list: List = [create_predictor(ep) for ep in endpoint_name_list]\n",
    "logger.info(predictor_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions to define and calculate metrics during the time of invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_sum(l: List) -> Union[int, float]:\n",
    "    return sum(filter(None, l))\n",
    "\n",
    "def safe_div(n: Union[int, float], d: Union[int, float]) -> Optional[Union[int, float]]:\n",
    "    return n/d if d else None\n",
    "\n",
    "## Represents the function to calculate all of the metrics at the time of inference\n",
    "def calculate_metrics(responses, chunk, elapsed_async, experiment_name, concurrency, payload_file) -> Dict:\n",
    "    \n",
    "    ## calculate errors based on the completion status of the inference prompt\n",
    "    errors = [r for r in responses if r['completion'] is None]\n",
    "    \n",
    "    ## Calculate the difference as the successes \n",
    "    successes = len(chunk) - len(errors)\n",
    "    \n",
    "    ## Count all of the prompts token count during inference\n",
    "    all_prompts_token_count = safe_sum([r['prompt_tokens'] for r in responses])\n",
    "    prompt_token_throughput = round(all_prompts_token_count / elapsed_async, 2)\n",
    "    prompt_token_count_mean = safe_div(all_prompts_token_count, successes)\n",
    "    all_completions_token_count = safe_sum([r['completion_tokens'] for r in responses])\n",
    "    completion_token_throughput = round(all_completions_token_count / elapsed_async, 2)\n",
    "    completion_token_count_mean = safe_div(all_completions_token_count, successes)\n",
    "    transactions_per_second = round(successes / elapsed_async, 2)\n",
    "    transactions_per_minute = int(transactions_per_second * 60)\n",
    "    \n",
    "    ## calculate the latency mean utilizing the safe_sum function defined above\n",
    "    latency_mean = safe_div(safe_sum([r['latency'] for r in responses]), successes)\n",
    "    \n",
    "    ## Function returns all these values at the time of the invocations\n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'concurrency': concurrency,\n",
    "        'payload_file': payload_file,\n",
    "        'errors': errors,\n",
    "        'successes': successes,\n",
    "        'error_rate': len(errors)/len(chunk),\n",
    "        'all_prompts_token_count': all_prompts_token_count,\n",
    "        'prompt_token_count_mean': prompt_token_count_mean,\n",
    "        'prompt_token_throughput': prompt_token_throughput,\n",
    "        'all_completions_token_count': all_completions_token_count,\n",
    "        'completion_token_count_mean': completion_token_count_mean,\n",
    "        'completion_token_throughput': completion_token_throughput,\n",
    "        'transactions': len(chunk),\n",
    "        'transactions_per_second': transactions_per_second,\n",
    "        'transactions_per_minute': transactions_per_minute,\n",
    "        'latency_mean': latency_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a blocker function and a series of asynchronous concurrent model prompt invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_metrics(endpoint_name=None,\n",
    "                    prompt=None,\n",
    "                    inference_params=None,\n",
    "                    completion=None,\n",
    "                    prompt_tokens=None,\n",
    "                    completion_tokens=None,\n",
    "                    latency=None) -> Dict:\n",
    "    return dict(endpoint_name=endpoint_name,                \n",
    "                prompt=prompt,\n",
    "                **inference_params,\n",
    "                completion=completion,\n",
    "                prompt_tokens=prompt_tokens,\n",
    "                completion_tokens=completion_tokens,\n",
    "                latency=latency)\n",
    "\n",
    "def get_inference(predictor, payload) -> Dict:\n",
    "    \n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "    latency = 0\n",
    "\n",
    "    try:\n",
    "        prompt_tokens = count_tokens(payload['inputs'])\n",
    "        logger.info(f\"get_inference, endpoint={predictor.endpoint_name}, prompt_tokens={prompt_tokens}\")\n",
    "\n",
    "        # get inference\n",
    "        st = time.perf_counter()        \n",
    "        response = predictor.predict(payload)        \n",
    "        latency = time.perf_counter() - st\n",
    "\n",
    "        if isinstance(response, bytes):\n",
    "            response = response.decode('utf-8')\n",
    "        response_json = json.loads(response)\n",
    "        if isinstance(response_json, list):\n",
    "            response_json = response_json[0]\n",
    "\n",
    "        completion = response_json.get(\"generated_text\", \"\")\n",
    "        completion_tokens = count_tokens(completion)\n",
    "\n",
    "        # Set metrics and logging for both cases\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               completion,\n",
    "                               prompt_tokens,\n",
    "                               completion_tokens,\n",
    "                               latency)\n",
    "        # logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, response={json.dumps(response, indent=2)}, latency={latency:.2f}\")\n",
    "        logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, completion_tokens={completion_tokens}, latency={latency:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error occurred with {predictor.endpoint_name}, exception={str(e)}\")\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               None,\n",
    "                               prompt_tokens,\n",
    "                               None,\n",
    "                               None)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a series of asynchronous functions to invoke and run inferences concurrently and asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents a function to start invoking models in separate thread asynchronously for the blocker function\n",
    "async def async_get_inference(predictor, payload: Dict) -> Dict:\n",
    "    return await asyncio.to_thread(get_inference, predictor, payload)\n",
    "\n",
    "## Gathers all of the tasks and sets of the concurrent calling of the asychronous invocations\n",
    "async def async_get_all_inferences(predictor, payload_list: List) -> List:\n",
    "    return await asyncio.gather(*[async_get_inference(predictor, payload) for payload in payload_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This function runs the asychronous function series above together for different experiments and concurrency levels.\n",
    "async def run_inferences(predictor: sagemaker.base_predictor.Predictor, chunk: List, experiment: Dict, concurrency: int, payload_file: str) -> Tuple[List, Dict]:\n",
    "    logger.info(f\"Processing chunk with concurrency={concurrency}\")\n",
    "    s = time.perf_counter()\n",
    "    responses = await async_get_all_inferences(predictor, chunk)\n",
    "    elapsed_async = time.perf_counter() - s\n",
    "\n",
    "    # Add more metadata about this experiment\n",
    "    for r in responses:\n",
    "        r['experiment_name'] = experiment['name']\n",
    "        r['concurrency'] = concurrency\n",
    "\n",
    "    metrics = calculate_metrics(responses, chunk, elapsed_async, experiment['name'], concurrency, payload_file)\n",
    "    return responses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create the predictors from the experiment we are iterating over\n",
    "def create_predictor_for_experiment(experiment: str, config: Dict, endpoint_info_list: List) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "\n",
    "    ## Here, we set the index and then iterate through the experiments\n",
    "    e_idx = config['experiments'].index(experiment) + 1\n",
    "\n",
    "    ## Iterate through the endpoint information to fetch the endpoint name\n",
    "    ep_info = [e for e in endpoint_info_list if e['experiment_name'] == experiment['name']]\n",
    "    if not ep_info:\n",
    "        logger.error(f\"endpoint for experiment={experiment['name']} not found, skipping\")\n",
    "        return None\n",
    "    ep_name = ep_info[0]['endpoint']['EndpointName']\n",
    "    logger.info(f\"experiment={e_idx}, name={experiment['name']}, ep_name={ep_name}\")\n",
    "\n",
    "    # create a predictor from each endpoint in experiments\n",
    "    return create_predictor(ep_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here, we will process combinations of concurrency levels, the payload files and then loop through the \n",
    "## different combinations to make payloads splitted in terms of the concurrency metric and how we can run \n",
    "## it and make inference\n",
    "\n",
    "def create_payload_dict(jline: str, experiment: Dict) -> Dict:\n",
    "    payload: Dict = json.loads(jline)\n",
    "    if experiment.get('remove_truncate', False) is True:\n",
    "        if payload['parameters'].get('truncate'):\n",
    "            del payload['parameters']['truncate']\n",
    "    return payload\n",
    "    \n",
    "    \n",
    "def create_combinations(experiment: Dict) -> List[Tuple]:\n",
    "    combinations_data = []\n",
    "\n",
    "    # Repeat for each concurrency level\n",
    "    combinations = list(itertools.product(experiment['concurrency_levels'], experiment['payload_files']))\n",
    "    logger.info(f\"there are {len(combinations)} combinations of {combinations} to run\")\n",
    "\n",
    "    for concurrency, payload_file in combinations:\n",
    "        # Construct the full S3 file path\n",
    "        s3_file_path = os.path.join(PROMPTS_DIR, payload_file)\n",
    "        logger.info(f\"s3 path where the payload files are being read from -> {s3_file_path}\")\n",
    "\n",
    "        # Read the payload file from S3\n",
    "        try:\n",
    "            response = s3_client.get_object(Bucket=BUCKET_NAME, Key=s3_file_path)\n",
    "            payload_file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "            # Create a payload list by processing each line\n",
    "            payload_list = [create_payload_dict(jline, experiment) for jline in payload_file_content.splitlines()]\n",
    "            logger.info(f\"read from s3://{BUCKET_NAME}/{s3_file_path}, contains {len(payload_list)} lines\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file from S3: {e}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"creating combinations for concurrency={concurrency}, payload_file={payload_file}, payload_list length={len(payload_list)}\")\n",
    "        \n",
    "        n = concurrency\n",
    "        \n",
    "        if len(payload_list) < n:\n",
    "            elements_to_add = n - len(payload_list)\n",
    "            element_to_replicate = payload_list[0]\n",
    "            # payload_list = payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "        # Split the original list into sublists which contain the number of requests we want to send concurrently        \n",
    "        payload_list_splitted = [payload_list[i * n:(i + 1) * n] for i in range((len(payload_list) + n - 1) // n )]  \n",
    "        \n",
    "        for p in payload_list_splitted:\n",
    "            if len(p) < n:\n",
    "                elements_to_add = n - len(p)\n",
    "                element_to_replicate = p[0]\n",
    "                # p = p.extend([element_to_replicate]*elements_to_add)\n",
    "                p.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "\n",
    "        # Only keep lists that have at least concurrency number of elements\n",
    "        len_before = len(payload_list_splitted)\n",
    "        payload_list_splitted = [p for p in payload_list_splitted if len(p) == concurrency]\n",
    "        logger.info(f\"after only retaining chunks of length {concurrency}, we have {len(payload_list_splitted)} chunks, previously we had {len_before} chunks\")\n",
    "        combinations_data.append((concurrency, payload_file, payload_list_splitted))\n",
    "    logger.info(f\"there are {len(combinations)} for {experiment}\")\n",
    "    return combinations_data\n",
    "\n",
    "# process_combinations(experiment, predictor, PROMPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:34,814] p14684 {663335596.py:13} INFO - experiment=1, name=mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0, ep_name=lmistral7b-g5-2xlarge-1706302351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:34,856] p14684 {2873079850.py:18} INFO - there are 10 combinations of [(1, 'payload_en_1-500.jsonl'), (1, 'payload_en_500-1000.jsonl'), (2, 'payload_en_1-500.jsonl'), (2, 'payload_en_500-1000.jsonl'), (4, 'payload_en_1-500.jsonl'), (4, 'payload_en_500-1000.jsonl'), (6, 'payload_en_1-500.jsonl'), (6, 'payload_en_500-1000.jsonl'), (8, 'payload_en_1-500.jsonl'), (8, 'payload_en_500-1000.jsonl')] to run\n",
      "[2024-01-26 19:27:34,857] p14684 {2873079850.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n",
      "[2024-01-26 19:27:35,045] p14684 {2873079850.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-26 19:27:35,045] p14684 {2873079850.py:38} INFO - creating combinations for concurrency=1, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-26 19:27:35,046] p14684 {2873079850.py:62} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 19:27:35,046] p14684 {2873079850.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-26 19:27:35,095] p14684 {2873079850.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-26 19:27:35,095] p14684 {2873079850.py:38} INFO - creating combinations for concurrency=1, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-26 19:27:35,096] p14684 {2873079850.py:62} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 19:27:35,096] p14684 {2873079850.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n",
      "[2024-01-26 19:27:35,149] p14684 {2873079850.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-26 19:27:35,150] p14684 {2873079850.py:38} INFO - creating combinations for concurrency=2, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-26 19:27:35,150] p14684 {2873079850.py:62} INFO - after only retaining chunks of length 2, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 19:27:35,151] p14684 {2873079850.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-26 19:27:35,204] p14684 {2873079850.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-26 19:27:35,204] p14684 {2873079850.py:38} INFO - creating combinations for concurrency=2, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-26 19:27:35,205] p14684 {2873079850.py:62} INFO - after only retaining chunks of length 2, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 19:27:35,205] p14684 {2873079850.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n",
      "[2024-01-26 19:27:35,258] p14684 {2873079850.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-26 19:27:35,262] p14684 {2873079850.py:38} INFO - creating combinations for concurrency=4, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-26 19:27:35,262] p14684 {2873079850.py:62} INFO - after only retaining chunks of length 4, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 19:27:35,263] p14684 {2873079850.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-26 19:27:35,364] p14684 {2873079850.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-26 19:27:35,364] p14684 {2873079850.py:38} INFO - creating combinations for concurrency=4, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-26 19:27:35,365] p14684 {2873079850.py:62} INFO - after only retaining chunks of length 4, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 19:27:35,366] p14684 {2873079850.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n",
      "[2024-01-26 19:27:35,418] p14684 {2873079850.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-26 19:27:35,418] p14684 {2873079850.py:38} INFO - creating combinations for concurrency=6, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-26 19:27:35,419] p14684 {2873079850.py:62} INFO - after only retaining chunks of length 6, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 19:27:35,419] p14684 {2873079850.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-26 19:27:35,514] p14684 {2873079850.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-26 19:27:35,514] p14684 {2873079850.py:38} INFO - creating combinations for concurrency=6, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-26 19:27:35,515] p14684 {2873079850.py:62} INFO - after only retaining chunks of length 6, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 19:27:35,515] p14684 {2873079850.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n",
      "[2024-01-26 19:27:35,568] p14684 {2873079850.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-26 19:27:35,569] p14684 {2873079850.py:38} INFO - creating combinations for concurrency=8, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-26 19:27:35,569] p14684 {2873079850.py:62} INFO - after only retaining chunks of length 8, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 19:27:35,569] p14684 {2873079850.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-26 19:27:35,631] p14684 {2873079850.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-26 19:27:35,631] p14684 {2873079850.py:38} INFO - creating combinations for concurrency=8, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-26 19:27:35,632] p14684 {2873079850.py:62} INFO - after only retaining chunks of length 8, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-26 19:27:35,632] p14684 {2873079850.py:64} INFO - there are 10 for {'name': 'mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'huggingface-llm-mistral-7b', 'model_version': '*', 'model_name': 'mistral7b', 'ep_name': 'lmistral7b-g5-2xlarge', 'instance_type': 'ml.g5.2xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl', 'payload_en_500-1000.jsonl'], 'concurrency_levels': [1, 2, 4, 6, 8], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '8191', 'MAX_TOTAL_TOKENS': '8192', 'MAX_BATCH_PREFILL_TOKENS': '8191', 'SM_NUM_GPUS': '1', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}\n",
      "[2024-01-26 19:27:35,633] p14684 {1751706209.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 19:27:35,633] p14684 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 19:27:35,736] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:27:39,380] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=3.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315259.383544.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:39,907] p14684 {1751706209.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 19:27:39,908] p14684 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-26 19:27:39,920] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315259.655541.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:43,795] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=3.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315263.7983181.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:44,348] p14684 {1751706209.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 19:27:44,348] p14684 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 19:27:44,357] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:27:44,358] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315264.0651672.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:48,159] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=3.80\n",
      "[2024-01-26 19:27:48,236] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=3.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315268.238461.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315268.482053.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:49,001] p14684 {1751706209.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 19:27:49,002] p14684 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-26 19:27:49,013] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:27:49,013] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315268.749988.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:50,102] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=1.08\n",
      "[2024-01-26 19:27:53,221] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315273.224117.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315273.473171.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:54,029] p14684 {1751706209.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 19:27:54,030] p14684 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 19:27:54,043] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:27:54,043] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:27:54,043] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:27:54,043] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315273.7399309.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:58,048] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=4.00\n",
      "[2024-01-26 19:27:58,048] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=4.00\n",
      "[2024-01-26 19:27:58,087] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=4.04\n",
      "[2024-01-26 19:27:58,088] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315278.0898619.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315278.324451.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315278.5683148.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315278.795916.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:27:59,259] p14684 {1751706209.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 19:27:59,260] p14684 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-26 19:27:59,272] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:27:59,272] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:27:59,274] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:27:59,274] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315279.0192618.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:00,830] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=1.55\n",
      "[2024-01-26 19:28:04,096] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.82\n",
      "[2024-01-26 19:28:04,096] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.82\n",
      "[2024-01-26 19:28:04,098] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=4.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315284.1034.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315284.406255.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315284.682734.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315284.94691.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:05,549] p14684 {1751706209.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 19:28:05,549] p14684 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 19:28:05,557] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:05,558] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:05,558] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:05,558] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:05,558] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:05,558] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315285.194865.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:08,211] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=39, latency=2.64\n",
      "[2024-01-26 19:28:09,715] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.16\n",
      "[2024-01-26 19:28:09,715] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.15\n",
      "[2024-01-26 19:28:09,716] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.16\n",
      "[2024-01-26 19:28:09,717] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.15\n",
      "[2024-01-26 19:28:10,424] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315290.4267972.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315290.677664.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315290.9388359.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315291.202114.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315291.451282.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315291.694627.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:12,195] p14684 {1751706209.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 19:28:12,195] p14684 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-26 19:28:12,210] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:12,211] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:12,211] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:12,211] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:12,211] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:12,212] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315291.942176.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:14,419] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.21\n",
      "[2024-01-26 19:28:14,420] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.20\n",
      "[2024-01-26 19:28:17,486] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.27\n",
      "[2024-01-26 19:28:17,487] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.27\n",
      "[2024-01-26 19:28:17,487] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.27\n",
      "[2024-01-26 19:28:17,498] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315297.500025.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315297.796799.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315298.095465.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315298.37982.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315298.651799.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315298.968029.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:19,537] p14684 {1751706209.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 19:28:19,538] p14684 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 19:28:19,556] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:19,558] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:19,558] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:19,559] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:19,561] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:19,562] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:19,562] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n",
      "[2024-01-26 19:28:19,563] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315299.221434.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:21,649] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=39, latency=2.08\n",
      "[2024-01-26 19:28:23,942] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=4.37\n",
      "[2024-01-26 19:28:23,942] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.37\n",
      "[2024-01-26 19:28:23,943] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=99, latency=4.37\n",
      "[2024-01-26 19:28:23,943] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=98, latency=4.38\n",
      "[2024-01-26 19:28:23,944] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.38\n",
      "[2024-01-26 19:28:23,944] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.38\n",
      "[2024-01-26 19:28:23,945] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=101, latency=4.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315303.952283.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315304.22862.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315304.485143.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315304.740923.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315305.025055.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315305.371649.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315305.641425.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315305.875691.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:26,435] p14684 {1751706209.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-26 19:28:26,436] p14684 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-26 19:28:26,450] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:26,453] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:26,454] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:26,456] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:26,459] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:26,461] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:26,461] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n",
      "[2024-01-26 19:28:26,461] p14684 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706302351, prompt_tokens=980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315306.161215.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:28,921] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.45\n",
      "[2024-01-26 19:28:28,922] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.46\n",
      "[2024-01-26 19:28:28,923] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.46\n",
      "[2024-01-26 19:28:28,924] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=16, latency=2.46\n",
      "[2024-01-26 19:28:32,205] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.74\n",
      "[2024-01-26 19:28:32,205] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.73\n",
      "[2024-01-26 19:28:32,206] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.74\n",
      "[2024-01-26 19:28:32,206] p14684 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706302351, completion_tokens=100, latency=5.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315312.21288.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315312.47529.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315312.754891.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315313.019111.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315313.2965.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315313.5826612.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315313.865887.json\n",
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315314.142499.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:34,743] p14684 {1751706209.py:49} INFO - experiment=1/1, name=mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0, done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference/1706315314.455976.json\n"
     ]
    }
   ],
   "source": [
    "# for each experiment\n",
    "#   - for each endpoint and concurrency in an experiment\n",
    "\n",
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "_ = list(map(clear_dir, [METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]))\n",
    "\n",
    "num_experiments: int = len(config['experiments'])\n",
    "for e_idx, experiment in enumerate(config['experiments']):\n",
    "    e_idx += 1  # Increment experiment index\n",
    "    # Call do_experiment function to create the predictor object\n",
    " \n",
    "    predictor = create_predictor_for_experiment(experiment, config, endpoint_info_list)\n",
    "    if predictor is None:\n",
    "        logger.error(f\"predictor could not be created for experiment={experiment}, moving to next...\")\n",
    "        continue\n",
    "\n",
    "    # Process combinations of concurrency levels and payload files\n",
    "    combination_data = create_combinations(experiment)\n",
    "\n",
    "    for concurrency, payload_file, split_payload in combination_data:\n",
    "        for chunk_index, chunk in enumerate(split_payload):\n",
    "            logger.info(f\"e_idx={e_idx}/{num_experiments}, chunk_index={chunk_index+1}/{len(split_payload)}\")\n",
    "\n",
    "            # Process each chunk and calculate metrics\n",
    "            responses, metrics = await run_inferences(predictor, chunk, experiment, concurrency, payload_file)\n",
    "            if metrics:\n",
    "                # Convert metrics to JSON string\n",
    "                metrics_json = json.dumps(metrics, indent=2)\n",
    "                # Define S3 file path\n",
    "                metrics_file_name = f\"{time.time()}.json\"\n",
    "                metrics_s3_path = os.path.join(METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "                # Write to S3\n",
    "                write_to_s3(metrics_json, BUCKET_NAME, \"\", METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "\n",
    "            if responses:\n",
    "                for r in responses:\n",
    "                    # Convert response to JSON string\n",
    "                    response_json = json.dumps(r, indent=2)\n",
    "                    # Define S3 file path\n",
    "                    response_file_name = f\"{time.time()}.json\"\n",
    "                    response_s3_path = os.path.join(METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "                    # Write to S3\n",
    "                    write_to_s3(response_json, BUCKET_NAME, \"\", METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "\n",
    "    logger.info(f\"experiment={e_idx}/{num_experiments}, name={experiment['name']}, done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:37,356] p14684 {2679022863.py:19} INFO - created dataframe of shape (42, 14) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia is from the ...</td>\n",
       "      <td>304</td>\n",
       "      <td>98</td>\n",
       "      <td>3.641538</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>3.873750</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>3.799924</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>3.872762</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>16</td>\n",
       "      <td>1.078368</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706302351   \n",
       "1  lmistral7b-g5-2xlarge-1706302351   \n",
       "2  lmistral7b-g5-2xlarge-1706302351   \n",
       "3  lmistral7b-g5-2xlarge-1706302351   \n",
       "4  lmistral7b-g5-2xlarge-1706302351   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100       304   \n",
       "1   0.92    120             100       980   \n",
       "2   0.92    120             100       304   \n",
       "3   0.92    120             100       304   \n",
       "4   0.92    120             100       980   \n",
       "\n",
       "                                          completion  prompt_tokens  \\\n",
       "0  \\n\\n```\\nThe genus Sinofranchetia is from the ...            304   \n",
       "1   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "2  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "3  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "4   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "\n",
       "   completion_tokens   latency  \\\n",
       "0                 98  3.641538   \n",
       "1                100  3.873750   \n",
       "2                101  3.799924   \n",
       "3                101  3.872762   \n",
       "4                 16  1.078368   \n",
       "\n",
       "                                     experiment_name  concurrency  \n",
       "0  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "1  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "2  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            2  \n",
       "3  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            2  \n",
       "4  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to list files in S3 bucket with a specific prefix\n",
    "def list_s3_files(bucket, prefix, suffix='.json'):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    return [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith(suffix)]\n",
    "\n",
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(BUCKET_NAME, METRICS_PER_INFERENCE_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = []\n",
    "for file_key in s3_files:\n",
    "    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=file_key)\n",
    "    file_content = response['Body'].read().decode('utf-8')\n",
    "    json_obj = json.loads(file_content)\n",
    "    json_list.append(json_obj)\n",
    "\n",
    "# Create DataFrame\n",
    "df_responses = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all responses\")\n",
    "df_responses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia is from the ...</td>\n",
       "      <td>304</td>\n",
       "      <td>98</td>\n",
       "      <td>3.641538</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>3.873750</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>3.799924</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>3.872762</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>16</td>\n",
       "      <td>1.078368</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>4.206579</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia is from the ...</td>\n",
       "      <td>304</td>\n",
       "      <td>99</td>\n",
       "      <td>4.003850</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia is from the ...</td>\n",
       "      <td>304</td>\n",
       "      <td>99</td>\n",
       "      <td>4.001087</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia is from the ...</td>\n",
       "      <td>304</td>\n",
       "      <td>98</td>\n",
       "      <td>4.038162</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>4.036621</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>4.822177</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>4.821616</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>4.818361</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>16</td>\n",
       "      <td>1.551029</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>4.152250</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>4.858157</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>4.155896</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>4.156175</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>4.152941</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia and Staunton...</td>\n",
       "      <td>304</td>\n",
       "      <td>39</td>\n",
       "      <td>2.643441</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>5.283754</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>16</td>\n",
       "      <td>2.205571</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>16</td>\n",
       "      <td>2.200241</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>5.269810</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>5.266170</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>5.268555</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>4.377650</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>4.381862</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia and Staunton...</td>\n",
       "      <td>304</td>\n",
       "      <td>39</td>\n",
       "      <td>2.084822</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia is from the ...</td>\n",
       "      <td>304</td>\n",
       "      <td>99</td>\n",
       "      <td>4.371490</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia is from the ...</td>\n",
       "      <td>304</td>\n",
       "      <td>98</td>\n",
       "      <td>4.375509</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>4.373703</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia is from the ...</td>\n",
       "      <td>304</td>\n",
       "      <td>99</td>\n",
       "      <td>4.370963</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>4.372982</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>16</td>\n",
       "      <td>2.461635</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>5.743484</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>16</td>\n",
       "      <td>2.460087</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>16</td>\n",
       "      <td>2.456678</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>5.738652</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>5.735238</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>16</td>\n",
       "      <td>2.451896</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>5.732741</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       endpoint_name  \\\n",
       "0   lmistral7b-g5-2xlarge-1706302351   \n",
       "1   lmistral7b-g5-2xlarge-1706302351   \n",
       "2   lmistral7b-g5-2xlarge-1706302351   \n",
       "3   lmistral7b-g5-2xlarge-1706302351   \n",
       "4   lmistral7b-g5-2xlarge-1706302351   \n",
       "5   lmistral7b-g5-2xlarge-1706302351   \n",
       "6   lmistral7b-g5-2xlarge-1706302351   \n",
       "7   lmistral7b-g5-2xlarge-1706302351   \n",
       "8   lmistral7b-g5-2xlarge-1706302351   \n",
       "9   lmistral7b-g5-2xlarge-1706302351   \n",
       "10  lmistral7b-g5-2xlarge-1706302351   \n",
       "11  lmistral7b-g5-2xlarge-1706302351   \n",
       "12  lmistral7b-g5-2xlarge-1706302351   \n",
       "13  lmistral7b-g5-2xlarge-1706302351   \n",
       "14  lmistral7b-g5-2xlarge-1706302351   \n",
       "15  lmistral7b-g5-2xlarge-1706302351   \n",
       "16  lmistral7b-g5-2xlarge-1706302351   \n",
       "17  lmistral7b-g5-2xlarge-1706302351   \n",
       "18  lmistral7b-g5-2xlarge-1706302351   \n",
       "19  lmistral7b-g5-2xlarge-1706302351   \n",
       "20  lmistral7b-g5-2xlarge-1706302351   \n",
       "21  lmistral7b-g5-2xlarge-1706302351   \n",
       "22  lmistral7b-g5-2xlarge-1706302351   \n",
       "23  lmistral7b-g5-2xlarge-1706302351   \n",
       "24  lmistral7b-g5-2xlarge-1706302351   \n",
       "25  lmistral7b-g5-2xlarge-1706302351   \n",
       "26  lmistral7b-g5-2xlarge-1706302351   \n",
       "27  lmistral7b-g5-2xlarge-1706302351   \n",
       "28  lmistral7b-g5-2xlarge-1706302351   \n",
       "29  lmistral7b-g5-2xlarge-1706302351   \n",
       "30  lmistral7b-g5-2xlarge-1706302351   \n",
       "31  lmistral7b-g5-2xlarge-1706302351   \n",
       "32  lmistral7b-g5-2xlarge-1706302351   \n",
       "33  lmistral7b-g5-2xlarge-1706302351   \n",
       "34  lmistral7b-g5-2xlarge-1706302351   \n",
       "35  lmistral7b-g5-2xlarge-1706302351   \n",
       "36  lmistral7b-g5-2xlarge-1706302351   \n",
       "37  lmistral7b-g5-2xlarge-1706302351   \n",
       "38  lmistral7b-g5-2xlarge-1706302351   \n",
       "39  lmistral7b-g5-2xlarge-1706302351   \n",
       "40  lmistral7b-g5-2xlarge-1706302351   \n",
       "41  lmistral7b-g5-2xlarge-1706302351   \n",
       "\n",
       "                                               prompt  do_sample  temperature  \\\n",
       "0   <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "1   <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "2   <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "3   <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "4   <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "5   <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "6   <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "7   <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "8   <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "9   <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "10  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "11  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "12  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "13  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "14  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "15  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "16  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "17  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "18  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "19  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "20  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "21  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "22  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "23  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "24  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "25  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "26  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "27  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "28  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "29  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "30  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "31  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "32  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "33  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "34  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "35  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "36  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "37  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "38  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "39  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "40  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "41  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "\n",
       "    top_p  top_k  max_new_tokens  truncate  \\\n",
       "0    0.92    120             100       304   \n",
       "1    0.92    120             100       980   \n",
       "2    0.92    120             100       304   \n",
       "3    0.92    120             100       304   \n",
       "4    0.92    120             100       980   \n",
       "5    0.92    120             100       980   \n",
       "6    0.92    120             100       304   \n",
       "7    0.92    120             100       304   \n",
       "8    0.92    120             100       304   \n",
       "9    0.92    120             100       304   \n",
       "10   0.92    120             100       980   \n",
       "11   0.92    120             100       980   \n",
       "12   0.92    120             100       980   \n",
       "13   0.92    120             100       980   \n",
       "14   0.92    120             100       304   \n",
       "15   0.92    120             100       304   \n",
       "16   0.92    120             100       304   \n",
       "17   0.92    120             100       304   \n",
       "18   0.92    120             100       304   \n",
       "19   0.92    120             100       304   \n",
       "20   0.92    120             100       980   \n",
       "21   0.92    120             100       980   \n",
       "22   0.92    120             100       980   \n",
       "23   0.92    120             100       980   \n",
       "24   0.92    120             100       980   \n",
       "25   0.92    120             100       980   \n",
       "26   0.92    120             100       304   \n",
       "27   0.92    120             100       304   \n",
       "28   0.92    120             100       304   \n",
       "29   0.92    120             100       304   \n",
       "30   0.92    120             100       304   \n",
       "31   0.92    120             100       304   \n",
       "32   0.92    120             100       304   \n",
       "33   0.92    120             100       304   \n",
       "34   0.92    120             100       980   \n",
       "35   0.92    120             100       980   \n",
       "36   0.92    120             100       980   \n",
       "37   0.92    120             100       980   \n",
       "38   0.92    120             100       980   \n",
       "39   0.92    120             100       980   \n",
       "40   0.92    120             100       980   \n",
       "41   0.92    120             100       980   \n",
       "\n",
       "                                           completion  prompt_tokens  \\\n",
       "0   \\n\\n```\\nThe genus Sinofranchetia is from the ...            304   \n",
       "1    Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "2   \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "3   \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "4    Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "5    Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "6   \\n\\n```\\nThe genus Sinofranchetia is from the ...            304   \n",
       "7   \\n\\n```\\nThe genus Sinofranchetia is from the ...            304   \n",
       "8   \\n\\n```\\nThe genus Sinofranchetia is from the ...            304   \n",
       "9   \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "10   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "11   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "12   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "13   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "14  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "15  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "16  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "17  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "18  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "19  \\n\\n```\\nThe genus Sinofranchetia and Staunton...            304   \n",
       "20   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "21   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "22   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "23   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "24   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "25   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "26  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "27  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "28  \\n\\n```\\nThe genus Sinofranchetia and Staunton...            304   \n",
       "29  \\n\\n```\\nThe genus Sinofranchetia is from the ...            304   \n",
       "30  \\n\\n```\\nThe genus Sinofranchetia is from the ...            304   \n",
       "31  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "32  \\n\\n```\\nThe genus Sinofranchetia is from the ...            304   \n",
       "33  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "34   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "35   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "36   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "37   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "38   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "39   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "40   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "41   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "\n",
       "    completion_tokens   latency  \\\n",
       "0                  98  3.641538   \n",
       "1                 100  3.873750   \n",
       "2                 101  3.799924   \n",
       "3                 101  3.872762   \n",
       "4                  16  1.078368   \n",
       "5                 100  4.206579   \n",
       "6                  99  4.003850   \n",
       "7                  99  4.001087   \n",
       "8                  98  4.038162   \n",
       "9                 101  4.036621   \n",
       "10                100  4.822177   \n",
       "11                100  4.821616   \n",
       "12                100  4.818361   \n",
       "13                 16  1.551029   \n",
       "14                101  4.152250   \n",
       "15                101  4.858157   \n",
       "16                101  4.155896   \n",
       "17                101  4.156175   \n",
       "18                101  4.152941   \n",
       "19                 39  2.643441   \n",
       "20                100  5.283754   \n",
       "21                 16  2.205571   \n",
       "22                 16  2.200241   \n",
       "23                100  5.269810   \n",
       "24                100  5.266170   \n",
       "25                100  5.268555   \n",
       "26                101  4.377650   \n",
       "27                101  4.381862   \n",
       "28                 39  2.084822   \n",
       "29                 99  4.371490   \n",
       "30                 98  4.375509   \n",
       "31                101  4.373703   \n",
       "32                 99  4.370963   \n",
       "33                101  4.372982   \n",
       "34                 16  2.461635   \n",
       "35                100  5.743484   \n",
       "36                 16  2.460087   \n",
       "37                 16  2.456678   \n",
       "38                100  5.738652   \n",
       "39                100  5.735238   \n",
       "40                 16  2.451896   \n",
       "41                100  5.732741   \n",
       "\n",
       "                                      experiment_name  concurrency  \n",
       "0   mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "1   mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "2   mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            2  \n",
       "3   mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            2  \n",
       "4   mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            2  \n",
       "5   mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            2  \n",
       "6   mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4  \n",
       "7   mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4  \n",
       "8   mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4  \n",
       "9   mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4  \n",
       "10  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4  \n",
       "11  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4  \n",
       "12  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4  \n",
       "13  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4  \n",
       "14  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "15  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "16  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "17  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "18  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "19  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "20  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "21  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "22  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "23  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "24  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "25  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6  \n",
       "26  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "27  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "28  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "29  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "30  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "31  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "32  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "33  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "34  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "35  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "36  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "37  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "38  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "39  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "40  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "41  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses[df_responses.endpoint_name.str.contains(\"g5\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:37,525] p14684 {2358223361.py:3} INFO - files recieved from s3 for per inference request --> {'ResponseMetadata': {'RequestId': 'D131NEJBF889KY2Q', 'HostId': 'GS7UWyTkPxmCaCjDBxLnrNZ/qikP+CF+dJ2vG0EUS/2A4K9WHp5DAqSFd/LMDL9PKbMx1C9tO/0=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'GS7UWyTkPxmCaCjDBxLnrNZ/qikP+CF+dJ2vG0EUS/2A4K9WHp5DAqSFd/LMDL9PKbMx1C9tO/0=', 'x-amz-request-id': 'D131NEJBF889KY2Q', 'date': 'Sat, 27 Jan 2024 00:28:38 GMT', 'x-amz-bucket-region': 'us-east-1', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'IsTruncated': False, 'Contents': [{'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315259.383544.json', 'LastModified': datetime.datetime(2024, 1, 27, 0, 27, 40, tzinfo=tzutc()), 'ETag': '\"112f92d300a059f960789dad35a918a5\"', 'Size': 556, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315263.7983181.json', 'LastModified': datetime.datetime(2024, 1, 27, 0, 27, 45, tzinfo=tzutc()), 'ETag': '\"3b8c5edfb6041187527c3f508c63c888\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315268.238461.json', 'LastModified': datetime.datetime(2024, 1, 27, 0, 27, 49, tzinfo=tzutc()), 'ETag': '\"3c78073a5c615aabdb407d071dce4940\"', 'Size': 559, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315273.224117.json', 'LastModified': datetime.datetime(2024, 1, 27, 0, 27, 54, tzinfo=tzutc()), 'ETag': '\"900c222bbbe8419b2eef1d4ca667f690\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315278.0898619.json', 'LastModified': datetime.datetime(2024, 1, 27, 0, 27, 59, tzinfo=tzutc()), 'ETag': '\"746088cfee4047e3ea94d7d38eb324df\"', 'Size': 561, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315284.1034.json', 'LastModified': datetime.datetime(2024, 1, 27, 0, 28, 5, tzinfo=tzutc()), 'ETag': '\"f068d2f0977c63273dc0e692b8a6b3cc\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315290.4267972.json', 'LastModified': datetime.datetime(2024, 1, 27, 0, 28, 11, tzinfo=tzutc()), 'ETag': '\"7884388a7fc49795620a2b1792cc6324\"', 'Size': 573, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315297.500025.json', 'LastModified': datetime.datetime(2024, 1, 27, 0, 28, 18, tzinfo=tzutc()), 'ETag': '\"7b984356da4144a6ffdfb630efac54ae\"', 'Size': 563, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315303.952283.json', 'LastModified': datetime.datetime(2024, 1, 27, 0, 28, 25, tzinfo=tzutc()), 'ETag': '\"6a726ae7c5dc56265d9c3c58a43a9117\"', 'Size': 562, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk/1706315312.21288.json', 'LastModified': datetime.datetime(2024, 1, 27, 0, 28, 33, tzinfo=tzutc()), 'ETag': '\"29c69033fd7c41e21f03d041cfec5048\"', 'Size': 563, 'StorageClass': 'STANDARD'}], 'Name': 'fmbt', 'Prefix': 'data/metrics/mistral-7b-tgi-g5-v1/per_chunk', 'MaxKeys': 1000, 'EncodingType': 'url', 'KeyCount': 10}\n",
      "[2024-01-26 19:28:38,143] p14684 {2358223361.py:19} INFO - created dataframe of shape (10, 16) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>errors</th>\n",
       "      <th>successes</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>all_prompts_token_count</th>\n",
       "      <th>prompt_token_count_mean</th>\n",
       "      <th>prompt_token_throughput</th>\n",
       "      <th>all_completions_token_count</th>\n",
       "      <th>completion_token_count_mean</th>\n",
       "      <th>completion_token_throughput</th>\n",
       "      <th>transactions</th>\n",
       "      <th>transactions_per_second</th>\n",
       "      <th>transactions_per_minute</th>\n",
       "      <th>latency_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304</td>\n",
       "      <td>304.0</td>\n",
       "      <td>81.08</td>\n",
       "      <td>98</td>\n",
       "      <td>98.00</td>\n",
       "      <td>26.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.27</td>\n",
       "      <td>16</td>\n",
       "      <td>3.641538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_500-1000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>980</td>\n",
       "      <td>980.0</td>\n",
       "      <td>251.99</td>\n",
       "      <td>100</td>\n",
       "      <td>100.00</td>\n",
       "      <td>25.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>15</td>\n",
       "      <td>3.873750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>2</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608</td>\n",
       "      <td>304.0</td>\n",
       "      <td>156.33</td>\n",
       "      <td>202</td>\n",
       "      <td>101.00</td>\n",
       "      <td>51.94</td>\n",
       "      <td>2</td>\n",
       "      <td>0.51</td>\n",
       "      <td>30</td>\n",
       "      <td>3.836343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>2</td>\n",
       "      <td>payload_en_500-1000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1960</td>\n",
       "      <td>980.0</td>\n",
       "      <td>464.47</td>\n",
       "      <td>116</td>\n",
       "      <td>58.00</td>\n",
       "      <td>27.49</td>\n",
       "      <td>2</td>\n",
       "      <td>0.47</td>\n",
       "      <td>28</td>\n",
       "      <td>2.642474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1216</td>\n",
       "      <td>304.0</td>\n",
       "      <td>299.59</td>\n",
       "      <td>397</td>\n",
       "      <td>99.25</td>\n",
       "      <td>97.81</td>\n",
       "      <td>4</td>\n",
       "      <td>0.99</td>\n",
       "      <td>59</td>\n",
       "      <td>4.019930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     experiment_name  concurrency  \\\n",
       "0  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1   \n",
       "1  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1   \n",
       "2  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            2   \n",
       "3  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            2   \n",
       "4  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4   \n",
       "\n",
       "                payload_file errors  successes  error_rate  \\\n",
       "0     payload_en_1-500.jsonl     []          1         0.0   \n",
       "1  payload_en_500-1000.jsonl     []          1         0.0   \n",
       "2     payload_en_1-500.jsonl     []          2         0.0   \n",
       "3  payload_en_500-1000.jsonl     []          2         0.0   \n",
       "4     payload_en_1-500.jsonl     []          4         0.0   \n",
       "\n",
       "   all_prompts_token_count  prompt_token_count_mean  prompt_token_throughput  \\\n",
       "0                      304                    304.0                    81.08   \n",
       "1                      980                    980.0                   251.99   \n",
       "2                      608                    304.0                   156.33   \n",
       "3                     1960                    980.0                   464.47   \n",
       "4                     1216                    304.0                   299.59   \n",
       "\n",
       "   all_completions_token_count  completion_token_count_mean  \\\n",
       "0                           98                        98.00   \n",
       "1                          100                       100.00   \n",
       "2                          202                       101.00   \n",
       "3                          116                        58.00   \n",
       "4                          397                        99.25   \n",
       "\n",
       "   completion_token_throughput  transactions  transactions_per_second  \\\n",
       "0                        26.14             1                     0.27   \n",
       "1                        25.71             1                     0.26   \n",
       "2                        51.94             2                     0.51   \n",
       "3                        27.49             2                     0.47   \n",
       "4                        97.81             4                     0.99   \n",
       "\n",
       "   transactions_per_minute  latency_mean  \n",
       "0                       16      3.641538  \n",
       "1                       15      3.873750  \n",
       "2                       30      3.836343  \n",
       "3                       28      2.642474  \n",
       "4                       59      4.019930  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_s3_files(bucket, prefix, suffix='.json'):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    logger.info(f\"files recieved from s3 for per inference request --> {response}\")\n",
    "    return [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith(suffix)]\n",
    "\n",
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(BUCKET_NAME, METRICS_PER_CHUNK_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = []\n",
    "for file_key in s3_files:\n",
    "    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=file_key)\n",
    "    file_content = response['Body'].read().decode('utf-8')\n",
    "    json_obj = json.loads(file_content)\n",
    "    json_list.append(json_obj)\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_metrics.shape} from all responses\")\n",
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_config.PrimaryContainer.Environment.ENDPOINT_SERVER_TIMEOUT', 'model_config.PrimaryContainer.Environment.HF_MODEL_ID', 'model_config.PrimaryContainer.Environment.MAX_BATCH_PREFILL_TOKENS', 'model_config.PrimaryContainer.Environment.MAX_INPUT_LENGTH', 'model_config.PrimaryContainer.Environment.MAX_TOTAL_TOKENS', 'model_config.PrimaryContainer.Environment.MODEL_CACHE_ROOT', 'model_config.PrimaryContainer.Environment.SAGEMAKER_ENV', 'model_config.PrimaryContainer.Environment.SAGEMAKER_MODEL_SERVER_WORKERS', 'model_config.PrimaryContainer.Environment.SAGEMAKER_PROGRAM', 'model_config.PrimaryContainer.Environment.SM_NUM_GPUS']\n",
      "Columns in df_responses: Index(['endpoint_name', 'prompt', 'do_sample', 'temperature', 'top_p', 'top_k',\n",
      "       'max_new_tokens', 'truncate', 'completion', 'prompt_tokens',\n",
      "       'completion_tokens', 'latency', 'experiment_name', 'concurrency'],\n",
      "      dtype='object')\n",
      "Columns in df_endpoints: Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri', 'ENDPOINT_SERVER_TIMEOUT', 'HF_MODEL_ID',\n",
      "       'MAX_BATCH_PREFILL_TOKENS', 'MAX_INPUT_LENGTH', 'MAX_TOTAL_TOKENS',\n",
      "       'MODEL_CACHE_ROOT', 'SAGEMAKER_ENV', 'SAGEMAKER_MODEL_SERVER_WORKERS',\n",
      "       'SAGEMAKER_PROGRAM', 'SM_NUM_GPUS'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>ENDPOINT_SERVER_TIMEOUT</th>\n",
       "      <th>HF_MODEL_ID</th>\n",
       "      <th>MAX_BATCH_PREFILL_TOKENS</th>\n",
       "      <th>MAX_INPUT_LENGTH</th>\n",
       "      <th>MAX_TOTAL_TOKENS</th>\n",
       "      <th>MODEL_CACHE_ROOT</th>\n",
       "      <th>SAGEMAKER_ENV</th>\n",
       "      <th>SAGEMAKER_MODEL_SERVER_WORKERS</th>\n",
       "      <th>SAGEMAKER_PROGRAM</th>\n",
       "      <th>SM_NUM_GPUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia is from the ...</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706302351   \n",
       "1  lmistral7b-g5-2xlarge-1706302351   \n",
       "2  lmistral7b-g5-2xlarge-1706302351   \n",
       "3  lmistral7b-g5-2xlarge-1706302351   \n",
       "4  lmistral7b-g5-2xlarge-1706302351   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100       304   \n",
       "1   0.92    120             100       980   \n",
       "2   0.92    120             100       304   \n",
       "3   0.92    120             100       304   \n",
       "4   0.92    120             100       980   \n",
       "\n",
       "                                          completion  prompt_tokens  ...  \\\n",
       "0  \\n\\n```\\nThe genus Sinofranchetia is from the ...            304  ...   \n",
       "1   Both WAGS Atlanta and WAGS are reality televi...            980  ...   \n",
       "2  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304  ...   \n",
       "3  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304  ...   \n",
       "4   Both WAGS Atlanta and WAGS are reality televi...            980  ...   \n",
       "\n",
       "   ENDPOINT_SERVER_TIMEOUT    HF_MODEL_ID MAX_BATCH_PREFILL_TOKENS  \\\n",
       "0                     3600  /opt/ml/model                     8191   \n",
       "1                     3600  /opt/ml/model                     8191   \n",
       "2                     3600  /opt/ml/model                     8191   \n",
       "3                     3600  /opt/ml/model                     8191   \n",
       "4                     3600  /opt/ml/model                     8191   \n",
       "\n",
       "   MAX_INPUT_LENGTH MAX_TOTAL_TOKENS MODEL_CACHE_ROOT SAGEMAKER_ENV  \\\n",
       "0              8191             8192    /opt/ml/model             1   \n",
       "1              8191             8192    /opt/ml/model             1   \n",
       "2              8191             8192    /opt/ml/model             1   \n",
       "3              8191             8192    /opt/ml/model             1   \n",
       "4              8191             8192    /opt/ml/model             1   \n",
       "\n",
       "  SAGEMAKER_MODEL_SERVER_WORKERS SAGEMAKER_PROGRAM SM_NUM_GPUS  \n",
       "0                              1      inference.py           1  \n",
       "1                              1      inference.py           1  \n",
       "2                              1      inference.py           1  \n",
       "3                              1      inference.py           1  \n",
       "4                              1      inference.py           1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_endpoints = pd.json_normalize(endpoint_info_list)\n",
    "df_endpoints['instance_type'] = df_endpoints['endpoint_config.ProductionVariants'].map(lambda x: x[0]['InstanceType'])\n",
    "df_endpoints\n",
    "cols_for_env = [c for c in df_endpoints.columns if 'Environment' in c]\n",
    "print(cols_for_env)\n",
    "cols_of_interest = ['experiment_name', \n",
    "                    'instance_type',\n",
    "                    'endpoint.EndpointName',\n",
    "                    'model_config.ModelName',\n",
    "                    'model_config.PrimaryContainer.Image',   \n",
    "                    'model_config.PrimaryContainer.ModelDataSource.S3DataSource.S3Uri']\n",
    "cols_of_interest.extend(cols_for_env)\n",
    "\n",
    "df_endpoints = df_endpoints[cols_of_interest]\n",
    "df_endpoints = df_endpoints[cols_of_interest]\n",
    "cols_of_interest_renamed = [c.split('.')[-1] for c in cols_of_interest]\n",
    "df_endpoints.columns = cols_of_interest_renamed\n",
    "\n",
    "# Check if 'experiment_name' column exists in both DataFrames\n",
    "print(\"Columns in df_responses:\", df_responses.columns)\n",
    "print(\"Columns in df_endpoints:\", df_endpoints.columns)\n",
    "\n",
    "# Merge operation\n",
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "\n",
    "# Inspect the result\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>ENDPOINT_SERVER_TIMEOUT</th>\n",
       "      <th>HF_MODEL_ID</th>\n",
       "      <th>MAX_BATCH_PREFILL_TOKENS</th>\n",
       "      <th>MAX_INPUT_LENGTH</th>\n",
       "      <th>MAX_TOTAL_TOKENS</th>\n",
       "      <th>MODEL_CACHE_ROOT</th>\n",
       "      <th>SAGEMAKER_ENV</th>\n",
       "      <th>SAGEMAKER_MODEL_SERVER_WORKERS</th>\n",
       "      <th>SAGEMAKER_PROGRAM</th>\n",
       "      <th>SM_NUM_GPUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nThe genus Sinofranchetia is from the ...</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706302351</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706302351   \n",
       "1  lmistral7b-g5-2xlarge-1706302351   \n",
       "2  lmistral7b-g5-2xlarge-1706302351   \n",
       "3  lmistral7b-g5-2xlarge-1706302351   \n",
       "4  lmistral7b-g5-2xlarge-1706302351   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100       304   \n",
       "1   0.92    120             100       980   \n",
       "2   0.92    120             100       304   \n",
       "3   0.92    120             100       304   \n",
       "4   0.92    120             100       980   \n",
       "\n",
       "                                          completion  prompt_tokens  ...  \\\n",
       "0  \\n\\n```\\nThe genus Sinofranchetia is from the ...            304  ...   \n",
       "1   Both WAGS Atlanta and WAGS are reality televi...            980  ...   \n",
       "2  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304  ...   \n",
       "3  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304  ...   \n",
       "4   Both WAGS Atlanta and WAGS are reality televi...            980  ...   \n",
       "\n",
       "   ENDPOINT_SERVER_TIMEOUT    HF_MODEL_ID MAX_BATCH_PREFILL_TOKENS  \\\n",
       "0                     3600  /opt/ml/model                     8191   \n",
       "1                     3600  /opt/ml/model                     8191   \n",
       "2                     3600  /opt/ml/model                     8191   \n",
       "3                     3600  /opt/ml/model                     8191   \n",
       "4                     3600  /opt/ml/model                     8191   \n",
       "\n",
       "   MAX_INPUT_LENGTH MAX_TOTAL_TOKENS MODEL_CACHE_ROOT SAGEMAKER_ENV  \\\n",
       "0              8191             8192    /opt/ml/model             1   \n",
       "1              8191             8192    /opt/ml/model             1   \n",
       "2              8191             8192    /opt/ml/model             1   \n",
       "3              8191             8192    /opt/ml/model             1   \n",
       "4              8191             8192    /opt/ml/model             1   \n",
       "\n",
       "  SAGEMAKER_MODEL_SERVER_WORKERS SAGEMAKER_PROGRAM SM_NUM_GPUS  \n",
       "0                              1      inference.py           1  \n",
       "1                              1      inference.py           1  \n",
       "2                              1      inference.py           1  \n",
       "3                              1      inference.py           1  \n",
       "4                              1      inference.py           1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:38,254] p14684 {2449880785.py:7} INFO - results s3 path for per inference csv --> data/metrics/mistral-7b-tgi-g5-v1/per_inference_request_results.csv\n",
      "[2024-01-26 19:28:38,664] p14684 {2449880785.py:9} INFO - saved results dataframe of shape=(42, 29) in s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference_request_results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/per_inference_request_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert df_results to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_results.to_csv(csv_buffer, index=False)\n",
    "csv_data_results = csv_buffer.getvalue()\n",
    "results_file_name = config['results']['per_inference_request_file'].format(datetime=date_time)\n",
    "results_s3_path = os.path.join(METRICS_DIR, results_file_name)\n",
    "logger.info(f\"results s3 path for per inference csv --> {results_s3_path}\")\n",
    "write_to_s3(csv_data_results, BUCKET_NAME, \"\", METRICS_DIR, results_file_name)\n",
    "logger.info(f\"saved results dataframe of shape={df_results.shape} in s3://{BUCKET_NAME}/{results_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-26 19:28:38,702] p14684 {3262244809.py:10} INFO - results s3 path for metrics csv --> data/metrics/mistral-7b-tgi-g5-v1/all_metrics.csv\n",
      "[2024-01-26 19:28:38,961] p14684 {3262244809.py:12} INFO - saved metrics results dataframe of shape=(10, 31) in s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/all_metrics.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to s3://fmbt/data/metrics/mistral-7b-tgi-g5-v1/all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "df_metrics = pd.merge(left=df_metrics, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "df_metrics.head()\n",
    "\n",
    "# Convert df_metrics to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_metrics.to_csv(csv_buffer, index=False)\n",
    "csv_data_metrics = csv_buffer.getvalue()\n",
    "metrics_file_name = config['results']['all_metrics_file'].format(datetime=date_time)\n",
    "metrics_s3_path = os.path.join(METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"results s3 path for metrics csv --> {metrics_s3_path}\")\n",
    "write_to_s3(csv_data_metrics, BUCKET_NAME, \"\", METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in s3://{BUCKET_NAME}/{metrics_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
