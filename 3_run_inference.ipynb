{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on all deployed endpoints: Various combinations of payloads, concurrency levels, model configurations\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of our solution design includes running inferences on all deployed model endpoints (with different configurations, concurrency levels and payload sizes). This notebook runs inferences in a manner that is calls endpoints concurrently and asychronously to generate responses and record metrics. Here are some of the key components:\n",
    "\n",
    "- **Accessing the deployed endpoints**, creating a predictor object for these endpoints to call them during inference time.\n",
    "\n",
    "- **Functions to define metrics**: This notebook sets stage for metrics to be recorded during the time of invocation of all these models for benchmarking purposes.\n",
    "\n",
    "- **Running Actual Inferences**: Once the metrics are defined, we set a blocker function that is responsible for creating inference on a single payload called get_inference. We then run a series of asynchronous functions that can be viewed in the code (link above), to create asychronous inferefences on the deployed models. The way we send requests are by creating combinations: this means creating combinations of payloads of different sizes that can be viewed in the config.yml file, with different concurrency levels (in this case we first go through all patches of payloads with a concurrency level of 1, then 2, and then 4). You can set this to your desired value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## auto reload all of the changes made in the config/globals.py file \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import itertools\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from globals import *\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker.predictor import Predictor\n",
    "from utils import load_config, count_tokens\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from typing import Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36myaml\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36menum\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Enum\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILEPATH_FILE: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig_filepath.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILE: \u001b[36mstr\u001b[39;49;00m = Path(CONFIG_FILEPATH_FILE).read_text()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCONFIG_FILE=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mCONFIG_FILE\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(CONFIG_FILE, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file:\u001b[37m\u001b[39;49;00m\n",
      "    config = yaml.safe_load(file)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "DATA_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PROMPTS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mprompts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_INFERENCE_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_inference\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_CHUNK_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_chunk\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "MODELS_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "DATASET_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "SCRIPTS_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mscripts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "DIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, DATASET_DIR, METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]\u001b[37m\u001b[39;49;00m\n",
      "TOKENIZER_DIR = \u001b[33m'\u001b[39;49;00m\u001b[33mllama2_tokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "_ = \u001b[36mlist\u001b[39;49;00m(\u001b[36mmap\u001b[39;49;00m(\u001b[34mlambda\u001b[39;49;00m x: os.makedirs(x, exist_ok=\u001b[34mTrue\u001b[39;49;00m), DIR_LIST))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_LIST_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(MODELS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mendpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "REQUEST_PAYLOAD_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(PROMPTS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mpayload.jsonl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTRUNCATE_POLICY\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m, Enum):\u001b[37m\u001b[39;49;00m\n",
      "    AT_PROMPT_TOKEN_LENGTH = \u001b[33m'\u001b[39;49;00m\u001b[33mat-prompt-token-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# misc. metrics related\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PLACE_HOLDER: \u001b[36mint\u001b[39;49;00m = -\u001b[34m1705338041\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# metric filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "COUNTS_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mexperiment_counts.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_DESC_MD_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_W_PRICING_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_w_pricing.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "INSTANCE_PRICING_PER_HOUR_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33minstance_pricing_per_hour.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_w_scores.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option_each_instance_type.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "BUSINESS_SUMMARY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mbusiness_summary.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# plot filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mError rates for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mTokens vs latency for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mtokens_vs_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconcurrency_vs_inference_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mConcurrency Vs latency for different instance type for selected dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "LATENCY_BUDGET: \u001b[36mint\u001b[39;49;00m = \u001b[34m20\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "OVERALL_RESULTS_MD: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m# Results for performance benchmarking\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m**Last modified (UTC): \u001b[39;49;00m\u001b[33m{dttm}\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Summary\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m{business_summary}\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Per instance results\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33mThe following table provides the best combinations for running inference for different sizes prompts on different instance types.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|Dataset   | Instance type   | Recommendation   |\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|---|---|---|\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Dataset=`{dataset}`, instance_type=`{instance_type}`\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULT_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThe best option for staying within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset is a `concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m`. A concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m achieves an `average latency of \u001b[39;49;00m\u001b[33m{latency_mean}\u001b[39;49;00m\u001b[33m seconds`, for an `average prompt size of \u001b[39;49;00m\u001b[33m{prompt_size}\u001b[39;49;00m\u001b[33m tokens` and `completion size of \u001b[39;49;00m\u001b[33m{completion_size}\u001b[39;49;00m\u001b[33m tokens` with `\u001b[39;49;00m\u001b[33m{tpm}\u001b[39;49;00m\u001b[33m transactions/minute`.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_ROW: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33m|`\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m`|`\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m`|\u001b[39;49;00m\u001b[33m{desc}\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_FAILURE_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThis experiment did not find any combination of concurrency level and other configuration settings that could provide a response within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Config.yml file that contains information that is used across this benchmarking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:28:28,545] p1345 {635462509.py:2} INFO - {\n",
      "  \"general\": {\n",
      "    \"name\": \"mistral-7b-tgi-g5-v1\",\n",
      "    \"model_name\": \"mistral7b\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"template_file\": \"prompt_template.txt\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\"\n",
      "  },\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1,\n",
      "      \"max_length_in_tokens\": 500,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 500,\n",
      "      \"max_length_in_tokens\": 1000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1000,\n",
      "      \"max_length_in_tokens\": 2000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 2000,\n",
      "      \"max_length_in_tokens\": 3000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 3000,\n",
      "      \"max_length_in_tokens\": 4000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 305,\n",
      "      \"max_length_in_tokens\": 3997,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    }\n",
      "  ],\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_3000-4000\",\n",
      "    \"weights\": {\n",
      "      \"price_per_tx_wt\": 0.65,\n",
      "      \"latenct_wt\": 0.35\n",
      "    }\n",
      "  },\n",
      "  \"pricing\": {\n",
      "    \"ml.g5.12xlarge\": 7.09,\n",
      "    \"ml.g5.24xlarge\": 10.18,\n",
      "    \"ml.g5.48xlarge\": 20.36,\n",
      "    \"ml.inf2.24xlarge\": 7.79,\n",
      "    \"ml.inf2.48xlarge\": 15.58,\n",
      "    \"ml.p4d.24xlarge\": 37.688\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.1,\n",
      "    \"top_p\": 0.92,\n",
      "    \"top_k\": 120,\n",
      "    \"max_new_tokens\": 100,\n",
      "    \"truncate\": \"at-prompt-token-length\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"huggingface-llm-mistral-7b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"mistral7b\",\n",
      "      \"ep_name\": \"lmistral7b-g5-2xlarge\",\n",
      "      \"instance_type\": \"ml.g5.2xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\",\n",
      "        \"payload_en_1000-2000.jsonl\",\n",
      "        \"payload_en_2000-3000.jsonl\",\n",
      "        \"payload_en_3000-4000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1,\n",
      "        2,\n",
      "        4,\n",
      "        6,\n",
      "        8\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"8191\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"8192\",\n",
      "        \"MAX_BATCH_PREFILL_TOKENS\": \"8191\",\n",
      "        \"SM_NUM_GPUS\": \"1\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"results\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the deployed model endpoints from the endpoints.json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:28:35,125] p1345 {3756670252.py:3} INFO - found information for 1 endpoints\n",
      "[2024-01-23 19:28:35,126] p1345 {3756670252.py:4} INFO - [\n",
      "  {\n",
      "    \"experiment_name\": \"mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "    \"endpoint\": {\n",
      "      \"EndpointName\": \"lmistral7b-g5-2xlarge-1706037700\",\n",
      "      \"EndpointArn\": \"arn:aws:sagemaker:us-east-1:015469603702:endpoint/lmistral7b-g5-2xlarge-1706037700\",\n",
      "      \"EndpointConfigName\": \"lmistral7b-g5-2xlarge-1706037700\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"AllTraffic\",\n",
      "          \"DeployedImages\": [\n",
      "            {\n",
      "              \"SpecifiedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "              \"ResolvedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb\",\n",
      "              \"ResolutionTime\": \"2024-01-23 19:21:42.916000+00:00\"\n",
      "            }\n",
      "          ],\n",
      "          \"CurrentWeight\": 1.0,\n",
      "          \"DesiredWeight\": 1.0,\n",
      "          \"CurrentInstanceCount\": 1,\n",
      "          \"DesiredInstanceCount\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"EndpointStatus\": \"InService\",\n",
      "      \"CreationTime\": \"2024-01-23 19:21:41.849000+00:00\",\n",
      "      \"LastModifiedTime\": \"2024-01-23 19:26:38.869000+00:00\",\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"a6262624-3153-4067-bd2a-27ab95081989\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"a6262624-3153-4067-bd2a-27ab95081989\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"809\",\n",
      "          \"date\": \"Tue, 23 Jan 2024 19:26:42 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"endpoint_config\": {\n",
      "      \"EndpointConfigName\": \"lmistral7b-g5-2xlarge-1706037700\",\n",
      "      \"EndpointConfigArn\": \"arn:aws:sagemaker:us-east-1:015469603702:endpoint-config/lmistral7b-g5-2xlarge-1706037700\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"AllTraffic\",\n",
      "          \"ModelName\": \"hf-llm-mistral-7b-2024-01-23-19-21-40-301\",\n",
      "          \"InitialInstanceCount\": 1,\n",
      "          \"InstanceType\": \"ml.g5.2xlarge\",\n",
      "          \"InitialVariantWeight\": 1.0,\n",
      "          \"ModelDataDownloadTimeoutInSeconds\": 1200,\n",
      "          \"ContainerStartupHealthCheckTimeoutInSeconds\": 1200\n",
      "        }\n",
      "      ],\n",
      "      \"CreationTime\": \"2024-01-23 19:21:41.466000+00:00\",\n",
      "      \"EnableNetworkIsolation\": false,\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"f92e0c54-1f72-4682-8716-38c6eae1ed21\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"f92e0c54-1f72-4682-8716-38c6eae1ed21\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"515\",\n",
      "          \"date\": \"Tue, 23 Jan 2024 19:26:42 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_config\": {\n",
      "      \"ModelName\": \"hf-llm-mistral-7b-2024-01-23-19-21-40-301\",\n",
      "      \"PrimaryContainer\": {\n",
      "        \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "        \"Mode\": \"SingleModel\",\n",
      "        \"ModelDataSource\": {\n",
      "          \"S3DataSource\": {\n",
      "            \"S3Uri\": \"s3://jumpstart-cache-prod-us-east-1/huggingface-llm/huggingface-llm-mistral-7b/artifacts/inference-prepack/v1.0.0/\",\n",
      "            \"S3DataType\": \"S3Prefix\",\n",
      "            \"CompressionType\": \"None\",\n",
      "            \"ModelAccessConfig\": {\n",
      "              \"AcceptEula\": true\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"Environment\": {\n",
      "          \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "          \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "          \"MAX_BATCH_PREFILL_TOKENS\": \"8191\",\n",
      "          \"MAX_INPUT_LENGTH\": \"8191\",\n",
      "          \"MAX_TOTAL_TOKENS\": \"8192\",\n",
      "          \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "          \"SAGEMAKER_ENV\": \"1\",\n",
      "          \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "          \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "          \"SM_NUM_GPUS\": \"1\"\n",
      "        }\n",
      "      },\n",
      "      \"ExecutionRoleArn\": \"arn:aws:iam::015469603702:role/service-role/AmazonSageMaker-ExecutionRole-20231116T132325\",\n",
      "      \"CreationTime\": \"2024-01-23 19:21:40.932000+00:00\",\n",
      "      \"ModelArn\": \"arn:aws:sagemaker:us-east-1:015469603702:model/hf-llm-mistral-7b-2024-01-23-19-21-40-301\",\n",
      "      \"EnableNetworkIsolation\": true,\n",
      "      \"DeploymentRecommendation\": {\n",
      "        \"RecommendationStatus\": \"COMPLETED\",\n",
      "        \"RealTimeInferenceRecommendations\": []\n",
      "      },\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"fc7ea6d7-0fa9-41fb-911f-44ce941c89ff\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"fc7ea6d7-0fa9-41fb-911f-44ce941c89ff\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"1168\",\n",
      "          \"date\": \"Tue, 23 Jan 2024 19:26:43 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# read the list of deployed endpoints\n",
    "endpoint_info_list = json.loads(Path(ENDPOINT_LIST_FPATH).read_text())\n",
    "logger.info(f\"found information for {len(endpoint_info_list)} endpoints\")\n",
    "logger.info(json.dumps(endpoint_info_list, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:28:41,095] p1345 {1455142584.py:3} INFO - there are 1 deployed endpoint(s), endpoint_name_list->['lmistral7b-g5-2xlarge-1706037700']\n"
     ]
    }
   ],
   "source": [
    "# List down the endpoint names that have been deployed\n",
    "endpoint_name_list = [e['endpoint']['EndpointName'] for e in endpoint_info_list]\n",
    "logger.info(f\"there are {len(endpoint_name_list)} deployed endpoint(s), endpoint_name_list->{endpoint_name_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating predictor objects from the deployed endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:28:43,288] p1345 {3970465137.py:15} INFO - [<sagemaker.base_predictor.Predictor object at 0x7ff6ec6c9db0>]\n"
     ]
    }
   ],
   "source": [
    "# create predictor objects\n",
    "\n",
    "## create a sagemaker predictor for these endpoints\n",
    "def create_predictor(endpoint_name: str) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "    # Create a SageMaker Predictor object\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "        serializer=JSONSerializer()\n",
    "    )\n",
    "    return predictor\n",
    "\n",
    "## Display the list of predictor objects that have been deployed ready for inferencing from\n",
    "predictor_list: List = [create_predictor(ep) for ep in endpoint_name_list]\n",
    "logger.info(predictor_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions to define and calculate metrics during the time of invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_sum(l: List) -> Union[int, float]:\n",
    "    return sum(filter(None, l))\n",
    "\n",
    "def safe_div(n: Union[int, float], d: Union[int, float]) -> Optional[Union[int, float]]:\n",
    "    return n/d if d else None\n",
    "\n",
    "## Represents the function to calculate all of the metrics at the time of inference\n",
    "def calculate_metrics(responses, chunk, elapsed_async, experiment_name, concurrency, payload_file) -> Dict:\n",
    "    \n",
    "    ## calculate errors based on the completion status of the inference prompt\n",
    "    errors = [r for r in responses if r['completion'] is None]\n",
    "    \n",
    "    ## Calculate the difference as the successes \n",
    "    successes = len(chunk) - len(errors)\n",
    "    \n",
    "    ## Count all of the prompts token count during inference\n",
    "    all_prompts_token_count = safe_sum([r['prompt_tokens'] for r in responses])\n",
    "    prompt_token_throughput = round(all_prompts_token_count / elapsed_async, 2)\n",
    "    prompt_token_count_mean = safe_div(all_prompts_token_count, successes)\n",
    "    all_completions_token_count = safe_sum([r['completion_tokens'] for r in responses])\n",
    "    completion_token_throughput = round(all_completions_token_count / elapsed_async, 2)\n",
    "    completion_token_count_mean = safe_div(all_completions_token_count, successes)\n",
    "    transactions_per_second = round(successes / elapsed_async, 2)\n",
    "    transactions_per_minute = int(transactions_per_second * 60)\n",
    "    \n",
    "    ## calculate the latency mean utilizing the safe_sum function defined above\n",
    "    latency_mean = safe_div(safe_sum([r['latency'] for r in responses]), successes)\n",
    "    \n",
    "    ## Function returns all these values at the time of the invocations\n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'concurrency': concurrency,\n",
    "        'payload_file': payload_file,\n",
    "        'errors': errors,\n",
    "        'successes': successes,\n",
    "        'error_rate': len(errors)/len(chunk),\n",
    "        'all_prompts_token_count': all_prompts_token_count,\n",
    "        'prompt_token_count_mean': prompt_token_count_mean,\n",
    "        'prompt_token_throughput': prompt_token_throughput,\n",
    "        'all_completions_token_count': all_completions_token_count,\n",
    "        'completion_token_count_mean': completion_token_count_mean,\n",
    "        'completion_token_throughput': completion_token_throughput,\n",
    "        'transactions': len(chunk),\n",
    "        'transactions_per_second': transactions_per_second,\n",
    "        'transactions_per_minute': transactions_per_minute,\n",
    "        'latency_mean': latency_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a blocker function and a series of asynchronous concurrent model prompt invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_metrics(endpoint_name=None,\n",
    "                    prompt=None,\n",
    "                    inference_params=None,\n",
    "                    completion=None,\n",
    "                    prompt_tokens=None,\n",
    "                    completion_tokens=None,\n",
    "                    latency=None) -> Dict:\n",
    "    return dict(endpoint_name=endpoint_name,                \n",
    "                prompt=prompt,\n",
    "                **inference_params,\n",
    "                completion=completion,\n",
    "                prompt_tokens=prompt_tokens,\n",
    "                completion_tokens=completion_tokens,\n",
    "                latency=latency)\n",
    "\n",
    "def get_inference(predictor, payload) -> Dict:\n",
    "    \n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "    latency = 0\n",
    "\n",
    "    try:\n",
    "        prompt_tokens = count_tokens(payload['inputs'])\n",
    "        logger.info(f\"get_inference, endpoint={predictor.endpoint_name}, prompt_tokens={prompt_tokens}\")\n",
    "\n",
    "        # get inference\n",
    "        st = time.perf_counter()        \n",
    "        response = predictor.predict(payload)        \n",
    "        latency = time.perf_counter() - st\n",
    "\n",
    "        if isinstance(response, bytes):\n",
    "            response = response.decode('utf-8')\n",
    "        response_json = json.loads(response)\n",
    "        if isinstance(response_json, list):\n",
    "            response_json = response_json[0]\n",
    "\n",
    "        completion = response_json.get(\"generated_text\", \"\")\n",
    "        completion_tokens = count_tokens(completion)\n",
    "\n",
    "        # Set metrics and logging for both cases\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               completion,\n",
    "                               prompt_tokens,\n",
    "                               completion_tokens,\n",
    "                               latency)\n",
    "        # logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, response={json.dumps(response, indent=2)}, latency={latency:.2f}\")\n",
    "        logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, completion_tokens={completion_tokens}, latency={latency:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error occurred with {predictor.endpoint_name}, exception={str(e)}\")\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               None,\n",
    "                               prompt_tokens,\n",
    "                               None,\n",
    "                               None)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a series of asynchronous functions to invoke and run inferences concurrently and asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents a function to start invoking models in separate thread asynchronously for the blocker function\n",
    "async def async_get_inference(predictor, payload: Dict) -> Dict:\n",
    "    return await asyncio.to_thread(get_inference, predictor, payload)\n",
    "\n",
    "## Gathers all of the tasks and sets of the concurrent calling of the asychronous invocations\n",
    "async def async_get_all_inferences(predictor, payload_list: List) -> List:\n",
    "    return await asyncio.gather(*[async_get_inference(predictor, payload) for payload in payload_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This function runs the asychronous function series above together for different experiments and concurrency levels.\n",
    "async def run_inferences(predictor: sagemaker.base_predictor.Predictor, chunk: List, experiment: Dict, concurrency: int, payload_file: str) -> Tuple[List, Dict]:\n",
    "    logger.info(f\"Processing chunk with concurrency={concurrency}\")\n",
    "    s = time.perf_counter()\n",
    "    responses = await async_get_all_inferences(predictor, chunk)\n",
    "    elapsed_async = time.perf_counter() - s\n",
    "\n",
    "    # Add more metadata about this experiment\n",
    "    for r in responses:\n",
    "        r['experiment_name'] = experiment['name']\n",
    "        r['concurrency'] = concurrency\n",
    "\n",
    "    metrics = calculate_metrics(responses, chunk, elapsed_async, experiment['name'], concurrency, payload_file)\n",
    "    return responses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create the predictors from the experiment we are iterating over\n",
    "def create_predictor_for_experiment(experiment: str, config: Dict, endpoint_info_list: List) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "\n",
    "    ## Here, we set the index and then iterate through the experiments\n",
    "    e_idx = config['experiments'].index(experiment) + 1\n",
    "\n",
    "    ## Iterate through the endpoint information to fetch the endpoint name\n",
    "    ep_info = [e for e in endpoint_info_list if e['experiment_name'] == experiment['name']]\n",
    "    if not ep_info:\n",
    "        logger.error(f\"endpoint for experiment={experiment['name']} not found, skipping\")\n",
    "        return None\n",
    "    ep_name = ep_info[0]['endpoint']['EndpointName']\n",
    "    logger.info(f\"experiment={e_idx}, name={experiment['name']}, ep_name={ep_name}\")\n",
    "\n",
    "    # create a predictor from each endpoint in experiments\n",
    "    return create_predictor(ep_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here, we will process combinations of concurrency levels, the payload files and then loop through the \n",
    "## different combinations to make payloads splitted in terms of the concurrency metric and how we can run \n",
    "## it and make inference\n",
    "\n",
    "def create_payload_dict(jline: str, experiment: Dict) -> Dict:\n",
    "    payload: Dict = json.loads(jline)\n",
    "    if experiment.get('remove_truncate', False) is True:\n",
    "        if payload['parameters'].get('truncate'):\n",
    "            del payload['parameters']['truncate']\n",
    "    return payload\n",
    "    \n",
    "    \n",
    "def create_combinations(experiment: Dict) -> List[Tuple]:\n",
    "    combinations_data = []\n",
    "\n",
    "    # Repeat for each concurrency level\n",
    "    combinations = list(itertools.product(experiment['concurrency_levels'], experiment['payload_files']))\n",
    "    logger.info(f\"there are {len(combinations)} combinations of {combinations} to run\")\n",
    "\n",
    "    for concurrency, payload_file in combinations:\n",
    "        # Read the payload file\n",
    "        fpath = os.path.join(PROMPTS_DIR, payload_file)\n",
    "        payload_list = [create_payload_dict(jline, experiment) for jline in Path(fpath).read_text().splitlines()]\n",
    "        logger.info(f\"read {fpath}, contains {len(payload_list)} lines\")      \n",
    "\n",
    "        logger.info(f\"creating combinations for concurrency={concurrency}, payload_file={payload_file}, payload_list length={len(payload_list)}\")\n",
    "        \n",
    "        # check if we have enough element in the list to run at least the concurrency count numberof transactions..\n",
    "        # for example if there are only 2 prompts and we want to run say 6 in parallel then take the first element (prompt)\n",
    "        # and replicate it 6-2=4 times and add it to the original list\n",
    "        n = concurrency\n",
    "        \n",
    "        if len(payload_list) < n:\n",
    "            elements_to_add = n - len(payload_list)\n",
    "            element_to_replicate = payload_list[0]\n",
    "            # payload_list = payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "        # Split the original list into sublists which contain the number of requests we want to send concurrently        \n",
    "        payload_list_splitted = [payload_list[i * n:(i + 1) * n] for i in range((len(payload_list) + n - 1) // n )]  \n",
    "        \n",
    "        for p in payload_list_splitted:\n",
    "            if len(p) < n:\n",
    "                elements_to_add = n - len(p)\n",
    "                element_to_replicate = p[0]\n",
    "                # p = p.extend([element_to_replicate]*elements_to_add)\n",
    "                p.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "\n",
    "        # Only keep lists that have at least concurrency number of elements\n",
    "        len_before = len(payload_list_splitted)\n",
    "        payload_list_splitted = [p for p in payload_list_splitted if len(p) == concurrency]\n",
    "        logger.info(f\"after only retaining chunks of length {concurrency}, we have {len(payload_list_splitted)} chunks, previously we had {len_before} chunks\")\n",
    "        combinations_data.append((concurrency, payload_file, payload_list_splitted))\n",
    "    logger.info(f\"there are {len(combinations)} for {experiment}\")\n",
    "    return combinations_data\n",
    "\n",
    "# process_combinations(experiment, predictor, PROMPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:29:31,082] p1345 {663335596.py:13} INFO - experiment=1, name=mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0, ep_name=lmistral7b-g5-2xlarge-1706037700\n",
      "[2024-01-23 19:29:31,108] p1345 {1352403322.py:18} INFO - there are 25 combinations of [(1, 'payload_en_1-500.jsonl'), (1, 'payload_en_500-1000.jsonl'), (1, 'payload_en_1000-2000.jsonl'), (1, 'payload_en_2000-3000.jsonl'), (1, 'payload_en_3000-4000.jsonl'), (2, 'payload_en_1-500.jsonl'), (2, 'payload_en_500-1000.jsonl'), (2, 'payload_en_1000-2000.jsonl'), (2, 'payload_en_2000-3000.jsonl'), (2, 'payload_en_3000-4000.jsonl'), (4, 'payload_en_1-500.jsonl'), (4, 'payload_en_500-1000.jsonl'), (4, 'payload_en_1000-2000.jsonl'), (4, 'payload_en_2000-3000.jsonl'), (4, 'payload_en_3000-4000.jsonl'), (6, 'payload_en_1-500.jsonl'), (6, 'payload_en_500-1000.jsonl'), (6, 'payload_en_1000-2000.jsonl'), (6, 'payload_en_2000-3000.jsonl'), (6, 'payload_en_3000-4000.jsonl'), (8, 'payload_en_1-500.jsonl'), (8, 'payload_en_500-1000.jsonl'), (8, 'payload_en_1000-2000.jsonl'), (8, 'payload_en_2000-3000.jsonl'), (8, 'payload_en_3000-4000.jsonl')] to run\n",
      "[2024-01-23 19:29:31,112] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 19:29:31,112] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 19:29:31,113] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 19:29:31,117] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 19:29:31,117] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 19:29:31,118] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 19:29:31,121] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 19:29:31,121] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 19:29:31,122] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 15 chunks, previously we had 15 chunks\n",
      "[2024-01-23 19:29:31,127] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 19:29:31,127] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 19:29:31,128] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 32 chunks, previously we had 32 chunks\n",
      "[2024-01-23 19:29:31,138] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 19:29:31,139] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=1, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 19:29:31,139] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 1, we have 57 chunks, previously we had 57 chunks\n",
      "[2024-01-23 19:29:31,141] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 19:29:31,142] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 19:29:31,142] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 19:29:31,144] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 19:29:31,144] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 19:29:31,145] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 19:29:31,147] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 19:29:31,148] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 19:29:31,148] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-23 19:29:31,153] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 19:29:31,153] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 19:29:31,154] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 16 chunks, previously we had 16 chunks\n",
      "[2024-01-23 19:29:31,163] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 19:29:31,163] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=2, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 19:29:31,164] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 2, we have 29 chunks, previously we had 29 chunks\n",
      "[2024-01-23 19:29:31,166] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 19:29:31,166] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 19:29:31,171] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 19:29:31,173] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 19:29:31,173] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 19:29:31,174] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 19:29:31,177] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 19:29:31,178] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 19:29:31,178] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 4 chunks, previously we had 4 chunks\n",
      "[2024-01-23 19:29:31,183] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 19:29:31,184] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 19:29:31,185] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-23 19:29:31,192] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 19:29:31,192] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=4, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 19:29:31,193] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 4, we have 15 chunks, previously we had 15 chunks\n",
      "[2024-01-23 19:29:31,195] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 19:29:31,195] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 19:29:31,196] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 19:29:31,197] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 19:29:31,198] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 19:29:31,199] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 19:29:31,201] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 19:29:31,202] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 19:29:31,202] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 3 chunks, previously we had 3 chunks\n",
      "[2024-01-23 19:29:31,207] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 19:29:31,207] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 19:29:31,208] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 6 chunks, previously we had 6 chunks\n",
      "[2024-01-23 19:29:31,216] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 19:29:31,217] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=6, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 19:29:31,218] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 6, we have 10 chunks, previously we had 10 chunks\n",
      "[2024-01-23 19:29:31,219] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-23 19:29:31,220] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-23 19:29:31,220] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 19:29:31,222] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-23 19:29:31,222] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-23 19:29:31,223] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-23 19:29:31,226] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_1000-2000.jsonl, contains 15 lines\n",
      "[2024-01-23 19:29:31,226] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_1000-2000.jsonl, payload_list length=15\n",
      "[2024-01-23 19:29:31,227] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 2 chunks, previously we had 2 chunks\n",
      "[2024-01-23 19:29:31,231] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_2000-3000.jsonl, contains 32 lines\n",
      "[2024-01-23 19:29:31,232] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_2000-3000.jsonl, payload_list length=32\n",
      "[2024-01-23 19:29:31,232] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 4 chunks, previously we had 4 chunks\n",
      "[2024-01-23 19:29:31,242] p1345 {1352403322.py:24} INFO - read data/prompts/payload_en_3000-4000.jsonl, contains 57 lines\n",
      "[2024-01-23 19:29:31,242] p1345 {1352403322.py:26} INFO - creating combinations for concurrency=8, payload_file=payload_en_3000-4000.jsonl, payload_list length=57\n",
      "[2024-01-23 19:29:31,243] p1345 {1352403322.py:53} INFO - after only retaining chunks of length 8, we have 8 chunks, previously we had 8 chunks\n",
      "[2024-01-23 19:29:31,244] p1345 {1352403322.py:55} INFO - there are 25 for {'name': 'mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'huggingface-llm-mistral-7b', 'model_version': '*', 'model_name': 'mistral7b', 'ep_name': 'lmistral7b-g5-2xlarge', 'instance_type': 'ml.g5.2xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl', 'payload_en_500-1000.jsonl', 'payload_en_1000-2000.jsonl', 'payload_en_2000-3000.jsonl', 'payload_en_3000-4000.jsonl'], 'concurrency_levels': [1, 2, 4, 6, 8], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '8191', 'MAX_TOTAL_TOKENS': '8192', 'MAX_BATCH_PREFILL_TOKENS': '8191', 'SM_NUM_GPUS': '1', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}\n",
      "[2024-01-23 19:29:31,247] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-23 19:29:31,247] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:31,254] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:29:31,889] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=13, latency=0.63\n",
      "[2024-01-23 19:29:31,908] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=1\n",
      "[2024-01-23 19:29:31,908] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-23 19:29:31,909] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:31,915] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:29:32,332] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=0.42\n",
      "[2024-01-23 19:29:32,350] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=1\n",
      "[2024-01-23 19:29:32,351] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/15\n",
      "[2024-01-23 19:29:32,352] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:32,358] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1339\n",
      "[2024-01-23 19:29:34,627] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=51, latency=2.27\n",
      "[2024-01-23 19:29:34,647] p1345 {3496713318.py:39} INFO - completed processing chunk 1/15 with concurrency=1\n",
      "[2024-01-23 19:29:34,648] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/15\n",
      "[2024-01-23 19:29:34,648] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:34,656] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1932\n",
      "[2024-01-23 19:29:39,137] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=4.48\n",
      "[2024-01-23 19:29:39,156] p1345 {3496713318.py:39} INFO - completed processing chunk 2/15 with concurrency=1\n",
      "[2024-01-23 19:29:39,157] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/15\n",
      "[2024-01-23 19:29:39,157] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:39,163] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1154\n",
      "[2024-01-23 19:29:40,214] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=19, latency=1.05\n",
      "[2024-01-23 19:29:40,235] p1345 {3496713318.py:39} INFO - completed processing chunk 3/15 with concurrency=1\n",
      "[2024-01-23 19:29:40,235] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/15\n",
      "[2024-01-23 19:29:40,236] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:40,242] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1646\n",
      "[2024-01-23 19:29:40,680] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.44\n",
      "[2024-01-23 19:29:40,699] p1345 {3496713318.py:39} INFO - completed processing chunk 4/15 with concurrency=1\n",
      "[2024-01-23 19:29:40,699] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/15\n",
      "[2024-01-23 19:29:40,700] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:40,707] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1397\n",
      "[2024-01-23 19:29:44,794] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=4.09\n",
      "[2024-01-23 19:29:44,813] p1345 {3496713318.py:39} INFO - completed processing chunk 5/15 with concurrency=1\n",
      "[2024-01-23 19:29:44,814] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/15\n",
      "[2024-01-23 19:29:44,815] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:44,822] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1746\n",
      "[2024-01-23 19:29:49,111] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=4.29\n",
      "[2024-01-23 19:29:49,131] p1345 {3496713318.py:39} INFO - completed processing chunk 6/15 with concurrency=1\n",
      "[2024-01-23 19:29:49,131] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=7/15\n",
      "[2024-01-23 19:29:49,132] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:49,139] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1373\n",
      "[2024-01-23 19:29:50,960] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=39, latency=1.82\n",
      "[2024-01-23 19:29:50,979] p1345 {3496713318.py:39} INFO - completed processing chunk 7/15 with concurrency=1\n",
      "[2024-01-23 19:29:50,980] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=8/15\n",
      "[2024-01-23 19:29:50,980] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:50,987] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1598\n",
      "[2024-01-23 19:29:52,149] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=20, latency=1.16\n",
      "[2024-01-23 19:29:52,169] p1345 {3496713318.py:39} INFO - completed processing chunk 8/15 with concurrency=1\n",
      "[2024-01-23 19:29:52,170] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=9/15\n",
      "[2024-01-23 19:29:52,170] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:52,178] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1743\n",
      "[2024-01-23 19:29:56,468] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=4.29\n",
      "[2024-01-23 19:29:56,486] p1345 {3496713318.py:39} INFO - completed processing chunk 9/15 with concurrency=1\n",
      "[2024-01-23 19:29:56,487] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=10/15\n",
      "[2024-01-23 19:29:56,490] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:29:56,496] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1539\n",
      "[2024-01-23 19:30:00,651] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=106, latency=4.15\n",
      "[2024-01-23 19:30:00,670] p1345 {3496713318.py:39} INFO - completed processing chunk 10/15 with concurrency=1\n",
      "[2024-01-23 19:30:00,671] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=11/15\n",
      "[2024-01-23 19:30:00,671] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:00,679] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1695\n",
      "[2024-01-23 19:30:04,957] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=92, latency=4.28\n",
      "[2024-01-23 19:30:04,978] p1345 {3496713318.py:39} INFO - completed processing chunk 11/15 with concurrency=1\n",
      "[2024-01-23 19:30:04,978] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=12/15\n",
      "[2024-01-23 19:30:04,979] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:04,986] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1421\n",
      "[2024-01-23 19:30:09,065] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=4.08\n",
      "[2024-01-23 19:30:09,084] p1345 {3496713318.py:39} INFO - completed processing chunk 12/15 with concurrency=1\n",
      "[2024-01-23 19:30:09,084] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=13/15\n",
      "[2024-01-23 19:30:09,085] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:09,093] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1918\n",
      "[2024-01-23 19:30:09,569] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.48\n",
      "[2024-01-23 19:30:09,589] p1345 {3496713318.py:39} INFO - completed processing chunk 13/15 with concurrency=1\n",
      "[2024-01-23 19:30:09,589] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=14/15\n",
      "[2024-01-23 19:30:09,590] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:09,598] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1910\n",
      "[2024-01-23 19:30:13,942] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=4.34\n",
      "[2024-01-23 19:30:13,963] p1345 {3496713318.py:39} INFO - completed processing chunk 14/15 with concurrency=1\n",
      "[2024-01-23 19:30:13,963] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=15/15\n",
      "[2024-01-23 19:30:13,964] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:13,972] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1939\n",
      "[2024-01-23 19:30:14,608] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=0.64\n",
      "[2024-01-23 19:30:14,628] p1345 {3496713318.py:39} INFO - completed processing chunk 15/15 with concurrency=1\n",
      "[2024-01-23 19:30:14,629] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/32\n",
      "[2024-01-23 19:30:14,629] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:14,638] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2637\n",
      "[2024-01-23 19:30:19,453] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=4.81\n",
      "[2024-01-23 19:30:19,472] p1345 {3496713318.py:39} INFO - completed processing chunk 1/32 with concurrency=1\n",
      "[2024-01-23 19:30:19,472] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/32\n",
      "[2024-01-23 19:30:19,473] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:19,483] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3000\n",
      "[2024-01-23 19:30:24,497] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=5.01\n",
      "[2024-01-23 19:30:24,518] p1345 {3496713318.py:39} INFO - completed processing chunk 2/32 with concurrency=1\n",
      "[2024-01-23 19:30:24,518] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/32\n",
      "[2024-01-23 19:30:24,519] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:24,526] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2148\n",
      "[2024-01-23 19:30:25,612] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=14, latency=1.09\n",
      "[2024-01-23 19:30:25,634] p1345 {3496713318.py:39} INFO - completed processing chunk 3/32 with concurrency=1\n",
      "[2024-01-23 19:30:25,634] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/32\n",
      "[2024-01-23 19:30:25,635] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:25,644] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2715\n",
      "[2024-01-23 19:30:30,500] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=4.86\n",
      "[2024-01-23 19:30:30,521] p1345 {3496713318.py:39} INFO - completed processing chunk 4/32 with concurrency=1\n",
      "[2024-01-23 19:30:30,522] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/32\n",
      "[2024-01-23 19:30:30,522] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:30,531] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2404\n",
      "[2024-01-23 19:30:35,200] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=96, latency=4.67\n",
      "[2024-01-23 19:30:35,226] p1345 {3496713318.py:39} INFO - completed processing chunk 5/32 with concurrency=1\n",
      "[2024-01-23 19:30:35,227] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/32\n",
      "[2024-01-23 19:30:35,227] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:35,237] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2150\n",
      "[2024-01-23 19:30:36,563] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=21, latency=1.32\n",
      "[2024-01-23 19:30:36,582] p1345 {3496713318.py:39} INFO - completed processing chunk 6/32 with concurrency=1\n",
      "[2024-01-23 19:30:36,583] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=7/32\n",
      "[2024-01-23 19:30:36,584] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:36,593] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2803\n",
      "[2024-01-23 19:30:41,471] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=105, latency=4.88\n",
      "[2024-01-23 19:30:41,490] p1345 {3496713318.py:39} INFO - completed processing chunk 7/32 with concurrency=1\n",
      "[2024-01-23 19:30:41,491] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=8/32\n",
      "[2024-01-23 19:30:41,491] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:41,500] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2369\n",
      "[2024-01-23 19:30:42,724] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=18, latency=1.22\n",
      "[2024-01-23 19:30:42,743] p1345 {3496713318.py:39} INFO - completed processing chunk 8/32 with concurrency=1\n",
      "[2024-01-23 19:30:42,744] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=9/32\n",
      "[2024-01-23 19:30:42,745] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:42,754] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2675\n",
      "[2024-01-23 19:30:47,646] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=4.89\n",
      "[2024-01-23 19:30:47,663] p1345 {3496713318.py:39} INFO - completed processing chunk 9/32 with concurrency=1\n",
      "[2024-01-23 19:30:47,664] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=10/32\n",
      "[2024-01-23 19:30:47,664] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:47,672] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2541\n",
      "[2024-01-23 19:30:52,400] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=4.73\n",
      "[2024-01-23 19:30:52,420] p1345 {3496713318.py:39} INFO - completed processing chunk 10/32 with concurrency=1\n",
      "[2024-01-23 19:30:52,420] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=11/32\n",
      "[2024-01-23 19:30:52,421] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:52,428] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2186\n",
      "[2024-01-23 19:30:54,429] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=37, latency=2.00\n",
      "[2024-01-23 19:30:54,448] p1345 {3496713318.py:39} INFO - completed processing chunk 11/32 with concurrency=1\n",
      "[2024-01-23 19:30:54,448] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=12/32\n",
      "[2024-01-23 19:30:54,449] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:54,458] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2775\n",
      "[2024-01-23 19:30:55,333] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=0.87\n",
      "[2024-01-23 19:30:55,353] p1345 {3496713318.py:39} INFO - completed processing chunk 12/32 with concurrency=1\n",
      "[2024-01-23 19:30:55,354] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=13/32\n",
      "[2024-01-23 19:30:55,354] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:30:55,363] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2686\n",
      "[2024-01-23 19:31:00,185] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=4.82\n",
      "[2024-01-23 19:31:00,204] p1345 {3496713318.py:39} INFO - completed processing chunk 13/32 with concurrency=1\n",
      "[2024-01-23 19:31:00,204] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=14/32\n",
      "[2024-01-23 19:31:00,205] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:00,214] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2500\n",
      "[2024-01-23 19:31:04,929] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=4.71\n",
      "[2024-01-23 19:31:04,954] p1345 {3496713318.py:39} INFO - completed processing chunk 14/32 with concurrency=1\n",
      "[2024-01-23 19:31:04,954] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=15/32\n",
      "[2024-01-23 19:31:04,955] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:04,964] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2443\n",
      "[2024-01-23 19:31:05,826] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=0.86\n",
      "[2024-01-23 19:31:05,845] p1345 {3496713318.py:39} INFO - completed processing chunk 15/32 with concurrency=1\n",
      "[2024-01-23 19:31:05,845] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=16/32\n",
      "[2024-01-23 19:31:05,846] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:05,855] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2321\n",
      "[2024-01-23 19:31:10,456] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.60\n",
      "[2024-01-23 19:31:10,479] p1345 {3496713318.py:39} INFO - completed processing chunk 16/32 with concurrency=1\n",
      "[2024-01-23 19:31:10,481] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=17/32\n",
      "[2024-01-23 19:31:10,482] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:10,495] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2428\n",
      "[2024-01-23 19:31:15,168] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=4.67\n",
      "[2024-01-23 19:31:15,186] p1345 {3496713318.py:39} INFO - completed processing chunk 17/32 with concurrency=1\n",
      "[2024-01-23 19:31:15,187] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=18/32\n",
      "[2024-01-23 19:31:15,188] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:15,197] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2458\n",
      "[2024-01-23 19:31:15,814] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.62\n",
      "[2024-01-23 19:31:15,833] p1345 {3496713318.py:39} INFO - completed processing chunk 18/32 with concurrency=1\n",
      "[2024-01-23 19:31:15,833] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=19/32\n",
      "[2024-01-23 19:31:15,834] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:15,843] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2101\n",
      "[2024-01-23 19:31:20,337] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.49\n",
      "[2024-01-23 19:31:20,354] p1345 {3496713318.py:39} INFO - completed processing chunk 19/32 with concurrency=1\n",
      "[2024-01-23 19:31:20,355] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=20/32\n",
      "[2024-01-23 19:31:20,355] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:20,364] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2440\n",
      "[2024-01-23 19:31:20,981] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.62\n",
      "[2024-01-23 19:31:21,000] p1345 {3496713318.py:39} INFO - completed processing chunk 20/32 with concurrency=1\n",
      "[2024-01-23 19:31:21,001] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=21/32\n",
      "[2024-01-23 19:31:21,001] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:21,011] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2711\n",
      "[2024-01-23 19:31:25,842] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=4.83\n",
      "[2024-01-23 19:31:25,860] p1345 {3496713318.py:39} INFO - completed processing chunk 21/32 with concurrency=1\n",
      "[2024-01-23 19:31:25,860] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=22/32\n",
      "[2024-01-23 19:31:25,861] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:25,871] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2771\n",
      "[2024-01-23 19:31:30,738] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=4.87\n",
      "[2024-01-23 19:31:30,762] p1345 {3496713318.py:39} INFO - completed processing chunk 22/32 with concurrency=1\n",
      "[2024-01-23 19:31:30,763] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=23/32\n",
      "[2024-01-23 19:31:30,764] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:30,774] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2691\n",
      "[2024-01-23 19:31:35,593] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=4.82\n",
      "[2024-01-23 19:31:35,614] p1345 {3496713318.py:39} INFO - completed processing chunk 23/32 with concurrency=1\n",
      "[2024-01-23 19:31:35,615] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=24/32\n",
      "[2024-01-23 19:31:35,615] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:35,625] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2624\n",
      "[2024-01-23 19:31:40,422] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.80\n",
      "[2024-01-23 19:31:40,441] p1345 {3496713318.py:39} INFO - completed processing chunk 24/32 with concurrency=1\n",
      "[2024-01-23 19:31:40,442] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=25/32\n",
      "[2024-01-23 19:31:40,442] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:40,450] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2062\n",
      "[2024-01-23 19:31:44,897] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.45\n",
      "[2024-01-23 19:31:44,919] p1345 {3496713318.py:39} INFO - completed processing chunk 25/32 with concurrency=1\n",
      "[2024-01-23 19:31:44,920] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=26/32\n",
      "[2024-01-23 19:31:44,921] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:44,929] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2213\n",
      "[2024-01-23 19:31:45,781] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=0.85\n",
      "[2024-01-23 19:31:45,801] p1345 {3496713318.py:39} INFO - completed processing chunk 26/32 with concurrency=1\n",
      "[2024-01-23 19:31:45,802] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=27/32\n",
      "[2024-01-23 19:31:45,802] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:45,811] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2608\n",
      "[2024-01-23 19:31:50,600] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=4.79\n",
      "[2024-01-23 19:31:50,619] p1345 {3496713318.py:39} INFO - completed processing chunk 27/32 with concurrency=1\n",
      "[2024-01-23 19:31:50,619] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=28/32\n",
      "[2024-01-23 19:31:50,620] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:50,629] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2770\n",
      "[2024-01-23 19:31:51,335] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.71\n",
      "[2024-01-23 19:31:51,357] p1345 {3496713318.py:39} INFO - completed processing chunk 28/32 with concurrency=1\n",
      "[2024-01-23 19:31:51,358] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=29/32\n",
      "[2024-01-23 19:31:51,358] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:51,368] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2564\n",
      "[2024-01-23 19:31:52,126] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=0.76\n",
      "[2024-01-23 19:31:52,146] p1345 {3496713318.py:39} INFO - completed processing chunk 29/32 with concurrency=1\n",
      "[2024-01-23 19:31:52,147] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=30/32\n",
      "[2024-01-23 19:31:52,148] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:52,156] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2333\n",
      "[2024-01-23 19:31:52,875] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=0.72\n",
      "[2024-01-23 19:31:52,892] p1345 {3496713318.py:39} INFO - completed processing chunk 30/32 with concurrency=1\n",
      "[2024-01-23 19:31:52,893] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=31/32\n",
      "[2024-01-23 19:31:52,894] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:52,903] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2613\n",
      "[2024-01-23 19:31:53,754] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=0.85\n",
      "[2024-01-23 19:31:53,772] p1345 {3496713318.py:39} INFO - completed processing chunk 31/32 with concurrency=1\n",
      "[2024-01-23 19:31:53,773] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=32/32\n",
      "[2024-01-23 19:31:53,773] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:53,782] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2358\n",
      "[2024-01-23 19:31:58,395] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=107, latency=4.61\n",
      "[2024-01-23 19:31:58,413] p1345 {3496713318.py:39} INFO - completed processing chunk 32/32 with concurrency=1\n",
      "[2024-01-23 19:31:58,413] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/57\n",
      "[2024-01-23 19:31:58,414] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:31:58,424] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3000\n",
      "[2024-01-23 19:32:03,437] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.01\n",
      "[2024-01-23 19:32:03,455] p1345 {3496713318.py:39} INFO - completed processing chunk 1/57 with concurrency=1\n",
      "[2024-01-23 19:32:03,456] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/57\n",
      "[2024-01-23 19:32:03,456] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:03,468] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3896\n",
      "[2024-01-23 19:32:09,039] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.57\n",
      "[2024-01-23 19:32:09,059] p1345 {3496713318.py:39} INFO - completed processing chunk 2/57 with concurrency=1\n",
      "[2024-01-23 19:32:09,060] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/57\n",
      "[2024-01-23 19:32:09,060] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:09,073] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3789\n",
      "[2024-01-23 19:32:11,578] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=34, latency=2.50\n",
      "[2024-01-23 19:32:11,599] p1345 {3496713318.py:39} INFO - completed processing chunk 3/57 with concurrency=1\n",
      "[2024-01-23 19:32:11,599] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/57\n",
      "[2024-01-23 19:32:11,600] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:11,610] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3450\n",
      "[2024-01-23 19:32:13,373] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=22, latency=1.76\n",
      "[2024-01-23 19:32:13,392] p1345 {3496713318.py:39} INFO - completed processing chunk 4/57 with concurrency=1\n",
      "[2024-01-23 19:32:13,392] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/57\n",
      "[2024-01-23 19:32:13,393] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:13,404] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3482\n",
      "[2024-01-23 19:32:18,730] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=5.33\n",
      "[2024-01-23 19:32:18,748] p1345 {3496713318.py:39} INFO - completed processing chunk 5/57 with concurrency=1\n",
      "[2024-01-23 19:32:18,749] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/57\n",
      "[2024-01-23 19:32:18,749] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:18,759] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3144\n",
      "[2024-01-23 19:32:20,029] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=12, latency=1.27\n",
      "[2024-01-23 19:32:20,049] p1345 {3496713318.py:39} INFO - completed processing chunk 6/57 with concurrency=1\n",
      "[2024-01-23 19:32:20,050] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=7/57\n",
      "[2024-01-23 19:32:20,050] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:20,061] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3639\n",
      "[2024-01-23 19:32:25,467] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.40\n",
      "[2024-01-23 19:32:25,495] p1345 {3496713318.py:39} INFO - completed processing chunk 7/57 with concurrency=1\n",
      "[2024-01-23 19:32:25,496] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=8/57\n",
      "[2024-01-23 19:32:25,497] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:25,506] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3014\n",
      "[2024-01-23 19:32:28,763] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=59, latency=3.26\n",
      "[2024-01-23 19:32:28,783] p1345 {3496713318.py:39} INFO - completed processing chunk 8/57 with concurrency=1\n",
      "[2024-01-23 19:32:28,784] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=9/57\n",
      "[2024-01-23 19:32:28,785] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:28,796] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3891\n",
      "[2024-01-23 19:32:34,352] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.55\n",
      "[2024-01-23 19:32:34,370] p1345 {3496713318.py:39} INFO - completed processing chunk 9/57 with concurrency=1\n",
      "[2024-01-23 19:32:34,371] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=10/57\n",
      "[2024-01-23 19:32:34,372] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:34,382] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3575\n",
      "[2024-01-23 19:32:36,017] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=16, latency=1.63\n",
      "[2024-01-23 19:32:36,037] p1345 {3496713318.py:39} INFO - completed processing chunk 10/57 with concurrency=1\n",
      "[2024-01-23 19:32:36,037] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=11/57\n",
      "[2024-01-23 19:32:36,038] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:36,047] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3419\n",
      "[2024-01-23 19:32:37,665] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=19, latency=1.62\n",
      "[2024-01-23 19:32:37,684] p1345 {3496713318.py:39} INFO - completed processing chunk 11/57 with concurrency=1\n",
      "[2024-01-23 19:32:37,685] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=12/57\n",
      "[2024-01-23 19:32:37,685] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:37,696] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3458\n",
      "[2024-01-23 19:32:42,961] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.26\n",
      "[2024-01-23 19:32:42,981] p1345 {3496713318.py:39} INFO - completed processing chunk 12/57 with concurrency=1\n",
      "[2024-01-23 19:32:42,982] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=13/57\n",
      "[2024-01-23 19:32:42,982] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:42,993] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3508\n",
      "[2024-01-23 19:32:46,626] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=63, latency=3.63\n",
      "[2024-01-23 19:32:46,647] p1345 {3496713318.py:39} INFO - completed processing chunk 13/57 with concurrency=1\n",
      "[2024-01-23 19:32:46,647] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=14/57\n",
      "[2024-01-23 19:32:46,648] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:46,657] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3436\n",
      "[2024-01-23 19:32:51,916] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.26\n",
      "[2024-01-23 19:32:51,934] p1345 {3496713318.py:39} INFO - completed processing chunk 14/57 with concurrency=1\n",
      "[2024-01-23 19:32:51,935] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=15/57\n",
      "[2024-01-23 19:32:51,935] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:51,946] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3249\n",
      "[2024-01-23 19:32:57,114] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.17\n",
      "[2024-01-23 19:32:57,139] p1345 {3496713318.py:39} INFO - completed processing chunk 15/57 with concurrency=1\n",
      "[2024-01-23 19:32:57,140] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=16/57\n",
      "[2024-01-23 19:32:57,141] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:32:57,150] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3087\n",
      "[2024-01-23 19:33:02,222] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=5.07\n",
      "[2024-01-23 19:33:02,252] p1345 {3496713318.py:39} INFO - completed processing chunk 16/57 with concurrency=1\n",
      "[2024-01-23 19:33:02,253] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=17/57\n",
      "[2024-01-23 19:33:02,253] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:02,265] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3680\n",
      "[2024-01-23 19:33:04,747] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=35, latency=2.48\n",
      "[2024-01-23 19:33:04,766] p1345 {3496713318.py:39} INFO - completed processing chunk 17/57 with concurrency=1\n",
      "[2024-01-23 19:33:04,766] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=18/57\n",
      "[2024-01-23 19:33:04,767] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:04,778] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3950\n",
      "[2024-01-23 19:33:10,334] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.55\n",
      "[2024-01-23 19:33:10,352] p1345 {3496713318.py:39} INFO - completed processing chunk 18/57 with concurrency=1\n",
      "[2024-01-23 19:33:10,353] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=19/57\n",
      "[2024-01-23 19:33:10,354] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:10,365] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3463\n",
      "[2024-01-23 19:33:11,459] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=1.09\n",
      "[2024-01-23 19:33:11,478] p1345 {3496713318.py:39} INFO - completed processing chunk 19/57 with concurrency=1\n",
      "[2024-01-23 19:33:11,479] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=20/57\n",
      "[2024-01-23 19:33:11,479] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:11,494] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3996\n",
      "[2024-01-23 19:33:17,072] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.58\n",
      "[2024-01-23 19:33:17,098] p1345 {3496713318.py:39} INFO - completed processing chunk 20/57 with concurrency=1\n",
      "[2024-01-23 19:33:17,098] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=21/57\n",
      "[2024-01-23 19:33:17,099] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:17,110] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3050\n",
      "[2024-01-23 19:33:18,061] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=0.95\n",
      "[2024-01-23 19:33:18,080] p1345 {3496713318.py:39} INFO - completed processing chunk 21/57 with concurrency=1\n",
      "[2024-01-23 19:33:18,081] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=22/57\n",
      "[2024-01-23 19:33:18,082] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:18,094] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3737\n",
      "[2024-01-23 19:33:23,508] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=5.41\n",
      "[2024-01-23 19:33:23,527] p1345 {3496713318.py:39} INFO - completed processing chunk 22/57 with concurrency=1\n",
      "[2024-01-23 19:33:23,527] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=23/57\n",
      "[2024-01-23 19:33:23,528] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:23,539] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3410\n",
      "[2024-01-23 19:33:28,832] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=105, latency=5.29\n",
      "[2024-01-23 19:33:28,852] p1345 {3496713318.py:39} INFO - completed processing chunk 23/57 with concurrency=1\n",
      "[2024-01-23 19:33:28,853] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=24/57\n",
      "[2024-01-23 19:33:28,854] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:28,865] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3979\n",
      "[2024-01-23 19:33:34,455] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=5.59\n",
      "[2024-01-23 19:33:34,475] p1345 {3496713318.py:39} INFO - completed processing chunk 24/57 with concurrency=1\n",
      "[2024-01-23 19:33:34,476] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=25/57\n",
      "[2024-01-23 19:33:34,476] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:34,486] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3171\n",
      "[2024-01-23 19:33:35,683] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=1.20\n",
      "[2024-01-23 19:33:35,702] p1345 {3496713318.py:39} INFO - completed processing chunk 25/57 with concurrency=1\n",
      "[2024-01-23 19:33:35,702] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=26/57\n",
      "[2024-01-23 19:33:35,703] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:35,714] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3655\n",
      "[2024-01-23 19:33:41,112] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=5.40\n",
      "[2024-01-23 19:33:41,132] p1345 {3496713318.py:39} INFO - completed processing chunk 26/57 with concurrency=1\n",
      "[2024-01-23 19:33:41,132] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=27/57\n",
      "[2024-01-23 19:33:41,133] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:41,145] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3662\n",
      "[2024-01-23 19:33:46,532] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.39\n",
      "[2024-01-23 19:33:46,551] p1345 {3496713318.py:39} INFO - completed processing chunk 27/57 with concurrency=1\n",
      "[2024-01-23 19:33:46,551] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=28/57\n",
      "[2024-01-23 19:33:46,552] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:46,561] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3098\n",
      "[2024-01-23 19:33:48,301] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=24, latency=1.74\n",
      "[2024-01-23 19:33:48,323] p1345 {3496713318.py:39} INFO - completed processing chunk 28/57 with concurrency=1\n",
      "[2024-01-23 19:33:48,324] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=29/57\n",
      "[2024-01-23 19:33:48,324] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:48,336] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3704\n",
      "[2024-01-23 19:33:53,739] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.40\n",
      "[2024-01-23 19:33:53,761] p1345 {3496713318.py:39} INFO - completed processing chunk 29/57 with concurrency=1\n",
      "[2024-01-23 19:33:53,761] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=30/57\n",
      "[2024-01-23 19:33:53,762] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:53,772] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3400\n",
      "[2024-01-23 19:33:59,018] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=5.25\n",
      "[2024-01-23 19:33:59,037] p1345 {3496713318.py:39} INFO - completed processing chunk 30/57 with concurrency=1\n",
      "[2024-01-23 19:33:59,037] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=31/57\n",
      "[2024-01-23 19:33:59,038] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:33:59,048] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3098\n",
      "[2024-01-23 19:34:00,135] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=1.09\n",
      "[2024-01-23 19:34:00,154] p1345 {3496713318.py:39} INFO - completed processing chunk 31/57 with concurrency=1\n",
      "[2024-01-23 19:34:00,155] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=32/57\n",
      "[2024-01-23 19:34:00,156] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:00,168] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3909\n",
      "[2024-01-23 19:34:01,412] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=1.24\n",
      "[2024-01-23 19:34:01,432] p1345 {3496713318.py:39} INFO - completed processing chunk 32/57 with concurrency=1\n",
      "[2024-01-23 19:34:01,432] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=33/57\n",
      "[2024-01-23 19:34:01,433] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:01,447] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3971\n",
      "[2024-01-23 19:34:07,023] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=77, latency=5.57\n",
      "[2024-01-23 19:34:07,042] p1345 {3496713318.py:39} INFO - completed processing chunk 33/57 with concurrency=1\n",
      "[2024-01-23 19:34:07,043] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=34/57\n",
      "[2024-01-23 19:34:07,043] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:07,053] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3132\n",
      "[2024-01-23 19:34:09,271] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=35, latency=2.22\n",
      "[2024-01-23 19:34:09,289] p1345 {3496713318.py:39} INFO - completed processing chunk 34/57 with concurrency=1\n",
      "[2024-01-23 19:34:09,290] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=35/57\n",
      "[2024-01-23 19:34:09,291] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:09,301] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3135\n",
      "[2024-01-23 19:34:10,227] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=0.93\n",
      "[2024-01-23 19:34:10,246] p1345 {3496713318.py:39} INFO - completed processing chunk 35/57 with concurrency=1\n",
      "[2024-01-23 19:34:10,247] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=36/57\n",
      "[2024-01-23 19:34:10,248] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:10,258] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3030\n",
      "[2024-01-23 19:34:11,333] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=1.07\n",
      "[2024-01-23 19:34:11,353] p1345 {3496713318.py:39} INFO - completed processing chunk 36/57 with concurrency=1\n",
      "[2024-01-23 19:34:11,354] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=37/57\n",
      "[2024-01-23 19:34:11,354] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:11,367] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3848\n",
      "[2024-01-23 19:34:12,596] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=1.23\n",
      "[2024-01-23 19:34:12,618] p1345 {3496713318.py:39} INFO - completed processing chunk 37/57 with concurrency=1\n",
      "[2024-01-23 19:34:12,619] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=38/57\n",
      "[2024-01-23 19:34:12,620] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:12,630] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3208\n",
      "[2024-01-23 19:34:17,721] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.09\n",
      "[2024-01-23 19:34:17,743] p1345 {3496713318.py:39} INFO - completed processing chunk 38/57 with concurrency=1\n",
      "[2024-01-23 19:34:17,744] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=39/57\n",
      "[2024-01-23 19:34:17,745] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:17,756] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3475\n",
      "[2024-01-23 19:34:18,893] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=1.14\n",
      "[2024-01-23 19:34:18,912] p1345 {3496713318.py:39} INFO - completed processing chunk 39/57 with concurrency=1\n",
      "[2024-01-23 19:34:18,912] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=40/57\n",
      "[2024-01-23 19:34:18,913] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:18,925] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3681\n",
      "[2024-01-23 19:34:24,323] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.40\n",
      "[2024-01-23 19:34:24,341] p1345 {3496713318.py:39} INFO - completed processing chunk 40/57 with concurrency=1\n",
      "[2024-01-23 19:34:24,341] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=41/57\n",
      "[2024-01-23 19:34:24,342] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:24,353] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3581\n",
      "[2024-01-23 19:34:25,273] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.92\n",
      "[2024-01-23 19:34:25,291] p1345 {3496713318.py:39} INFO - completed processing chunk 41/57 with concurrency=1\n",
      "[2024-01-23 19:34:25,292] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=42/57\n",
      "[2024-01-23 19:34:25,293] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:25,303] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3302\n",
      "[2024-01-23 19:34:26,282] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=0.98\n",
      "[2024-01-23 19:34:26,304] p1345 {3496713318.py:39} INFO - completed processing chunk 42/57 with concurrency=1\n",
      "[2024-01-23 19:34:26,304] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=43/57\n",
      "[2024-01-23 19:34:26,305] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:26,316] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3614\n",
      "[2024-01-23 19:34:27,684] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=11, latency=1.37\n",
      "[2024-01-23 19:34:27,703] p1345 {3496713318.py:39} INFO - completed processing chunk 43/57 with concurrency=1\n",
      "[2024-01-23 19:34:27,703] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=44/57\n",
      "[2024-01-23 19:34:27,704] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:27,715] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3537\n",
      "[2024-01-23 19:34:29,786] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=26, latency=2.07\n",
      "[2024-01-23 19:34:29,812] p1345 {3496713318.py:39} INFO - completed processing chunk 44/57 with concurrency=1\n",
      "[2024-01-23 19:34:29,813] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=45/57\n",
      "[2024-01-23 19:34:29,813] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:29,824] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3165\n",
      "[2024-01-23 19:34:30,838] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=1.01\n",
      "[2024-01-23 19:34:30,857] p1345 {3496713318.py:39} INFO - completed processing chunk 45/57 with concurrency=1\n",
      "[2024-01-23 19:34:30,857] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=46/57\n",
      "[2024-01-23 19:34:30,858] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:30,869] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3476\n",
      "[2024-01-23 19:34:31,963] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=1.09\n",
      "[2024-01-23 19:34:31,988] p1345 {3496713318.py:39} INFO - completed processing chunk 46/57 with concurrency=1\n",
      "[2024-01-23 19:34:31,988] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=47/57\n",
      "[2024-01-23 19:34:31,989] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:31,999] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3235\n",
      "[2024-01-23 19:34:37,096] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=74, latency=5.10\n",
      "[2024-01-23 19:34:37,115] p1345 {3496713318.py:39} INFO - completed processing chunk 47/57 with concurrency=1\n",
      "[2024-01-23 19:34:37,116] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=48/57\n",
      "[2024-01-23 19:34:37,117] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:37,127] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3363\n",
      "[2024-01-23 19:34:42,341] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=77, latency=5.21\n",
      "[2024-01-23 19:34:42,361] p1345 {3496713318.py:39} INFO - completed processing chunk 48/57 with concurrency=1\n",
      "[2024-01-23 19:34:42,362] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=49/57\n",
      "[2024-01-23 19:34:42,362] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:42,374] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3882\n",
      "[2024-01-23 19:34:43,335] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.96\n",
      "[2024-01-23 19:34:43,361] p1345 {3496713318.py:39} INFO - completed processing chunk 49/57 with concurrency=1\n",
      "[2024-01-23 19:34:43,361] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=50/57\n",
      "[2024-01-23 19:34:43,362] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:43,373] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3616\n",
      "[2024-01-23 19:34:44,520] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=1.15\n",
      "[2024-01-23 19:34:44,541] p1345 {3496713318.py:39} INFO - completed processing chunk 50/57 with concurrency=1\n",
      "[2024-01-23 19:34:44,542] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=51/57\n",
      "[2024-01-23 19:34:44,542] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:44,554] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3567\n",
      "[2024-01-23 19:34:49,888] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.33\n",
      "[2024-01-23 19:34:49,908] p1345 {3496713318.py:39} INFO - completed processing chunk 51/57 with concurrency=1\n",
      "[2024-01-23 19:34:49,909] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=52/57\n",
      "[2024-01-23 19:34:49,909] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:49,920] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3330\n",
      "[2024-01-23 19:34:50,898] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=0.98\n",
      "[2024-01-23 19:34:50,918] p1345 {3496713318.py:39} INFO - completed processing chunk 52/57 with concurrency=1\n",
      "[2024-01-23 19:34:50,918] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=53/57\n",
      "[2024-01-23 19:34:50,919] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:50,930] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3566\n",
      "[2024-01-23 19:34:52,077] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=1.15\n",
      "[2024-01-23 19:34:52,096] p1345 {3496713318.py:39} INFO - completed processing chunk 53/57 with concurrency=1\n",
      "[2024-01-23 19:34:52,096] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=54/57\n",
      "[2024-01-23 19:34:52,097] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:52,107] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3497\n",
      "[2024-01-23 19:34:52,980] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.87\n",
      "[2024-01-23 19:34:52,999] p1345 {3496713318.py:39} INFO - completed processing chunk 54/57 with concurrency=1\n",
      "[2024-01-23 19:34:53,000] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=55/57\n",
      "[2024-01-23 19:34:53,000] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:53,010] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3003\n",
      "[2024-01-23 19:34:53,916] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=0.91\n",
      "[2024-01-23 19:34:53,934] p1345 {3496713318.py:39} INFO - completed processing chunk 55/57 with concurrency=1\n",
      "[2024-01-23 19:34:53,935] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=56/57\n",
      "[2024-01-23 19:34:53,936] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:53,947] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3783\n",
      "[2024-01-23 19:34:55,248] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=1.30\n",
      "[2024-01-23 19:34:55,266] p1345 {3496713318.py:39} INFO - completed processing chunk 56/57 with concurrency=1\n",
      "[2024-01-23 19:34:55,266] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=57/57\n",
      "[2024-01-23 19:34:55,267] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-23 19:34:55,280] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:35:00,398] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.12\n",
      "[2024-01-23 19:35:00,419] p1345 {3496713318.py:39} INFO - completed processing chunk 57/57 with concurrency=1\n",
      "[2024-01-23 19:35:00,420] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-23 19:35:00,420] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:00,425] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:35:00,430] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:35:04,200] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=3.77\n",
      "[2024-01-23 19:35:04,200] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=3.77\n",
      "[2024-01-23 19:35:04,229] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=2\n",
      "[2024-01-23 19:35:04,230] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-23 19:35:04,230] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:04,236] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:35:04,239] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:35:05,629] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=23, latency=1.39\n",
      "[2024-01-23 19:35:08,382] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.14\n",
      "[2024-01-23 19:35:08,409] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=2\n",
      "[2024-01-23 19:35:08,410] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/8\n",
      "[2024-01-23 19:35:08,410] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:08,417] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1339\n",
      "[2024-01-23 19:35:08,423] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1932\n",
      "[2024-01-23 19:35:13,356] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=4.93\n",
      "[2024-01-23 19:35:13,356] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=4.94\n",
      "[2024-01-23 19:35:13,385] p1345 {3496713318.py:39} INFO - completed processing chunk 1/8 with concurrency=2\n",
      "[2024-01-23 19:35:13,385] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/8\n",
      "[2024-01-23 19:35:13,386] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:13,394] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1154\n",
      "[2024-01-23 19:35:13,396] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1646\n",
      "[2024-01-23 19:35:15,206] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=28, latency=1.81\n",
      "[2024-01-23 19:35:15,784] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=43, latency=2.39\n",
      "[2024-01-23 19:35:15,812] p1345 {3496713318.py:39} INFO - completed processing chunk 2/8 with concurrency=2\n",
      "[2024-01-23 19:35:15,812] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/8\n",
      "[2024-01-23 19:35:15,813] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:15,823] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1746\n",
      "[2024-01-23 19:35:15,826] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1397\n",
      "[2024-01-23 19:35:20,648] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.82\n",
      "[2024-01-23 19:35:20,649] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.82\n",
      "[2024-01-23 19:35:20,677] p1345 {3496713318.py:39} INFO - completed processing chunk 3/8 with concurrency=2\n",
      "[2024-01-23 19:35:20,677] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/8\n",
      "[2024-01-23 19:35:20,678] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:20,687] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1598\n",
      "[2024-01-23 19:35:20,689] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1373\n",
      "[2024-01-23 19:35:22,548] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=27, latency=1.85\n",
      "[2024-01-23 19:35:25,307] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=4.62\n",
      "[2024-01-23 19:35:25,334] p1345 {3496713318.py:39} INFO - completed processing chunk 4/8 with concurrency=2\n",
      "[2024-01-23 19:35:25,335] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/8\n",
      "[2024-01-23 19:35:25,335] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:25,345] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1743\n",
      "[2024-01-23 19:35:25,350] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1539\n",
      "[2024-01-23 19:35:26,457] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=1.11\n",
      "[2024-01-23 19:35:30,019] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=4.67\n",
      "[2024-01-23 19:35:30,047] p1345 {3496713318.py:39} INFO - completed processing chunk 5/8 with concurrency=2\n",
      "[2024-01-23 19:35:30,048] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/8\n",
      "[2024-01-23 19:35:30,048] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:30,058] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1421\n",
      "[2024-01-23 19:35:30,061] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1695\n",
      "[2024-01-23 19:35:31,904] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=25, latency=1.84\n",
      "[2024-01-23 19:35:34,721] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.66\n",
      "[2024-01-23 19:35:34,751] p1345 {3496713318.py:39} INFO - completed processing chunk 6/8 with concurrency=2\n",
      "[2024-01-23 19:35:34,751] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=7/8\n",
      "[2024-01-23 19:35:34,752] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:34,760] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1918\n",
      "[2024-01-23 19:35:34,763] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1910\n",
      "[2024-01-23 19:35:36,056] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=1.29\n",
      "[2024-01-23 19:35:39,583] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.82\n",
      "[2024-01-23 19:35:39,610] p1345 {3496713318.py:39} INFO - completed processing chunk 7/8 with concurrency=2\n",
      "[2024-01-23 19:35:39,611] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=8/8\n",
      "[2024-01-23 19:35:39,612] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:39,623] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1939\n",
      "[2024-01-23 19:35:39,623] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1939\n",
      "[2024-01-23 19:35:40,924] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=1.30\n",
      "[2024-01-23 19:35:40,924] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=1.29\n",
      "[2024-01-23 19:35:40,957] p1345 {3496713318.py:39} INFO - completed processing chunk 8/8 with concurrency=2\n",
      "[2024-01-23 19:35:40,957] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/16\n",
      "[2024-01-23 19:35:40,958] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:40,969] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2637\n",
      "[2024-01-23 19:35:40,974] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3000\n",
      "[2024-01-23 19:35:42,893] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=12, latency=1.92\n",
      "[2024-01-23 19:35:46,566] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.60\n",
      "[2024-01-23 19:35:46,594] p1345 {3496713318.py:39} INFO - completed processing chunk 1/16 with concurrency=2\n",
      "[2024-01-23 19:35:46,595] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/16\n",
      "[2024-01-23 19:35:46,596] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:46,605] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2148\n",
      "[2024-01-23 19:35:46,610] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2715\n",
      "[2024-01-23 19:35:48,601] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=19, latency=1.99\n",
      "[2024-01-23 19:35:52,045] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=5.43\n",
      "[2024-01-23 19:35:52,072] p1345 {3496713318.py:39} INFO - completed processing chunk 2/16 with concurrency=2\n",
      "[2024-01-23 19:35:52,073] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/16\n",
      "[2024-01-23 19:35:52,073] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:52,083] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2150\n",
      "[2024-01-23 19:35:52,087] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2404\n",
      "[2024-01-23 19:35:57,475] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.38\n",
      "[2024-01-23 19:35:57,475] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.39\n",
      "[2024-01-23 19:35:57,505] p1345 {3496713318.py:39} INFO - completed processing chunk 3/16 with concurrency=2\n",
      "[2024-01-23 19:35:57,506] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/16\n",
      "[2024-01-23 19:35:57,506] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:35:57,519] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2803\n",
      "[2024-01-23 19:35:57,519] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2369\n",
      "[2024-01-23 19:35:59,472] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=17, latency=1.95\n",
      "[2024-01-23 19:36:02,898] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=5.38\n",
      "[2024-01-23 19:36:02,927] p1345 {3496713318.py:39} INFO - completed processing chunk 4/16 with concurrency=2\n",
      "[2024-01-23 19:36:02,928] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/16\n",
      "[2024-01-23 19:36:02,928] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:02,941] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2541\n",
      "[2024-01-23 19:36:02,945] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2675\n",
      "[2024-01-23 19:36:04,446] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=1.50\n",
      "[2024-01-23 19:36:08,368] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=5.42\n",
      "[2024-01-23 19:36:08,397] p1345 {3496713318.py:39} INFO - completed processing chunk 5/16 with concurrency=2\n",
      "[2024-01-23 19:36:08,397] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/16\n",
      "[2024-01-23 19:36:08,398] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:08,407] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2186\n",
      "[2024-01-23 19:36:08,412] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2775\n",
      "[2024-01-23 19:36:10,443] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=16, latency=2.03\n",
      "[2024-01-23 19:36:13,687] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.28\n",
      "[2024-01-23 19:36:13,716] p1345 {3496713318.py:39} INFO - completed processing chunk 6/16 with concurrency=2\n",
      "[2024-01-23 19:36:13,717] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=7/16\n",
      "[2024-01-23 19:36:13,717] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:13,730] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2500\n",
      "[2024-01-23 19:36:13,731] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2686\n",
      "[2024-01-23 19:36:15,279] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=1.55\n",
      "[2024-01-23 19:36:19,157] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.42\n",
      "[2024-01-23 19:36:19,191] p1345 {3496713318.py:39} INFO - completed processing chunk 7/16 with concurrency=2\n",
      "[2024-01-23 19:36:19,191] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=8/16\n",
      "[2024-01-23 19:36:19,192] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:19,202] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2321\n",
      "[2024-01-23 19:36:19,209] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2443\n",
      "[2024-01-23 19:36:20,642] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=1.43\n",
      "[2024-01-23 19:36:24,409] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.21\n",
      "[2024-01-23 19:36:24,436] p1345 {3496713318.py:39} INFO - completed processing chunk 8/16 with concurrency=2\n",
      "[2024-01-23 19:36:24,437] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=9/16\n",
      "[2024-01-23 19:36:24,437] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:24,450] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2428\n",
      "[2024-01-23 19:36:24,454] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2458\n",
      "[2024-01-23 19:36:26,129] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=13, latency=1.68\n",
      "[2024-01-23 19:36:29,728] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=5.27\n",
      "[2024-01-23 19:36:29,755] p1345 {3496713318.py:39} INFO - completed processing chunk 9/16 with concurrency=2\n",
      "[2024-01-23 19:36:29,756] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=10/16\n",
      "[2024-01-23 19:36:29,756] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:29,769] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2440\n",
      "[2024-01-23 19:36:29,770] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2101\n",
      "[2024-01-23 19:36:31,044] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=1.27\n",
      "[2024-01-23 19:36:31,165] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=1.39\n",
      "[2024-01-23 19:36:31,193] p1345 {3496713318.py:39} INFO - completed processing chunk 10/16 with concurrency=2\n",
      "[2024-01-23 19:36:31,194] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=11/16\n",
      "[2024-01-23 19:36:31,195] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:31,206] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2711\n",
      "[2024-01-23 19:36:31,211] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2771\n",
      "[2024-01-23 19:36:32,748] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=1.54\n",
      "[2024-01-23 19:36:32,874] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=1.67\n",
      "[2024-01-23 19:36:32,903] p1345 {3496713318.py:39} INFO - completed processing chunk 11/16 with concurrency=2\n",
      "[2024-01-23 19:36:32,904] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=12/16\n",
      "[2024-01-23 19:36:32,904] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:32,917] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2691\n",
      "[2024-01-23 19:36:32,921] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2624\n",
      "[2024-01-23 19:36:34,264] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=1.34\n",
      "[2024-01-23 19:36:34,514] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=1.60\n",
      "[2024-01-23 19:36:34,543] p1345 {3496713318.py:39} INFO - completed processing chunk 12/16 with concurrency=2\n",
      "[2024-01-23 19:36:34,543] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=13/16\n",
      "[2024-01-23 19:36:34,544] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:34,554] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2062\n",
      "[2024-01-23 19:36:34,559] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2213\n",
      "[2024-01-23 19:36:35,915] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=1.35\n",
      "[2024-01-23 19:36:35,996] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=1.44\n",
      "[2024-01-23 19:36:36,024] p1345 {3496713318.py:39} INFO - completed processing chunk 13/16 with concurrency=2\n",
      "[2024-01-23 19:36:36,025] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=14/16\n",
      "[2024-01-23 19:36:36,025] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:36,039] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2608\n",
      "[2024-01-23 19:36:36,043] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2770\n",
      "[2024-01-23 19:36:36,719] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.68\n",
      "[2024-01-23 19:36:41,560] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=5.52\n",
      "[2024-01-23 19:36:41,588] p1345 {3496713318.py:39} INFO - completed processing chunk 14/16 with concurrency=2\n",
      "[2024-01-23 19:36:41,588] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=15/16\n",
      "[2024-01-23 19:36:41,589] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:41,600] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2333\n",
      "[2024-01-23 19:36:41,604] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2564\n",
      "[2024-01-23 19:36:43,014] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=1.41\n",
      "[2024-01-23 19:36:43,140] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=1.53\n",
      "[2024-01-23 19:36:43,166] p1345 {3496713318.py:39} INFO - completed processing chunk 15/16 with concurrency=2\n",
      "[2024-01-23 19:36:43,167] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=16/16\n",
      "[2024-01-23 19:36:43,168] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:43,181] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2613\n",
      "[2024-01-23 19:36:43,184] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2358\n",
      "[2024-01-23 19:36:48,736] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=109, latency=5.55\n",
      "[2024-01-23 19:36:48,736] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=5.55\n",
      "[2024-01-23 19:36:48,768] p1345 {3496713318.py:39} INFO - completed processing chunk 16/16 with concurrency=2\n",
      "[2024-01-23 19:36:48,769] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/29\n",
      "[2024-01-23 19:36:48,769] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:48,780] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3000\n",
      "[2024-01-23 19:36:48,787] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3896\n",
      "[2024-01-23 19:36:55,273] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=6.48\n",
      "[2024-01-23 19:36:55,273] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=94, latency=6.49\n",
      "[2024-01-23 19:36:55,301] p1345 {3496713318.py:39} INFO - completed processing chunk 1/29 with concurrency=2\n",
      "[2024-01-23 19:36:55,302] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/29\n",
      "[2024-01-23 19:36:55,302] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:36:55,324] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3789\n",
      "[2024-01-23 19:36:55,324] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3450\n",
      "[2024-01-23 19:36:57,784] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=16, latency=2.46\n",
      "[2024-01-23 19:37:01,553] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=105, latency=6.22\n",
      "[2024-01-23 19:37:01,583] p1345 {3496713318.py:39} INFO - completed processing chunk 2/29 with concurrency=2\n",
      "[2024-01-23 19:37:01,584] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/29\n",
      "[2024-01-23 19:37:01,584] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:01,597] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3482\n",
      "[2024-01-23 19:37:01,604] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3144\n",
      "[2024-01-23 19:37:03,652] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=2.05\n",
      "[2024-01-23 19:37:07,708] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=96, latency=6.11\n",
      "[2024-01-23 19:37:07,734] p1345 {3496713318.py:39} INFO - completed processing chunk 3/29 with concurrency=2\n",
      "[2024-01-23 19:37:07,735] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/29\n",
      "[2024-01-23 19:37:07,736] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:07,750] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3014\n",
      "[2024-01-23 19:37:07,755] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3639\n",
      "[2024-01-23 19:37:10,701] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=29, latency=2.95\n",
      "[2024-01-23 19:37:13,937] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.18\n",
      "[2024-01-23 19:37:13,966] p1345 {3496713318.py:39} INFO - completed processing chunk 4/29 with concurrency=2\n",
      "[2024-01-23 19:37:13,967] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/29\n",
      "[2024-01-23 19:37:13,967] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:13,983] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3891\n",
      "[2024-01-23 19:37:13,988] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3575\n",
      "[2024-01-23 19:37:16,144] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=2.16\n",
      "[2024-01-23 19:37:20,354] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.37\n",
      "[2024-01-23 19:37:20,381] p1345 {3496713318.py:39} INFO - completed processing chunk 5/29 with concurrency=2\n",
      "[2024-01-23 19:37:20,381] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/29\n",
      "[2024-01-23 19:37:20,382] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:20,397] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3419\n",
      "[2024-01-23 19:37:20,401] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3458\n",
      "[2024-01-23 19:37:22,612] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=13, latency=2.21\n",
      "[2024-01-23 19:37:24,291] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=50, latency=3.89\n",
      "[2024-01-23 19:37:24,317] p1345 {3496713318.py:39} INFO - completed processing chunk 6/29 with concurrency=2\n",
      "[2024-01-23 19:37:24,317] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=7/29\n",
      "[2024-01-23 19:37:24,318] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:24,332] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3508\n",
      "[2024-01-23 19:37:24,333] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3436\n",
      "[2024-01-23 19:37:30,704] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=6.37\n",
      "[2024-01-23 19:37:30,704] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=6.37\n",
      "[2024-01-23 19:37:30,732] p1345 {3496713318.py:39} INFO - completed processing chunk 7/29 with concurrency=2\n",
      "[2024-01-23 19:37:30,733] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=8/29\n",
      "[2024-01-23 19:37:30,733] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:30,749] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3087\n",
      "[2024-01-23 19:37:30,749] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3249\n",
      "[2024-01-23 19:37:36,868] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=6.11\n",
      "[2024-01-23 19:37:36,870] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.12\n",
      "[2024-01-23 19:37:36,906] p1345 {3496713318.py:39} INFO - completed processing chunk 8/29 with concurrency=2\n",
      "[2024-01-23 19:37:36,906] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=9/29\n",
      "[2024-01-23 19:37:36,907] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:36,924] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3680\n",
      "[2024-01-23 19:37:36,927] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3950\n",
      "[2024-01-23 19:37:43,585] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=110, latency=6.66\n",
      "[2024-01-23 19:37:43,586] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=96, latency=6.66\n",
      "[2024-01-23 19:37:43,615] p1345 {3496713318.py:39} INFO - completed processing chunk 9/29 with concurrency=2\n",
      "[2024-01-23 19:37:43,615] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=10/29\n",
      "[2024-01-23 19:37:43,616] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:43,634] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3463\n",
      "[2024-01-23 19:37:43,641] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3996\n",
      "[2024-01-23 19:37:45,650] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.01\n",
      "[2024-01-23 19:37:46,110] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=17, latency=2.46\n",
      "[2024-01-23 19:37:46,141] p1345 {3496713318.py:39} INFO - completed processing chunk 10/29 with concurrency=2\n",
      "[2024-01-23 19:37:46,141] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=11/29\n",
      "[2024-01-23 19:37:46,142] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:46,154] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3050\n",
      "[2024-01-23 19:37:46,162] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3737\n",
      "[2024-01-23 19:37:48,453] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=15, latency=2.29\n",
      "[2024-01-23 19:37:52,154] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=6.00\n",
      "[2024-01-23 19:37:52,186] p1345 {3496713318.py:39} INFO - completed processing chunk 11/29 with concurrency=2\n",
      "[2024-01-23 19:37:52,187] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=12/29\n",
      "[2024-01-23 19:37:52,187] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:52,202] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3979\n",
      "[2024-01-23 19:37:52,208] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3410\n",
      "[2024-01-23 19:37:58,828] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=96, latency=6.63\n",
      "[2024-01-23 19:37:58,830] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=91, latency=6.62\n",
      "[2024-01-23 19:37:58,858] p1345 {3496713318.py:39} INFO - completed processing chunk 12/29 with concurrency=2\n",
      "[2024-01-23 19:37:58,859] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=13/29\n",
      "[2024-01-23 19:37:58,860] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:37:58,874] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3655\n",
      "[2024-01-23 19:37:58,878] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3171\n",
      "[2024-01-23 19:38:01,718] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=23, latency=2.84\n",
      "[2024-01-23 19:38:04,976] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=6.10\n",
      "[2024-01-23 19:38:05,003] p1345 {3496713318.py:39} INFO - completed processing chunk 13/29 with concurrency=2\n",
      "[2024-01-23 19:38:05,003] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=14/29\n",
      "[2024-01-23 19:38:05,004] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:38:05,020] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3662\n",
      "[2024-01-23 19:38:05,024] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3098\n",
      "[2024-01-23 19:38:06,819] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=3, latency=1.79\n",
      "[2024-01-23 19:38:11,181] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=6.16\n",
      "[2024-01-23 19:38:11,209] p1345 {3496713318.py:39} INFO - completed processing chunk 14/29 with concurrency=2\n",
      "[2024-01-23 19:38:11,209] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=15/29\n",
      "[2024-01-23 19:38:11,210] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:38:11,226] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3400\n",
      "[2024-01-23 19:38:11,227] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3704\n",
      "[2024-01-23 19:38:17,652] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=6.42\n",
      "[2024-01-23 19:38:17,652] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.42\n",
      "[2024-01-23 19:38:17,682] p1345 {3496713318.py:39} INFO - completed processing chunk 15/29 with concurrency=2\n",
      "[2024-01-23 19:38:17,683] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=16/29\n",
      "[2024-01-23 19:38:17,683] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:38:17,697] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3098\n",
      "[2024-01-23 19:38:17,704] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3909\n",
      "[2024-01-23 19:38:19,515] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=1.81\n",
      "[2024-01-23 19:38:23,705] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.01\n",
      "[2024-01-23 19:38:23,732] p1345 {3496713318.py:39} INFO - completed processing chunk 16/29 with concurrency=2\n",
      "[2024-01-23 19:38:23,732] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=17/29\n",
      "[2024-01-23 19:38:23,733] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:38:23,751] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3132\n",
      "[2024-01-23 19:38:23,753] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3971\n",
      "[2024-01-23 19:38:30,270] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=105, latency=6.51\n",
      "[2024-01-23 19:38:30,270] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.52\n",
      "[2024-01-23 19:38:30,977] p1345 {3496713318.py:39} INFO - completed processing chunk 17/29 with concurrency=2\n",
      "[2024-01-23 19:38:30,977] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=18/29\n",
      "[2024-01-23 19:38:30,978] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:38:30,993] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3030\n",
      "[2024-01-23 19:38:30,999] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3135\n",
      "[2024-01-23 19:38:37,011] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=6.02\n",
      "[2024-01-23 19:38:37,012] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.01\n",
      "[2024-01-23 19:38:37,043] p1345 {3496713318.py:39} INFO - completed processing chunk 18/29 with concurrency=2\n",
      "[2024-01-23 19:38:37,044] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=19/29\n",
      "[2024-01-23 19:38:37,044] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:38:37,062] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3208\n",
      "[2024-01-23 19:38:37,063] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3848\n",
      "[2024-01-23 19:38:43,451] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=6.39\n",
      "[2024-01-23 19:38:43,451] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.38\n",
      "[2024-01-23 19:38:43,480] p1345 {3496713318.py:39} INFO - completed processing chunk 19/29 with concurrency=2\n",
      "[2024-01-23 19:38:43,481] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=20/29\n",
      "[2024-01-23 19:38:43,481] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:38:43,497] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3681\n",
      "[2024-01-23 19:38:43,502] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3475\n",
      "[2024-01-23 19:38:49,935] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=6.43\n",
      "[2024-01-23 19:38:49,935] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=78, latency=6.44\n",
      "[2024-01-23 19:38:49,966] p1345 {3496713318.py:39} INFO - completed processing chunk 20/29 with concurrency=2\n",
      "[2024-01-23 19:38:49,966] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=21/29\n",
      "[2024-01-23 19:38:49,967] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:38:49,983] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3302\n",
      "[2024-01-23 19:38:49,987] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3581\n",
      "[2024-01-23 19:38:52,140] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=12, latency=2.16\n",
      "[2024-01-23 19:38:52,231] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=14, latency=2.24\n",
      "[2024-01-23 19:38:52,259] p1345 {3496713318.py:39} INFO - completed processing chunk 21/29 with concurrency=2\n",
      "[2024-01-23 19:38:52,260] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=22/29\n",
      "[2024-01-23 19:38:52,261] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:38:52,275] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3614\n",
      "[2024-01-23 19:38:52,281] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3537\n",
      "[2024-01-23 19:38:54,275] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.00\n",
      "[2024-01-23 19:38:58,501] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=6.22\n",
      "[2024-01-23 19:38:58,529] p1345 {3496713318.py:39} INFO - completed processing chunk 22/29 with concurrency=2\n",
      "[2024-01-23 19:38:58,529] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=23/29\n",
      "[2024-01-23 19:38:58,530] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:38:58,547] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3165\n",
      "[2024-01-23 19:38:58,550] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3476\n",
      "[2024-01-23 19:39:04,765] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=6.22\n",
      "[2024-01-23 19:39:04,765] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=6.21\n",
      "[2024-01-23 19:39:04,797] p1345 {3496713318.py:39} INFO - completed processing chunk 23/29 with concurrency=2\n",
      "[2024-01-23 19:39:04,798] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=24/29\n",
      "[2024-01-23 19:39:04,798] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:39:04,815] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3235\n",
      "[2024-01-23 19:39:04,816] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3363\n",
      "[2024-01-23 19:39:06,861] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=2.04\n",
      "[2024-01-23 19:39:10,812] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.99\n",
      "[2024-01-23 19:39:10,839] p1345 {3496713318.py:39} INFO - completed processing chunk 24/29 with concurrency=2\n",
      "[2024-01-23 19:39:10,840] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=25/29\n",
      "[2024-01-23 19:39:10,840] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:39:10,859] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3882\n",
      "[2024-01-23 19:39:10,862] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3616\n",
      "[2024-01-23 19:39:13,275] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=11, latency=2.41\n",
      "[2024-01-23 19:39:14,230] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=34, latency=3.37\n",
      "[2024-01-23 19:39:14,262] p1345 {3496713318.py:39} INFO - completed processing chunk 25/29 with concurrency=2\n",
      "[2024-01-23 19:39:14,263] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=26/29\n",
      "[2024-01-23 19:39:14,263] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:39:14,277] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3330\n",
      "[2024-01-23 19:39:14,283] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3567\n",
      "[2024-01-23 19:39:16,434] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=2.15\n",
      "[2024-01-23 19:39:20,374] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=6.10\n",
      "[2024-01-23 19:39:20,402] p1345 {3496713318.py:39} INFO - completed processing chunk 26/29 with concurrency=2\n",
      "[2024-01-23 19:39:20,403] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=27/29\n",
      "[2024-01-23 19:39:20,403] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:39:20,421] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3497\n",
      "[2024-01-23 19:39:20,425] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3566\n",
      "[2024-01-23 19:39:26,783] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=6.36\n",
      "[2024-01-23 19:39:26,784] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=6.36\n",
      "[2024-01-23 19:39:26,814] p1345 {3496713318.py:39} INFO - completed processing chunk 27/29 with concurrency=2\n",
      "[2024-01-23 19:39:26,815] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=28/29\n",
      "[2024-01-23 19:39:26,816] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:39:26,840] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3783\n",
      "[2024-01-23 19:39:26,841] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3003\n",
      "[2024-01-23 19:39:28,761] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=1.92\n",
      "[2024-01-23 19:39:32,794] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=5.95\n",
      "[2024-01-23 19:39:32,827] p1345 {3496713318.py:39} INFO - completed processing chunk 28/29 with concurrency=2\n",
      "[2024-01-23 19:39:32,827] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=29/29\n",
      "[2024-01-23 19:39:32,828] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=2\n",
      "[2024-01-23 19:39:32,841] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:39:32,845] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:39:34,542] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=1.70\n",
      "[2024-01-23 19:39:38,726] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=116, latency=5.88\n",
      "[2024-01-23 19:39:38,755] p1345 {3496713318.py:39} INFO - completed processing chunk 29/29 with concurrency=2\n",
      "[2024-01-23 19:39:38,755] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-23 19:39:38,756] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:39:38,767] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:39:38,769] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:39:38,770] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:39:38,770] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:39:39,332] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=0.56\n",
      "[2024-01-23 19:39:39,332] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=0.56\n",
      "[2024-01-23 19:39:39,441] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=11, latency=0.67\n",
      "[2024-01-23 19:39:42,504] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=3.73\n",
      "[2024-01-23 19:39:42,551] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=4\n",
      "[2024-01-23 19:39:42,552] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-23 19:39:42,552] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:39:42,562] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:39:42,565] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:39:42,567] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:39:42,568] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:39:44,380] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=24, latency=1.81\n",
      "[2024-01-23 19:39:47,334] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=4.76\n",
      "[2024-01-23 19:39:47,335] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.76\n",
      "[2024-01-23 19:39:47,336] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.77\n",
      "[2024-01-23 19:39:47,383] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=4\n",
      "[2024-01-23 19:39:47,383] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/4\n",
      "[2024-01-23 19:39:47,384] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:39:47,393] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1154\n",
      "[2024-01-23 19:39:47,404] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1646\n",
      "[2024-01-23 19:39:47,404] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1932\n",
      "[2024-01-23 19:39:47,404] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1339\n",
      "[2024-01-23 19:39:49,316] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=12, latency=1.91\n",
      "[2024-01-23 19:39:50,977] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=50, latency=3.58\n",
      "[2024-01-23 19:39:52,913] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.50\n",
      "[2024-01-23 19:39:52,913] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=5.51\n",
      "[2024-01-23 19:39:52,962] p1345 {3496713318.py:39} INFO - completed processing chunk 1/4 with concurrency=4\n",
      "[2024-01-23 19:39:52,962] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/4\n",
      "[2024-01-23 19:39:52,963] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:39:52,971] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1397\n",
      "[2024-01-23 19:39:52,974] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1746\n",
      "[2024-01-23 19:39:52,983] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1373\n",
      "[2024-01-23 19:39:52,984] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1598\n",
      "[2024-01-23 19:39:58,525] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.54\n",
      "[2024-01-23 19:39:58,526] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.54\n",
      "[2024-01-23 19:39:58,526] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.54\n",
      "[2024-01-23 19:39:58,526] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.55\n",
      "[2024-01-23 19:39:58,572] p1345 {3496713318.py:39} INFO - completed processing chunk 2/4 with concurrency=4\n",
      "[2024-01-23 19:39:58,573] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/4\n",
      "[2024-01-23 19:39:58,573] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:39:58,587] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1539\n",
      "[2024-01-23 19:39:58,591] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1695\n",
      "[2024-01-23 19:39:58,592] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1743\n",
      "[2024-01-23 19:39:58,600] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1421\n",
      "[2024-01-23 19:40:00,307] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=1.71\n",
      "[2024-01-23 19:40:04,174] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=5.57\n",
      "[2024-01-23 19:40:04,176] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=5.59\n",
      "[2024-01-23 19:40:04,176] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=5.58\n",
      "[2024-01-23 19:40:04,233] p1345 {3496713318.py:39} INFO - completed processing chunk 3/4 with concurrency=4\n",
      "[2024-01-23 19:40:04,233] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/4\n",
      "[2024-01-23 19:40:04,234] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:40:04,243] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1918\n",
      "[2024-01-23 19:40:04,246] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1910\n",
      "[2024-01-23 19:40:04,258] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1918\n",
      "[2024-01-23 19:40:04,258] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1939\n",
      "[2024-01-23 19:40:06,437] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=2.18\n",
      "[2024-01-23 19:40:07,016] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=18, latency=2.77\n",
      "[2024-01-23 19:40:10,146] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.89\n",
      "[2024-01-23 19:40:10,146] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.89\n",
      "[2024-01-23 19:40:10,200] p1345 {3496713318.py:39} INFO - completed processing chunk 4/4 with concurrency=4\n",
      "[2024-01-23 19:40:10,201] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/8\n",
      "[2024-01-23 19:40:10,202] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:40:10,215] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3000\n",
      "[2024-01-23 19:40:10,222] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2637\n",
      "[2024-01-23 19:40:10,223] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2148\n",
      "[2024-01-23 19:40:10,224] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2715\n",
      "[2024-01-23 19:40:13,058] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=2.83\n",
      "[2024-01-23 19:40:17,302] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=7.07\n",
      "[2024-01-23 19:40:17,303] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=7.07\n",
      "[2024-01-23 19:40:17,303] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=7.08\n",
      "[2024-01-23 19:40:17,353] p1345 {3496713318.py:39} INFO - completed processing chunk 1/8 with concurrency=4\n",
      "[2024-01-23 19:40:17,353] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/8\n",
      "[2024-01-23 19:40:17,354] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:40:17,369] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2404\n",
      "[2024-01-23 19:40:17,370] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2150\n",
      "[2024-01-23 19:40:17,377] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2803\n",
      "[2024-01-23 19:40:17,378] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2369\n",
      "[2024-01-23 19:40:19,851] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=3, latency=2.48\n",
      "[2024-01-23 19:40:21,265] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=36, latency=3.88\n",
      "[2024-01-23 19:40:24,099] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=6.72\n",
      "[2024-01-23 19:40:24,099] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.73\n",
      "[2024-01-23 19:40:24,145] p1345 {3496713318.py:39} INFO - completed processing chunk 2/8 with concurrency=4\n",
      "[2024-01-23 19:40:24,146] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/8\n",
      "[2024-01-23 19:40:24,146] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:40:24,161] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2675\n",
      "[2024-01-23 19:40:24,166] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2541\n",
      "[2024-01-23 19:40:24,170] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2186\n",
      "[2024-01-23 19:40:24,171] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2775\n",
      "[2024-01-23 19:40:28,574] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=34, latency=4.39\n",
      "[2024-01-23 19:40:31,049] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=6.88\n",
      "[2024-01-23 19:40:31,049] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=6.87\n",
      "[2024-01-23 19:40:31,049] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.88\n",
      "[2024-01-23 19:40:31,096] p1345 {3496713318.py:39} INFO - completed processing chunk 3/8 with concurrency=4\n",
      "[2024-01-23 19:40:31,096] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/8\n",
      "[2024-01-23 19:40:31,097] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:40:31,112] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2500\n",
      "[2024-01-23 19:40:31,119] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2686\n",
      "[2024-01-23 19:40:31,122] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2321\n",
      "[2024-01-23 19:40:31,123] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2443\n",
      "[2024-01-23 19:40:37,936] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=96, latency=6.82\n",
      "[2024-01-23 19:40:37,938] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=6.82\n",
      "[2024-01-23 19:40:37,938] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.81\n",
      "[2024-01-23 19:40:37,938] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=6.81\n",
      "[2024-01-23 19:40:37,985] p1345 {3496713318.py:39} INFO - completed processing chunk 4/8 with concurrency=4\n",
      "[2024-01-23 19:40:37,986] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/8\n",
      "[2024-01-23 19:40:37,987] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:40:37,998] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2428\n",
      "[2024-01-23 19:40:38,007] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2458\n",
      "[2024-01-23 19:40:38,009] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2440\n",
      "[2024-01-23 19:40:38,010] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2101\n",
      "[2024-01-23 19:40:40,425] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=2.41\n",
      "[2024-01-23 19:40:44,578] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=6.57\n",
      "[2024-01-23 19:40:44,579] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=6.57\n",
      "[2024-01-23 19:40:44,579] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=105, latency=6.56\n",
      "[2024-01-23 19:40:44,632] p1345 {3496713318.py:39} INFO - completed processing chunk 5/8 with concurrency=4\n",
      "[2024-01-23 19:40:44,633] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/8\n",
      "[2024-01-23 19:40:44,633] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:40:44,653] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2711\n",
      "[2024-01-23 19:40:44,658] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2691\n",
      "[2024-01-23 19:40:44,658] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2771\n",
      "[2024-01-23 19:40:44,658] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2624\n",
      "[2024-01-23 19:40:47,535] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=2.87\n",
      "[2024-01-23 19:40:47,581] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=2.92\n",
      "[2024-01-23 19:40:51,622] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=6.96\n",
      "[2024-01-23 19:40:51,623] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.96\n",
      "[2024-01-23 19:40:51,673] p1345 {3496713318.py:39} INFO - completed processing chunk 6/8 with concurrency=4\n",
      "[2024-01-23 19:40:51,674] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=7/8\n",
      "[2024-01-23 19:40:51,675] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:40:51,688] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2213\n",
      "[2024-01-23 19:40:51,699] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2770\n",
      "[2024-01-23 19:40:51,700] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2062\n",
      "[2024-01-23 19:40:51,703] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2608\n",
      "[2024-01-23 19:40:54,240] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.53\n",
      "[2024-01-23 19:40:54,329] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=2.63\n",
      "[2024-01-23 19:40:54,419] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=2.73\n",
      "[2024-01-23 19:40:58,247] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=6.55\n",
      "[2024-01-23 19:40:58,294] p1345 {3496713318.py:39} INFO - completed processing chunk 7/8 with concurrency=4\n",
      "[2024-01-23 19:40:58,294] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=8/8\n",
      "[2024-01-23 19:40:58,295] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:40:58,315] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2613\n",
      "[2024-01-23 19:40:58,318] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2358\n",
      "[2024-01-23 19:40:58,320] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2333\n",
      "[2024-01-23 19:40:58,323] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2564\n",
      "[2024-01-23 19:41:00,866] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=2.54\n",
      "[2024-01-23 19:41:00,867] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.54\n",
      "[2024-01-23 19:41:01,042] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=2.72\n",
      "[2024-01-23 19:41:04,780] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=6.46\n",
      "[2024-01-23 19:41:04,826] p1345 {3496713318.py:39} INFO - completed processing chunk 8/8 with concurrency=4\n",
      "[2024-01-23 19:41:04,827] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/15\n",
      "[2024-01-23 19:41:04,827] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:41:04,847] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3000\n",
      "[2024-01-23 19:41:04,861] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3789\n",
      "[2024-01-23 19:41:04,861] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3450\n",
      "[2024-01-23 19:41:04,862] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3896\n",
      "[2024-01-23 19:41:07,864] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=3.00\n",
      "[2024-01-23 19:41:13,178] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=8.31\n",
      "[2024-01-23 19:41:13,179] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=8.31\n",
      "[2024-01-23 19:41:13,995] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=9.15\n",
      "[2024-01-23 19:41:14,042] p1345 {3496713318.py:39} INFO - completed processing chunk 1/15 with concurrency=4\n",
      "[2024-01-23 19:41:14,043] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/15\n",
      "[2024-01-23 19:41:14,043] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:41:14,060] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3482\n",
      "[2024-01-23 19:41:14,070] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3144\n",
      "[2024-01-23 19:41:14,073] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3014\n",
      "[2024-01-23 19:41:14,076] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3639\n",
      "[2024-01-23 19:41:18,866] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=34, latency=4.79\n",
      "[2024-01-23 19:41:22,091] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=8.03\n",
      "[2024-01-23 19:41:22,092] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=8.02\n",
      "[2024-01-23 19:41:22,951] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.87\n",
      "[2024-01-23 19:41:22,999] p1345 {3496713318.py:39} INFO - completed processing chunk 2/15 with concurrency=4\n",
      "[2024-01-23 19:41:22,999] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/15\n",
      "[2024-01-23 19:41:23,000] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:41:23,018] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3575\n",
      "[2024-01-23 19:41:23,024] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3419\n",
      "[2024-01-23 19:41:23,028] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3458\n",
      "[2024-01-23 19:41:23,032] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3891\n",
      "[2024-01-23 19:41:27,768] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=26, latency=4.73\n",
      "[2024-01-23 19:41:28,010] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=26, latency=4.99\n",
      "[2024-01-23 19:41:31,344] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.31\n",
      "[2024-01-23 19:41:32,217] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=9.18\n",
      "[2024-01-23 19:41:32,261] p1345 {3496713318.py:39} INFO - completed processing chunk 3/15 with concurrency=4\n",
      "[2024-01-23 19:41:32,262] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/15\n",
      "[2024-01-23 19:41:32,262] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:41:32,281] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3436\n",
      "[2024-01-23 19:41:32,285] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3508\n",
      "[2024-01-23 19:41:32,292] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3249\n",
      "[2024-01-23 19:41:32,296] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3087\n",
      "[2024-01-23 19:41:36,635] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=4.34\n",
      "[2024-01-23 19:41:38,596] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=64, latency=6.31\n",
      "[2024-01-23 19:41:40,224] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=7.94\n",
      "[2024-01-23 19:41:40,225] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=7.93\n",
      "[2024-01-23 19:41:40,276] p1345 {3496713318.py:39} INFO - completed processing chunk 4/15 with concurrency=4\n",
      "[2024-01-23 19:41:40,277] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/15\n",
      "[2024-01-23 19:41:40,277] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:41:40,305] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3996\n",
      "[2024-01-23 19:41:40,312] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3680\n",
      "[2024-01-23 19:41:40,306] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3950\n",
      "[2024-01-23 19:41:40,316] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3463\n",
      "[2024-01-23 19:41:45,162] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=4.84\n",
      "[2024-01-23 19:41:48,788] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.48\n",
      "[2024-01-23 19:41:48,788] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.48\n",
      "[2024-01-23 19:41:48,789] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=8.47\n",
      "[2024-01-23 19:41:48,837] p1345 {3496713318.py:39} INFO - completed processing chunk 5/15 with concurrency=4\n",
      "[2024-01-23 19:41:48,838] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/15\n",
      "[2024-01-23 19:41:48,839] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:41:48,860] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3410\n",
      "[2024-01-23 19:41:48,866] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3050\n",
      "[2024-01-23 19:41:48,869] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3737\n",
      "[2024-01-23 19:41:48,871] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3979\n",
      "[2024-01-23 19:41:51,561] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.69\n",
      "[2024-01-23 19:41:51,983] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=14, latency=3.11\n",
      "[2024-01-23 19:41:57,149] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.28\n",
      "[2024-01-23 19:41:57,704] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=8.82\n",
      "[2024-01-23 19:41:57,749] p1345 {3496713318.py:39} INFO - completed processing chunk 6/15 with concurrency=4\n",
      "[2024-01-23 19:41:57,750] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=7/15\n",
      "[2024-01-23 19:41:57,751] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:41:57,770] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3098\n",
      "[2024-01-23 19:41:57,777] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3171\n",
      "[2024-01-23 19:41:57,778] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3662\n",
      "[2024-01-23 19:41:57,785] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3655\n",
      "[2024-01-23 19:42:00,525] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=2.75\n",
      "[2024-01-23 19:42:05,847] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=8.07\n",
      "[2024-01-23 19:42:05,848] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=8.07\n",
      "[2024-01-23 19:42:06,709] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.92\n",
      "[2024-01-23 19:42:06,758] p1345 {3496713318.py:39} INFO - completed processing chunk 7/15 with concurrency=4\n",
      "[2024-01-23 19:42:06,759] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=8/15\n",
      "[2024-01-23 19:42:06,760] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:42:06,775] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3400\n",
      "[2024-01-23 19:42:06,789] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3098\n",
      "[2024-01-23 19:42:06,793] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3909\n",
      "[2024-01-23 19:42:06,795] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3704\n",
      "[2024-01-23 19:42:15,063] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=93, latency=8.27\n",
      "[2024-01-23 19:42:15,064] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=8.27\n",
      "[2024-01-23 19:42:15,064] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.28\n",
      "[2024-01-23 19:42:15,924] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=9.13\n",
      "[2024-01-23 19:42:15,969] p1345 {3496713318.py:39} INFO - completed processing chunk 8/15 with concurrency=4\n",
      "[2024-01-23 19:42:15,970] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=9/15\n",
      "[2024-01-23 19:42:15,971] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:42:15,992] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3132\n",
      "[2024-01-23 19:42:15,995] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3971\n",
      "[2024-01-23 19:42:16,001] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3030\n",
      "[2024-01-23 19:42:16,007] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3135\n",
      "[2024-01-23 19:42:19,166] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=18, latency=3.17\n",
      "[2024-01-23 19:42:20,177] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=4.17\n",
      "[2024-01-23 19:42:24,007] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=8.00\n",
      "[2024-01-23 19:42:24,007] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.01\n",
      "[2024-01-23 19:42:24,066] p1345 {3496713318.py:39} INFO - completed processing chunk 9/15 with concurrency=4\n",
      "[2024-01-23 19:42:24,067] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=10/15\n",
      "[2024-01-23 19:42:24,067] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:42:24,087] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3208\n",
      "[2024-01-23 19:42:24,094] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3475\n",
      "[2024-01-23 19:42:24,097] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3848\n",
      "[2024-01-23 19:42:24,105] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3681\n",
      "[2024-01-23 19:42:26,962] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=2.86\n",
      "[2024-01-23 19:42:26,962] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=2.86\n",
      "[2024-01-23 19:42:32,237] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=8.14\n",
      "[2024-01-23 19:42:32,510] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=8.40\n",
      "[2024-01-23 19:42:32,555] p1345 {3496713318.py:39} INFO - completed processing chunk 10/15 with concurrency=4\n",
      "[2024-01-23 19:42:32,557] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=11/15\n",
      "[2024-01-23 19:42:32,557] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:42:32,574] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3302\n",
      "[2024-01-23 19:42:32,587] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3537\n",
      "[2024-01-23 19:42:32,591] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3581\n",
      "[2024-01-23 19:42:32,591] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3614\n",
      "[2024-01-23 19:42:35,156] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=2.56\n",
      "[2024-01-23 19:42:35,344] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.76\n",
      "[2024-01-23 19:42:36,381] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=3.79\n",
      "[2024-01-23 19:42:36,472] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=3.88\n",
      "[2024-01-23 19:42:36,519] p1345 {3496713318.py:39} INFO - completed processing chunk 11/15 with concurrency=4\n",
      "[2024-01-23 19:42:36,519] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=12/15\n",
      "[2024-01-23 19:42:36,520] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:42:36,542] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3476\n",
      "[2024-01-23 19:42:36,549] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3235\n",
      "[2024-01-23 19:42:36,552] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3363\n",
      "[2024-01-23 19:42:36,551] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3165\n",
      "[2024-01-23 19:42:39,212] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=2.66\n",
      "[2024-01-23 19:42:41,056] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=4.50\n",
      "[2024-01-23 19:42:44,403] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=96, latency=7.86\n",
      "[2024-01-23 19:42:44,403] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=7.85\n",
      "[2024-01-23 19:42:44,450] p1345 {3496713318.py:39} INFO - completed processing chunk 12/15 with concurrency=4\n",
      "[2024-01-23 19:42:44,450] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=13/15\n",
      "[2024-01-23 19:42:44,451] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:42:44,471] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3616\n",
      "[2024-01-23 19:42:44,483] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3882\n",
      "[2024-01-23 19:42:44,488] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3330\n",
      "[2024-01-23 19:42:44,494] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3567\n",
      "[2024-01-23 19:42:52,736] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=8.25\n",
      "[2024-01-23 19:42:52,788] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=8.30\n",
      "[2024-01-23 19:42:52,789] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.29\n",
      "[2024-01-23 19:42:53,640] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=9.16\n",
      "[2024-01-23 19:42:53,687] p1345 {3496713318.py:39} INFO - completed processing chunk 13/15 with concurrency=4\n",
      "[2024-01-23 19:42:53,687] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=14/15\n",
      "[2024-01-23 19:42:53,688] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:42:53,704] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3497\n",
      "[2024-01-23 19:42:53,714] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3003\n",
      "[2024-01-23 19:42:53,725] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3783\n",
      "[2024-01-23 19:42:53,730] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3566\n",
      "[2024-01-23 19:42:56,629] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=2.90\n",
      "[2024-01-23 19:42:58,276] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=4.57\n",
      "[2024-01-23 19:43:01,807] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=8.07\n",
      "[2024-01-23 19:43:01,808] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=8.08\n",
      "[2024-01-23 19:43:01,858] p1345 {3496713318.py:39} INFO - completed processing chunk 14/15 with concurrency=4\n",
      "[2024-01-23 19:43:01,858] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=15/15\n",
      "[2024-01-23 19:43:01,859] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=4\n",
      "[2024-01-23 19:43:01,885] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:43:01,891] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:43:01,891] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:43:01,892] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:43:04,885] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=17, latency=2.99\n",
      "[2024-01-23 19:43:09,560] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=7.67\n",
      "[2024-01-23 19:43:09,561] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=93, latency=7.67\n",
      "[2024-01-23 19:43:10,396] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=109, latency=8.50\n",
      "[2024-01-23 19:43:10,443] p1345 {3496713318.py:39} INFO - completed processing chunk 15/15 with concurrency=4\n",
      "[2024-01-23 19:43:10,444] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-23 19:43:10,444] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:43:10,453] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:43:10,456] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:43:10,456] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:43:10,460] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:43:10,461] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:43:10,462] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:43:10,939] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.47\n",
      "[2024-01-23 19:43:12,141] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=35, latency=1.67\n",
      "[2024-01-23 19:43:12,361] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=40, latency=1.90\n",
      "[2024-01-23 19:43:12,651] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=46, latency=2.18\n",
      "[2024-01-23 19:43:14,522] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.06\n",
      "[2024-01-23 19:43:14,522] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=4.05\n",
      "[2024-01-23 19:43:14,581] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=6\n",
      "[2024-01-23 19:43:14,582] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-23 19:43:14,583] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:43:14,727] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:43:14,734] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:43:14,735] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:43:14,735] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:43:14,737] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:43:14,746] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:43:16,307] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=1.56\n",
      "[2024-01-23 19:43:17,048] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=25, latency=2.31\n",
      "[2024-01-23 19:43:17,432] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=38, latency=2.68\n",
      "[2024-01-23 19:43:17,701] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=43, latency=2.96\n",
      "[2024-01-23 19:43:19,953] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.22\n",
      "[2024-01-23 19:43:19,955] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.21\n",
      "[2024-01-23 19:43:20,014] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=6\n",
      "[2024-01-23 19:43:20,015] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/3\n",
      "[2024-01-23 19:43:20,016] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:43:20,022] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1339\n",
      "[2024-01-23 19:43:20,031] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1932\n",
      "[2024-01-23 19:43:20,032] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1646\n",
      "[2024-01-23 19:43:20,032] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1154\n",
      "[2024-01-23 19:43:20,038] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1397\n",
      "[2024-01-23 19:43:20,046] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1746\n",
      "[2024-01-23 19:43:22,706] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=11, latency=2.66\n",
      "[2024-01-23 19:43:25,512] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=79, latency=5.47\n",
      "[2024-01-23 19:43:25,892] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=88, latency=5.84\n",
      "[2024-01-23 19:43:26,395] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.34\n",
      "[2024-01-23 19:43:26,395] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=6.35\n",
      "[2024-01-23 19:43:26,396] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=6.36\n",
      "[2024-01-23 19:43:26,461] p1345 {3496713318.py:39} INFO - completed processing chunk 1/3 with concurrency=6\n",
      "[2024-01-23 19:43:26,462] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/3\n",
      "[2024-01-23 19:43:26,462] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:43:26,470] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1598\n",
      "[2024-01-23 19:43:26,472] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1373\n",
      "[2024-01-23 19:43:26,482] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1539\n",
      "[2024-01-23 19:43:26,491] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1695\n",
      "[2024-01-23 19:43:26,491] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1421\n",
      "[2024-01-23 19:43:26,496] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1743\n",
      "[2024-01-23 19:43:29,186] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=12, latency=2.69\n",
      "[2024-01-23 19:43:29,436] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=19, latency=2.95\n",
      "[2024-01-23 19:43:29,560] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=21, latency=3.08\n",
      "[2024-01-23 19:43:32,793] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=6.29\n",
      "[2024-01-23 19:43:32,794] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=6.29\n",
      "[2024-01-23 19:43:32,794] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=6.30\n",
      "[2024-01-23 19:43:32,858] p1345 {3496713318.py:39} INFO - completed processing chunk 2/3 with concurrency=6\n",
      "[2024-01-23 19:43:32,859] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/3\n",
      "[2024-01-23 19:43:32,859] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:43:32,870] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1910\n",
      "[2024-01-23 19:43:32,887] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1918\n",
      "[2024-01-23 19:43:32,891] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1918\n",
      "[2024-01-23 19:43:32,891] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1939\n",
      "[2024-01-23 19:43:32,892] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1918\n",
      "[2024-01-23 19:43:32,892] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1918\n",
      "[2024-01-23 19:43:35,495] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=2.60\n",
      "[2024-01-23 19:43:36,535] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=3.63\n",
      "[2024-01-23 19:43:39,203] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=88, latency=6.33\n",
      "[2024-01-23 19:43:39,704] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=6.80\n",
      "[2024-01-23 19:43:39,705] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=6.81\n",
      "[2024-01-23 19:43:39,705] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=6.82\n",
      "[2024-01-23 19:43:39,767] p1345 {3496713318.py:39} INFO - completed processing chunk 3/3 with concurrency=6\n",
      "[2024-01-23 19:43:39,768] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/6\n",
      "[2024-01-23 19:43:39,768] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:43:39,791] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2148\n",
      "[2024-01-23 19:43:39,796] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3000\n",
      "[2024-01-23 19:43:39,802] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2715\n",
      "[2024-01-23 19:43:39,803] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2404\n",
      "[2024-01-23 19:43:39,803] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2150\n",
      "[2024-01-23 19:43:39,803] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2637\n",
      "[2024-01-23 19:43:43,027] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=15, latency=3.22\n",
      "[2024-01-23 19:43:45,392] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=42, latency=5.59\n",
      "[2024-01-23 19:43:47,962] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=8.15\n",
      "[2024-01-23 19:43:47,964] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=106, latency=8.17\n",
      "[2024-01-23 19:43:48,796] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.98\n",
      "[2024-01-23 19:43:48,796] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=8.99\n",
      "[2024-01-23 19:43:48,863] p1345 {3496713318.py:39} INFO - completed processing chunk 1/6 with concurrency=6\n",
      "[2024-01-23 19:43:48,864] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/6\n",
      "[2024-01-23 19:43:48,864] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:43:48,877] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2369\n",
      "[2024-01-23 19:43:48,881] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2675\n",
      "[2024-01-23 19:43:48,895] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2186\n",
      "[2024-01-23 19:43:48,903] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2775\n",
      "[2024-01-23 19:43:48,896] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2541\n",
      "[2024-01-23 19:43:48,895] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2803\n",
      "[2024-01-23 19:43:51,871] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=14, latency=2.97\n",
      "[2024-01-23 19:43:57,043] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=8.15\n",
      "[2024-01-23 19:43:57,043] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=105, latency=8.14\n",
      "[2024-01-23 19:43:57,044] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=8.15\n",
      "[2024-01-23 19:43:57,884] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=107, latency=8.97\n",
      "[2024-01-23 19:43:57,884] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=8.98\n",
      "[2024-01-23 19:43:57,949] p1345 {3496713318.py:39} INFO - completed processing chunk 2/6 with concurrency=6\n",
      "[2024-01-23 19:43:57,950] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/6\n",
      "[2024-01-23 19:43:57,951] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:43:57,970] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2443\n",
      "[2024-01-23 19:43:57,974] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2500\n",
      "[2024-01-23 19:43:57,980] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2428\n",
      "[2024-01-23 19:43:57,982] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2458\n",
      "[2024-01-23 19:43:57,982] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2686\n",
      "[2024-01-23 19:43:57,983] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2321\n",
      "[2024-01-23 19:44:00,506] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.52\n",
      "[2024-01-23 19:44:04,586] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=51, latency=6.59\n",
      "[2024-01-23 19:44:05,949] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=7.96\n",
      "[2024-01-23 19:44:05,951] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=7.96\n",
      "[2024-01-23 19:44:05,953] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=7.96\n",
      "[2024-01-23 19:44:06,724] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=8.73\n",
      "[2024-01-23 19:44:06,785] p1345 {3496713318.py:39} INFO - completed processing chunk 3/6 with concurrency=6\n",
      "[2024-01-23 19:44:06,785] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/6\n",
      "[2024-01-23 19:44:06,786] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:44:06,805] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2711\n",
      "[2024-01-23 19:44:06,811] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2101\n",
      "[2024-01-23 19:44:06,817] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2440\n",
      "[2024-01-23 19:44:06,826] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2771\n",
      "[2024-01-23 19:44:06,827] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2691\n",
      "[2024-01-23 19:44:06,827] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2624\n",
      "[2024-01-23 19:44:09,432] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.61\n",
      "[2024-01-23 19:44:09,653] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=2.85\n",
      "[2024-01-23 19:44:11,028] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=13, latency=4.20\n",
      "[2024-01-23 19:44:11,294] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=4.46\n",
      "[2024-01-23 19:44:14,871] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=8.04\n",
      "[2024-01-23 19:44:15,251] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=8.42\n",
      "[2024-01-23 19:44:15,316] p1345 {3496713318.py:39} INFO - completed processing chunk 4/6 with concurrency=6\n",
      "[2024-01-23 19:44:15,317] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/6\n",
      "[2024-01-23 19:44:15,317] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:44:15,338] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2608\n",
      "[2024-01-23 19:44:15,340] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2213\n",
      "[2024-01-23 19:44:15,347] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2062\n",
      "[2024-01-23 19:44:15,347] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2333\n",
      "[2024-01-23 19:44:15,347] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2564\n",
      "[2024-01-23 19:44:15,350] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2770\n",
      "[2024-01-23 19:44:17,591] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=2.24\n",
      "[2024-01-23 19:44:17,764] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.41\n",
      "[2024-01-23 19:44:19,183] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=3.83\n",
      "[2024-01-23 19:44:19,492] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=12, latency=4.13\n",
      "[2024-01-23 19:44:19,493] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=15, latency=4.14\n",
      "[2024-01-23 19:44:23,228] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=7.86\n",
      "[2024-01-23 19:44:23,297] p1345 {3496713318.py:39} INFO - completed processing chunk 5/6 with concurrency=6\n",
      "[2024-01-23 19:44:23,297] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/6\n",
      "[2024-01-23 19:44:23,298] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:44:23,322] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2358\n",
      "[2024-01-23 19:44:23,325] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2613\n",
      "[2024-01-23 19:44:23,330] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2613\n",
      "[2024-01-23 19:44:23,331] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2613\n",
      "[2024-01-23 19:44:23,332] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2613\n",
      "[2024-01-23 19:44:23,339] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2613\n",
      "[2024-01-23 19:44:25,903] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=2.58\n",
      "[2024-01-23 19:44:28,065] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=4.72\n",
      "[2024-01-23 19:44:28,065] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=4.72\n",
      "[2024-01-23 19:44:31,402] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=82, latency=8.08\n",
      "[2024-01-23 19:44:31,403] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.07\n",
      "[2024-01-23 19:44:31,403] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=8.06\n",
      "[2024-01-23 19:44:31,469] p1345 {3496713318.py:39} INFO - completed processing chunk 6/6 with concurrency=6\n",
      "[2024-01-23 19:44:31,469] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/10\n",
      "[2024-01-23 19:44:31,470] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:44:31,502] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3896\n",
      "[2024-01-23 19:44:31,509] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3144\n",
      "[2024-01-23 19:44:31,511] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3789\n",
      "[2024-01-23 19:44:31,513] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3000\n",
      "[2024-01-23 19:44:31,516] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3450\n",
      "[2024-01-23 19:44:31,516] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3482\n",
      "[2024-01-23 19:44:38,929] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=28, latency=7.41\n",
      "[2024-01-23 19:44:41,579] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=10.07\n",
      "[2024-01-23 19:44:41,579] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=10.06\n",
      "[2024-01-23 19:44:41,580] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=10.06\n",
      "[2024-01-23 19:44:42,464] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=10.94\n",
      "[2024-01-23 19:44:43,313] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=11.78\n",
      "[2024-01-23 19:44:43,374] p1345 {3496713318.py:39} INFO - completed processing chunk 1/10 with concurrency=6\n",
      "[2024-01-23 19:44:43,374] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/10\n",
      "[2024-01-23 19:44:43,375] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:44:43,410] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3891\n",
      "[2024-01-23 19:44:43,410] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3575\n",
      "[2024-01-23 19:44:43,413] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3639\n",
      "[2024-01-23 19:44:43,415] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3419\n",
      "[2024-01-23 19:44:43,415] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3014\n",
      "[2024-01-23 19:44:43,416] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3458\n",
      "[2024-01-23 19:44:46,496] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=11, latency=3.07\n",
      "[2024-01-23 19:44:48,320] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=12, latency=4.90\n",
      "[2024-01-23 19:44:50,870] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=46, latency=7.45\n",
      "[2024-01-23 19:44:51,108] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=41, latency=7.68\n",
      "[2024-01-23 19:44:53,791] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=10.37\n",
      "[2024-01-23 19:44:54,636] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=11.21\n",
      "[2024-01-23 19:44:54,701] p1345 {3496713318.py:39} INFO - completed processing chunk 2/10 with concurrency=6\n",
      "[2024-01-23 19:44:54,701] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/10\n",
      "[2024-01-23 19:44:54,702] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:44:54,734] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3249\n",
      "[2024-01-23 19:44:54,740] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3436\n",
      "[2024-01-23 19:44:54,745] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3950\n",
      "[2024-01-23 19:44:54,746] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3508\n",
      "[2024-01-23 19:44:54,746] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3087\n",
      "[2024-01-23 19:44:54,747] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3680\n",
      "[2024-01-23 19:44:59,976] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=5.23\n",
      "[2024-01-23 19:45:00,123] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=25, latency=5.38\n",
      "[2024-01-23 19:45:04,218] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=84, latency=9.47\n",
      "[2024-01-23 19:45:04,648] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=95, latency=9.91\n",
      "[2024-01-23 19:45:05,547] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=10.79\n",
      "[2024-01-23 19:45:06,407] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=11.65\n",
      "[2024-01-23 19:45:06,478] p1345 {3496713318.py:39} INFO - completed processing chunk 3/10 with concurrency=6\n",
      "[2024-01-23 19:45:06,479] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/10\n",
      "[2024-01-23 19:45:06,480] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:45:06,514] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3050\n",
      "[2024-01-23 19:45:06,520] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3737\n",
      "[2024-01-23 19:45:06,524] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3996\n",
      "[2024-01-23 19:45:06,526] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3463\n",
      "[2024-01-23 19:45:06,526] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3410\n",
      "[2024-01-23 19:45:06,527] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3979\n",
      "[2024-01-23 19:45:09,298] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.76\n",
      "[2024-01-23 19:45:11,928] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=16, latency=5.39\n",
      "[2024-01-23 19:45:13,680] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=35, latency=7.14\n",
      "[2024-01-23 19:45:16,570] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=10.04\n",
      "[2024-01-23 19:45:16,570] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=10.04\n",
      "[2024-01-23 19:45:17,547] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=11.01\n",
      "[2024-01-23 19:45:17,622] p1345 {3496713318.py:39} INFO - completed processing chunk 4/10 with concurrency=6\n",
      "[2024-01-23 19:45:17,623] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/10\n",
      "[2024-01-23 19:45:17,624] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:45:17,648] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3662\n",
      "[2024-01-23 19:45:17,650] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3655\n",
      "[2024-01-23 19:45:17,664] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3171\n",
      "[2024-01-23 19:45:17,668] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3098\n",
      "[2024-01-23 19:45:17,671] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3400\n",
      "[2024-01-23 19:45:17,671] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3704\n",
      "[2024-01-23 19:45:20,603] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=2.95\n",
      "[2024-01-23 19:45:25,426] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=31, latency=7.75\n",
      "[2024-01-23 19:45:27,529] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=9.88\n",
      "[2024-01-23 19:45:27,529] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=9.86\n",
      "[2024-01-23 19:45:27,856] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=10.19\n",
      "[2024-01-23 19:45:27,858] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=10.18\n",
      "[2024-01-23 19:45:27,924] p1345 {3496713318.py:39} INFO - completed processing chunk 5/10 with concurrency=6\n",
      "[2024-01-23 19:45:27,924] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/10\n",
      "[2024-01-23 19:45:27,925] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:45:27,961] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3098\n",
      "[2024-01-23 19:45:27,967] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3132\n",
      "[2024-01-23 19:45:27,970] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3135\n",
      "[2024-01-23 19:45:27,973] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3971\n",
      "[2024-01-23 19:45:27,973] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3030\n",
      "[2024-01-23 19:45:27,975] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3909\n",
      "[2024-01-23 19:45:28,740] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.78\n",
      "[2024-01-23 19:45:34,145] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=6.16\n",
      "[2024-01-23 19:45:37,742] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=9.76\n",
      "[2024-01-23 19:45:37,742] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=9.77\n",
      "[2024-01-23 19:45:37,742] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=9.77\n",
      "[2024-01-23 19:45:37,742] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=9.76\n",
      "[2024-01-23 19:45:37,809] p1345 {3496713318.py:39} INFO - completed processing chunk 6/10 with concurrency=6\n",
      "[2024-01-23 19:45:37,809] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=7/10\n",
      "[2024-01-23 19:45:37,810] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:45:37,835] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3208\n",
      "[2024-01-23 19:45:37,842] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3848\n",
      "[2024-01-23 19:45:37,851] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3475\n",
      "[2024-01-23 19:45:37,852] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3681\n",
      "[2024-01-23 19:45:37,866] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3581\n",
      "[2024-01-23 19:45:37,868] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3302\n",
      "[2024-01-23 19:45:40,563] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.71\n",
      "[2024-01-23 19:45:42,420] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=4.57\n",
      "[2024-01-23 19:45:42,610] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=4.75\n",
      "[2024-01-23 19:45:42,610] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=4.74\n",
      "[2024-01-23 19:45:43,783] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=5.92\n",
      "[2024-01-23 19:45:47,386] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=78, latency=9.54\n",
      "[2024-01-23 19:45:47,454] p1345 {3496713318.py:39} INFO - completed processing chunk 7/10 with concurrency=6\n",
      "[2024-01-23 19:45:47,455] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=8/10\n",
      "[2024-01-23 19:45:47,455] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:45:47,491] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3537\n",
      "[2024-01-23 19:45:47,496] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3165\n",
      "[2024-01-23 19:45:47,499] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3614\n",
      "[2024-01-23 19:45:47,499] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3476\n",
      "[2024-01-23 19:45:47,507] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3235\n",
      "[2024-01-23 19:45:47,507] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3363\n",
      "[2024-01-23 19:45:50,283] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=2.78\n",
      "[2024-01-23 19:45:51,967] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=4.47\n",
      "[2024-01-23 19:45:52,156] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=4.65\n",
      "[2024-01-23 19:45:52,344] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=4.83\n",
      "[2024-01-23 19:45:53,376] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=5.86\n",
      "[2024-01-23 19:45:56,998] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=92, latency=9.49\n",
      "[2024-01-23 19:45:57,060] p1345 {3496713318.py:39} INFO - completed processing chunk 8/10 with concurrency=6\n",
      "[2024-01-23 19:45:57,061] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=9/10\n",
      "[2024-01-23 19:45:57,062] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:45:57,095] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3882\n",
      "[2024-01-23 19:45:57,095] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3567\n",
      "[2024-01-23 19:45:57,111] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3616\n",
      "[2024-01-23 19:45:57,113] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3497\n",
      "[2024-01-23 19:45:57,113] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3566\n",
      "[2024-01-23 19:45:57,113] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3330\n",
      "[2024-01-23 19:45:59,944] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=2.85\n",
      "[2024-01-23 19:46:01,822] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=4.71\n",
      "[2024-01-23 19:46:01,822] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=4.72\n",
      "[2024-01-23 19:46:01,823] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=11, latency=4.71\n",
      "[2024-01-23 19:46:02,997] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=5.88\n",
      "[2024-01-23 19:46:06,886] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=9.76\n",
      "[2024-01-23 19:46:06,954] p1345 {3496713318.py:39} INFO - completed processing chunk 9/10 with concurrency=6\n",
      "[2024-01-23 19:46:06,955] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=10/10\n",
      "[2024-01-23 19:46:06,955] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=6\n",
      "[2024-01-23 19:46:06,970] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3003\n",
      "[2024-01-23 19:46:06,989] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3003\n",
      "[2024-01-23 19:46:06,997] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3783\n",
      "[2024-01-23 19:46:06,997] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:46:06,998] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3003\n",
      "[2024-01-23 19:46:06,999] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3003\n",
      "[2024-01-23 19:46:10,152] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=16, latency=3.15\n",
      "[2024-01-23 19:46:13,509] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=6.50\n",
      "[2024-01-23 19:46:16,281] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=87, latency=9.28\n",
      "[2024-01-23 19:46:16,330] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=9.34\n",
      "[2024-01-23 19:46:16,331] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=9.34\n",
      "[2024-01-23 19:46:16,979] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=9.97\n",
      "[2024-01-23 19:46:17,045] p1345 {3496713318.py:39} INFO - completed processing chunk 10/10 with concurrency=6\n",
      "[2024-01-23 19:46:17,046] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-23 19:46:17,047] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:46:17,058] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:46:17,058] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:46:17,058] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:46:17,061] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:46:17,064] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:46:17,065] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:46:21,175] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=4.10\n",
      "[2024-01-23 19:46:21,176] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=4.10\n",
      "[2024-01-23 19:46:21,177] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=4.11\n",
      "[2024-01-23 19:46:21,177] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=4.10\n",
      "[2024-01-23 19:46:21,177] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=4.11\n",
      "[2024-01-23 19:46:21,178] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=107, latency=4.11\n",
      "[2024-01-23 19:46:21,182] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:46:21,185] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=304\n",
      "[2024-01-23 19:46:21,304] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.12\n",
      "[2024-01-23 19:46:24,772] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=3.58\n",
      "[2024-01-23 19:46:24,848] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=8\n",
      "[2024-01-23 19:46:24,848] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-23 19:46:24,849] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:46:24,858] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:46:24,860] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:46:24,862] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:46:24,865] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:46:24,866] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:46:24,869] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:46:26,708] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=11, latency=1.83\n",
      "[2024-01-23 19:46:26,713] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:46:26,865] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=17, latency=1.99\n",
      "[2024-01-23 19:46:26,870] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=980\n",
      "[2024-01-23 19:46:28,054] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=16, latency=1.34\n",
      "[2024-01-23 19:46:28,095] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=36, latency=3.22\n",
      "[2024-01-23 19:46:30,220] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=96, latency=5.35\n",
      "[2024-01-23 19:46:30,607] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.73\n",
      "[2024-01-23 19:46:30,607] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=5.73\n",
      "[2024-01-23 19:46:31,299] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=105, latency=4.43\n",
      "[2024-01-23 19:46:31,381] p1345 {3496713318.py:39} INFO - completed processing chunk 1/1 with concurrency=8\n",
      "[2024-01-23 19:46:31,381] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/2\n",
      "[2024-01-23 19:46:31,382] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:46:31,396] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1932\n",
      "[2024-01-23 19:46:31,398] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1154\n",
      "[2024-01-23 19:46:31,400] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1646\n",
      "[2024-01-23 19:46:31,403] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1397\n",
      "[2024-01-23 19:46:31,404] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1746\n",
      "[2024-01-23 19:46:31,405] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1339\n",
      "[2024-01-23 19:46:33,989] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=2.58\n",
      "[2024-01-23 19:46:33,996] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1373\n",
      "[2024-01-23 19:46:34,798] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=0.80\n",
      "[2024-01-23 19:46:34,803] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1598\n",
      "[2024-01-23 19:46:36,348] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=49, latency=4.94\n",
      "[2024-01-23 19:46:37,382] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=37, latency=2.58\n",
      "[2024-01-23 19:46:38,542] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=7.12\n",
      "[2024-01-23 19:46:38,542] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=7.13\n",
      "[2024-01-23 19:46:38,543] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=7.13\n",
      "[2024-01-23 19:46:38,544] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=93, latency=7.13\n",
      "[2024-01-23 19:46:38,626] p1345 {3496713318.py:39} INFO - completed processing chunk 1/2 with concurrency=8\n",
      "[2024-01-23 19:46:38,627] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/2\n",
      "[2024-01-23 19:46:38,627] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:46:38,635] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1743\n",
      "[2024-01-23 19:46:38,637] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1539\n",
      "[2024-01-23 19:46:38,650] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1421\n",
      "[2024-01-23 19:46:38,651] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1695\n",
      "[2024-01-23 19:46:38,650] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1918\n",
      "[2024-01-23 19:46:38,652] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1910\n",
      "[2024-01-23 19:46:40,840] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.18\n",
      "[2024-01-23 19:46:40,840] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=2.18\n",
      "[2024-01-23 19:46:40,849] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1939\n",
      "[2024-01-23 19:46:40,852] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=1743\n",
      "[2024-01-23 19:46:42,212] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=3.54\n",
      "[2024-01-23 19:46:42,505] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=1.65\n",
      "[2024-01-23 19:46:46,130] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=108, latency=7.47\n",
      "[2024-01-23 19:46:46,130] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=7.47\n",
      "[2024-01-23 19:46:46,130] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=7.47\n",
      "[2024-01-23 19:46:46,328] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=5.47\n",
      "[2024-01-23 19:46:46,405] p1345 {3496713318.py:39} INFO - completed processing chunk 2/2 with concurrency=8\n",
      "[2024-01-23 19:46:46,405] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/4\n",
      "[2024-01-23 19:46:46,406] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:46:46,421] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2637\n",
      "[2024-01-23 19:46:46,430] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3000\n",
      "[2024-01-23 19:46:46,434] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2404\n",
      "[2024-01-23 19:46:46,436] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2150\n",
      "[2024-01-23 19:46:46,436] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2148\n",
      "[2024-01-23 19:46:46,436] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2715\n",
      "[2024-01-23 19:46:48,852] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=2.41\n",
      "[2024-01-23 19:46:48,861] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2803\n",
      "[2024-01-23 19:46:55,334] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=8.90\n",
      "[2024-01-23 19:46:55,335] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=8.89\n",
      "[2024-01-23 19:46:55,337] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=8.89\n",
      "[2024-01-23 19:46:55,342] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2369\n",
      "[2024-01-23 19:46:56,793] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=106, latency=10.35\n",
      "[2024-01-23 19:46:56,795] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=10.34\n",
      "[2024-01-23 19:46:57,636] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=8.77\n",
      "[2024-01-23 19:47:00,177] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=4.83\n",
      "[2024-01-23 19:47:00,258] p1345 {3496713318.py:39} INFO - completed processing chunk 1/4 with concurrency=8\n",
      "[2024-01-23 19:47:00,259] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/4\n",
      "[2024-01-23 19:47:00,260] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:47:00,279] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2675\n",
      "[2024-01-23 19:47:00,284] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2186\n",
      "[2024-01-23 19:47:00,286] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2541\n",
      "[2024-01-23 19:47:00,289] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2686\n",
      "[2024-01-23 19:47:00,291] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2775\n",
      "[2024-01-23 19:47:00,292] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2500\n",
      "[2024-01-23 19:47:02,863] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.57\n",
      "[2024-01-23 19:47:02,872] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2443\n",
      "[2024-01-23 19:47:05,124] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=2.25\n",
      "[2024-01-23 19:47:05,132] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2321\n",
      "[2024-01-23 19:47:06,147] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=1.01\n",
      "[2024-01-23 19:47:07,267] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=42, latency=6.96\n",
      "[2024-01-23 19:47:09,545] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=9.25\n",
      "[2024-01-23 19:47:09,545] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=9.24\n",
      "[2024-01-23 19:47:09,545] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=9.25\n",
      "[2024-01-23 19:47:09,716] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=9.41\n",
      "[2024-01-23 19:47:09,800] p1345 {3496713318.py:39} INFO - completed processing chunk 2/4 with concurrency=8\n",
      "[2024-01-23 19:47:09,801] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/4\n",
      "[2024-01-23 19:47:09,801] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:47:09,827] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2101\n",
      "[2024-01-23 19:47:09,828] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2440\n",
      "[2024-01-23 19:47:09,829] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2428\n",
      "[2024-01-23 19:47:09,837] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2458\n",
      "[2024-01-23 19:47:09,837] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2771\n",
      "[2024-01-23 19:47:09,835] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2711\n",
      "[2024-01-23 19:47:12,373] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=2.54\n",
      "[2024-01-23 19:47:12,382] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2691\n",
      "[2024-01-23 19:47:14,456] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=11, latency=4.61\n",
      "[2024-01-23 19:47:14,464] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2624\n",
      "[2024-01-23 19:47:14,636] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=4.79\n",
      "[2024-01-23 19:47:19,124] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=9.28\n",
      "[2024-01-23 19:47:19,124] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=9.28\n",
      "[2024-01-23 19:47:19,303] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=9.47\n",
      "[2024-01-23 19:47:19,303] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=95, latency=6.92\n",
      "[2024-01-23 19:47:20,133] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=5.67\n",
      "[2024-01-23 19:47:20,220] p1345 {3496713318.py:39} INFO - completed processing chunk 3/4 with concurrency=8\n",
      "[2024-01-23 19:47:20,221] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/4\n",
      "[2024-01-23 19:47:20,221] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:47:20,231] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2062\n",
      "[2024-01-23 19:47:20,253] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2770\n",
      "[2024-01-23 19:47:20,249] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2564\n",
      "[2024-01-23 19:47:20,246] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2213\n",
      "[2024-01-23 19:47:20,257] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2608\n",
      "[2024-01-23 19:47:20,257] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2333\n",
      "[2024-01-23 19:47:22,803] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=2.54\n",
      "[2024-01-23 19:47:22,803] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=2.55\n",
      "[2024-01-23 19:47:22,814] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2613\n",
      "[2024-01-23 19:47:22,817] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=2358\n",
      "[2024-01-23 19:47:24,078] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=3.82\n",
      "[2024-01-23 19:47:24,079] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=3.83\n",
      "[2024-01-23 19:47:25,372] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=5.11\n",
      "[2024-01-23 19:47:29,522] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=77, latency=9.26\n",
      "[2024-01-23 19:47:29,613] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=6.79\n",
      "[2024-01-23 19:47:29,614] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=6.80\n",
      "[2024-01-23 19:47:29,707] p1345 {3496713318.py:39} INFO - completed processing chunk 4/4 with concurrency=8\n",
      "[2024-01-23 19:47:29,707] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=1/8\n",
      "[2024-01-23 19:47:29,708] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:47:29,738] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3896\n",
      "[2024-01-23 19:47:29,746] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3000\n",
      "[2024-01-23 19:47:29,748] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3450\n",
      "[2024-01-23 19:47:29,753] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3482\n",
      "[2024-01-23 19:47:29,754] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3789\n",
      "[2024-01-23 19:47:29,755] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3144\n",
      "[2024-01-23 19:47:38,399] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=33, latency=8.64\n",
      "[2024-01-23 19:47:38,409] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3639\n",
      "[2024-01-23 19:47:40,704] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=10.95\n",
      "[2024-01-23 19:47:40,707] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=10.95\n",
      "[2024-01-23 19:47:40,709] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=10.97\n",
      "[2024-01-23 19:47:40,714] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3014\n",
      "[2024-01-23 19:47:42,364] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=12.61\n",
      "[2024-01-23 19:47:42,364] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=12.60\n",
      "[2024-01-23 19:47:42,796] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=29, latency=2.08\n",
      "[2024-01-23 19:47:44,767] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=6.36\n",
      "[2024-01-23 19:47:44,854] p1345 {3496713318.py:39} INFO - completed processing chunk 1/8 with concurrency=8\n",
      "[2024-01-23 19:47:44,855] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=2/8\n",
      "[2024-01-23 19:47:44,855] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:47:44,880] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3575\n",
      "[2024-01-23 19:47:44,880] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3891\n",
      "[2024-01-23 19:47:44,894] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3436\n",
      "[2024-01-23 19:47:44,898] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3419\n",
      "[2024-01-23 19:47:44,900] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3508\n",
      "[2024-01-23 19:47:44,901] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3458\n",
      "[2024-01-23 19:47:50,849] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=33, latency=5.95\n",
      "[2024-01-23 19:47:50,860] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3249\n",
      "[2024-01-23 19:47:53,076] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=8.17\n",
      "[2024-01-23 19:47:53,084] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3087\n",
      "[2024-01-23 19:47:53,224] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=25, latency=8.32\n",
      "[2024-01-23 19:47:55,655] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=23, latency=2.57\n",
      "[2024-01-23 19:47:56,582] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=11.70\n",
      "[2024-01-23 19:47:56,582] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=103, latency=11.70\n",
      "[2024-01-23 19:47:57,461] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=96, latency=12.56\n",
      "[2024-01-23 19:47:58,293] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=7.43\n",
      "[2024-01-23 19:47:58,375] p1345 {3496713318.py:39} INFO - completed processing chunk 2/8 with concurrency=8\n",
      "[2024-01-23 19:47:58,376] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=3/8\n",
      "[2024-01-23 19:47:58,376] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:47:58,391] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3950\n",
      "[2024-01-23 19:47:58,411] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3463\n",
      "[2024-01-23 19:47:58,413] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3680\n",
      "[2024-01-23 19:47:58,426] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3737\n",
      "[2024-01-23 19:47:58,422] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3996\n",
      "[2024-01-23 19:47:58,428] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3050\n",
      "[2024-01-23 19:48:03,897] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=24, latency=5.48\n",
      "[2024-01-23 19:48:03,907] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3410\n",
      "[2024-01-23 19:48:06,859] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=8.43\n",
      "[2024-01-23 19:48:06,871] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3979\n",
      "[2024-01-23 19:48:10,560] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=101, latency=12.14\n",
      "[2024-01-23 19:48:10,560] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=12.14\n",
      "[2024-01-23 19:48:11,488] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=13.06\n",
      "[2024-01-23 19:48:11,490] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=13.06\n",
      "[2024-01-23 19:48:12,401] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=100, latency=8.49\n",
      "[2024-01-23 19:48:13,280] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=6.41\n",
      "[2024-01-23 19:48:13,368] p1345 {3496713318.py:39} INFO - completed processing chunk 3/8 with concurrency=8\n",
      "[2024-01-23 19:48:13,369] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=4/8\n",
      "[2024-01-23 19:48:13,370] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:48:13,399] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3704\n",
      "[2024-01-23 19:48:13,400] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3171\n",
      "[2024-01-23 19:48:13,410] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3662\n",
      "[2024-01-23 19:48:13,415] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3400\n",
      "[2024-01-23 19:48:13,418] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3655\n",
      "[2024-01-23 19:48:13,419] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3098\n",
      "[2024-01-23 19:48:16,303] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=2.90\n",
      "[2024-01-23 19:48:16,313] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3098\n",
      "[2024-01-23 19:48:18,060] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=4.66\n",
      "[2024-01-23 19:48:18,071] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3909\n",
      "[2024-01-23 19:48:18,109] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=4.69\n",
      "[2024-01-23 19:48:19,892] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=6.47\n",
      "[2024-01-23 19:48:19,940] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=3.63\n",
      "[2024-01-23 19:48:21,917] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=3.85\n",
      "[2024-01-23 19:48:24,921] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=11.51\n",
      "[2024-01-23 19:48:25,182] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=97, latency=11.76\n",
      "[2024-01-23 19:48:25,267] p1345 {3496713318.py:39} INFO - completed processing chunk 4/8 with concurrency=8\n",
      "[2024-01-23 19:48:25,268] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=5/8\n",
      "[2024-01-23 19:48:25,269] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:48:25,304] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3132\n",
      "[2024-01-23 19:48:25,306] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3971\n",
      "[2024-01-23 19:48:25,308] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3208\n",
      "[2024-01-23 19:48:25,313] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3848\n",
      "[2024-01-23 19:48:25,314] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3030\n",
      "[2024-01-23 19:48:25,318] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3135\n",
      "[2024-01-23 19:48:28,019] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.71\n",
      "[2024-01-23 19:48:28,029] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3475\n",
      "[2024-01-23 19:48:29,732] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=1, latency=4.41\n",
      "[2024-01-23 19:48:29,743] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3681\n",
      "[2024-01-23 19:48:29,783] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=4.47\n",
      "[2024-01-23 19:48:31,495] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=3.46\n",
      "[2024-01-23 19:48:31,733] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=14, latency=6.41\n",
      "[2024-01-23 19:48:31,925] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=15, latency=6.60\n",
      "[2024-01-23 19:48:36,912] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=93, latency=11.59\n",
      "[2024-01-23 19:48:37,500] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=74, latency=7.76\n",
      "[2024-01-23 19:48:37,588] p1345 {3496713318.py:39} INFO - completed processing chunk 5/8 with concurrency=8\n",
      "[2024-01-23 19:48:37,589] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=6/8\n",
      "[2024-01-23 19:48:37,589] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:48:37,625] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3302\n",
      "[2024-01-23 19:48:37,632] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3537\n",
      "[2024-01-23 19:48:37,635] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3614\n",
      "[2024-01-23 19:48:37,636] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3581\n",
      "[2024-01-23 19:48:37,636] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3476\n",
      "[2024-01-23 19:48:37,636] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3165\n",
      "[2024-01-23 19:48:40,479] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=2.84\n",
      "[2024-01-23 19:48:40,491] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3235\n",
      "[2024-01-23 19:48:42,502] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=10, latency=4.85\n",
      "[2024-01-23 19:48:42,511] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3363\n",
      "[2024-01-23 19:48:44,991] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=7, latency=7.35\n",
      "[2024-01-23 19:48:45,134] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=11, latency=4.64\n",
      "[2024-01-23 19:48:46,592] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=6, latency=4.08\n",
      "[2024-01-23 19:48:48,994] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=11.37\n",
      "[2024-01-23 19:48:48,995] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=93, latency=11.36\n",
      "[2024-01-23 19:48:49,265] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=79, latency=11.62\n",
      "[2024-01-23 19:48:49,351] p1345 {3496713318.py:39} INFO - completed processing chunk 6/8 with concurrency=8\n",
      "[2024-01-23 19:48:49,351] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=7/8\n",
      "[2024-01-23 19:48:49,352] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:48:49,388] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3567\n",
      "[2024-01-23 19:48:49,393] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3882\n",
      "[2024-01-23 19:48:49,391] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3616\n",
      "[2024-01-23 19:48:49,401] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3330\n",
      "[2024-01-23 19:48:49,405] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3497\n",
      "[2024-01-23 19:48:49,405] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3566\n",
      "[2024-01-23 19:48:52,233] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=5, latency=2.84\n",
      "[2024-01-23 19:48:52,243] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3003\n",
      "[2024-01-23 19:48:54,089] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=4.69\n",
      "[2024-01-23 19:48:54,089] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=4.70\n",
      "[2024-01-23 19:48:54,100] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3783\n",
      "[2024-01-23 19:48:56,087] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=9, latency=6.68\n",
      "[2024-01-23 19:48:56,417] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=22, latency=7.01\n",
      "[2024-01-23 19:48:57,850] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=3.75\n",
      "[2024-01-23 19:49:01,119] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=98, latency=11.72\n",
      "[2024-01-23 19:49:01,251] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=99, latency=9.01\n",
      "[2024-01-23 19:49:01,334] p1345 {3496713318.py:39} INFO - completed processing chunk 7/8 with concurrency=8\n",
      "[2024-01-23 19:49:01,334] p1345 {3496713318.py:26} INFO - e_idx=1/1, chunk_index=8/8\n",
      "[2024-01-23 19:49:01,335] p1345 {1750118496.py:3} INFO - Processing chunk with concurrency=8\n",
      "[2024-01-23 19:49:01,485] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:49:01,485] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:49:01,485] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:49:01,507] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:49:01,516] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:49:01,510] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:49:03,984] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=4, latency=2.49\n",
      "[2024-01-23 19:49:03,994] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:49:05,823] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=4.31\n",
      "[2024-01-23 19:49:05,833] p1345 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706037700, prompt_tokens=3271\n",
      "[2024-01-23 19:49:05,871] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=8, latency=4.35\n",
      "[2024-01-23 19:49:07,573] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=16, latency=6.07\n",
      "[2024-01-23 19:49:12,454] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=74, latency=10.93\n",
      "[2024-01-23 19:49:12,780] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=104, latency=8.78\n",
      "[2024-01-23 19:49:12,780] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=102, latency=11.27\n",
      "[2024-01-23 19:49:13,613] p1345 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706037700, completion_tokens=105, latency=7.78\n",
      "[2024-01-23 19:49:13,699] p1345 {3496713318.py:39} INFO - completed processing chunk 8/8 with concurrency=8\n",
      "[2024-01-23 19:49:13,700] p1345 {3496713318.py:41} INFO - experiment=1/1, name=mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0, done\n"
     ]
    }
   ],
   "source": [
    "# for each experiment\n",
    "#   - for each endpoint and concurrency in an experiment\n",
    "\n",
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "_ = list(map(clear_dir, [METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]))\n",
    "\n",
    "num_experiments: int = len(config['experiments'])\n",
    "for e_idx, experiment in enumerate(config['experiments']):\n",
    "    e_idx += 1  # Increment experiment index\n",
    "    # Call do_experiment function to create the predictor object\n",
    " \n",
    "    predictor = create_predictor_for_experiment(experiment, config, endpoint_info_list)\n",
    "    if predictor is None:\n",
    "        logger.error(f\"predictor could not be created for experiment={experiment}, moving to next...\")\n",
    "        continue\n",
    "\n",
    "    # Process combinations of concurrency levels and payload files\n",
    "    combination_data = create_combinations(experiment)\n",
    "\n",
    "    for concurrency, payload_file, split_payload in combination_data:\n",
    "        for chunk_index, chunk in enumerate(split_payload):\n",
    "            logger.info(f\"e_idx={e_idx}/{num_experiments}, chunk_index={chunk_index+1}/{len(split_payload)}\")\n",
    "\n",
    "            # Process each chunk and calculate metrics\n",
    "            responses, metrics = await run_inferences(predictor, chunk, experiment, concurrency, payload_file)\n",
    "            if metrics:\n",
    "                #per_concurrency_level_response_metrics.append(metrics)\n",
    "                fpath: str = os.path.join(METRICS_PER_CHUNK_DIR, f\"{time.time()}.json\")\n",
    "                Path(fpath).write_text(json.dumps(metrics, indent=2))\n",
    "            if responses:\n",
    "                for r in responses:\n",
    "                    fpath: str = os.path.join(METRICS_PER_INFERENCE_DIR, f\"{time.time()}.json\")\n",
    "                    Path(fpath).write_text(json.dumps(r, indent=2))\n",
    "            \n",
    "            logger.info(f\"completed processing chunk {chunk_index+1}/{len(split_payload)} with concurrency={concurrency}\")\n",
    "\n",
    "    logger.info(f\"experiment={e_idx}/{num_experiments}, name={experiment['name']}, done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:49:14,664] p1345 {3838589563.py:4} INFO - created dataframe of shape (586, 14) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>2321</td>\n",
       "      <td>\\n```\\n\\n```\\n\\nQuestion: Passage:\\nBiggest We...</td>\n",
       "      <td>2321</td>\n",
       "      <td>100</td>\n",
       "      <td>4.599911</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3436</td>\n",
       "      <td>Where Are You? I'm Here is earlier than Patna...</td>\n",
       "      <td>3436</td>\n",
       "      <td>33</td>\n",
       "      <td>5.953868</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3655</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>3655</td>\n",
       "      <td>4</td>\n",
       "      <td>6.470846</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>2691</td>\n",
       "      <td>\\n\\n\\n[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for...</td>\n",
       "      <td>2691</td>\n",
       "      <td>101</td>\n",
       "      <td>4.817750</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3271</td>\n",
       "      <td>\\n\\n\\n[/INST]\\nAnswer:\\nThe Owl and the Pussy-...</td>\n",
       "      <td>3271</td>\n",
       "      <td>103</td>\n",
       "      <td>7.673864</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706037700   \n",
       "1  lmistral7b-g5-2xlarge-1706037700   \n",
       "2  lmistral7b-g5-2xlarge-1706037700   \n",
       "3  lmistral7b-g5-2xlarge-1706037700   \n",
       "4  lmistral7b-g5-2xlarge-1706037700   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100      2321   \n",
       "1   0.92    120             100      3436   \n",
       "2   0.92    120             100      3655   \n",
       "3   0.92    120             100      2691   \n",
       "4   0.92    120             100      3271   \n",
       "\n",
       "                                          completion  prompt_tokens  \\\n",
       "0  \\n```\\n\\n```\\n\\nQuestion: Passage:\\nBiggest We...           2321   \n",
       "1   Where Are You? I'm Here is earlier than Patna...           3436   \n",
       "2                                               Yes.           3655   \n",
       "3  \\n\\n\\n[INST] <<SYS>>\\nYou are an assistant for...           2691   \n",
       "4  \\n\\n\\n[/INST]\\nAnswer:\\nThe Owl and the Pussy-...           3271   \n",
       "\n",
       "   completion_tokens   latency  \\\n",
       "0                100  4.599911   \n",
       "1                 33  5.953868   \n",
       "2                  4  6.470846   \n",
       "3                101  4.817750   \n",
       "4                103  7.673864   \n",
       "\n",
       "                                     experiment_name  concurrency  \n",
       "0  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "1  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "2  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "3  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "4  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all per chunk files\n",
    "json_list: List[Dict] = [json.loads(Path(f_name).read_text()) for f_name in glob.glob(os.path.join(METRICS_PER_INFERENCE_DIR, \"*.json\"))]\n",
    "df_responses = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all responses\")\n",
    "df_responses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:49:15,360] p1345 {1971204328.py:4} INFO - created dataframe of shape (586, 14) from all inference responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>2321</td>\n",
       "      <td>\\n```\\n\\n```\\n\\nQuestion: Passage:\\nBiggest We...</td>\n",
       "      <td>2321</td>\n",
       "      <td>100</td>\n",
       "      <td>4.599911</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3436</td>\n",
       "      <td>Where Are You? I'm Here is earlier than Patna...</td>\n",
       "      <td>3436</td>\n",
       "      <td>33</td>\n",
       "      <td>5.953868</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3655</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>3655</td>\n",
       "      <td>4</td>\n",
       "      <td>6.470846</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>2691</td>\n",
       "      <td>\\n\\n\\n[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for...</td>\n",
       "      <td>2691</td>\n",
       "      <td>101</td>\n",
       "      <td>4.817750</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3271</td>\n",
       "      <td>\\n\\n\\n[/INST]\\nAnswer:\\nThe Owl and the Pussy-...</td>\n",
       "      <td>3271</td>\n",
       "      <td>103</td>\n",
       "      <td>7.673864</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706037700   \n",
       "1  lmistral7b-g5-2xlarge-1706037700   \n",
       "2  lmistral7b-g5-2xlarge-1706037700   \n",
       "3  lmistral7b-g5-2xlarge-1706037700   \n",
       "4  lmistral7b-g5-2xlarge-1706037700   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100      2321   \n",
       "1   0.92    120             100      3436   \n",
       "2   0.92    120             100      3655   \n",
       "3   0.92    120             100      2691   \n",
       "4   0.92    120             100      3271   \n",
       "\n",
       "                                          completion  prompt_tokens  \\\n",
       "0  \\n```\\n\\n```\\n\\nQuestion: Passage:\\nBiggest We...           2321   \n",
       "1   Where Are You? I'm Here is earlier than Patna...           3436   \n",
       "2                                               Yes.           3655   \n",
       "3  \\n\\n\\n[INST] <<SYS>>\\nYou are an assistant for...           2691   \n",
       "4  \\n\\n\\n[/INST]\\nAnswer:\\nThe Owl and the Pussy-...           3271   \n",
       "\n",
       "   completion_tokens   latency  \\\n",
       "0                100  4.599911   \n",
       "1                 33  5.953868   \n",
       "2                  4  6.470846   \n",
       "3                101  4.817750   \n",
       "4                103  7.673864   \n",
       "\n",
       "                                     experiment_name  concurrency  \n",
       "0  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "1  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "2  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8  \n",
       "3  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "4  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all per inference files\n",
    "json_list: List[Dict] = [json.loads(Path(f_name).read_text()) for f_name in glob.glob(os.path.join(METRICS_PER_INFERENCE_DIR, \"*.json\"))]\n",
    "df_responses = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all inference responses\")\n",
    "df_responses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [endpoint_name, prompt, do_sample, temperature, top_p, top_k, max_new_tokens, truncate, completion, prompt_tokens, completion_tokens, latency, experiment_name, concurrency]\n",
       "Index: []"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses[df_responses.endpoint_name.str.contains(\"inf2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:49:15,848] p1345 {976317414.py:4} INFO - created dataframe of shape (227, 16) from all chunk responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>errors</th>\n",
       "      <th>successes</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>all_prompts_token_count</th>\n",
       "      <th>prompt_token_count_mean</th>\n",
       "      <th>prompt_token_throughput</th>\n",
       "      <th>all_completions_token_count</th>\n",
       "      <th>completion_token_count_mean</th>\n",
       "      <th>completion_token_throughput</th>\n",
       "      <th>transactions</th>\n",
       "      <th>transactions_per_second</th>\n",
       "      <th>transactions_per_minute</th>\n",
       "      <th>latency_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>8</td>\n",
       "      <td>payload_en_2000-3000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20226</td>\n",
       "      <td>2528.25</td>\n",
       "      <td>1468.72</td>\n",
       "      <td>718</td>\n",
       "      <td>89.75</td>\n",
       "      <td>52.14</td>\n",
       "      <td>8</td>\n",
       "      <td>0.58</td>\n",
       "      <td>34</td>\n",
       "      <td>7.923026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>2</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6626</td>\n",
       "      <td>3313.00</td>\n",
       "      <td>1082.04</td>\n",
       "      <td>105</td>\n",
       "      <td>52.50</td>\n",
       "      <td>17.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.33</td>\n",
       "      <td>19</td>\n",
       "      <td>4.077796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>6</td>\n",
       "      <td>payload_en_1000-2000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9369</td>\n",
       "      <td>1561.50</td>\n",
       "      <td>1479.16</td>\n",
       "      <td>351</td>\n",
       "      <td>58.50</td>\n",
       "      <td>55.42</td>\n",
       "      <td>6</td>\n",
       "      <td>0.95</td>\n",
       "      <td>57</td>\n",
       "      <td>4.601383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13239</td>\n",
       "      <td>3309.75</td>\n",
       "      <td>1679.03</td>\n",
       "      <td>213</td>\n",
       "      <td>53.25</td>\n",
       "      <td>27.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.51</td>\n",
       "      <td>30</td>\n",
       "      <td>5.716153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>4</td>\n",
       "      <td>payload_en_3000-4000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14395</td>\n",
       "      <td>3598.75</td>\n",
       "      <td>1566.69</td>\n",
       "      <td>403</td>\n",
       "      <td>100.75</td>\n",
       "      <td>43.86</td>\n",
       "      <td>4</td>\n",
       "      <td>0.44</td>\n",
       "      <td>26</td>\n",
       "      <td>8.499557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     experiment_name  concurrency  \\\n",
       "0  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            8   \n",
       "1  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            2   \n",
       "2  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            6   \n",
       "3  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4   \n",
       "4  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            4   \n",
       "\n",
       "                 payload_file errors  successes  error_rate  \\\n",
       "0  payload_en_2000-3000.jsonl     []          8         0.0   \n",
       "1  payload_en_3000-4000.jsonl     []          2         0.0   \n",
       "2  payload_en_1000-2000.jsonl     []          6         0.0   \n",
       "3  payload_en_3000-4000.jsonl     []          4         0.0   \n",
       "4  payload_en_3000-4000.jsonl     []          4         0.0   \n",
       "\n",
       "   all_prompts_token_count  prompt_token_count_mean  prompt_token_throughput  \\\n",
       "0                    20226                  2528.25                  1468.72   \n",
       "1                     6626                  3313.00                  1082.04   \n",
       "2                     9369                  1561.50                  1479.16   \n",
       "3                    13239                  3309.75                  1679.03   \n",
       "4                    14395                  3598.75                  1566.69   \n",
       "\n",
       "   all_completions_token_count  completion_token_count_mean  \\\n",
       "0                          718                        89.75   \n",
       "1                          105                        52.50   \n",
       "2                          351                        58.50   \n",
       "3                          213                        53.25   \n",
       "4                          403                       100.75   \n",
       "\n",
       "   completion_token_throughput  transactions  transactions_per_second  \\\n",
       "0                        52.14             8                     0.58   \n",
       "1                        17.15             2                     0.33   \n",
       "2                        55.42             6                     0.95   \n",
       "3                        27.01             4                     0.51   \n",
       "4                        43.86             4                     0.44   \n",
       "\n",
       "   transactions_per_minute  latency_mean  \n",
       "0                       34      7.923026  \n",
       "1                       19      4.077796  \n",
       "2                       57      4.601383  \n",
       "3                       30      5.716153  \n",
       "4                       26      8.499557  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all per inference files\n",
    "json_list: List[Dict] = [json.loads(Path(f_name).read_text()) for f_name in glob.glob(os.path.join(METRICS_PER_CHUNK_DIR, \"*.json\"))]\n",
    "df_metrics = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_metrics.shape} from all chunk responses\")\n",
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_config.PrimaryContainer.Environment.ENDPOINT_SERVER_TIMEOUT', 'model_config.PrimaryContainer.Environment.HF_MODEL_ID', 'model_config.PrimaryContainer.Environment.MAX_BATCH_PREFILL_TOKENS', 'model_config.PrimaryContainer.Environment.MAX_INPUT_LENGTH', 'model_config.PrimaryContainer.Environment.MAX_TOTAL_TOKENS', 'model_config.PrimaryContainer.Environment.MODEL_CACHE_ROOT', 'model_config.PrimaryContainer.Environment.SAGEMAKER_ENV', 'model_config.PrimaryContainer.Environment.SAGEMAKER_MODEL_SERVER_WORKERS', 'model_config.PrimaryContainer.Environment.SAGEMAKER_PROGRAM', 'model_config.PrimaryContainer.Environment.SM_NUM_GPUS']\n",
      "Columns in df_responses: Index(['endpoint_name', 'prompt', 'do_sample', 'temperature', 'top_p', 'top_k',\n",
      "       'max_new_tokens', 'truncate', 'completion', 'prompt_tokens',\n",
      "       'completion_tokens', 'latency', 'experiment_name', 'concurrency'],\n",
      "      dtype='object')\n",
      "Columns in df_endpoints: Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri', 'ENDPOINT_SERVER_TIMEOUT', 'HF_MODEL_ID',\n",
      "       'MAX_BATCH_PREFILL_TOKENS', 'MAX_INPUT_LENGTH', 'MAX_TOTAL_TOKENS',\n",
      "       'MODEL_CACHE_ROOT', 'SAGEMAKER_ENV', 'SAGEMAKER_MODEL_SERVER_WORKERS',\n",
      "       'SAGEMAKER_PROGRAM', 'SM_NUM_GPUS'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>ENDPOINT_SERVER_TIMEOUT</th>\n",
       "      <th>HF_MODEL_ID</th>\n",
       "      <th>MAX_BATCH_PREFILL_TOKENS</th>\n",
       "      <th>MAX_INPUT_LENGTH</th>\n",
       "      <th>MAX_TOTAL_TOKENS</th>\n",
       "      <th>MODEL_CACHE_ROOT</th>\n",
       "      <th>SAGEMAKER_ENV</th>\n",
       "      <th>SAGEMAKER_MODEL_SERVER_WORKERS</th>\n",
       "      <th>SAGEMAKER_PROGRAM</th>\n",
       "      <th>SM_NUM_GPUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>2321</td>\n",
       "      <td>\\n```\\n\\n```\\n\\nQuestion: Passage:\\nBiggest We...</td>\n",
       "      <td>2321</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3436</td>\n",
       "      <td>Where Are You? I'm Here is earlier than Patna...</td>\n",
       "      <td>3436</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3655</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>3655</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>2691</td>\n",
       "      <td>\\n\\n\\n[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for...</td>\n",
       "      <td>2691</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3271</td>\n",
       "      <td>\\n\\n\\n[/INST]\\nAnswer:\\nThe Owl and the Pussy-...</td>\n",
       "      <td>3271</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706037700   \n",
       "1  lmistral7b-g5-2xlarge-1706037700   \n",
       "2  lmistral7b-g5-2xlarge-1706037700   \n",
       "3  lmistral7b-g5-2xlarge-1706037700   \n",
       "4  lmistral7b-g5-2xlarge-1706037700   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100      2321   \n",
       "1   0.92    120             100      3436   \n",
       "2   0.92    120             100      3655   \n",
       "3   0.92    120             100      2691   \n",
       "4   0.92    120             100      3271   \n",
       "\n",
       "                                          completion  prompt_tokens  ...  \\\n",
       "0  \\n```\\n\\n```\\n\\nQuestion: Passage:\\nBiggest We...           2321  ...   \n",
       "1   Where Are You? I'm Here is earlier than Patna...           3436  ...   \n",
       "2                                               Yes.           3655  ...   \n",
       "3  \\n\\n\\n[INST] <<SYS>>\\nYou are an assistant for...           2691  ...   \n",
       "4  \\n\\n\\n[/INST]\\nAnswer:\\nThe Owl and the Pussy-...           3271  ...   \n",
       "\n",
       "   ENDPOINT_SERVER_TIMEOUT    HF_MODEL_ID MAX_BATCH_PREFILL_TOKENS  \\\n",
       "0                     3600  /opt/ml/model                     8191   \n",
       "1                     3600  /opt/ml/model                     8191   \n",
       "2                     3600  /opt/ml/model                     8191   \n",
       "3                     3600  /opt/ml/model                     8191   \n",
       "4                     3600  /opt/ml/model                     8191   \n",
       "\n",
       "   MAX_INPUT_LENGTH MAX_TOTAL_TOKENS MODEL_CACHE_ROOT SAGEMAKER_ENV  \\\n",
       "0              8191             8192    /opt/ml/model             1   \n",
       "1              8191             8192    /opt/ml/model             1   \n",
       "2              8191             8192    /opt/ml/model             1   \n",
       "3              8191             8192    /opt/ml/model             1   \n",
       "4              8191             8192    /opt/ml/model             1   \n",
       "\n",
       "  SAGEMAKER_MODEL_SERVER_WORKERS SAGEMAKER_PROGRAM SM_NUM_GPUS  \n",
       "0                              1      inference.py           1  \n",
       "1                              1      inference.py           1  \n",
       "2                              1      inference.py           1  \n",
       "3                              1      inference.py           1  \n",
       "4                              1      inference.py           1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_endpoints = pd.json_normalize(endpoint_info_list)\n",
    "df_endpoints['instance_type'] = df_endpoints['endpoint_config.ProductionVariants'].map(lambda x: x[0]['InstanceType'])\n",
    "df_endpoints\n",
    "cols_for_env = [c for c in df_endpoints.columns if 'Environment' in c]\n",
    "print(cols_for_env)\n",
    "cols_of_interest = ['experiment_name', \n",
    "                    'instance_type',\n",
    "                    'endpoint.EndpointName',\n",
    "                    'model_config.ModelName',\n",
    "                    'model_config.PrimaryContainer.Image',   \n",
    "                    'model_config.PrimaryContainer.ModelDataSource.S3DataSource.S3Uri']\n",
    "cols_of_interest.extend(cols_for_env)\n",
    "\n",
    "df_endpoints = df_endpoints[cols_of_interest]\n",
    "df_endpoints = df_endpoints[cols_of_interest]\n",
    "cols_of_interest_renamed = [c.split('.')[-1] for c in cols_of_interest]\n",
    "df_endpoints.columns = cols_of_interest_renamed\n",
    "\n",
    "# Check if 'experiment_name' column exists in both DataFrames\n",
    "print(\"Columns in df_responses:\", df_responses.columns)\n",
    "print(\"Columns in df_endpoints:\", df_endpoints.columns)\n",
    "\n",
    "# Merge operation\n",
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "\n",
    "# Inspect the result\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>ENDPOINT_SERVER_TIMEOUT</th>\n",
       "      <th>HF_MODEL_ID</th>\n",
       "      <th>MAX_BATCH_PREFILL_TOKENS</th>\n",
       "      <th>MAX_INPUT_LENGTH</th>\n",
       "      <th>MAX_TOTAL_TOKENS</th>\n",
       "      <th>MODEL_CACHE_ROOT</th>\n",
       "      <th>SAGEMAKER_ENV</th>\n",
       "      <th>SAGEMAKER_MODEL_SERVER_WORKERS</th>\n",
       "      <th>SAGEMAKER_PROGRAM</th>\n",
       "      <th>SM_NUM_GPUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>2321</td>\n",
       "      <td>\\n```\\n\\n```\\n\\nQuestion: Passage:\\nBiggest We...</td>\n",
       "      <td>2321</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3436</td>\n",
       "      <td>Where Are You? I'm Here is earlier than Patna...</td>\n",
       "      <td>3436</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3655</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>3655</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>2691</td>\n",
       "      <td>\\n\\n\\n[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for...</td>\n",
       "      <td>2691</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706037700</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>3271</td>\n",
       "      <td>\\n\\n\\n[/INST]\\nAnswer:\\nThe Owl and the Pussy-...</td>\n",
       "      <td>3271</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706037700   \n",
       "1  lmistral7b-g5-2xlarge-1706037700   \n",
       "2  lmistral7b-g5-2xlarge-1706037700   \n",
       "3  lmistral7b-g5-2xlarge-1706037700   \n",
       "4  lmistral7b-g5-2xlarge-1706037700   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "2  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "3  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "4  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.7   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100      2321   \n",
       "1   0.92    120             100      3436   \n",
       "2   0.92    120             100      3655   \n",
       "3   0.92    120             100      2691   \n",
       "4   0.92    120             100      3271   \n",
       "\n",
       "                                          completion  prompt_tokens  ...  \\\n",
       "0  \\n```\\n\\n```\\n\\nQuestion: Passage:\\nBiggest We...           2321  ...   \n",
       "1   Where Are You? I'm Here is earlier than Patna...           3436  ...   \n",
       "2                                               Yes.           3655  ...   \n",
       "3  \\n\\n\\n[INST] <<SYS>>\\nYou are an assistant for...           2691  ...   \n",
       "4  \\n\\n\\n[/INST]\\nAnswer:\\nThe Owl and the Pussy-...           3271  ...   \n",
       "\n",
       "   ENDPOINT_SERVER_TIMEOUT    HF_MODEL_ID MAX_BATCH_PREFILL_TOKENS  \\\n",
       "0                     3600  /opt/ml/model                     8191   \n",
       "1                     3600  /opt/ml/model                     8191   \n",
       "2                     3600  /opt/ml/model                     8191   \n",
       "3                     3600  /opt/ml/model                     8191   \n",
       "4                     3600  /opt/ml/model                     8191   \n",
       "\n",
       "   MAX_INPUT_LENGTH MAX_TOTAL_TOKENS MODEL_CACHE_ROOT SAGEMAKER_ENV  \\\n",
       "0              8191             8192    /opt/ml/model             1   \n",
       "1              8191             8192    /opt/ml/model             1   \n",
       "2              8191             8192    /opt/ml/model             1   \n",
       "3              8191             8192    /opt/ml/model             1   \n",
       "4              8191             8192    /opt/ml/model             1   \n",
       "\n",
       "  SAGEMAKER_MODEL_SERVER_WORKERS SAGEMAKER_PROGRAM SM_NUM_GPUS  \n",
       "0                              1      inference.py           1  \n",
       "1                              1      inference.py           1  \n",
       "2                              1      inference.py           1  \n",
       "3                              1      inference.py           1  \n",
       "4                              1      inference.py           1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:49:16,263] p1345 {3435721265.py:3} INFO - saved results dataframe of shape=(586, 29) in data/metrics/mistral-7b-tgi-g5-v1/per_inference_request_results.csv\n"
     ]
    }
   ],
   "source": [
    "fpath: str = os.path.join(METRICS_DIR, config['results']['per_inference_request_file']).format(datetime=date_time)\n",
    "df_results.to_csv(fpath, index=False)\n",
    "logger.info(f\"saved results dataframe of shape={df_results.shape} in {fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:49:16,326] p1345 {3880536130.py:5} INFO - saved metrics results dataframe of shape=(227, 31) in data/metrics/mistral-7b-tgi-g5-v1/all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "df_metrics = pd.merge(left=df_metrics, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "df_metrics.head()\n",
    "fpath: str = os.path.join(METRICS_DIR, config['results']['all_metrics_file']).format(datetime=date_time)\n",
    "df_metrics.to_csv(fpath, index=False)\n",
    "logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in {fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
