{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on all deployed endpoints: Various combinations of payloads, concurrency levels, model configurations\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of our solution design includes running inferences on all deployed model endpoints (with different configurations, concurrency levels and payload sizes). This notebook runs inferences in a manner that is calls endpoints concurrently and asychronously to generate responses and record metrics. Here are some of the key components:\n",
    "\n",
    "- **Accessing the deployed endpoints**, creating a predictor object for these endpoints to call them during inference time.\n",
    "\n",
    "- **Functions to define metrics**: This notebook sets stage for metrics to be recorded during the time of invocation of all these models for benchmarking purposes.\n",
    "\n",
    "- **Running Actual Inferences**: Once the metrics are defined, we set a blocker function that is responsible for creating inference on a single payload called get_inference. We then run a series of asynchronous functions that can be viewed in the code (link above), to create asychronous inferefences on the deployed models. The way we send requests are by creating combinations: this means creating combinations of payloads of different sizes that can be viewed in the config.yml file, with different concurrency levels (in this case we first go through all patches of payloads with a concurrency level of 1, then 2, and then 4). You can set this to your desired value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG_FILE=configs/config-mistral-7b-tgi-g5.yml\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "## auto reload all of the changes made in the config/globals.py file \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!touch globals.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG_FILE=configs/config-mistral-7b-tgi-g5.yml\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "import copy\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import itertools\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from globals import *\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer\n",
    "from sagemaker.predictor import Predictor\n",
    "from utils import load_config, count_tokens, write_to_s3, read_from_s3, get_s3_object\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from typing import Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pygmentize globals.py to view and use any of the globally initialized variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36myaml\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36menum\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Enum\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILEPATH_FILE: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig_filepath.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# S3 client initialization\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "s3_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "CONFIG_FILE: \u001b[36mstr\u001b[39;49;00m = Path(CONFIG_FILEPATH_FILE).read_text()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCONFIG_FILE=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mCONFIG_FILE\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(CONFIG_FILE, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m file:\u001b[37m\u001b[39;49;00m\n",
      "    config = yaml.safe_load(file)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "DATA_DIR: \u001b[36mstr\u001b[39;49;00m = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mdata_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "PROMPTS_DIR = os.path.join(DATA_DIR, config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprompts_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## --------------------- Metrics directory based on date and time ---------------------------\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "current_time = datetime.now()\u001b[37m\u001b[39;49;00m\n",
      "formatted_time = current_time.strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY/\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm/\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH/\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_DIR = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mdata_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m/metrics/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mformatted_time\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_INFERENCE_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_inference\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "METRICS_PER_CHUNK_DIR  = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mper_chunk\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## --------------------- Models directory based on date and time ---------------------------\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# MODELS_DIR = f\"{config['dir_paths']['data_prefix']}/models/{config['general']['name']}\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "MODELS_DIR = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mdata_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m/models/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mformatted_time\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig[\u001b[33m'\u001b[39;49;00m\u001b[33mgeneral\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Use this to upload to the s3 bucket (extracted from the config file)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "BUCKET_NAME = config[\u001b[33m'\u001b[39;49;00m\u001b[33maws\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mbucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## S3 prefix\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PREFIX_NAME = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mdata_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## SOURCE data is where your actual data resides in s3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SOURCE_DATA = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33msource_data_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Read the prompt template that the user uploads\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PROMPT_TEMPLATE_S3_PREFIX = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprompt_template_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "DATASET_DIR = os.path.join(DATA_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mdataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "SCRIPTS_DIR: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mscripts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "DIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, DATASET_DIR, METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# TOKENIZER_DIR = 'llama2_tokenizer'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## this is for custom tokenizers\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENIZER_DIR_S3 = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mtokenizer_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "TOKENIZER = \u001b[33m'\u001b[39;49;00m\u001b[33mtokenizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "DEPLOYMENT_SCRIPT_S3 = config[\u001b[33m'\u001b[39;49;00m\u001b[33mdir_paths\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mscripts_prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "_ = \u001b[36mlist\u001b[39;49;00m(\u001b[36mmap\u001b[39;49;00m(\u001b[34mlambda\u001b[39;49;00m x: os.makedirs(x, exist_ok=\u001b[34mTrue\u001b[39;49;00m), DIR_LIST))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "ENDPOINT_LIST_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(MODELS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mendpoints.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "REQUEST_PAYLOAD_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(PROMPTS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mpayload.jsonl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_FPATH:\u001b[36mstr\u001b[39;49;00m = os.path.join(METRICS_DIR, \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTRUNCATE_POLICY\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m, Enum):\u001b[37m\u001b[39;49;00m\n",
      "    AT_PROMPT_TOKEN_LENGTH = \u001b[33m'\u001b[39;49;00m\u001b[33mat-prompt-token-length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# misc. metrics related\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "PLACE_HOLDER: \u001b[36mint\u001b[39;49;00m = -\u001b[34m1705338041\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# metric filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "COUNTS_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mexperiment_counts.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULTS_DESC_MD_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mresults.md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_W_PRICING_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_w_pricing.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "INSTANCE_PRICING_PER_HOUR_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33minstance_pricing_per_hour.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_w_scores.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "SUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33msummary_metrics_for_dataset_best_option_each_instance_type.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "BUSINESS_SUMMARY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mbusiness_summary.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# plot filenames\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mError rates for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "ERROR_RATES_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33merror_rates.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mTokens vs latency for different concurrency levels and instance types\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "TOKENS_VS_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mtokens_vs_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mconcurrency_vs_inference_latency.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "CONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33mConcurrency Vs latency for different instance type for selected dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "LATENCY_BUDGET: \u001b[36mint\u001b[39;49;00m = \u001b[34m20\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "OVERALL_RESULTS_MD: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m# Results for performance benchmarking\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m**Last modified (UTC): \u001b[39;49;00m\u001b[33m{dttm}\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Summary\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m{business_summary}\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m## Per instance results\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33mThe following table provides the best combinations for running inference for different sizes prompts on different instance types.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|Dataset   | Instance type   | Recommendation   |\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m|---|---|---|\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m## Dataset=`{dataset}`, instance_type=`{instance_type}`\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "RESULT_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThe best option for staying within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset is a `concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m`. A concurrency level of \u001b[39;49;00m\u001b[33m{concurrency}\u001b[39;49;00m\u001b[33m achieves an `average latency of \u001b[39;49;00m\u001b[33m{latency_mean}\u001b[39;49;00m\u001b[33m seconds`, for an `average prompt size of \u001b[39;49;00m\u001b[33m{prompt_size}\u001b[39;49;00m\u001b[33m tokens` and `completion size of \u001b[39;49;00m\u001b[33m{completion_size}\u001b[39;49;00m\u001b[33m tokens` with `\u001b[39;49;00m\u001b[33m{tpm}\u001b[39;49;00m\u001b[33m transactions/minute`.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_ROW: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\u001b[39;49;00m\u001b[33m|`\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m`|`\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m`|\u001b[39;49;00m\u001b[33m{desc}\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "RESULT_FAILURE_DESC: \u001b[36mstr\u001b[39;49;00m = \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mThis experiment did not find any combination of concurrency level and other configuration settings that could provide a response within a latency budget of `\u001b[39;49;00m\u001b[33m{latency_budget}\u001b[39;49;00m\u001b[33m seconds` on a `\u001b[39;49;00m\u001b[33m{instance_type}\u001b[39;49;00m\u001b[33m` for the `\u001b[39;49;00m\u001b[33m{dataset}\u001b[39;49;00m\u001b[33m` dataset.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Config.yml file that contains information that is used across this benchmarking environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:15,573] p58222 {635462509.py:2} INFO - {\n",
      "  \"general\": {\n",
      "    \"name\": \"mistral-7b-tgi-g5-v1\",\n",
      "    \"model_name\": \"mistral7b\"\n",
      "  },\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"arn:aws:iam::218208277580:role/service-role/AmazonSageMaker-ExecutionRole-20230807T175994\",\n",
      "    \"bucket\": \"fmbt\"\n",
      "  },\n",
      "  \"dir_paths\": {\n",
      "    \"data_prefix\": \"data\",\n",
      "    \"source_data_prefix\": \"source_data\",\n",
      "    \"prompt_template_dir\": \"prompt_template\",\n",
      "    \"prompt_template_file\": \"prompt_template.txt\",\n",
      "    \"tokenizer_prefix\": \"tokenizer\",\n",
      "    \"scripts_prefix\": \"scripts\",\n",
      "    \"prompts_prefix\": \"prompts\",\n",
      "    \"all_prompts_file\": \"all_prompts.csv\",\n",
      "    \"metrics_dir\": \"metrics_{datetime}\",\n",
      "    \"models_dir\": \"models_{datetime}\"\n",
      "  },\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1,\n",
      "      \"max_length_in_tokens\": 500,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 500,\n",
      "      \"max_length_in_tokens\": 1000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 1000,\n",
      "      \"max_length_in_tokens\": 2000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 2000,\n",
      "      \"max_length_in_tokens\": 3000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 3000,\n",
      "      \"max_length_in_tokens\": 4000,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    },\n",
      "    {\n",
      "      \"language\": \"en\",\n",
      "      \"min_length_in_tokens\": 305,\n",
      "      \"max_length_in_tokens\": 3997,\n",
      "      \"payload_file\": \"payload_{lang}_{min}-{max}.jsonl\"\n",
      "    }\n",
      "  ],\n",
      "  \"metrics\": {\n",
      "    \"dataset_of_interest\": \"en_500-1000\",\n",
      "    \"weights\": {\n",
      "      \"price_per_tx_wt\": 0.65,\n",
      "      \"latenct_wt\": 0.35\n",
      "    }\n",
      "  },\n",
      "  \"pricing\": {\n",
      "    \"ml.g5.2xlarge\": 1.515,\n",
      "    \"ml.g5.12xlarge\": 7.09,\n",
      "    \"ml.g5.24xlarge\": 10.18,\n",
      "    \"ml.g5.48xlarge\": 20.36,\n",
      "    \"ml.inf2.24xlarge\": 7.79,\n",
      "    \"ml.inf2.48xlarge\": 15.58,\n",
      "    \"ml.p4d.24xlarge\": 37.688\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"do_sample\": true,\n",
      "    \"temperature\": 0.1,\n",
      "    \"top_p\": 0.92,\n",
      "    \"top_k\": 120,\n",
      "    \"max_new_tokens\": 100,\n",
      "    \"truncate\": \"at-prompt-token-length\"\n",
      "  },\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "      \"model_id\": \"huggingface-llm-mistral-7b\",\n",
      "      \"model_version\": \"*\",\n",
      "      \"model_name\": \"mistral7b\",\n",
      "      \"ep_name\": \"lmistral7b-g5-2xlarge\",\n",
      "      \"instance_type\": \"ml.g5.2xlarge\",\n",
      "      \"image_uri\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "      \"deploy\": true,\n",
      "      \"instance_count\": 1,\n",
      "      \"deployment_script\": \"jumpstart.py\",\n",
      "      \"payload_files\": [\n",
      "        \"payload_en_1-500.jsonl\",\n",
      "        \"payload_en_500-1000.jsonl\"\n",
      "      ],\n",
      "      \"concurrency_levels\": [\n",
      "        1\n",
      "      ],\n",
      "      \"accept_eula\": true,\n",
      "      \"env\": {\n",
      "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "        \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "        \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "        \"SAGEMAKER_ENV\": \"1\",\n",
      "        \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "        \"MAX_INPUT_LENGTH\": \"8191\",\n",
      "        \"MAX_TOTAL_TOKENS\": \"8192\",\n",
      "        \"MAX_BATCH_PREFILL_TOKENS\": \"8191\",\n",
      "        \"SM_NUM_GPUS\": \"1\",\n",
      "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"results\": {\n",
      "    \"per_inference_request_file\": \"per_inference_request_results.csv\",\n",
      "    \"all_metrics_file\": \"all_metrics.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting access to the s3 bucket where endpoints.json for different models resides\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the deployed model endpoints from the endpoints.json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:15,883] p58222 {429287404.py:3} INFO - endpoint path -> s3://fmbt/data/models/2024/01/29/19/25/mistral-7b-tgi-g5-v1/lmistral7b-g5-2xlarge-1706574342_endpoints.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:16,051] p58222 {429287404.py:6} INFO - found information for 1 endpoints\n",
      "[2024-01-29 19:31:16,051] p58222 {429287404.py:7} INFO - [\n",
      "  {\n",
      "    \"experiment_name\": \"mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\",\n",
      "    \"endpoint\": {\n",
      "      \"EndpointName\": \"lmistral7b-g5-2xlarge-1706574342\",\n",
      "      \"EndpointArn\": \"arn:aws:sagemaker:us-east-1:218208277580:endpoint/lmistral7b-g5-2xlarge-1706574342\",\n",
      "      \"EndpointConfigName\": \"lmistral7b-g5-2xlarge-1706574342\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"AllTraffic\",\n",
      "          \"DeployedImages\": [\n",
      "            {\n",
      "              \"SpecifiedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "              \"ResolvedImage\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference@sha256:2739b630b95d8a95e6b4665e66d8243dd43b99c4fdb865feff13aab9c1da06eb\",\n",
      "              \"ResolutionTime\": \"2024-01-29 19:25:45.024000-05:00\"\n",
      "            }\n",
      "          ],\n",
      "          \"CurrentWeight\": 1.0,\n",
      "          \"DesiredWeight\": 1.0,\n",
      "          \"CurrentInstanceCount\": 1,\n",
      "          \"DesiredInstanceCount\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"EndpointStatus\": \"InService\",\n",
      "      \"CreationTime\": \"2024-01-29 19:25:43.928000-05:00\",\n",
      "      \"LastModifiedTime\": \"2024-01-29 19:30:30.695000-05:00\",\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"dc443b87-6b6b-4a89-8440-106e9779933e\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"dc443b87-6b6b-4a89-8440-106e9779933e\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"809\",\n",
      "          \"date\": \"Tue, 30 Jan 2024 00:30:44 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"endpoint_config\": {\n",
      "      \"EndpointConfigName\": \"lmistral7b-g5-2xlarge-1706574342\",\n",
      "      \"EndpointConfigArn\": \"arn:aws:sagemaker:us-east-1:218208277580:endpoint-config/lmistral7b-g5-2xlarge-1706574342\",\n",
      "      \"ProductionVariants\": [\n",
      "        {\n",
      "          \"VariantName\": \"AllTraffic\",\n",
      "          \"ModelName\": \"hf-llm-mistral-7b-2024-01-30-00-25-42-426\",\n",
      "          \"InitialInstanceCount\": 1,\n",
      "          \"InstanceType\": \"ml.g5.2xlarge\",\n",
      "          \"InitialVariantWeight\": 1.0,\n",
      "          \"ModelDataDownloadTimeoutInSeconds\": 1200,\n",
      "          \"ContainerStartupHealthCheckTimeoutInSeconds\": 1200\n",
      "        }\n",
      "      ],\n",
      "      \"CreationTime\": \"2024-01-29 19:25:43.523000-05:00\",\n",
      "      \"EnableNetworkIsolation\": false,\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"bfeef867-dc83-4ba3-bf59-83a65be03233\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"bfeef867-dc83-4ba3-bf59-83a65be03233\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"515\",\n",
      "          \"date\": \"Tue, 30 Jan 2024 00:30:44 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_config\": {\n",
      "      \"ModelName\": \"hf-llm-mistral-7b-2024-01-30-00-25-42-426\",\n",
      "      \"PrimaryContainer\": {\n",
      "        \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\",\n",
      "        \"Mode\": \"SingleModel\",\n",
      "        \"ModelDataSource\": {\n",
      "          \"S3DataSource\": {\n",
      "            \"S3Uri\": \"s3://jumpstart-cache-prod-us-east-1/huggingface-llm/huggingface-llm-mistral-7b/artifacts/inference-prepack/v1.0.0/\",\n",
      "            \"S3DataType\": \"S3Prefix\",\n",
      "            \"CompressionType\": \"None\",\n",
      "            \"ModelAccessConfig\": {\n",
      "              \"AcceptEula\": true\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"Environment\": {\n",
      "          \"ENDPOINT_SERVER_TIMEOUT\": \"3600\",\n",
      "          \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
      "          \"MAX_BATCH_PREFILL_TOKENS\": \"8191\",\n",
      "          \"MAX_INPUT_LENGTH\": \"8191\",\n",
      "          \"MAX_TOTAL_TOKENS\": \"8192\",\n",
      "          \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "          \"SAGEMAKER_ENV\": \"1\",\n",
      "          \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "          \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "          \"SM_NUM_GPUS\": \"1\"\n",
      "        }\n",
      "      },\n",
      "      \"ExecutionRoleArn\": \"arn:aws:iam::218208277580:role/service-role/AmazonSageMaker-ExecutionRole-20230807T175994\",\n",
      "      \"CreationTime\": \"2024-01-29 19:25:43.043000-05:00\",\n",
      "      \"ModelArn\": \"arn:aws:sagemaker:us-east-1:218208277580:model/hf-llm-mistral-7b-2024-01-30-00-25-42-426\",\n",
      "      \"EnableNetworkIsolation\": true,\n",
      "      \"DeploymentRecommendation\": {\n",
      "        \"RecommendationStatus\": \"COMPLETED\",\n",
      "        \"RealTimeInferenceRecommendations\": []\n",
      "      },\n",
      "      \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"eee62628-f5ad-4e1b-98ed-50771d5a1418\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "          \"x-amzn-requestid\": \"eee62628-f5ad-4e1b-98ed-50771d5a1418\",\n",
      "          \"content-type\": \"application/x-amz-json-1.1\",\n",
      "          \"content-length\": \"1168\",\n",
      "          \"date\": \"Tue, 30 Jan 2024 00:30:44 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "with open('endpoint_path.txt', 'r') as file:\n",
    "    ENDPOINT_S3_PATH = file.read().strip()\n",
    "    logger.info(f\"endpoint path -> {ENDPOINT_S3_PATH}\")\n",
    "\n",
    "endpoint_info_list = json.loads(get_s3_object(ENDPOINT_S3_PATH))\n",
    "logger.info(f\"found information for {len(endpoint_info_list)} endpoints\")\n",
    "logger.info(json.dumps(endpoint_info_list, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:16,074] p58222 {1455142584.py:3} INFO - there are 1 deployed endpoint(s), endpoint_name_list->['lmistral7b-g5-2xlarge-1706574342']\n"
     ]
    }
   ],
   "source": [
    "# List down the endpoint names that have been deployed\n",
    "endpoint_name_list = [e['endpoint']['EndpointName'] for e in endpoint_info_list]\n",
    "logger.info(f\"there are {len(endpoint_name_list)} deployed endpoint(s), endpoint_name_list->{endpoint_name_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating predictor objects from the deployed endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:16,343] p58222 {3970465137.py:15} INFO - [<sagemaker.base_predictor.Predictor object at 0x28cd7aed0>]\n"
     ]
    }
   ],
   "source": [
    "# create predictor objects\n",
    "\n",
    "## create a sagemaker predictor for these endpoints\n",
    "def create_predictor(endpoint_name: str) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "    # Create a SageMaker Predictor object\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "        serializer=JSONSerializer()\n",
    "    )\n",
    "    return predictor\n",
    "\n",
    "## Display the list of predictor objects that have been deployed ready for inferencing from\n",
    "predictor_list: List = [create_predictor(ep) for ep in endpoint_name_list]\n",
    "logger.info(predictor_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions to define and calculate metrics during the time of invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_sum(l: List) -> Union[int, float]:\n",
    "    return sum(filter(None, l))\n",
    "\n",
    "def safe_div(n: Union[int, float], d: Union[int, float]) -> Optional[Union[int, float]]:\n",
    "    return n/d if d else None\n",
    "\n",
    "## Represents the function to calculate all of the metrics at the time of inference\n",
    "def calculate_metrics(responses, chunk, elapsed_async, experiment_name, concurrency, payload_file) -> Dict:\n",
    "    \n",
    "    ## calculate errors based on the completion status of the inference prompt\n",
    "    errors = [r for r in responses if r['completion'] is None]\n",
    "    \n",
    "    ## Calculate the difference as the successes \n",
    "    successes = len(chunk) - len(errors)\n",
    "    \n",
    "    ## Count all of the prompts token count during inference\n",
    "    all_prompts_token_count = safe_sum([r['prompt_tokens'] for r in responses])\n",
    "    prompt_token_throughput = round(all_prompts_token_count / elapsed_async, 2)\n",
    "    prompt_token_count_mean = safe_div(all_prompts_token_count, successes)\n",
    "    all_completions_token_count = safe_sum([r['completion_tokens'] for r in responses])\n",
    "    completion_token_throughput = round(all_completions_token_count / elapsed_async, 2)\n",
    "    completion_token_count_mean = safe_div(all_completions_token_count, successes)\n",
    "    transactions_per_second = round(successes / elapsed_async, 2)\n",
    "    transactions_per_minute = int(transactions_per_second * 60)\n",
    "    \n",
    "    ## calculate the latency mean utilizing the safe_sum function defined above\n",
    "    latency_mean = safe_div(safe_sum([r['latency'] for r in responses]), successes)\n",
    "    \n",
    "    ## Function returns all these values at the time of the invocations\n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'concurrency': concurrency,\n",
    "        'payload_file': payload_file,\n",
    "        'errors': errors,\n",
    "        'successes': successes,\n",
    "        'error_rate': len(errors)/len(chunk),\n",
    "        'all_prompts_token_count': all_prompts_token_count,\n",
    "        'prompt_token_count_mean': prompt_token_count_mean,\n",
    "        'prompt_token_throughput': prompt_token_throughput,\n",
    "        'all_completions_token_count': all_completions_token_count,\n",
    "        'completion_token_count_mean': completion_token_count_mean,\n",
    "        'completion_token_throughput': completion_token_throughput,\n",
    "        'transactions': len(chunk),\n",
    "        'transactions_per_second': transactions_per_second,\n",
    "        'transactions_per_minute': transactions_per_minute,\n",
    "        'latency_mean': latency_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a blocker function and a series of asynchronous concurrent model prompt invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_metrics(endpoint_name=None,\n",
    "                    prompt=None,\n",
    "                    inference_params=None,\n",
    "                    completion=None,\n",
    "                    prompt_tokens=None,\n",
    "                    completion_tokens=None,\n",
    "                    latency=None) -> Dict:\n",
    "    return dict(endpoint_name=endpoint_name,                \n",
    "                prompt=prompt,\n",
    "                **inference_params,\n",
    "                completion=completion,\n",
    "                prompt_tokens=prompt_tokens,\n",
    "                completion_tokens=completion_tokens,\n",
    "                latency=latency)\n",
    "\n",
    "def get_inference(predictor, payload) -> Dict:\n",
    "    \n",
    "    smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "    latency = 0\n",
    "\n",
    "    try:\n",
    "        prompt_tokens = count_tokens(payload['inputs'])\n",
    "        logger.info(f\"get_inference, endpoint={predictor.endpoint_name}, prompt_tokens={prompt_tokens}\")\n",
    "\n",
    "        # get inference\n",
    "        st = time.perf_counter()        \n",
    "        response = predictor.predict(payload)        \n",
    "        latency = time.perf_counter() - st\n",
    "\n",
    "        if isinstance(response, bytes):\n",
    "            response = response.decode('utf-8')\n",
    "        response_json = json.loads(response)\n",
    "        if isinstance(response_json, list):\n",
    "            response_json = response_json[0]\n",
    "\n",
    "        completion = response_json.get(\"generated_text\", \"\")\n",
    "        completion_tokens = count_tokens(completion)\n",
    "\n",
    "        # Set metrics and logging for both cases\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               completion,\n",
    "                               prompt_tokens,\n",
    "                               completion_tokens,\n",
    "                               latency)\n",
    "        # logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, response={json.dumps(response, indent=2)}, latency={latency:.2f}\")\n",
    "        logger.info(f\"get_inference, done, endpoint={predictor.endpoint_name}, completion_tokens={completion_tokens}, latency={latency:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error occurred with {predictor.endpoint_name}, exception={str(e)}\")\n",
    "        response = set_metrics(predictor.endpoint_name,\n",
    "                               payload['inputs'],\n",
    "                               payload['parameters'],\n",
    "                               None,\n",
    "                               prompt_tokens,\n",
    "                               None,\n",
    "                               None)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a series of asynchronous functions to invoke and run inferences concurrently and asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents a function to start invoking models in separate thread asynchronously for the blocker function\n",
    "async def async_get_inference(predictor, payload: Dict) -> Dict:\n",
    "    return await asyncio.to_thread(get_inference, predictor, payload)\n",
    "\n",
    "## Gathers all of the tasks and sets of the concurrent calling of the asychronous invocations\n",
    "async def async_get_all_inferences(predictor, payload_list: List) -> List:\n",
    "    return await asyncio.gather(*[async_get_inference(predictor, payload) for payload in payload_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This function runs the asychronous function series above together for different experiments and concurrency levels.\n",
    "async def run_inferences(predictor: sagemaker.base_predictor.Predictor, chunk: List, experiment: Dict, concurrency: int, payload_file: str) -> Tuple[List, Dict]:\n",
    "    logger.info(f\"Processing chunk with concurrency={concurrency}\")\n",
    "    s = time.perf_counter()\n",
    "    responses = await async_get_all_inferences(predictor, chunk)\n",
    "    elapsed_async = time.perf_counter() - s\n",
    "\n",
    "    # Add more metadata about this experiment\n",
    "    for r in responses:\n",
    "        r['experiment_name'] = experiment['name']\n",
    "        r['concurrency'] = concurrency\n",
    "\n",
    "    metrics = calculate_metrics(responses, chunk, elapsed_async, experiment['name'], concurrency, payload_file)\n",
    "    return responses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to create the predictors from the experiment we are iterating over\n",
    "def create_predictor_for_experiment(experiment: str, config: Dict, endpoint_info_list: List) -> Optional[sagemaker.base_predictor.Predictor]:\n",
    "\n",
    "    ## Here, we set the index and then iterate through the experiments\n",
    "    e_idx = config['experiments'].index(experiment) + 1\n",
    "\n",
    "    ## Iterate through the endpoint information to fetch the endpoint name\n",
    "    ep_info = [e for e in endpoint_info_list if e['experiment_name'] == experiment['name']]\n",
    "    if not ep_info:\n",
    "        logger.error(f\"endpoint for experiment={experiment['name']} not found, skipping\")\n",
    "        return None\n",
    "    ep_name = ep_info[0]['endpoint']['EndpointName']\n",
    "    logger.info(f\"experiment={e_idx}, name={experiment['name']}, ep_name={ep_name}\")\n",
    "\n",
    "    # create a predictor from each endpoint in experiments\n",
    "    return create_predictor(ep_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here, we will process combinations of concurrency levels, the payload files and then loop through the \n",
    "## different combinations to make payloads splitted in terms of the concurrency metric and how we can run \n",
    "## it and make inference\n",
    "\n",
    "def create_payload_dict(jline: str, experiment: Dict) -> Dict:\n",
    "    payload: Dict = json.loads(jline)\n",
    "    if experiment.get('remove_truncate', False) is True:\n",
    "        if payload['parameters'].get('truncate'):\n",
    "            del payload['parameters']['truncate']\n",
    "    return payload\n",
    "    \n",
    "    \n",
    "def create_combinations(experiment: Dict) -> List[Tuple]:\n",
    "    combinations_data = []\n",
    "\n",
    "    # Repeat for each concurrency level\n",
    "    combinations = list(itertools.product(experiment['concurrency_levels'], experiment['payload_files']))\n",
    "    logger.info(f\"there are {len(combinations)} combinations of {combinations} to run\")\n",
    "\n",
    "    for concurrency, payload_file in combinations:\n",
    "        # Construct the full S3 file path\n",
    "        s3_file_path = os.path.join(PROMPTS_DIR, payload_file)\n",
    "        logger.info(f\"s3 path where the payload files are being read from -> {s3_file_path}\")\n",
    "\n",
    "        # Read the payload file from S3\n",
    "        try:\n",
    "            response = s3_client.get_object(Bucket=config['aws']['bucket'], Key=s3_file_path)\n",
    "            payload_file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "            # Create a payload list by processing each line\n",
    "            payload_list = [create_payload_dict(jline, experiment) for jline in payload_file_content.splitlines()]\n",
    "            logger.info(f\"read from s3://{config['aws']['bucket']}/{s3_file_path}, contains {len(payload_list)} lines\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file from S3: {e}\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"creating combinations for concurrency={concurrency}, payload_file={payload_file}, payload_list length={len(payload_list)}\")\n",
    "        \n",
    "        n = concurrency\n",
    "        \n",
    "        if len(payload_list) < n:\n",
    "            elements_to_add = n - len(payload_list)\n",
    "            element_to_replicate = payload_list[0]\n",
    "            # payload_list = payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            payload_list.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "        # Split the original list into sublists which contain the number of requests we want to send concurrently        \n",
    "        payload_list_splitted = [payload_list[i * n:(i + 1) * n] for i in range((len(payload_list) + n - 1) // n )]  \n",
    "        \n",
    "        for p in payload_list_splitted:\n",
    "            if len(p) < n:\n",
    "                elements_to_add = n - len(p)\n",
    "                element_to_replicate = p[0]\n",
    "                # p = p.extend([element_to_replicate]*elements_to_add)\n",
    "                p.extend([element_to_replicate]*elements_to_add)\n",
    "            \n",
    "\n",
    "        # Only keep lists that have at least concurrency number of elements\n",
    "        len_before = len(payload_list_splitted)\n",
    "        payload_list_splitted = [p for p in payload_list_splitted if len(p) == concurrency]\n",
    "        logger.info(f\"after only retaining chunks of length {concurrency}, we have {len(payload_list_splitted)} chunks, previously we had {len_before} chunks\")\n",
    "        combinations_data.append((concurrency, payload_file, payload_list_splitted))\n",
    "    logger.info(f\"there are {len(combinations)} for {experiment}\")\n",
    "    return combinations_data\n",
    "\n",
    "# process_combinations(experiment, predictor, PROMPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:17,857] p58222 {663335596.py:13} INFO - experiment=1, name=mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0, ep_name=lmistral7b-g5-2xlarge-1706574342\n",
      "[2024-01-29 19:31:17,872] p58222 {2135215762.py:18} INFO - there are 2 combinations of [(1, 'payload_en_1-500.jsonl'), (1, 'payload_en_500-1000.jsonl')] to run\n",
      "[2024-01-29 19:31:17,872] p58222 {2135215762.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_1-500.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:18,106] p58222 {2135215762.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_1-500.jsonl, contains 1 lines\n",
      "[2024-01-29 19:31:18,107] p58222 {2135215762.py:38} INFO - creating combinations for concurrency=1, payload_file=payload_en_1-500.jsonl, payload_list length=1\n",
      "[2024-01-29 19:31:18,107] p58222 {2135215762.py:62} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-29 19:31:18,107] p58222 {2135215762.py:23} INFO - s3 path where the payload files are being read from -> data/prompts/payload_en_500-1000.jsonl\n",
      "[2024-01-29 19:31:18,155] p58222 {2135215762.py:32} INFO - read from s3://fmbt/data/prompts/payload_en_500-1000.jsonl, contains 1 lines\n",
      "[2024-01-29 19:31:18,156] p58222 {2135215762.py:38} INFO - creating combinations for concurrency=1, payload_file=payload_en_500-1000.jsonl, payload_list length=1\n",
      "[2024-01-29 19:31:18,156] p58222 {2135215762.py:62} INFO - after only retaining chunks of length 1, we have 1 chunks, previously we had 1 chunks\n",
      "[2024-01-29 19:31:18,157] p58222 {2135215762.py:64} INFO - there are 2 for {'name': 'mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0', 'model_id': 'huggingface-llm-mistral-7b', 'model_version': '*', 'model_name': 'mistral7b', 'ep_name': 'lmistral7b-g5-2xlarge', 'instance_type': 'ml.g5.2xlarge', 'image_uri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04', 'deploy': True, 'instance_count': 1, 'deployment_script': 'jumpstart.py', 'payload_files': ['payload_en_1-500.jsonl', 'payload_en_500-1000.jsonl'], 'concurrency_levels': [1], 'accept_eula': True, 'env': {'SAGEMAKER_PROGRAM': 'inference.py', 'ENDPOINT_SERVER_TIMEOUT': '3600', 'MODEL_CACHE_ROOT': '/opt/ml/model', 'SAGEMAKER_ENV': '1', 'HF_MODEL_ID': '/opt/ml/model', 'MAX_INPUT_LENGTH': '8191', 'MAX_TOTAL_TOKENS': '8192', 'MAX_BATCH_PREFILL_TOKENS': '8191', 'SM_NUM_GPUS': '1', 'SAGEMAKER_MODEL_SERVER_WORKERS': '1'}}\n",
      "[2024-01-29 19:31:18,158] p58222 {2183876069.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-29 19:31:18,160] p58222 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-29 19:31:18,166] p58222 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706574342, prompt_tokens=304\n",
      "[2024-01-29 19:31:21,795] p58222 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706574342, completion_tokens=101, latency=3.63\n",
      "[2024-01-29 19:31:22,073] p58222 {2183876069.py:26} INFO - e_idx=1/1, chunk_index=1/1\n",
      "[2024-01-29 19:31:22,073] p58222 {1750118496.py:3} INFO - Processing chunk with concurrency=1\n",
      "[2024-01-29 19:31:22,078] p58222 {701838357.py:23} INFO - get_inference, endpoint=lmistral7b-g5-2xlarge-1706574342, prompt_tokens=980\n",
      "[2024-01-29 19:31:25,942] p58222 {701838357.py:48} INFO - get_inference, done, endpoint=lmistral7b-g5-2xlarge-1706574342, completion_tokens=100, latency=3.86\n",
      "[2024-01-29 19:31:26,270] p58222 {2183876069.py:49} INFO - experiment=1/1, name=mistral-7b-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0, done\n"
     ]
    }
   ],
   "source": [
    "# for each experiment\n",
    "#   - for each endpoint and concurrency in an experiment\n",
    "\n",
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "_ = list(map(clear_dir, [METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]))\n",
    "\n",
    "num_experiments: int = len(config['experiments'])\n",
    "for e_idx, experiment in enumerate(config['experiments']):\n",
    "    e_idx += 1  # Increment experiment index\n",
    "    # Call do_experiment function to create the predictor object\n",
    " \n",
    "    predictor = create_predictor_for_experiment(experiment, config, endpoint_info_list)\n",
    "    if predictor is None:\n",
    "        logger.error(f\"predictor could not be created for experiment={experiment}, moving to next...\")\n",
    "        continue\n",
    "\n",
    "    # Process combinations of concurrency levels and payload files\n",
    "    combination_data = create_combinations(experiment)\n",
    "\n",
    "    for concurrency, payload_file, split_payload in combination_data:\n",
    "        for chunk_index, chunk in enumerate(split_payload):\n",
    "            logger.info(f\"e_idx={e_idx}/{num_experiments}, chunk_index={chunk_index+1}/{len(split_payload)}\")\n",
    "\n",
    "            # Process each chunk and calculate metrics\n",
    "            responses, metrics = await run_inferences(predictor, chunk, experiment, concurrency, payload_file)\n",
    "            if metrics:\n",
    "                # Convert metrics to JSON string\n",
    "                metrics_json = json.dumps(metrics, indent=2)\n",
    "                # Define S3 file path\n",
    "                metrics_file_name = f\"{time.time()}.json\"\n",
    "                metrics_s3_path = os.path.join(METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "                # Write to S3\n",
    "                write_to_s3(metrics_json, config['aws']['bucket'], \"\", METRICS_PER_CHUNK_DIR, metrics_file_name)\n",
    "\n",
    "            if responses:\n",
    "                for r in responses:\n",
    "                    # Convert response to JSON string\n",
    "                    response_json = json.dumps(r, indent=2)\n",
    "                    # Define S3 file path\n",
    "                    response_file_name = f\"{time.time()}.json\"\n",
    "                    response_s3_path = os.path.join(METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "                    # Write to S3\n",
    "                    write_to_s3(response_json, config['aws']['bucket'], \"\", METRICS_PER_INFERENCE_DIR, response_file_name)\n",
    "\n",
    "    logger.info(f\"experiment={e_idx}/{num_experiments}, name={experiment['name']}, done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:26,486] p58222 {2402469004.py:19} INFO - created dataframe of shape (2, 14) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706574342</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>3.628395</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706574342</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>3.861141</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706574342   \n",
       "1  lmistral7b-g5-2xlarge-1706574342   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100       304   \n",
       "1   0.92    120             100       980   \n",
       "\n",
       "                                          completion  prompt_tokens  \\\n",
       "0  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "1   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "\n",
       "   completion_tokens   latency  \\\n",
       "0                101  3.628395   \n",
       "1                100  3.861141   \n",
       "\n",
       "                                     experiment_name  concurrency  \n",
       "0  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "1  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to list files in S3 bucket with a specific prefix\n",
    "def list_s3_files(bucket, prefix, suffix='.json'):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    return [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith(suffix)]\n",
    "\n",
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(config['aws']['bucket'], METRICS_PER_INFERENCE_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = []\n",
    "for file_key in s3_files:\n",
    "    response = s3_client.get_object(Bucket=config['aws']['bucket'], Key=file_key)\n",
    "    file_content = response['Body'].read().decode('utf-8')\n",
    "    json_obj = json.loads(file_content)\n",
    "    json_list.append(json_obj)\n",
    "\n",
    "# Create DataFrame\n",
    "df_responses = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_responses.shape} from all responses\")\n",
    "df_responses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>latency</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706574342</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>3.628395</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706574342</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>100</td>\n",
       "      <td>3.861141</td>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706574342   \n",
       "1  lmistral7b-g5-2xlarge-1706574342   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100       304   \n",
       "1   0.92    120             100       980   \n",
       "\n",
       "                                          completion  prompt_tokens  \\\n",
       "0  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304   \n",
       "1   Both WAGS Atlanta and WAGS are reality televi...            980   \n",
       "\n",
       "   completion_tokens   latency  \\\n",
       "0                101  3.628395   \n",
       "1                100  3.861141   \n",
       "\n",
       "                                     experiment_name  concurrency  \n",
       "0  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  \n",
       "1  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses[df_responses.endpoint_name.str.contains(\"g5\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:26,567] p58222 {4027816906.py:3} INFO - files recieved from s3 for per inference request --> {'ResponseMetadata': {'RequestId': 'M5PNP3G3ZK0S7GM7', 'HostId': 'Ys6tyM1KZ3IZnUTeof0mCvSM9/95SeyI3kt9+bSwAoG7mUa7hOPaa7Nq4LhoK81O5TQwYc42vOc=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'Ys6tyM1KZ3IZnUTeof0mCvSM9/95SeyI3kt9+bSwAoG7mUa7hOPaa7Nq4LhoK81O5TQwYc42vOc=', 'x-amz-request-id': 'M5PNP3G3ZK0S7GM7', 'date': 'Tue, 30 Jan 2024 00:31:27 GMT', 'x-amz-bucket-region': 'us-east-1', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'IsTruncated': False, 'Contents': [{'Key': 'data/metrics/2024/01/29/19/31/mistral-7b-tgi-g5-v1/per_chunk/1706574681.79703.json', 'LastModified': datetime.datetime(2024, 1, 30, 0, 31, 22, tzinfo=tzutc()), 'ETag': '\"24300efb5fe77ae3ac6ebdcaf6980fb2\"', 'Size': 559, 'StorageClass': 'STANDARD'}, {'Key': 'data/metrics/2024/01/29/19/31/mistral-7b-tgi-g5-v1/per_chunk/1706574685.9443488.json', 'LastModified': datetime.datetime(2024, 1, 30, 0, 31, 27, tzinfo=tzutc()), 'ETag': '\"9432645ef4aefd310f4e8130c5c5f248\"', 'Size': 561, 'StorageClass': 'STANDARD'}], 'Name': 'fmbt', 'Prefix': 'data/metrics/2024/01/29/19/31/mistral-7b-tgi-g5-v1/per_chunk', 'MaxKeys': 1000, 'EncodingType': 'url', 'KeyCount': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:26,635] p58222 {4027816906.py:19} INFO - created dataframe of shape (2, 16) from all responses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>concurrency</th>\n",
       "      <th>payload_file</th>\n",
       "      <th>errors</th>\n",
       "      <th>successes</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>all_prompts_token_count</th>\n",
       "      <th>prompt_token_count_mean</th>\n",
       "      <th>prompt_token_throughput</th>\n",
       "      <th>all_completions_token_count</th>\n",
       "      <th>completion_token_count_mean</th>\n",
       "      <th>completion_token_throughput</th>\n",
       "      <th>transactions</th>\n",
       "      <th>transactions_per_second</th>\n",
       "      <th>transactions_per_minute</th>\n",
       "      <th>latency_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_1-500.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304</td>\n",
       "      <td>304.0</td>\n",
       "      <td>83.66</td>\n",
       "      <td>101</td>\n",
       "      <td>101.0</td>\n",
       "      <td>27.79</td>\n",
       "      <td>1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>16</td>\n",
       "      <td>3.628395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral-7b-g5-huggingface-pytorch-tgi-inferenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>payload_en_500-1000.jsonl</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>980</td>\n",
       "      <td>980.0</td>\n",
       "      <td>253.20</td>\n",
       "      <td>100</td>\n",
       "      <td>100.0</td>\n",
       "      <td>25.84</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>15</td>\n",
       "      <td>3.861141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     experiment_name  concurrency  \\\n",
       "0  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1   \n",
       "1  mistral-7b-g5-huggingface-pytorch-tgi-inferenc...            1   \n",
       "\n",
       "                payload_file errors  successes  error_rate  \\\n",
       "0     payload_en_1-500.jsonl     []          1         0.0   \n",
       "1  payload_en_500-1000.jsonl     []          1         0.0   \n",
       "\n",
       "   all_prompts_token_count  prompt_token_count_mean  prompt_token_throughput  \\\n",
       "0                      304                    304.0                    83.66   \n",
       "1                      980                    980.0                   253.20   \n",
       "\n",
       "   all_completions_token_count  completion_token_count_mean  \\\n",
       "0                          101                        101.0   \n",
       "1                          100                        100.0   \n",
       "\n",
       "   completion_token_throughput  transactions  transactions_per_second  \\\n",
       "0                        27.79             1                     0.28   \n",
       "1                        25.84             1                     0.26   \n",
       "\n",
       "   transactions_per_minute  latency_mean  \n",
       "0                       16      3.628395  \n",
       "1                       15      3.861141  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_s3_files(bucket, prefix, suffix='.json'):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    logger.info(f\"files recieved from s3 for per inference request --> {response}\")\n",
    "    return [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith(suffix)]\n",
    "\n",
    "# List .json files in the specified S3 directory\n",
    "s3_files = list_s3_files(config['aws']['bucket'], METRICS_PER_CHUNK_DIR)\n",
    "\n",
    "# Read and parse each JSON file from S3\n",
    "json_list = []\n",
    "for file_key in s3_files:\n",
    "    response = s3_client.get_object(Bucket=config['aws']['bucket'], Key=file_key)\n",
    "    file_content = response['Body'].read().decode('utf-8')\n",
    "    json_obj = json.loads(file_content)\n",
    "    json_list.append(json_obj)\n",
    "\n",
    "# Create DataFrame\n",
    "df_metrics = pd.DataFrame(json_list)\n",
    "logger.info(f\"created dataframe of shape {df_metrics.shape} from all responses\")\n",
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_config.PrimaryContainer.Environment.ENDPOINT_SERVER_TIMEOUT', 'model_config.PrimaryContainer.Environment.HF_MODEL_ID', 'model_config.PrimaryContainer.Environment.MAX_BATCH_PREFILL_TOKENS', 'model_config.PrimaryContainer.Environment.MAX_INPUT_LENGTH', 'model_config.PrimaryContainer.Environment.MAX_TOTAL_TOKENS', 'model_config.PrimaryContainer.Environment.MODEL_CACHE_ROOT', 'model_config.PrimaryContainer.Environment.SAGEMAKER_ENV', 'model_config.PrimaryContainer.Environment.SAGEMAKER_MODEL_SERVER_WORKERS', 'model_config.PrimaryContainer.Environment.SAGEMAKER_PROGRAM', 'model_config.PrimaryContainer.Environment.SM_NUM_GPUS']\n",
      "Columns in df_responses: Index(['endpoint_name', 'prompt', 'do_sample', 'temperature', 'top_p', 'top_k',\n",
      "       'max_new_tokens', 'truncate', 'completion', 'prompt_tokens',\n",
      "       'completion_tokens', 'latency', 'experiment_name', 'concurrency'],\n",
      "      dtype='object')\n",
      "Columns in df_endpoints: Index(['experiment_name', 'instance_type', 'EndpointName', 'ModelName',\n",
      "       'Image', 'S3Uri', 'ENDPOINT_SERVER_TIMEOUT', 'HF_MODEL_ID',\n",
      "       'MAX_BATCH_PREFILL_TOKENS', 'MAX_INPUT_LENGTH', 'MAX_TOTAL_TOKENS',\n",
      "       'MODEL_CACHE_ROOT', 'SAGEMAKER_ENV', 'SAGEMAKER_MODEL_SERVER_WORKERS',\n",
      "       'SAGEMAKER_PROGRAM', 'SM_NUM_GPUS'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>ENDPOINT_SERVER_TIMEOUT</th>\n",
       "      <th>HF_MODEL_ID</th>\n",
       "      <th>MAX_BATCH_PREFILL_TOKENS</th>\n",
       "      <th>MAX_INPUT_LENGTH</th>\n",
       "      <th>MAX_TOTAL_TOKENS</th>\n",
       "      <th>MODEL_CACHE_ROOT</th>\n",
       "      <th>SAGEMAKER_ENV</th>\n",
       "      <th>SAGEMAKER_MODEL_SERVER_WORKERS</th>\n",
       "      <th>SAGEMAKER_PROGRAM</th>\n",
       "      <th>SM_NUM_GPUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706574342</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706574342</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706574342   \n",
       "1  lmistral7b-g5-2xlarge-1706574342   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100       304   \n",
       "1   0.92    120             100       980   \n",
       "\n",
       "                                          completion  prompt_tokens  ...  \\\n",
       "0  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304  ...   \n",
       "1   Both WAGS Atlanta and WAGS are reality televi...            980  ...   \n",
       "\n",
       "   ENDPOINT_SERVER_TIMEOUT    HF_MODEL_ID MAX_BATCH_PREFILL_TOKENS  \\\n",
       "0                     3600  /opt/ml/model                     8191   \n",
       "1                     3600  /opt/ml/model                     8191   \n",
       "\n",
       "   MAX_INPUT_LENGTH MAX_TOTAL_TOKENS MODEL_CACHE_ROOT SAGEMAKER_ENV  \\\n",
       "0              8191             8192    /opt/ml/model             1   \n",
       "1              8191             8192    /opt/ml/model             1   \n",
       "\n",
       "  SAGEMAKER_MODEL_SERVER_WORKERS SAGEMAKER_PROGRAM SM_NUM_GPUS  \n",
       "0                              1      inference.py           1  \n",
       "1                              1      inference.py           1  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_endpoints = pd.json_normalize(endpoint_info_list)\n",
    "df_endpoints['instance_type'] = df_endpoints['endpoint_config.ProductionVariants'].map(lambda x: x[0]['InstanceType'])\n",
    "df_endpoints\n",
    "cols_for_env = [c for c in df_endpoints.columns if 'Environment' in c]\n",
    "print(cols_for_env)\n",
    "cols_of_interest = ['experiment_name', \n",
    "                    'instance_type',\n",
    "                    'endpoint.EndpointName',\n",
    "                    'model_config.ModelName',\n",
    "                    'model_config.PrimaryContainer.Image',   \n",
    "                    'model_config.PrimaryContainer.ModelDataSource.S3DataSource.S3Uri']\n",
    "cols_of_interest.extend(cols_for_env)\n",
    "\n",
    "df_endpoints = df_endpoints[cols_of_interest]\n",
    "df_endpoints = df_endpoints[cols_of_interest]\n",
    "cols_of_interest_renamed = [c.split('.')[-1] for c in cols_of_interest]\n",
    "df_endpoints.columns = cols_of_interest_renamed\n",
    "\n",
    "# Check if 'experiment_name' column exists in both DataFrames\n",
    "print(\"Columns in df_responses:\", df_responses.columns)\n",
    "print(\"Columns in df_endpoints:\", df_endpoints.columns)\n",
    "\n",
    "# Merge operation\n",
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "\n",
    "# Inspect the result\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint_name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>do_sample</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "      <th>max_new_tokens</th>\n",
       "      <th>truncate</th>\n",
       "      <th>completion</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>ENDPOINT_SERVER_TIMEOUT</th>\n",
       "      <th>HF_MODEL_ID</th>\n",
       "      <th>MAX_BATCH_PREFILL_TOKENS</th>\n",
       "      <th>MAX_INPUT_LENGTH</th>\n",
       "      <th>MAX_TOTAL_TOKENS</th>\n",
       "      <th>MODEL_CACHE_ROOT</th>\n",
       "      <th>SAGEMAKER_ENV</th>\n",
       "      <th>SAGEMAKER_MODEL_SERVER_WORKERS</th>\n",
       "      <th>SAGEMAKER_PROGRAM</th>\n",
       "      <th>SM_NUM_GPUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706574342</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>304</td>\n",
       "      <td>\\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...</td>\n",
       "      <td>304</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lmistral7b-g5-2xlarge-1706574342</td>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an assistant for qu...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>980</td>\n",
       "      <td>Both WAGS Atlanta and WAGS are reality televi...</td>\n",
       "      <td>980</td>\n",
       "      <td>...</td>\n",
       "      <td>3600</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>8191</td>\n",
       "      <td>8191</td>\n",
       "      <td>8192</td>\n",
       "      <td>/opt/ml/model</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>inference.py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      endpoint_name  \\\n",
       "0  lmistral7b-g5-2xlarge-1706574342   \n",
       "1  lmistral7b-g5-2xlarge-1706574342   \n",
       "\n",
       "                                              prompt  do_sample  temperature  \\\n",
       "0  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "1  <s>[INST] <<SYS>>\\nYou are an assistant for qu...       True          0.1   \n",
       "\n",
       "   top_p  top_k  max_new_tokens  truncate  \\\n",
       "0   0.92    120             100       304   \n",
       "1   0.92    120             100       980   \n",
       "\n",
       "                                          completion  prompt_tokens  ...  \\\n",
       "0  \\n\\n```\\nPassage 1:\\nStauntonia\\nStauntonia is...            304  ...   \n",
       "1   Both WAGS Atlanta and WAGS are reality televi...            980  ...   \n",
       "\n",
       "   ENDPOINT_SERVER_TIMEOUT    HF_MODEL_ID MAX_BATCH_PREFILL_TOKENS  \\\n",
       "0                     3600  /opt/ml/model                     8191   \n",
       "1                     3600  /opt/ml/model                     8191   \n",
       "\n",
       "   MAX_INPUT_LENGTH MAX_TOTAL_TOKENS MODEL_CACHE_ROOT SAGEMAKER_ENV  \\\n",
       "0              8191             8192    /opt/ml/model             1   \n",
       "1              8191             8192    /opt/ml/model             1   \n",
       "\n",
       "  SAGEMAKER_MODEL_SERVER_WORKERS SAGEMAKER_PROGRAM SM_NUM_GPUS  \n",
       "0                              1      inference.py           1  \n",
       "1                              1      inference.py           1  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.merge(left=df_responses, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:26,782] p58222 {1276732506.py:7} INFO - results s3 path for per inference csv --> data/metrics/2024/01/29/19/31/mistral-7b-tgi-g5-v1/per_inference_request_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:26,961] p58222 {1276732506.py:9} INFO - saved results dataframe of shape=(2, 29) in s3://fmbt/data/metrics/2024/01/29/19/31/mistral-7b-tgi-g5-v1/per_inference_request_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert df_results to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_results.to_csv(csv_buffer, index=False)\n",
    "csv_data_results = csv_buffer.getvalue()\n",
    "results_file_name = config['results']['per_inference_request_file'].format(datetime=date_time)\n",
    "results_s3_path = os.path.join(METRICS_DIR, results_file_name)\n",
    "logger.info(f\"results s3 path for per inference csv --> {results_s3_path}\")\n",
    "write_to_s3(csv_data_results, config['aws']['bucket'], \"\", METRICS_DIR, results_file_name)\n",
    "logger.info(f\"saved results dataframe of shape={df_results.shape} in s3://{BUCKET_NAME}/{results_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/metrics/2024/01/29/19/31/mistral-7b-tgi-g5-v1'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRICS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:27,008] p58222 {234513869.py:10} INFO - results s3 path for metrics csv --> data/metrics/2024/01/29/19/31/mistral-7b-tgi-g5-v1/all_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-29 19:31:27,151] p58222 {234513869.py:12} INFO - saved metrics results dataframe of shape=(2, 31) in s3://fmbt/data/metrics/2024/01/29/19/31/mistral-7b-tgi-g5-v1/all_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "df_metrics = pd.merge(left=df_metrics, right=df_endpoints, how='left', left_on='experiment_name', right_on='experiment_name')\n",
    "df_metrics.head()\n",
    "\n",
    "# Convert df_metrics to CSV and write to S3\n",
    "csv_buffer = io.StringIO()\n",
    "df_metrics.to_csv(csv_buffer, index=False)\n",
    "csv_data_metrics = csv_buffer.getvalue()\n",
    "metrics_file_name = config['results']['all_metrics_file'].format(datetime=date_time)\n",
    "metrics_s3_path = os.path.join(METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"results s3 path for metrics csv --> {metrics_s3_path}\")\n",
    "write_to_s3(csv_data_metrics, config['aws']['bucket'], \"\", METRICS_DIR, metrics_file_name)\n",
    "logger.info(f\"saved metrics results dataframe of shape={df_metrics.shape} in s3://{config['aws']['bucket']}/{metrics_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
