{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Overview","text":"Amazon Bedrock | Amazon SageMaker | Amazon EKS | Amazon EC2 <p>A key challenge with FMs is the ability to benchmark their performance in terms of inference latency, throughput and cost so as to determine which model running with what combination of the hardware and serving stack provides the best price-performance combination for a given workload.</p> <p>Stated as business problem, the ask is \u201cWhat is the dollar cost per transaction for a given generative AI workload that serves a given number of users while keeping the response time under a target threshold?\u201d</p> <p>But to really answer this question, we need to answer an engineering question (an optimization problem, actually) corresponding to this business problem: \u201cWhat is the minimum number of instances N, of most cost optimal instance type T, that are needed to serve a workload W while keeping the average transaction latency under L seconds?\u201d</p> <p>W: = {R transactions per-minute, average prompt token length P, average generation token length G}</p> <p>This foundation model benchmarking tool (a.k.a. <code>FMBench</code>) is a tool to answer the above engineering question and thus answer the original business question about how to get the best price performance for a given workload. Here is one of the plots generated by <code>FMBench</code> to help answer the above question (the instance types in the legend have been blurred out on purpose, you can find them in the actual plot generated on running <code>FMBench</code>).</p> <p></p>"},{"location":"index.html#full-list-of-benchmarked-models","title":"Full list of benchmarked models","text":"Model SageMaker g4dn/g5/p3 SageMaker Inf2 SageMaker P4 SageMaker P5 Bedrock On-demand throughput Bedrock provisioned throughput Anthropic Claude-3 Sonnet \u2705 \u2705 Anthropic Claude-3 Haiku \u2705 Mistral-7b-instruct \u2705 \u2705 \u2705 \u2705 Mistral-7b-AWQ \u2705 Mixtral-8x7b-instruct \u2705 Llama3-8b instruct \u2705 \u2705 \u2705 \u2705 \u2705 Llama3-70b instruct \u2705 \u2705 \u2705 \u2705 Llama2-13b chat \u2705 \u2705 \u2705 \u2705 Llama2-70b chat \u2705 \u2705 \u2705 \u2705 Amazon Titan text lite \u2705 Amazon Titan text express \u2705 Cohere Command text \u2705 Cohere Command light text \u2705 AI21 J2 Mid \u2705 AI21 J2 Ultra \u2705 Gemma-2b \u2705 Phi-3-mini-4k-instruct \u2705 distilbert-base-uncased \u2705"},{"location":"advanced.html","title":"Advanced functionality","text":"<p>Beyond running <code>FMBench</code> with the configuraton files provided, you may want try out bringing your own dataset or endpoint to <code>FMBench</code>. </p>"},{"location":"advanced.html#bring-your-own-endpoint-aka-support-for-external-endpoints","title":"Bring your own endpoint (a.k.a. support for external endpoints)","text":"<p>If you have an endpoint deployed on say <code>Amazon EKS</code> or <code>Amazon EC2</code> or have your models hosted on a fully-managed service such as <code>Amazon Bedrock</code>, you can still bring your endpoint to <code>FMBench</code> and run tests against your endpoint. To do this you need to do the following:</p> <ol> <li> <p>Create a derived class from <code>FMBenchPredictor</code> abstract class and provide implementation for the constructor, the <code>get_predictions</code> method and the <code>endpoint_name</code> property. See <code>SageMakerPredictor</code> for an example. Save this file locally as say <code>my_custom_predictor.py</code>.</p> </li> <li> <p>Upload your new Python file (<code>my_custom_predictor.py</code>) for your custom FMBench predictor to your <code>FMBench</code> read bucket and the scripts prefix specified in the <code>s3_read_data</code> section (<code>read_bucket</code> and <code>scripts_prefix</code>).</p> </li> <li> <p>Edit the configuration file you are using for your <code>FMBench</code> for the following:</p> <ul> <li>Skip the deployment step by setting the <code>2_deploy_model.ipynb</code> step under <code>run_steps</code> to <code>no</code>.</li> <li>Set the <code>inference_script</code> under any experiment in the <code>experiments</code> section for which you want to use your new custom inference script to point to your new Python file (<code>my_custom_predictor.py</code>) that contains your custom predictor.</li> </ul> </li> </ol>"},{"location":"advanced.html#bring-your-own-rest-predictor-data-on-eks-version","title":"Bring your own <code>REST Predictor</code> (<code>data-on-eks</code> version)","text":"<p><code>FMBench</code> now provides an example of bringing your own endpoint as a <code>REST Predictor</code> for benchmarking. View this <code>script</code> as an example. This script is an inference file for the <code>NousResearch/Llama-2-13b-chat-hf</code> model deployed on an Amazon EKS cluster using Ray Serve. The model is deployed via <code>data-on-eks</code> which is a comprehensive resource for scaling your data and machine learning workloads on Amazon EKS and unlocking the power of Gen AI. Using <code>data-on-eks</code>, you can harness the capabilities of AWS Trainium, AWS Inferentia and NVIDIA GPUs to scale and optimize your Gen AI workloads and benchmark those models on FMBench with ease. </p>"},{"location":"advanced.html#bring-your-own-dataset","title":"Bring your own dataset","text":"<p>By default <code>FMBench</code> uses the <code>LongBench dataset</code> dataset for testing the models, but this is not the only dataset you can test with. You may want to test with other datasets available on HuggingFace or use your own datasets for testing. You can do this by converting your dataset to the <code>JSON lines</code> format. We provide a code sample for converting any HuggingFace dataset into JSON lines format and uploading it to the S3 bucket used by <code>FMBench</code> in the <code>bring_your_own_dataset</code> notebook. Follow the steps described in the notebook to bring your own dataset for testing with <code>FMBench</code>.</p>"},{"location":"advanced.html#support-for-open-orca-dataset","title":"Support for Open-Orca dataset","text":"<p>Support for Open-Orca dataset and corresponding prompts for Llama3, Llama2 and Mistral, see:</p> <ol> <li>bring_your_own_dataset.ipynb</li> <li>prompt templates</li> <li>Llama3 config file with OpenOrca</li> </ol>"},{"location":"advanced.html#building-the-fmbench-python-package","title":"Building the <code>FMBench</code> Python package","text":"<p>If you would like to build a dev version of <code>FMBench</code> for your own development and testing purposes, the following steps describe how to do that.</p> <ol> <li> <p>Clone the <code>FMBench</code> repo from GitHub.</p> </li> <li> <p>Make any code changes as needed.</p> </li> <li> <p>Install <code>poetry</code>.</p> <p><code>{.bash} pip install poetry</code></p> </li> <li> <p>Change directory to the <code>FMBench</code> repo directory and run poetry build.</p> <p><code>{.bash} poetry build</code></p> </li> <li> <p>The <code>.whl</code> file is generated in the <code>dist</code> folder. Install the <code>.whl</code> in your current Python environment.</p> <p><code>{.bash} pip install dist/fmbench-X.Y.Z-py3-none-any.whl</code></p> </li> <li> <p>Run <code>FMBench</code> as usual through the <code>FMBench</code> CLI command.</p> </li> </ol>"},{"location":"benchmarking.html","title":"Benchmark models","text":""},{"location":"benchmarking.html#benchmark-models-deployed-on-different-aws-generative-ai-services","title":"Benchmark models deployed on different AWS Generative AI services","text":"<p><code>FMBench</code> comes packaged with configuration files for benchmarking models on different AWS Generative AI services.</p>"},{"location":"benchmarking.html#benchmark-models-on-bedrock","title":"Benchmark models on Bedrock","text":"<p>Choose any config file from the <code>bedrock</code> folder and either run these directly or use them as templates for creating new config files specific to your use-case. Here is an example for benchmarking the <code>Llama3</code> models on Bedrock.</p> <pre><code>fmbench --config-file https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/src/fmbench/configs/bedrock/config-bedrock-llama3.yml &gt; fmbench.log 2&gt;&amp;1\n</code></pre>"},{"location":"benchmarking.html#benchmark-models-on-sagemaker","title":"Benchmark models on SageMaker","text":"<p>Choose any config file from the model specific folders, for example the <code>Llama3</code> folder for <code>Llama3</code> family of models. These configuration files also include instructions for <code>FMBench</code> to first deploy the model on SageMaker using your configured instance type and inference parameters of choice and then run the benchmarking. Here is an example for benchmarking <code>Llama3-8b</code> model on an <code>ml.inf2.24xlarge</code> and <code>ml.g5.12xlarge</code> instance. </p> <pre><code>fmbench --config-file https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/src/fmbench/configs/llama3/8b/config-llama3-8b-inf2-g5.yml &gt; fmbench.log 2&gt;&amp;1\n</code></pre>"},{"location":"benchmarking.html#benchmark-models-on-eks","title":"Benchmark models on EKS","text":"<p>You can use <code>FMBench</code> to benchmark models on hosted on EKS. This can be done in one of two ways:  - Deploy the model on your EKS cluster independantly of <code>FMBench</code> and then benchmark it through the Bring your own endpoint mode.  - Deploy the model on your EKS cluster through <code>FMBench</code> and then benchmark it.</p> <p>The steps for deploying the model on your EKS cluster are described below.</p> <p>\ud83d\udc49 EKS cluster creation itself is not a part of the <code>FMBench</code> functionality, the cluster needs to exist before you run the following steps. Steps for cluster creation are provided in this file but it would be best to consult the DoEKS repo on GitHub for comprehensive instructions.</p> <ol> <li> <p>Add the following IAM policies to your existing <code>FMBench</code> Role:</p> <ol> <li> <p>AmazonEKSClusterPolicy: This policy provides Kubernetes the permissions it requires to manage resources on your behalf.</p> </li> <li> <p>AmazonEKS_CNI_Policy: This policy provides the Amazon VPC CNI Plugin (amazon-vpc-cni-k8s) the permissions it requires to modify the IP address configuration on your EKS worker nodes. This permission set allows the CNI to list, describe, and modify Elastic Network Interfaces on your behalf.</p> </li> <li> <p>AmazonEKSWorkerNodePolicy: This policy allows Amazon EKS worker nodes to connect to Amazon EKS Clusters.</p> </li> </ol> </li> <li> <p>Once the EKS cluster is available you can use either the following two files or create your own config files using these files as examples for running benchmarking for these models. These config files require that the EKS cluster has been created as per the steps in these instructions.</p> <ol> <li> <p>config-llama3-8b-eks-inf2.yml: Deploy Llama3 on Trn1/Inf2 instances.</p> </li> <li> <p>config-mistral-7b-eks-inf2.yml: Deploy Mistral 7b on Trn1/Inf2 instances.</p> </li> </ol> <p>For more information about the blueprints used by FMBench to deploy these models, view: DoEKS docs gen-ai.</p> </li> <li> <p>Run the <code>Llama3-8b</code> benchmarking using the command below (replace the config file as needed for a different model). This will first deploy the model on your EKS cluster and then run benchmarking on the deployed model.</p> <p><code>{.bash} fmbench --config-file https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/src/fmbench/configs/llama3/8b/config-llama3-8b-eks-inf2.yml &gt; fmbench.log 2&gt;&amp;1</code></p> </li> <li> <p>As the model is getting deployed you might want to run the following <code>kubectl</code> commands to monitor the deployment progress. Set the model_namespace to <code>llama3</code> or <code>mistral</code> or a different model as appropriate.</p> <ol> <li><code>kubectl get pods -n &lt;model_namespace&gt; -w</code>: Watch the pods in the model specific namespace.</li> <li><code>kubectl -n karpenter get pods</code>: Get the pods in the karpenter namespace.</li> <li><code>kubectl describe pod -n &lt;model_namespace&gt; &lt;pod-name&gt;</code>: Describe a specific pod in the mistral namespace to view the live logs.</li> </ol> </li> </ol>"},{"location":"benchmarking.html#benchmark-models-on-ec2","title":"Benchmark models on EC2","text":"<p>You can use <code>FMBench</code> to benchmark models on hosted on EC2. This can be done in one of two ways:  - Deploy the model on your EC2 instance independantly of <code>FMBench</code> and then benchmark it through the Bring your own endpoint mode.  - Deploy the model on your EC2 instance through <code>FMBench</code> and then benchmark it.</p> <p>The steps for deploying the model on your EC2 instance are described below. </p> <p>\ud83d\udc49 In this configuration both the model being benchmarked and <code>FMBench</code> are deployed on the same EC2 instance.</p> <ol> <li> <p>Create a new EC2 instance suitable for hosting an LMI as per the steps described here.</p> </li> <li> <p>Install <code>FMBench</code> on this instance and run benchmarking for a desired model using one of the config files included in the <code>FMbench</code> repo or create your own.</p> <ol> <li> <p>Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs Anaconda on the instance which is then used to create a new <code>conda</code> environment for <code>FMBench</code>. See instructions for downloading anaconda here</p> <p><code>{.bash} curl -O https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh chmod +x Anaconda3-2023.09-0-Linux-x86_64.sh ./Anaconda3-2023.09-0-Linux-x86_64.sh export PATH=/home/ubuntu/anaconda3/bin:$PATH</code></p> </li> <li> <p>Setup the <code>fmbench_python311</code> conda environment.</p> <p><code>{.bash} conda create --name fmbench_python311 -y python=3.11 ipykernel source activate fmbench_python311; pip install -U fmbench</code></p> </li> <li> <p>Create local directory structure needed for <code>FMBench</code> and copy all publicly available dependencies from the AWS S3 bucket for <code>FMBench</code>. This is done by running the <code>copy_s3_content.sh</code> script available as part of the <code>FMBench</code> repo.</p> <p><code>{.bash} curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh</code></p> </li> <li> <p>To download the model files from HuggingFace, create a <code>hf_token.txt</code> file in the <code>/tmp/fmbench-read/scripts/</code> directory containing the Hugging Face token you would like to use. In the command below replace the <code>hf_yourtokenstring</code> with your hugging Face token.</p> <p><code>{.bash} echo hf_yourtokenstring &gt; /tmp/fmbench-read/scripts/hf_token.txt</code></p> </li> <li> <p>Run <code>FMBench</code> with a packaged or a custom config file. This step will also deploy the model on the EC2 instance. The <code>--write-bucket</code> parameter value is just a placeholder and an actual S3 bucket is not required</p> <p><code>{.bash} fmbench --config-file /tmp/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b.yml --local-mode yes --write-bucket placeholder &gt; fmbench.log 2&gt;&amp;1</code></p> </li> <li> <p>For example, to run <code>FMBench</code> on a <code>llama3-8b-Instruct</code> model on an <code>inf2.48xlarge</code> instance, run the command  command below. The config file for this example can be viewed here.</p> <p><code>{.bash} fmbench --config-file /tmp/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml --local-mode yes --write-bucket placeholder &gt; fmbench.log 2&gt;&amp;1</code></p> </li> <li> <p>Open a new Terminal and navigate to the <code>foundation-model-benchmarking-tool</code> directory and do a <code>tail</code> on <code>fmbench.log</code> to see a live log of the run.</p> <p><code>{.bash} tail -f fmbench.log</code></p> </li> <li> <p>All metrics are stored in the <code>/tmp/fmbench-write</code> directory created automatically by the <code>fmbench</code> package. Once the run completes all files are copied locally in a <code>results-*</code> folder as usual.</p> </li> </ol> </li> </ol>"},{"location":"gettingstarted.html","title":"Getting started","text":"<p><code>FMBench</code> is available as a Python package on PyPi and is run as a command line tool once it is installed. All data that includes metrics, reports and results are stored in an Amazon S3 bucket.</p> <p>While technically you can run <code>FMBench</code> on any AWS compute but practically speaking we either run it on a SageMaker Notebook or on EC2. Both these options are described below.</p> <p>\ud83d\udc49 The following sections are discussing running <code>FMBench</code> the tool, as different from where the FM is actually deployed. For example, we could run <code>FMBench</code> on EC2 but the model being deployed is on SageMaker or even Bedrock. </p>"},{"location":"gettingstarted.html#quickstart","title":"Quickstart","text":"<p>FMBench on a SageMaker Notebook</p> <ol> <li> <p>Each <code>FMBench</code> run works with a configuration file that contains the information about the model, the deployment steps, and the tests to run. A typical <code>FMBench</code> workflow involves either directly using an already provided config file from the <code>configs</code> folder in the <code>FMBench</code> GitHub repo or editing an already provided config file as per your own requirements (say you want to try benchmarking on a different instance type, or a different inference container etc.).</p> <p>\ud83d\udc49 A simple config file with key parameters annotated is included in this repo, see <code>config-llama2-7b-g5-quick.yml</code>. This file benchmarks performance of Llama2-7b on an <code>ml.g5.xlarge</code> instance and an <code>ml.g5.2xlarge</code> instance. You can use this config file as it is for this Quickstart.</p> </li> <li> <p>Launch the AWS CloudFormation template included in this repository using one of the buttons from the table below. The CloudFormation template creates the following resources within your AWS account: Amazon S3 buckets, Amazon IAM role and an Amazon SageMaker Notebook with this repository cloned. A read S3 bucket is created which contains all the files (configuration files, datasets) required to run <code>FMBench</code> and a write S3 bucket is created which will hold the metrics and reports generated by <code>FMBench</code>. The CloudFormation stack takes about 5-minutes to create.</p> </li> </ol> AWS Region Link us-east-1 (N. Virginia) us-west-2 (Oregon) us-gov-west-1 (GovCloud West) <ol> <li> <p>Once the CloudFormation stack is created, navigate to SageMaker Notebooks and open the <code>fmbench-notebook</code>.</p> </li> <li> <p>On the <code>fmbench-notebook</code> open a Terminal and run the following commands.</p> <p><code>{.bash} conda create --name fmbench_python311 -y python=3.11 ipykernel source activate fmbench_python311; pip install -U fmbench</code></p> </li> <li> <p>Now you are ready to <code>fmbench</code> with the following command line. We will use a sample config file placed in the S3 bucket by the CloudFormation stack for a quick first run.</p> <ol> <li> <p>We benchmark performance for the <code>Llama2-7b</code> model on a <code>ml.g5.xlarge</code> and a <code>ml.g5.2xlarge</code> instance type, using the <code>huggingface-pytorch-tgi-inference</code> inference container. This test would take about 30 minutes to complete and cost about $0.20.</p> </li> <li> <p>It uses a simple relationship of 750 words equals 1000 tokens, to get a more accurate representation of token counts use the <code>Llama2 tokenizer</code> (instructions are provided in the next section). It is strongly recommended that for more accurate results on token throughput you use a tokenizer specific to the model you are testing rather than the default tokenizer. See instructions provided later in this document on how to use a custom tokenizer.</p> <p><code>{.bash} account=`aws sts get-caller-identity | jq .Account | tr -d '\"'` region=`aws configure get region` fmbench --config-file s3://sagemaker-fmbench-read-${region}-${account}/configs/llama2/7b/config-llama2-7b-g5-quick.yml &gt; fmbench.log 2&gt;&amp;1</code></p> </li> <li> <p>Open another terminal window and do a <code>tail -f</code> on the <code>fmbench.log</code> file to see all the traces being generated at runtime.</p> <p><code>{.bash} tail -f fmbench.log</code></p> </li> <li> <p>\ud83d\udc49 For streaming support on SageMaker and Bedrock checkout these config files:</p> <ol> <li>config-llama3-8b-g5-streaming.yml</li> <li>config-bedrock-llama3-streaming.yml</li> </ol> </li> </ol> </li> <li> <p>The generated reports and metrics are available in the <code>sagemaker-fmbench-write-&lt;replace_w_your_aws_region&gt;-&lt;replace_w_your_aws_account_id&gt;</code> bucket. The metrics and report files are also downloaded locally and in the <code>results</code> directory (created by <code>FMBench</code>) and the benchmarking report is available as a markdown file called <code>report.md</code> in the <code>results</code> directory. You can view the rendered Markdown report in the SageMaker notebook itself or download the metrics and report files to your machine for offline analysis.</p> </li> </ol> <p>If you would like to understand what is being done under the hood by the CloudFormation template, see the DIY version with gory details</p>"},{"location":"gettingstarted.html#fmbench-on-govcloud","title":"<code>FMBench</code> on GovCloud","text":"<p>No special steps are required for running <code>FMBench</code> on GovCloud. The CloudFormation link for <code>us-gov-west-1</code> has been provided in the section above.</p> <ol> <li>Not all models available via Bedrock or other services may be available in GovCloud. The following commands show how to run <code>FMBench</code> to benchmark the Amazon Titan Text Express model in the GovCloud. See the Amazon Bedrock GovCloud page for more details.</li> </ol> <pre><code>account=`aws sts get-caller-identity | jq .Account | tr -d '\"'`\nregion=`aws configure get region`\nfmbench --config-file s3://sagemaker-fmbench-read-${region}-${account}/configs/bedrock/config-bedrock-titan-text-express.yml &gt; fmbench.log 2&gt;&amp;1\n</code></pre>"},{"location":"gettingstarted.html#run-fmbench-on-amazon-ec2","title":"Run <code>FMBench</code> on Amazon EC2","text":"<p>For some enterprise scenarios it might be desirable to run <code>FMBench</code> directly on an EC2 instance with no dependency on S3. Here are the steps to do this:</p> <ol> <li> <p>Have a <code>t3.xlarge</code> (or larger) instance in the <code>Running</code> stage. Make sure that the instance has at least 50GB of disk space and the IAM role associated with your EC2 instance has <code>AmazonSageMakerFullAccess</code> policy associated with it and <code>sagemaker.amazonaws.com</code> added to its Trust relationships.     <code>{.bash}     {             \"Effect\": \"Allow\",             \"Principal\": {                 \"Service\": \"sagemaker.amazonaws.com\"             },             \"Action\": \"sts:AssumeRole\"     }</code></p> </li> <li> <p>Setup the <code>fmbench_python311</code> conda environment. This step required conda to be installed on the EC2 instance, see instructions for downloading Anaconda.</p> <p><code>{.bash} conda create --name fmbench_python311 -y python=3.11 ipykernel source activate fmbench_python311; pip install -U fmbench</code></p> </li> <li> <p>Create local directory structure needed for <code>FMBench</code> and copy all publicly available dependencies from the AWS S3 bucket for <code>FMBench</code>. This is done by running the <code>copy_s3_content.sh</code> script available as part of the <code>FMBench</code> repo.</p> <p><code>{.bash} curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh</code></p> </li> <li> <p>Run <code>FMBench</code> with a quickstart config file.</p> <p><code>{.bash} fmbench --config-file /tmp/fmbench-read/configs/llama2/7b/config-llama2-7b-g5-quick.yml --local-mode yes &gt; fmbench.log 2&gt;&amp;1</code></p> </li> <li> <p>Open a new Terminal and navigate to the <code>foundation-model-benchmarking-tool</code> directory and do a <code>tail</code> on <code>fmbench.log</code> to see a live log of the run.</p> <p><code>{.bash} tail -f fmbench.log</code></p> </li> <li> <p>All metrics are stored in the <code>/tmp/fmbench-write</code> directory created automatically by the <code>fmbench</code> package. Once the run completes all files are copied locally in a <code>results-*</code> folder as usual.</p> </li> </ol>"},{"location":"releases.html","title":"Releases","text":""},{"location":"releases.html#models-benchmarked","title":"Models benchmarked","text":"<p>Configuration files are available in the configs folder for the following models in this repo.</p>"},{"location":"releases.html#llama3-on-amazon-sagemaker","title":"Llama3 on Amazon SageMaker","text":"<p>Llama3 is now available on SageMaker (read blog post), and you can now benchmark it using <code>FMBench</code>. Here are the config files for benchmarking <code>Llama3-8b-instruct</code> and <code>Llama3-70b-instruct</code> on <code>ml.p4d.24xlarge</code>, <code>ml.inf2.24xlarge</code> and <code>ml.g5.12xlarge</code> instances.</p> <ul> <li>Config file for <code>Llama3-8b-instruct</code> on  <code>ml.p4d.24xlarge</code> and <code>ml.g5.12xlarge</code>.</li> <li>Config file for <code>Llama3-70b-instruct</code> on  <code>ml.p4d.24xlarge</code> and <code>ml.g5.48xlarge</code>.</li> <li>Config file for <code>Llama3-8b-instruct</code> on  <code>ml.inf2.24xlarge</code> and <code>ml.g5.12xlarge</code>.</li> </ul>"},{"location":"releases.html#new-in-this-release","title":"New in this release","text":""},{"location":"releases.html#v1050","title":"v1.0.50","text":"<ol> <li><code>Llama3-8b</code> on Amazon EC2 <code>inf2.48xlarge</code> config file.</li> <li>Update to new version of DJL LMI (0.28.0).</li> </ol>"},{"location":"releases.html#v1049","title":"v1.0.49","text":"<ol> <li>Streaming support for Amazon SageMaker and Amazon Bedrock.</li> <li>Per-token latency metrics such as time to first token (TTFT) and mean time per-output token (TPOT).</li> <li>Misc. bug fixes.</li> </ol>"},{"location":"releases.html#v1048","title":"v1.0.48","text":"<ol> <li>Faster result file download at the end of a test run.</li> <li><code>Phi-3-mini-4k-instruct</code> configuration file.</li> <li>Tokenizer and misc. bug fixes.</li> </ol> <p>Release history</p>"},{"location":"resources.html","title":"Resources","text":""},{"location":"resources.html#pending-enhancements","title":"Pending enhancements","text":"<p>View the ISSUES on GitHub and add any you might think be an beneficial iteration to this benchmarking harness.</p>"},{"location":"resources.html#security","title":"Security","text":"<p>See CONTRIBUTING for more information.</p>"},{"location":"resources.html#license","title":"License","text":"<p>This library is licensed under the MIT-0 License. See the LICENSE file.</p>"},{"location":"results.html","title":"Results","text":"<p>Depending upon the experiments in the config file, the <code>FMBench</code> run may take a few minutes to several hours. Once the run completes, you can find the report and metrics in the local <code>results-*</code> folder in the directory from where <code>FMBench</code> was run. The rpeort and metrics are also written to the write S3 bucket set in the config file.</p> <p>Here is a screenshot of the <code>report.md</code> file generated by <code>FMBench</code>. </p>"},{"location":"results.html#an-internal-fmbench-website","title":"An internal <code>FMBench</code> website","text":"<p>You can create an internal <code>FMBench</code> website to view results from multiple runs in a single place. All <code>FMBench</code> reports are generated as a Markdown file, these files can be rendered together in a website that is viewable in a web browser on your machine. We use <code>Quarto</code> to do this. The steps below describe the process you can follow.</p> <p>[Prerequisites] If you have followed the <code>Quickstart</code> then these are already taken care of for you. 1. You will need to clone the <code>FMBench</code> code repo from GitHub. 1. The <code>results-*</code> folders that contain the reports and metrics from a run are present in the root folder of the <code>FMBench</code> code repo. 1. Run the <code>render_fmbench_website.py</code> Python script using the following command. This will generate a <code>_quarto.yml</code> file and render the website in the <code>fmbench-website</code> folder in the root directory of your <code>FMBench</code> repo. The website is rendered using the <code>Quarto</code> container downloaded from <code>registry.gitlab.com/quarto-forge/docker/quarto</code>.</p> <pre><code>```{.python}\nsource activate fmbench_python311\ncurl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/render_fmbench_website.py\npython render_fmbench_website.py\n```\n</code></pre> <ol> <li> <p>The website is created in the local directory <code>fmbench-website</code>. You can copy this folder into a webserver that you have OR the easiest option is to zip up this folder and download to your local machine and use the Python3  <code>http.server</code> to host the website.</p> <p><code>{.bash} cd fmbench-website; zip -r9 ../fmbench-website.zip *;cd -</code></p> </li> <li> <p>Download <code>fmbench-website.zip</code> to your local machine. Extract the contents from the <code>fmbench-website.zip</code> file. Navigate to <code>fmbench-website</code> directory and run the Python3 webserver. This will start a local webserver. You should see traces being printed out on the console indicating that the webserver has started.</p> <p><code>{.bash} python http.server 8080</code></p> </li> <li> <p>Open <code>http://localhost:8080/</code> in your browser and you should be able to see the <code>FMBench</code> website with all the reports that were present in the <code>results-*</code> folder in your <code>FMBench</code> installation. The following screenshot shows a picture of the <code>FMBench</code> website with links to multiple reports.</p> </li> </ol> <p></p>"}]}